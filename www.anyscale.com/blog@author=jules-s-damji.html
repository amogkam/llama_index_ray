<!DOCTYPE html><html lang="en-US"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Blog | Anyscale</title><meta name="description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta property="og:title" content="Blog | Anyscale"/><meta property="og:description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta property="og:site_name" content="Anyscale"/><meta property="og:image" content=""/><meta property="fb:app_id" content=""/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta name="twitter:site" content=""/><meta name="twitter:title" content="Blog | Anyscale"/><meta name="twitter:image" content=""/><meta name="next-head-count" content="14"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><link rel="icon" href="static/favicon.ico"/><script>
(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-P8H6KQG');
</script><script src="https://tag.clearbitscripts.com/v1/pk_7a96be585f731437b35c7bad25c432b7/tags.js"></script><script src="https://www.googleoptimize.com/optimize.js?id=OPT-NPGB9W3"></script><script>
  !function(f,b,e,v,n,t,s)
  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
  n.queue=[];t=b.createElement(e);t.async=!0;
  t.src=v;s=b.getElementsByTagName(e)[0];
  s.parentNode.insertBefore(t,s)}(window,document,'script',
  'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '168099398554987');
  fbq('track', 'PageView');
</script><link rel="stylesheet" href="static/font-awesome-4.7.0/css/font-awesome.min.css"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="true"/><script src="https://fast.wistia.com/assets/external/E-v1.js" async=""></script><meta name="ahrefs-site-verification" content="4be3c5d78cdad8823d038256ac06d4336ae7a84632dac0ed57975703fc14fe6b"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="_next/static/css/dbba207740458c19.css" as="style"/><link rel="stylesheet" href="_next/static/css/dbba207740458c19.css" data-n-g=""/><link rel="preload" href="_next/static/css/13fbfc51931a4b43.css" as="style"/><link rel="stylesheet" href="_next/static/css/13fbfc51931a4b43.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="_next/static/chunks/webpack-94adde92c5e1c75b.js" defer=""></script><script src="_next/static/chunks/framework-95c3ca6de27174dd.js" defer=""></script><script src="_next/static/chunks/main-774d7304f1973b2a.js" defer=""></script><script src="_next/static/chunks/pages/_app-fd8cb5a9775605f5.js" defer=""></script><script src="_next/static/chunks/0f1ac474-1cd8c801d37caef0.js" defer=""></script><script src="_next/static/chunks/1812-a871a62a8e8aa88c.js" defer=""></script><script src="_next/static/chunks/9065-e24d5adee6cbba35.js" defer=""></script><script src="_next/static/chunks/6139-f3c4647afbd26b94.js" defer=""></script><script src="_next/static/chunks/8423-007cfdc371b27898.js" defer=""></script><script src="_next/static/chunks/3167-65b612e959dd1945.js" defer=""></script><script src="_next/static/chunks/9027-e83e1bb65c284840.js" defer=""></script><script src="_next/static/chunks/pages/blog-1eafbb689a124ac5.js" defer=""></script><script src="_next/static/-oT8_WeW9m_lAxr26cJvY/_buildManifest.js" defer=""></script><script src="_next/static/-oT8_WeW9m_lAxr26cJvY/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Rubik&display=swap">@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4i1Uw.woff) format('woff')}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFWkUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFU0UzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFVUUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFWUUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFV0UzdYPFkZVO.woff) format('woff');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href="https://fonts.googleapis.com/css2?family=Outfit:wght@400;500;600;700&display=swap">@font-face{font-family:'Outfit';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4TC1C4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4QK1C4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4e6yC4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4deyC4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,400;0,700;1,400;1,700&display=swap">@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTeEw.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINscg.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKtdU.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7MIU.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGqZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuE6ZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuFKZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGKZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGaZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuF6ZJW9XjDg.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINce_fuJGl18QRY.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINccvfuJGl18QRY.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcdffuJGl18QRY.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINceffuJGl18QRY.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcePfuJGl18QRY.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcdvfuJGl18Q.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdzeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdXeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdLeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhd7eFaxOedfTDw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhd_eFaxOedfTDw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdHeFaxOedc.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIxsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIVsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIJsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AI5sdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AI9sdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIFsdP3pBms.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body class=""><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-P8H6KQG" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next" data-reactroot=""><svg xmlns="http://www.w3.org/2000/svg" class="sr-only"><symbol id="svg-arrow-right-gradient"><path d="M1.5 7.5h18" stroke="url(#paint0_linear_3927_132230)" stroke-width="1.5" stroke-linecap="round"></path><path d="m16.6562 3.65625 3.8442 3.84412-3.8442 3.84413" stroke="url(#paint1_linear_3927_132230)" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><defs><linearGradient id="paint0_linear_3927_132230" x1="1.06972" y1="8.58929" x2="12.2319" y2="-1.53819" gradientUnits="userSpaceOnUse"><stop stop-color="#8B20BE"></stop><stop offset="1" stop-color="#3579FF" stop-opacity=".97"></stop></linearGradient><linearGradient id="paint1_linear_3927_132230" x1="12.377" y1="7.75171" x2="16.9307" y2="11.8684" gradientUnits="userSpaceOnUse"><stop stop-color="#8B20BE"></stop><stop offset="1" stop-color="#3579FF" stop-opacity=".97"></stop></linearGradient></defs></symbol></svg><div class="Banner_container__zEmFV"><a href="https://raysummit.anyscale.com/?utm_source=anyscale&amp;utm_medium=website&amp;utm_campaign=ray_summit_2023&amp;utm_content=homepage_ticker" rel="noreferrer nofollow" target="_blank"><span class="Banner_banner-text__wr6Xi"><span class="Banner_main-text__cpJ6H">Are you passionate about Ray, scalable AI and distributed computing? Join us at Ray Summit!</span><span class="Banner_cta-button__0JCtt"> Find out more! <!-- --> <svg width="23" height="14" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1.5 6.75a.75.75 0 0 0 0 1.5v-1.5Zm18 1.5a.75.75 0 0 0 0-1.5v1.5Zm-18 0h18v-1.5h-18v1.5Z" fill="#fff"></path><path d="M17.187 3.126a.75.75 0 0 0-1.061 1.06l1.06-1.06ZM20.5 7.5l.53.53a.75.75 0 0 0 0-1.06l-.53.53Zm-4.374 3.314a.75.75 0 1 0 1.06 1.06l-1.06-1.06Zm0-6.627L19.97 8.03l1.06-1.061-3.843-3.844-1.061 1.06ZM19.97 6.97l-3.844 3.844 1.06 1.06 3.845-3.843L19.97 6.97Z" fill="#fff"></path></svg></span></span></a><button class="Banner_close-button__9ksfY" type="button" aria-label="Close Banner"></button></div><header class="inner-shadow sticky top-0 z-[1000] h-24 transform py-5 transition-transform duration-500 bg-primary translate-y-0"><div class="container mx-auto flex items-center justify-between"><a href="index.html" class="text-white"><svg width="141" height="32" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><g clip-path="url(#anyscale-logo_svg__a)" fill="currentColor"><path d="M10.653 17.46H3.865C1.697 17.46 0 19.16 0 21.332v6.797c0 2.17 1.697 3.87 3.865 3.87h6.788c2.169 0 3.866-1.7 3.866-3.87V21.33c0-2.171-1.792-3.87-3.866-3.87Zm1.509 10.573c0 .85-.66 1.51-1.509 1.51H3.865a1.49 1.49 0 0 1-1.508-1.51v-6.796c0-.85.66-1.51 1.508-1.51h6.788c.849 0 1.509.66 1.509 1.51v6.796Z"></path><path d="M28.095 11.988h-8.108V3.87C19.892 1.7 18.196 0 16.027 0H3.865C1.697 0 0 1.7 0 3.87v6.608c0 2.171 1.697 3.87 3.865 3.87h13.67v13.687c0 2.171 1.698 3.87 3.866 3.87h6.6c2.168 0 3.865-1.699 3.865-3.87V15.858c0-2.076-1.697-3.775-3.771-3.87Zm-24.23 0a1.49 1.49 0 0 1-1.508-1.51V3.87c0-.85.66-1.51 1.508-1.51h12.162c.849 0 1.509.66 1.509 1.51v8.118H3.866Zm25.738 16.047c0 .85-.66 1.51-1.508 1.51h-6.6a1.49 1.49 0 0 1-1.508-1.51V14.348h8.108c.848 0 1.508.66 1.508 1.51v12.177ZM79.194 17.462l-3.017-8.118h-3.3l4.714 11.327-2.546 5.853h3.112l7.165-17.18H82.21l-3.016 8.118ZM93.335 14.819c-.66-.284-1.414-.472-2.075-.661-.66-.189-1.225-.378-1.79-.66-.378-.19-.566-.473-.566-.85 0-.378.188-.661.472-.85.847-.472 1.79-.378 2.639.094.377.284.566.661.566 1.133h2.828c0-1.133-.566-2.171-1.414-2.832-.943-.755-2.169-1.133-3.3-1.038-.849 0-1.697.094-2.45.472-.66.283-1.227.66-1.604 1.227-.377.472-.565 1.133-.565 1.794 0 .66.188 1.321.565 1.887.377.472.849.85 1.414 1.039.66.283 1.414.472 2.168.66.66.19 1.226.378 1.886.661.377.19.566.472.566.85 0 .377-.189.66-.471.85-.377.283-.85.377-1.32.377-.472 0-1.038-.094-1.415-.472-.377-.283-.565-.66-.66-1.133h-2.922c0 .755.282 1.416.754 1.982.472.567 1.131 1.133 1.791 1.416.849.378 1.698.567 2.546.567.849 0 1.697-.095 2.45-.472.661-.283 1.227-.66 1.604-1.227.377-.567.565-1.133.565-1.794 0-.66-.188-1.321-.66-1.888-.565-.66-1.037-.944-1.602-1.132ZM102.575 11.702c.566 0 1.036.095 1.508.472.377.283.66.755.849 1.227h3.205a5.366 5.366 0 0 0-1.886-3.02 5.348 5.348 0 0 0-3.582-1.039c-1.037 0-2.074.284-3.017.755a5.486 5.486 0 0 0-2.074 2.077 7.427 7.427 0 0 0 0 6.325c.472.85 1.131 1.604 2.074 2.076.943.472 1.98.756 3.017.756 1.32 0 2.546-.378 3.582-1.133a5.366 5.366 0 0 0 1.886-3.02h-3.111c-.283 1.037-1.32 1.698-2.357 1.604-.754 0-1.509-.283-1.98-.944a5.119 5.119 0 0 1 0-5.192 1.914 1.914 0 0 1 1.886-.944ZM118.412 10.1a.405.405 0 0 1-.377.378c-.094 0-.188 0-.188-.095a3.382 3.382 0 0 0-.944-.66c-.659-.378-1.507-.567-2.262-.567-.943 0-1.885.283-2.734.755a4.459 4.459 0 0 0-1.885 2.077c-.472.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.376.85 1.037 1.604 1.885 2.17.849.473 1.697.756 2.64.756.754 0 1.603-.189 2.263-.567.659-.283 1.131-.755 1.602-1.321v1.699h2.923V9.345h-2.923v.755Zm-.377 6.891c-.283.472-.66.944-1.132 1.227-.47.283-1.037.378-1.507.378-.567 0-1.037-.189-1.509-.378-.472-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.094-1.228.472-1.794.848-1.51 2.64-1.982 4.147-1.227h.095c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM55.907 10.1a.406.406 0 0 1-.377.378c-.095 0-.19 0-.19-.095a3.384 3.384 0 0 0-.942-.66c-.66-.378-1.508-.567-2.263-.567-.942 0-1.885.283-2.733.755a4.463 4.463 0 0 0-1.886 2.077c-.471.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.377.85 1.037 1.604 1.886 2.17.848.473 1.697.756 2.64.756.753 0 1.602-.189 2.262-.567.66-.283 1.131-.755 1.603-1.321v1.699h2.922V9.345h-2.922v.755Zm-.472 6.891c-.283.472-.66.944-1.131 1.227-.471.283-1.037.378-1.509.378-.565 0-1.037-.189-1.508-.378-.471-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.095-1.228.472-1.794.848-1.51 2.64-1.982 4.148-1.227h.094c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM126.708 5.477h-2.923v15.575h2.923V5.477ZM140.191 14.914c0-1.038-.189-2.077-.755-3.02-.471-.85-1.131-1.605-2.074-1.983a6.78 6.78 0 0 0-6.033 0 5.485 5.485 0 0 0-2.075 2.077c-.565 1.038-.754 2.077-.754 3.21 0 1.132.189 2.17.754 3.114a5.485 5.485 0 0 0 2.075 2.077c.942.472 1.979.755 3.016.755a6.56 6.56 0 0 0 3.489-1.038c.943-.661 1.602-1.7 1.885-2.738h-3.016c-.377.944-1.321 1.51-2.358 1.416-.659 0-1.414-.283-1.885-.755-.566-.472-.848-1.227-.848-1.982h8.484c.095-.378.095-.755.095-1.133Zm-8.579-.85c.093-.755.377-1.415.942-1.887.566-.472 1.226-.661 1.886-.661.754 0 1.414.189 1.979.66.566.473.849 1.133.849 1.794l-5.656.095ZM67.692 9.159a4.7 4.7 0 0 0-1.98.472c-.377.188-.754.377-1.037.66a.286.286 0 0 1-.377 0c-.094-.094-.094-.094-.094-.188v-.756H61.28v11.705h2.923v-6.418c-.094-.755.188-1.51.754-2.171.471-.472 1.226-.756 1.886-.756.66 0 1.414.284 1.885.755.472.567.754 1.322.66 2.172v6.418h2.923v-6.89c.094-1.322-.283-2.644-1.226-3.682-.848-.85-2.168-1.416-3.394-1.321Z"></path></g><defs><clipPath id="anyscale-logo_svg__a"><path fill="currentColor" d="M0 0h140.19v32H0z"></path></clipPath></defs></svg></a><div class="lg:relative lg:flex lg:flex-row hidden"><div class="max-h-[calc(100vh-192px)] items-center overflow-y-auto py-[1px] shadow-[0_-1px_0_#DAE3F2] lg:flex lg:overflow-visible lg:p-0 lg:shadow-none"><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Products</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="platform.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Anyscale Platform</div></a><a href="ray-open-source.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Open Source</div></a></div></div></div></div></div></div><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Solutions</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="container mx-auto mb-4 lg:mb-0 mt-2 lg:mt-0 lg:mr-12"><div class="header-card h-full rounded-[5px] p-6 lg:w-[296px]"><h5 id="scalable-ai-and-python">Scalable AI and Python</h5>
<p>Leverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more.</p></div></div><div class="grid lg:flex grid-cols-1 md:grid-cols-2 gap-x-10 lg:gap-x-12 container mx-auto"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><p class="text-base leading-normal font-normal text-darkGrey-300 px-6 lg:px-4 mb-4">Scalable AI and Python</p><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2"><a href="data-ingestion.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Data Ingestion</div></a><a href="reinforcement-learning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Reinforcement Learning</div></a><a href="ray-air.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray AIR</div></a><a href="model-serving.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Model Serving</div></a><a href="hyperparameter-tuning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Hyperparameter Tuning</div></a></div></div></div><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><p class="text-base leading-normal font-normal text-darkGrey-300 px-6 lg:px-4 mb-4">Use Cases</p><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2"><a href="large-language-models.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Large Language Models</div></a><a href="demand-forecasting.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Demand Forecasting/Pricing</div></a><a href="industrial-automation.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Industrial Automation</div></a><a href="machine-learning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Scalable ML Platforms</div></a><a href="natural-language-processing.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">NLP</div></a><a href="recommendation-system.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Recommendation System</div></a></div></div></div></div></div></div></div><a class="block font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4 shadow-[0_1px_0_#DAE3F2] lg:shadow-none" href="ray-ecosystem.html"><div class="w-full container mx-auto">Ecosystem</div></a><a class="block font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4 shadow-[0_1px_0_#DAE3F2] lg:shadow-none" href="user-stories.html"><div class="w-full container mx-auto">Success Stories</div></a><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Learn</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="blog/how-ray-solves-common-production-challenges-for-generative-ai.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Blog</div></a><a href="https://raysummit.anyscale.com/?utm_source=anyscale&amp;utm_medium=website&amp;utm_campaign=ray_summit_2023&amp;utm_content=home_nav-pulldown"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Summit 2023</div></a><a href="event-category/rl-summit.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Videos &amp; Webinars</div></a><a href="https://github.com/ray-project/ray-educational-materials"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Training</div></a><a href="https://docs.anyscale.com"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Anyscale Docs</div></a></div></div></div></div></div></div><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Company</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="about.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">About us</div></a><a href="press.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">News</div></a><a href="careers.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Careers</div></a><a href="community.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Community</div></a></div></div></div></div></div></div></div><div class="absolute bottom-0 left-0 right-0 bg-white py-5 shadow-[0_-1px_0_#DAE3F2] lg:relative lg:ml-5 lg:bg-transparent lg:p-0 lg:shadow-none"><div class="container mx-auto flex justify-end"><a class="btn bg-gradient min-w-[320px] justify-center bg-blue-900 text-center lg:min-w-0" href="beta.html">Try It Now</a></div></div></div><div class="cursor-pointer py-4 pl-6 lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M20.05 11H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95ZM20.05 16H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95ZM20.05 6H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95Z" fill="#fff"></path></svg></div></div></header><div class="Layout_page-sections__2__gb overflow-x-hidden"><div class="ArticlesList_container__mBEpW"><div class="ArticlesList_inner__QWc69"><div class="ArticlesList_header__45BKa"><h1>Posts by Jules S. Damji</h1><div class="ArticlesList_spacer__8l_nL"></div></div><div class="BlogFilters_root__mrUMs"><div class="BlogFilters_inner__87PZK"><div class="BlogFilters_item__1GpID"><div class="wrapper"><select aria-label="Types" class="SelectDropdown_select__hNpf2"><option selected="">All Types</option><option value="news">News</option><option value="culture">Culture</option><option value="engineering">Engineering</option><option value="user-story">User Story</option></select><div class="SelectDropdown_select__hNpf2 SelectDropdown_select-dropdown__Jabke"><button type="button" aria-haspopup="true" aria-expanded="false" style="width:245px">All Types</button><ul role="menu" class="SelectDropdown_menu__3wacL"><li class="SelectDropdown_item__LO5La SelectDropdown_active__YhRi7" tabindex="0" role="menuitem">All Types</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">News</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Culture</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Engineering</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">User Story</li></ul></div></div></div><div class="BlogFilters_item__1GpID"><div class="wrapper"><select aria-label="Tags" class="SelectDropdown_select__hNpf2"><option selected="">All Products / Libraries</option><option value="anyscale">Anyscale</option><option value="ray_core">Ray Core</option><option value="ray-datasets">Ray Datasets</option><option value="ray_train">Ray Train</option><option value="ray-tune">Ray Tune</option><option value="ray_serve">Ray Serve</option><option value="rllib">Ray RLlib</option></select><div class="SelectDropdown_select__hNpf2 SelectDropdown_select-dropdown__Jabke"><button type="button" aria-haspopup="true" aria-expanded="false" style="width:245px">All Products / Libraries</button><ul role="menu" class="SelectDropdown_menu__3wacL"><li class="SelectDropdown_item__LO5La SelectDropdown_active__YhRi7" tabindex="0" role="menuitem">All Products / Libraries</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Anyscale</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Core</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Datasets</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Train</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Tune</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Serve</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray RLlib</li></ul></div></div></div></div></div><div class="empty"><h2>No posts found.</h2><p><a href="blog/how-ray-solves-common-production-challenges-for-generative-ai.html">View all</a></p></div><div class="ArticlesList_list__uP0RC"></div><div class="Pagination_container__FdBHw"></div></div></div></div><footer class="relative bg-primary pt-30 pb-30 md:pt-36 md:pb-[180px] lg:pt-20"><div class="container mx-auto flex flex-col space-y-12 md:space-y-16 lg:flex-row lg:justify-between lg:space-y-0"><div class="flex flex-col gap-16"><div class="flex flex-col gap-6"><div class="h-10 text-white"><svg width="141" height="32" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="origin-top-left scale-125"><g clip-path="url(#anyscale-logo_svg__a)" fill="currentColor"><path d="M10.653 17.46H3.865C1.697 17.46 0 19.16 0 21.332v6.797c0 2.17 1.697 3.87 3.865 3.87h6.788c2.169 0 3.866-1.7 3.866-3.87V21.33c0-2.171-1.792-3.87-3.866-3.87Zm1.509 10.573c0 .85-.66 1.51-1.509 1.51H3.865a1.49 1.49 0 0 1-1.508-1.51v-6.796c0-.85.66-1.51 1.508-1.51h6.788c.849 0 1.509.66 1.509 1.51v6.796Z"></path><path d="M28.095 11.988h-8.108V3.87C19.892 1.7 18.196 0 16.027 0H3.865C1.697 0 0 1.7 0 3.87v6.608c0 2.171 1.697 3.87 3.865 3.87h13.67v13.687c0 2.171 1.698 3.87 3.866 3.87h6.6c2.168 0 3.865-1.699 3.865-3.87V15.858c0-2.076-1.697-3.775-3.771-3.87Zm-24.23 0a1.49 1.49 0 0 1-1.508-1.51V3.87c0-.85.66-1.51 1.508-1.51h12.162c.849 0 1.509.66 1.509 1.51v8.118H3.866Zm25.738 16.047c0 .85-.66 1.51-1.508 1.51h-6.6a1.49 1.49 0 0 1-1.508-1.51V14.348h8.108c.848 0 1.508.66 1.508 1.51v12.177ZM79.194 17.462l-3.017-8.118h-3.3l4.714 11.327-2.546 5.853h3.112l7.165-17.18H82.21l-3.016 8.118ZM93.335 14.819c-.66-.284-1.414-.472-2.075-.661-.66-.189-1.225-.378-1.79-.66-.378-.19-.566-.473-.566-.85 0-.378.188-.661.472-.85.847-.472 1.79-.378 2.639.094.377.284.566.661.566 1.133h2.828c0-1.133-.566-2.171-1.414-2.832-.943-.755-2.169-1.133-3.3-1.038-.849 0-1.697.094-2.45.472-.66.283-1.227.66-1.604 1.227-.377.472-.565 1.133-.565 1.794 0 .66.188 1.321.565 1.887.377.472.849.85 1.414 1.039.66.283 1.414.472 2.168.66.66.19 1.226.378 1.886.661.377.19.566.472.566.85 0 .377-.189.66-.471.85-.377.283-.85.377-1.32.377-.472 0-1.038-.094-1.415-.472-.377-.283-.565-.66-.66-1.133h-2.922c0 .755.282 1.416.754 1.982.472.567 1.131 1.133 1.791 1.416.849.378 1.698.567 2.546.567.849 0 1.697-.095 2.45-.472.661-.283 1.227-.66 1.604-1.227.377-.567.565-1.133.565-1.794 0-.66-.188-1.321-.66-1.888-.565-.66-1.037-.944-1.602-1.132ZM102.575 11.702c.566 0 1.036.095 1.508.472.377.283.66.755.849 1.227h3.205a5.366 5.366 0 0 0-1.886-3.02 5.348 5.348 0 0 0-3.582-1.039c-1.037 0-2.074.284-3.017.755a5.486 5.486 0 0 0-2.074 2.077 7.427 7.427 0 0 0 0 6.325c.472.85 1.131 1.604 2.074 2.076.943.472 1.98.756 3.017.756 1.32 0 2.546-.378 3.582-1.133a5.366 5.366 0 0 0 1.886-3.02h-3.111c-.283 1.037-1.32 1.698-2.357 1.604-.754 0-1.509-.283-1.98-.944a5.119 5.119 0 0 1 0-5.192 1.914 1.914 0 0 1 1.886-.944ZM118.412 10.1a.405.405 0 0 1-.377.378c-.094 0-.188 0-.188-.095a3.382 3.382 0 0 0-.944-.66c-.659-.378-1.507-.567-2.262-.567-.943 0-1.885.283-2.734.755a4.459 4.459 0 0 0-1.885 2.077c-.472.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.376.85 1.037 1.604 1.885 2.17.849.473 1.697.756 2.64.756.754 0 1.603-.189 2.263-.567.659-.283 1.131-.755 1.602-1.321v1.699h2.923V9.345h-2.923v.755Zm-.377 6.891c-.283.472-.66.944-1.132 1.227-.47.283-1.037.378-1.507.378-.567 0-1.037-.189-1.509-.378-.472-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.094-1.228.472-1.794.848-1.51 2.64-1.982 4.147-1.227h.095c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM55.907 10.1a.406.406 0 0 1-.377.378c-.095 0-.19 0-.19-.095a3.384 3.384 0 0 0-.942-.66c-.66-.378-1.508-.567-2.263-.567-.942 0-1.885.283-2.733.755a4.463 4.463 0 0 0-1.886 2.077c-.471.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.377.85 1.037 1.604 1.886 2.17.848.473 1.697.756 2.64.756.753 0 1.602-.189 2.262-.567.66-.283 1.131-.755 1.603-1.321v1.699h2.922V9.345h-2.922v.755Zm-.472 6.891c-.283.472-.66.944-1.131 1.227-.471.283-1.037.378-1.509.378-.565 0-1.037-.189-1.508-.378-.471-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.095-1.228.472-1.794.848-1.51 2.64-1.982 4.148-1.227h.094c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM126.708 5.477h-2.923v15.575h2.923V5.477ZM140.191 14.914c0-1.038-.189-2.077-.755-3.02-.471-.85-1.131-1.605-2.074-1.983a6.78 6.78 0 0 0-6.033 0 5.485 5.485 0 0 0-2.075 2.077c-.565 1.038-.754 2.077-.754 3.21 0 1.132.189 2.17.754 3.114a5.485 5.485 0 0 0 2.075 2.077c.942.472 1.979.755 3.016.755a6.56 6.56 0 0 0 3.489-1.038c.943-.661 1.602-1.7 1.885-2.738h-3.016c-.377.944-1.321 1.51-2.358 1.416-.659 0-1.414-.283-1.885-.755-.566-.472-.848-1.227-.848-1.982h8.484c.095-.378.095-.755.095-1.133Zm-8.579-.85c.093-.755.377-1.415.942-1.887.566-.472 1.226-.661 1.886-.661.754 0 1.414.189 1.979.66.566.473.849 1.133.849 1.794l-5.656.095ZM67.692 9.159a4.7 4.7 0 0 0-1.98.472c-.377.188-.754.377-1.037.66a.286.286 0 0 1-.377 0c-.094-.094-.094-.094-.094-.188v-.756H61.28v11.705h2.923v-6.418c-.094-.755.188-1.51.754-2.171.471-.472 1.226-.756 1.886-.756.66 0 1.414.284 1.885.755.472.567.754 1.322.66 2.172v6.418h2.923v-6.89c.094-1.322-.283-2.644-1.226-3.682-.848-.85-2.168-1.416-3.394-1.321Z"></path></g><defs><clipPath id="anyscale-logo_svg__a"><path fill="currentColor" d="M0 0h140.19v32H0z"></path></clipPath></defs></svg></div><div class="font-light leading-normal text-lightBlue-300 opacity-70 hidden lg:flex"><span>Â© Anyscale, Inc 2022</span></div></div><div class="z-10 hidden flex-col gap-10 lg:flex"><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Anyscale</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://www.linkedin.com/company/joinanyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M15.784 14.168a1.667 1.667 0 1 1-3.334-.002 1.667 1.667 0 0 1 3.334.002Zm.05 2.9H12.5V27.5h3.334V17.067Zm5.266 0h-3.317V27.5h3.284v-5.475c0-3.05 3.975-3.334 3.975 0V27.5h3.291v-6.609c0-5.141-5.883-4.95-7.266-2.424l.033-1.4Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://www.facebook.com/AnyscaleCompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M21.667 21.247h2.083l.834-3.333h-2.917v-1.667c0-.858 0-1.666 1.666-1.666h1.25v-2.8a23.49 23.49 0 0 0-2.38-.117c-2.263 0-3.87 1.38-3.87 3.917v2.333h-2.5v3.333h2.5v7.084h3.334v-7.084Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://twitter.com/anyscalecompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/anyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Ray</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://twitter.com/raydistributed"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/ray-project/ray"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div></div></div><div class="grid grid-cols-2 gap-x-8 gap-y-12 md:grid-cols-3 md:gap-y-10 md:gap-x-16"><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Products</div><div><ul>
<li><a href="platform.html">Anyscale Compute Platform</a></li>
<li><a href="ray-open-source.html">Ray Open Source</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Events</div><div><ul>
<li><a href="event-category/rl-summit.html">Webinars</a></li>
<li><a href="event-category/rl-summit.html">Meetups</a></li>
<li><a href="event-category/rl-summit.html">Summits</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Company</div><div><ul>
<li><a href="about.html">About Us</a></li>
<li><a href="press.html">News</a></li>
<li><a href="careers.html">Careers</a></li>
<li><a href="community.html">Community</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Learn</div><div><ul>
<li><a href="blog/how-ray-solves-common-production-challenges-for-generative-ai.html">Blog</a></li>
<li><a href="event-category/rl-summit.html">Demos &amp; Webinars</a></li>
<li><a href="event-category/rl-summit.html">Anyscale Academy</a></li>
<li><a href="https://docs.anyscale.com">Anyscale Docs</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Scalable AI and Python</div><div><ul>
<li><a href="data-ingestion.html">Data Ingestion</a></li>
<li><a href="reinforcement-learning.html">Reinforcement Learning</a></li>
<li><a href="ray-air.html">Ray AIR</a></li>
<li><a href="model-serving.html">Model Serving</a></li>
<li><a href="hyperparameter-tuning.html">Hyperparameter Tuning</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Use Cases</div><div><ul>
<li><a href="demand-forecasting.html">Demand Forecasting/Pricing</a></li>
<li><a href="industrial-automation.html">Industrial Automation</a></li>
<li><a href="machine-learning.html">Scalable ML Platforms</a></li>
<li><a href="natural-language-processing.html">NLP</a></li>
<li><a href="recommendation-system.html">Recommendation System</a></li>
</ul>
</div></div></div><div class="grid-col-1 grid gap-12 md:grid-cols-3 md:gap-16 lg:hidden"><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Anyscale</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://www.linkedin.com/company/joinanyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M15.784 14.168a1.667 1.667 0 1 1-3.334-.002 1.667 1.667 0 0 1 3.334.002Zm.05 2.9H12.5V27.5h3.334V17.067Zm5.266 0h-3.317V27.5h3.284v-5.475c0-3.05 3.975-3.334 3.975 0V27.5h3.291v-6.609c0-5.141-5.883-4.95-7.266-2.424l.033-1.4Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://www.facebook.com/AnyscaleCompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M21.667 21.247h2.083l.834-3.333h-2.917v-1.667c0-.858 0-1.666 1.666-1.666h1.25v-2.8a23.49 23.49 0 0 0-2.38-.117c-2.263 0-3.87 1.38-3.87 3.917v2.333h-2.5v3.333h2.5v7.084h3.334v-7.084Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://twitter.com/anyscalecompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/anyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Ray</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://twitter.com/raydistributed"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/ray-project/ray"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="md:self-end md:text-right"><div class="font-light leading-normal text-lightBlue-300 opacity-70"><span>Â© Anyscale, Inc 2022</span></div></div></div></div><div class="pointer-events-none absolute bottom-0 left-0"><svg width="612" height="297" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="hidden lg:block"><g opacity="0.5" clip-path="url(#dots_svg__a)"><mask id="dots_svg__c" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="0" y="0" width="612" height="297"><path transform="rotate(90 611.5 .5)" fill="url(#dots_svg__b)" stroke="#F2F3F7" d="M611.5.5h296v611h-296z"></path></mask><g mask="url(#dots_svg__c)" stroke="#A2B7D4"><g opacity="0.2" stroke-linecap="round"><path d="M598 350v-688M582 350v-688M566 350v-688M550 350v-688M534 350v-688M518 350v-688M502 350v-688M486 350v-688M470 350v-688M454 350v-688M438 350v-688M422 350v-688M406 350v-688M390 350v-688M374 350v-688M358 350v-688M342 350v-688M326 350v-688M310 350v-688M294 350v-688M278 350v-688M262 350v-688M246 350v-688M230 350v-688M214 350v-688M198 350v-688M182 350v-688M166 350v-688M150 350v-688M134 350v-688M118 350v-688M102 350v-688M86 350v-688M70 350v-688M54 350v-688M38 350v-688M22 350v-688M6 350v-688"></path></g><g opacity="0.2" stroke-linecap="round"><path d="M1014 14H0M1014 30H0M1014 46H0M1014 62H0M1014 78H0M1014 94H0M1014 110H0M1014 126H0M1014 142H0M1014 158H0M1014 174H0M1014 190H4M1014 206H4M1014 222H4M1014 238H4M1014 254H4M1014 270H4M1014 286H4"></path></g><path fill="#4C6996" d="M615.5 12.5v3h-3v-3zM615.5 28.5v3h-3v-3zM615.5 44.5v3h-3v-3zM615.5 60.5v3h-3v-3zM615.5 76.5v3h-3v-3zM615.5 92.5v3h-3v-3zM615.5 108.5v3h-3v-3zM615.5 124.5v3h-3v-3zM615.5 140.5v3h-3v-3zM615.5 156.5v3h-3v-3zM615.5 172.5v3h-3v-3zM615.5 188.5v3h-3v-3zM615.5 204.5v3h-3v-3zM615.5 220.5v3h-3v-3zM615.5 236.5v3h-3v-3zM615.5 252.5v3h-3v-3zM615.5 268.5v3h-3v-3zM615.5 284.5v3h-3v-3zM599.5 12.5v3h-3v-3zM599.5 28.5v3h-3v-3zM599.5 44.5v3h-3v-3zM599.5 60.5v3h-3v-3zM599.5 76.5v3h-3v-3zM599.5 92.5v3h-3v-3zM599.5 108.5v3h-3v-3zM599.5 124.5v3h-3v-3zM599.5 140.5v3h-3v-3zM599.5 156.5v3h-3v-3zM599.5 172.5v3h-3v-3zM599.5 188.5v3h-3v-3zM599.5 204.5v3h-3v-3zM599.5 220.5v3h-3v-3zM599.5 236.5v3h-3v-3zM599.5 252.5v3h-3v-3zM599.5 268.5v3h-3v-3zM599.5 284.5v3h-3v-3zM583.5 12.5v3h-3v-3zM583.5 28.5v3h-3v-3zM583.5 44.5v3h-3v-3zM583.5 60.5v3h-3v-3zM583.5 76.5v3h-3v-3zM583.5 92.5v3h-3v-3zM583.5 108.5v3h-3v-3zM583.5 124.5v3h-3v-3zM583.5 140.5v3h-3v-3zM583.5 156.5v3h-3v-3zM583.5 172.5v3h-3v-3zM583.5 188.5v3h-3v-3zM583.5 204.5v3h-3v-3zM583.5 220.5v3h-3v-3zM583.5 236.5v3h-3v-3zM583.5 252.5v3h-3v-3zM583.5 268.5v3h-3v-3zM583.5 284.5v3h-3v-3zM567.5 12.5v3h-3v-3zM567.5 28.5v3h-3v-3zM567.5 44.5v3h-3v-3zM567.5 60.5v3h-3v-3zM567.5 76.5v3h-3v-3zM567.5 92.5v3h-3v-3zM567.5 108.5v3h-3v-3zM567.5 124.5v3h-3v-3zM567.5 140.5v3h-3v-3zM567.5 156.5v3h-3v-3zM567.5 172.5v3h-3v-3zM567.5 188.5v3h-3v-3zM567.5 204.5v3h-3v-3zM567.5 220.5v3h-3v-3zM567.5 236.5v3h-3v-3zM567.5 252.5v3h-3v-3zM567.5 268.5v3h-3v-3zM567.5 284.5v3h-3v-3zM551.5 12.5v3h-3v-3zM551.5 28.5v3h-3v-3zM551.5 44.5v3h-3v-3zM551.5 60.5v3h-3v-3zM551.5 76.5v3h-3v-3zM551.5 92.5v3h-3v-3zM551.5 108.5v3h-3v-3zM551.5 124.5v3h-3v-3zM551.5 140.5v3h-3v-3zM551.5 156.5v3h-3v-3zM551.5 172.5v3h-3v-3zM551.5 188.5v3h-3v-3zM551.5 204.5v3h-3v-3zM551.5 220.5v3h-3v-3zM551.5 236.5v3h-3v-3zM551.5 252.5v3h-3v-3zM551.5 268.5v3h-3v-3zM551.5 284.5v3h-3v-3zM535.5 12.5v3h-3v-3zM535.5 28.5v3h-3v-3zM535.5 44.5v3h-3v-3zM535.5 60.5v3h-3v-3zM535.5 76.5v3h-3v-3zM535.5 92.5v3h-3v-3zM535.5 108.5v3h-3v-3zM535.5 124.5v3h-3v-3zM535.5 140.5v3h-3v-3zM535.5 156.5v3h-3v-3zM535.5 172.5v3h-3v-3zM535.5 188.5v3h-3v-3zM535.5 204.5v3h-3v-3zM535.5 220.5v3h-3v-3zM535.5 236.5v3h-3v-3zM535.5 252.5v3h-3v-3zM535.5 268.5v3h-3v-3zM535.5 284.5v3h-3v-3zM519.5 12.5v3h-3v-3zM519.5 28.5v3h-3v-3zM519.5 44.5v3h-3v-3zM519.5 60.5v3h-3v-3zM519.5 76.5v3h-3v-3zM519.5 92.5v3h-3v-3zM519.5 108.5v3h-3v-3zM519.5 124.5v3h-3v-3zM519.5 140.5v3h-3v-3zM519.5 156.5v3h-3v-3zM519.5 172.5v3h-3v-3zM519.5 188.5v3h-3v-3zM519.5 204.5v3h-3v-3zM519.5 220.5v3h-3v-3zM519.5 236.5v3h-3v-3zM519.5 252.5v3h-3v-3zM519.5 268.5v3h-3v-3zM519.5 284.5v3h-3v-3zM503.5 12.5v3h-3v-3zM503.5 28.5v3h-3v-3zM503.5 44.5v3h-3v-3zM503.5 60.5v3h-3v-3zM503.5 76.5v3h-3v-3zM503.5 92.5v3h-3v-3zM503.5 108.5v3h-3v-3zM503.5 124.5v3h-3v-3zM503.5 140.5v3h-3v-3zM503.5 156.5v3h-3v-3zM503.5 172.5v3h-3v-3zM503.5 188.5v3h-3v-3zM503.5 204.5v3h-3v-3zM503.5 220.5v3h-3v-3zM503.5 236.5v3h-3v-3zM503.5 252.5v3h-3v-3zM503.5 268.5v3h-3v-3zM503.5 284.5v3h-3v-3zM487.5 12.5v3h-3v-3zM487.5 28.5v3h-3v-3zM487.5 44.5v3h-3v-3zM487.5 60.5v3h-3v-3zM487.5 76.5v3h-3v-3zM487.5 92.5v3h-3v-3zM487.5 108.5v3h-3v-3zM487.5 124.5v3h-3v-3zM487.5 140.5v3h-3v-3zM487.5 156.5v3h-3v-3zM487.5 172.5v3h-3v-3zM487.5 188.5v3h-3v-3zM487.5 204.5v3h-3v-3zM487.5 220.5v3h-3v-3zM487.5 236.5v3h-3v-3zM487.5 252.5v3h-3v-3zM487.5 268.5v3h-3v-3zM487.5 284.5v3h-3v-3zM471.5 12.5v3h-3v-3zM471.5 28.5v3h-3v-3zM471.5 44.5v3h-3v-3zM471.5 60.5v3h-3v-3zM471.5 76.5v3h-3v-3zM471.5 92.5v3h-3v-3zM471.5 108.5v3h-3v-3zM471.5 124.5v3h-3v-3zM471.5 140.5v3h-3v-3zM471.5 156.5v3h-3v-3zM471.5 172.5v3h-3v-3zM471.5 188.5v3h-3v-3zM471.5 204.5v3h-3v-3zM471.5 220.5v3h-3v-3zM471.5 236.5v3h-3v-3zM471.5 252.5v3h-3v-3zM471.5 268.5v3h-3v-3zM471.5 284.5v3h-3v-3zM455.5 12.5v3h-3v-3zM455.5 28.5v3h-3v-3zM455.5 44.5v3h-3v-3zM455.5 60.5v3h-3v-3zM455.5 76.5v3h-3v-3zM455.5 92.5v3h-3v-3zM455.5 108.5v3h-3v-3zM455.5 124.5v3h-3v-3zM455.5 140.5v3h-3v-3zM455.5 156.5v3h-3v-3zM455.5 172.5v3h-3v-3zM455.5 188.5v3h-3v-3zM455.5 204.5v3h-3v-3zM455.5 220.5v3h-3v-3zM455.5 236.5v3h-3v-3zM455.5 252.5v3h-3v-3zM455.5 268.5v3h-3v-3zM455.5 284.5v3h-3v-3zM439.5 12.5v3h-3v-3zM439.5 28.5v3h-3v-3zM439.5 44.5v3h-3v-3zM439.5 60.5v3h-3v-3zM439.5 76.5v3h-3v-3zM439.5 92.5v3h-3v-3zM439.5 108.5v3h-3v-3zM439.5 124.5v3h-3v-3zM439.5 140.5v3h-3v-3zM439.5 156.5v3h-3v-3zM439.5 172.5v3h-3v-3zM439.5 188.5v3h-3v-3zM439.5 204.5v3h-3v-3zM439.5 220.5v3h-3v-3zM439.5 236.5v3h-3v-3zM439.5 252.5v3h-3v-3zM439.5 268.5v3h-3v-3zM439.5 284.5v3h-3v-3zM423.5 12.5v3h-3v-3zM423.5 28.5v3h-3v-3zM423.5 44.5v3h-3v-3zM423.5 60.5v3h-3v-3zM423.5 76.5v3h-3v-3zM423.5 92.5v3h-3v-3zM423.5 108.5v3h-3v-3zM423.5 124.5v3h-3v-3zM423.5 140.5v3h-3v-3zM423.5 156.5v3h-3v-3zM423.5 172.5v3h-3v-3zM423.5 188.5v3h-3v-3zM423.5 204.5v3h-3v-3zM423.5 220.5v3h-3v-3zM423.5 236.5v3h-3v-3zM423.5 252.5v3h-3v-3zM423.5 268.5v3h-3v-3zM423.5 284.5v3h-3v-3zM407.5 12.5v3h-3v-3zM407.5 28.5v3h-3v-3zM407.5 44.5v3h-3v-3zM407.5 60.5v3h-3v-3zM407.5 76.5v3h-3v-3zM407.5 92.5v3h-3v-3zM407.5 108.5v3h-3v-3zM407.5 124.5v3h-3v-3zM407.5 140.5v3h-3v-3zM407.5 156.5v3h-3v-3zM407.5 172.5v3h-3v-3zM407.5 188.5v3h-3v-3zM407.5 204.5v3h-3v-3zM407.5 220.5v3h-3v-3zM407.5 236.5v3h-3v-3zM407.5 252.5v3h-3v-3zM407.5 268.5v3h-3v-3zM407.5 284.5v3h-3v-3zM391.5 12.5v3h-3v-3zM391.5 28.5v3h-3v-3zM391.5 44.5v3h-3v-3zM391.5 60.5v3h-3v-3zM391.5 76.5v3h-3v-3zM391.5 92.5v3h-3v-3zM391.5 108.5v3h-3v-3zM391.5 124.5v3h-3v-3zM391.5 140.5v3h-3v-3zM391.5 156.5v3h-3v-3zM391.5 172.5v3h-3v-3zM391.5 188.5v3h-3v-3zM391.5 204.5v3h-3v-3zM391.5 220.5v3h-3v-3zM391.5 236.5v3h-3v-3zM391.5 252.5v3h-3v-3zM391.5 268.5v3h-3v-3zM391.5 284.5v3h-3v-3zM375.5 12.5v3h-3v-3zM375.5 28.5v3h-3v-3zM375.5 44.5v3h-3v-3zM375.5 60.5v3h-3v-3zM375.5 76.5v3h-3v-3zM375.5 92.5v3h-3v-3zM375.5 108.5v3h-3v-3zM375.5 124.5v3h-3v-3zM375.5 140.5v3h-3v-3zM375.5 156.5v3h-3v-3zM375.5 172.5v3h-3v-3zM375.5 188.5v3h-3v-3zM375.5 204.5v3h-3v-3zM375.5 220.5v3h-3v-3z"></path><rect x="375.5" y="236.5" width="3" height="3" rx="1.5" transform="rotate(90 375.5 236.5)" fill="#4C6996"></rect><path fill="#4C6996" d="M375.5 252.5v3h-3v-3zM375.5 268.5v3h-3v-3zM375.5 284.5v3h-3v-3zM359.5 12.5v3h-3v-3zM359.5 28.5v3h-3v-3zM359.5 44.5v3h-3v-3zM359.5 60.5v3h-3v-3zM359.5 76.5v3h-3v-3zM359.5 92.5v3h-3v-3zM359.5 108.5v3h-3v-3zM359.5 124.5v3h-3v-3zM359.5 140.5v3h-3v-3zM359.5 156.5v3h-3v-3zM359.5 172.5v3h-3v-3zM359.5 188.5v3h-3v-3zM359.5 204.5v3h-3v-3zM359.5 220.5v3h-3v-3zM359.5 236.5v3h-3v-3zM359.5 252.5v3h-3v-3zM359.5 268.5v3h-3v-3zM359.5 284.5v3h-3v-3zM343.5 12.5v3h-3v-3zM343.5 28.5v3h-3v-3zM343.5 44.5v3h-3v-3zM343.5 60.5v3h-3v-3zM343.5 76.5v3h-3v-3zM343.5 92.5v3h-3v-3zM343.5 108.5v3h-3v-3zM343.5 124.5v3h-3v-3zM343.5 140.5v3h-3v-3zM343.5 156.5v3h-3v-3zM343.5 172.5v3h-3v-3zM343.5 188.5v3h-3v-3zM343.5 204.5v3h-3v-3zM343.5 220.5v3h-3v-3zM343.5 236.5v3h-3v-3zM343.5 252.5v3h-3v-3zM343.5 268.5v3h-3v-3zM343.5 284.5v3h-3v-3zM327.5 12.5v3h-3v-3zM327.5 28.5v3h-3v-3zM327.5 44.5v3h-3v-3zM327.5 60.5v3h-3v-3zM327.5 76.5v3h-3v-3zM327.5 92.5v3h-3v-3zM327.5 108.5v3h-3v-3zM327.5 124.5v3h-3v-3zM327.5 140.5v3h-3v-3zM327.5 156.5v3h-3v-3zM327.5 172.5v3h-3v-3zM327.5 188.5v3h-3v-3zM327.5 204.5v3h-3v-3zM327.5 220.5v3h-3v-3zM327.5 236.5v3h-3v-3zM327.5 252.5v3h-3v-3zM327.5 268.5v3h-3v-3zM327.5 284.5v3h-3v-3zM311.5 12.5v3h-3v-3zM311.5 28.5v3h-3v-3zM311.5 44.5v3h-3v-3zM311.5 60.5v3h-3v-3zM311.5 76.5v3h-3v-3zM311.5 92.5v3h-3v-3zM311.5 108.5v3h-3v-3zM311.5 124.5v3h-3v-3zM311.5 140.5v3h-3v-3zM311.5 156.5v3h-3v-3zM311.5 172.5v3h-3v-3zM311.5 188.5v3h-3v-3zM311.5 204.5v3h-3v-3zM311.5 220.5v3h-3v-3zM311.5 236.5v3h-3v-3zM311.5 252.5v3h-3v-3zM311.5 268.5v3h-3v-3zM311.5 284.5v3h-3v-3zM295.5 12.5v3h-3v-3zM295.5 28.5v3h-3v-3zM295.5 44.5v3h-3v-3zM295.5 60.5v3h-3v-3zM295.5 76.5v3h-3v-3zM295.5 92.5v3h-3v-3zM295.5 108.5v3h-3v-3zM295.5 124.5v3h-3v-3zM295.5 140.5v3h-3v-3zM295.5 156.5v3h-3v-3zM295.5 172.5v3h-3v-3zM295.5 188.5v3h-3v-3zM295.5 204.5v3h-3v-3zM295.5 220.5v3h-3v-3zM295.5 236.5v3h-3v-3zM295.5 252.5v3h-3v-3zM295.5 268.5v3h-3v-3zM295.5 284.5v3h-3v-3zM279.5 12.5v3h-3v-3zM279.5 28.5v3h-3v-3zM279.5 44.5v3h-3v-3zM279.5 60.5v3h-3v-3zM279.5 76.5v3h-3v-3zM279.5 92.5v3h-3v-3zM279.5 108.5v3h-3v-3zM279.5 124.5v3h-3v-3zM279.5 140.5v3h-3v-3zM279.5 156.5v3h-3v-3zM279.5 172.5v3h-3v-3zM279.5 188.5v3h-3v-3zM279.5 204.5v3h-3v-3zM279.5 220.5v3h-3v-3zM279.5 236.5v3h-3v-3zM279.5 252.5v3h-3v-3zM279.5 268.5v3h-3v-3zM279.5 284.5v3h-3v-3zM263.5 12.5v3h-3v-3zM263.5 28.5v3h-3v-3zM263.5 44.5v3h-3v-3zM263.5 60.5v3h-3v-3zM263.5 76.5v3h-3v-3zM263.5 92.5v3h-3v-3zM263.5 108.5v3h-3v-3zM263.5 124.5v3h-3v-3zM263.5 140.5v3h-3v-3zM263.5 156.5v3h-3v-3zM263.5 172.5v3h-3v-3zM263.5 188.5v3h-3v-3zM263.5 204.5v3h-3v-3zM263.5 220.5v3h-3v-3zM263.5 236.5v3h-3v-3zM263.5 252.5v3h-3v-3zM263.5 268.5v3h-3v-3zM263.5 284.5v3h-3v-3zM247.5 12.5v3h-3v-3zM247.5 28.5v3h-3v-3zM247.5 44.5v3h-3v-3zM247.5 60.5v3h-3v-3zM247.5 76.5v3h-3v-3zM247.5 92.5v3h-3v-3zM247.5 108.5v3h-3v-3zM247.5 124.5v3h-3v-3zM247.5 140.5v3h-3v-3zM247.5 156.5v3h-3v-3zM247.5 172.5v3h-3v-3zM247.5 188.5v3h-3v-3zM247.5 204.5v3h-3v-3zM247.5 220.5v3h-3v-3zM247.5 236.5v3h-3v-3zM247.5 252.5v3h-3v-3zM247.5 268.5v3h-3v-3zM247.5 284.5v3h-3v-3zM231.5 12.5v3h-3v-3zM231.5 28.5v3h-3v-3zM231.5 44.5v3h-3v-3zM231.5 60.5v3h-3v-3zM231.5 76.5v3h-3v-3zM231.5 92.5v3h-3v-3zM231.5 108.5v3h-3v-3zM231.5 124.5v3h-3v-3zM231.5 140.5v3h-3v-3zM231.5 156.5v3h-3v-3zM231.5 172.5v3h-3v-3zM231.5 188.5v3h-3v-3zM231.5 204.5v3h-3v-3zM231.5 220.5v3h-3v-3zM231.5 236.5v3h-3v-3zM231.5 252.5v3h-3v-3zM231.5 268.5v3h-3v-3zM231.5 284.5v3h-3v-3zM215.5 12.5v3h-3v-3zM215.5 28.5v3h-3v-3zM215.5 44.5v3h-3v-3zM215.5 60.5v3h-3v-3zM215.5 76.5v3h-3v-3zM215.5 92.5v3h-3v-3zM215.5 108.5v3h-3v-3zM215.5 124.5v3h-3v-3zM215.5 140.5v3h-3v-3zM215.5 156.5v3h-3v-3zM215.5 172.5v3h-3v-3zM215.5 188.5v3h-3v-3zM215.5 204.5v3h-3v-3zM215.5 220.5v3h-3v-3zM215.5 236.5v3h-3v-3zM215.5 252.5v3h-3v-3zM215.5 268.5v3h-3v-3zM215.5 284.5v3h-3v-3zM199.5 12.5v3h-3v-3zM199.5 28.5v3h-3v-3zM199.5 44.5v3h-3v-3zM199.5 60.5v3h-3v-3zM199.5 76.5v3h-3v-3zM199.5 92.5v3h-3v-3zM199.5 108.5v3h-3v-3zM199.5 124.5v3h-3v-3zM199.5 140.5v3h-3v-3zM199.5 156.5v3h-3v-3zM199.5 172.5v3h-3v-3zM199.5 188.5v3h-3v-3zM199.5 204.5v3h-3v-3zM199.5 220.5v3h-3v-3zM199.5 236.5v3h-3v-3zM199.5 252.5v3h-3v-3zM199.5 268.5v3h-3v-3zM199.5 284.5v3h-3v-3zM183.5 12.5v3h-3v-3zM183.5 28.5v3h-3v-3zM183.5 44.5v3h-3v-3zM183.5 60.5v3h-3v-3zM183.5 76.5v3h-3v-3zM183.5 92.5v3h-3v-3zM183.5 108.5v3h-3v-3zM183.5 124.5v3h-3v-3zM183.5 140.5v3h-3v-3zM183.5 156.5v3h-3v-3zM183.5 172.5v3h-3v-3zM183.5 188.5v3h-3v-3zM183.5 204.5v3h-3v-3zM183.5 220.5v3h-3v-3zM183.5 236.5v3h-3v-3zM183.5 252.5v3h-3v-3zM183.5 268.5v3h-3v-3zM183.5 284.5v3h-3v-3zM167.5 12.5v3h-3v-3zM167.5 28.5v3h-3v-3zM167.5 44.5v3h-3v-3zM167.5 60.5v3h-3v-3zM167.5 76.5v3h-3v-3zM167.5 92.5v3h-3v-3zM167.5 108.5v3h-3v-3zM167.5 124.5v3h-3v-3zM167.5 140.5v3h-3v-3zM167.5 156.5v3h-3v-3zM167.5 172.5v3h-3v-3zM167.5 188.5v3h-3v-3zM167.5 204.5v3h-3v-3zM167.5 220.5v3h-3v-3zM167.5 236.5v3h-3v-3zM167.5 252.5v3h-3v-3zM167.5 268.5v3h-3v-3zM167.5 284.5v3h-3v-3zM151.5 12.5v3h-3v-3zM151.5 28.5v3h-3v-3zM151.5 44.5v3h-3v-3zM151.5 60.5v3h-3v-3zM151.5 76.5v3h-3v-3zM151.5 92.5v3h-3v-3zM151.5 108.5v3h-3v-3zM151.5 124.5v3h-3v-3zM151.5 140.5v3h-3v-3zM151.5 156.5v3h-3v-3zM151.5 172.5v3h-3v-3zM151.5 188.5v3h-3v-3zM151.5 204.5v3h-3v-3zM151.5 220.5v3h-3v-3zM151.5 236.5v3h-3v-3zM151.5 252.5v3h-3v-3zM151.5 268.5v3h-3v-3zM151.5 284.5v3h-3v-3zM135.5 12.5v3h-3v-3zM135.5 28.5v3h-3v-3zM135.5 44.5v3h-3v-3zM135.5 60.5v3h-3v-3zM135.5 76.5v3h-3v-3zM135.5 92.5v3h-3v-3zM135.5 108.5v3h-3v-3zM135.5 124.5v3h-3v-3zM135.5 140.5v3h-3v-3zM135.5 156.5v3h-3v-3zM135.5 172.5v3h-3v-3zM135.5 188.5v3h-3v-3zM135.5 204.5v3h-3v-3zM135.5 220.5v3h-3v-3zM135.5 236.5v3h-3v-3zM135.5 252.5v3h-3v-3zM135.5 268.5v3h-3v-3zM135.5 284.5v3h-3v-3zM119.5 12.5v3h-3v-3zM119.5 28.5v3h-3v-3zM119.5 44.5v3h-3v-3zM119.5 60.5v3h-3v-3zM119.5 76.5v3h-3v-3zM119.5 92.5v3h-3v-3zM119.5 108.5v3h-3v-3zM119.5 124.5v3h-3v-3zM119.5 140.5v3h-3v-3zM119.5 156.5v3h-3v-3zM119.5 172.5v3h-3v-3zM119.5 188.5v3h-3v-3zM119.5 204.5v3h-3v-3zM119.5 220.5v3h-3v-3zM119.5 236.5v3h-3v-3zM119.5 252.5v3h-3v-3zM119.5 268.5v3h-3v-3zM119.5 284.5v3h-3v-3zM103.5 12.5v3h-3v-3zM103.5 28.5v3h-3v-3zM103.5 44.5v3h-3v-3zM103.5 60.5v3h-3v-3zM103.5 76.5v3h-3v-3zM103.5 92.5v3h-3v-3zM103.5 108.5v3h-3v-3zM103.5 124.5v3h-3v-3zM103.5 140.5v3h-3v-3zM103.5 156.5v3h-3v-3zM103.5 172.5v3h-3v-3zM103.5 188.5v3h-3v-3zM103.5 204.5v3h-3v-3zM103.5 220.5v3h-3v-3zM103.5 236.5v3h-3v-3zM103.5 252.5v3h-3v-3zM103.5 268.5v3h-3v-3zM103.5 284.5v3h-3v-3zM87.5 12.5v3h-3v-3zM87.5 28.5v3h-3v-3zM87.5 44.5v3h-3v-3zM87.5 60.5v3h-3v-3zM87.5 76.5v3h-3v-3zM87.5 92.5v3h-3v-3zM87.5 108.5v3h-3v-3zM87.5 124.5v3h-3v-3zM87.5 140.5v3h-3v-3zM87.5 156.5v3h-3v-3zM87.5 172.5v3h-3v-3zM87.5 188.5v3h-3v-3zM87.5 204.5v3h-3v-3zM87.5 220.5v3h-3v-3zM87.5 236.5v3h-3v-3zM87.5 252.5v3h-3v-3zM87.5 268.5v3h-3v-3zM87.5 284.5v3h-3v-3zM71.5 12.5v3h-3v-3zM71.5 28.5v3h-3v-3zM71.5 44.5v3h-3v-3zM71.5 60.5v3h-3v-3zM71.5 76.5v3h-3v-3zM71.5 92.5v3h-3v-3zM71.5 108.5v3h-3v-3zM71.5 124.5v3h-3v-3zM71.5 140.5v3h-3v-3zM71.5 156.5v3h-3v-3zM71.5 172.5v3h-3v-3zM71.5 188.5v3h-3v-3zM71.5 204.5v3h-3v-3zM71.5 220.5v3h-3v-3zM71.5 236.5v3h-3v-3zM71.5 252.5v3h-3v-3zM71.5 268.5v3h-3v-3zM71.5 284.5v3h-3v-3zM55.5 12.5v3h-3v-3zM55.5 28.5v3h-3v-3zM55.5 44.5v3h-3v-3zM55.5 60.5v3h-3v-3zM55.5 76.5v3h-3v-3zM55.5 92.5v3h-3v-3zM55.5 108.5v3h-3v-3zM55.5 124.5v3h-3v-3zM55.5 140.5v3h-3v-3zM55.5 156.5v3h-3v-3zM55.5 172.5v3h-3v-3zM55.5 188.5v3h-3v-3zM55.5 204.5v3h-3v-3zM55.5 220.5v3h-3v-3zM55.5 236.5v3h-3v-3zM55.5 252.5v3h-3v-3zM55.5 268.5v3h-3v-3zM55.5 284.5v3h-3v-3zM39.5 12.5v3h-3v-3zM39.5 28.5v3h-3v-3zM39.5 44.5v3h-3v-3zM39.5 60.5v3h-3v-3zM39.5 76.5v3h-3v-3zM39.5 92.5v3h-3v-3zM39.5 108.5v3h-3v-3zM39.5 124.5v3h-3v-3zM39.5 140.5v3h-3v-3zM39.5 156.5v3h-3v-3zM39.5 172.5v3h-3v-3zM39.5 188.5v3h-3v-3zM39.5 204.5v3h-3v-3zM39.5 220.5v3h-3v-3zM39.5 236.5v3h-3v-3zM39.5 252.5v3h-3v-3zM39.5 268.5v3h-3v-3zM39.5 284.5v3h-3v-3zM23.5 12.5v3h-3v-3zM23.5 28.5v3h-3v-3zM23.5 44.5v3h-3v-3zM23.5 60.5v3h-3v-3zM23.5 76.5v3h-3v-3zM23.5 92.5v3h-3v-3zM23.5 108.5v3h-3v-3zM23.5 124.5v3h-3v-3zM23.5 140.5v3h-3v-3zM23.5 156.5v3h-3v-3zM23.5 172.5v3h-3v-3zM23.5 188.5v3h-3v-3zM23.5 204.5v3h-3v-3zM23.5 220.5v3h-3v-3zM23.5 236.5v3h-3v-3zM23.5 252.5v3h-3v-3zM23.5 268.5v3h-3v-3zM23.5 284.5v3h-3v-3zM7.5 12.5v3h-3v-3zM7.5 28.5v3h-3v-3zM7.5 44.5v3h-3v-3zM7.5 60.5v3h-3v-3zM7.5 76.5v3h-3v-3zM7.5 92.5v3h-3v-3zM7.5 108.5v3h-3v-3zM7.5 124.5v3h-3v-3zM7.5 140.5v3h-3v-3zM7.5 156.5v3h-3v-3zM7.5 172.5v3h-3v-3zM7.5 188.5v3h-3v-3zM7.5 204.5v3h-3v-3zM7.5 220.5v3h-3v-3zM7.5 236.5v3h-3v-3zM7.5 252.5v3h-3v-3zM7.5 268.5v3h-3v-3zM7.5 284.5v3h-3v-3z"></path></g></g><defs><radialGradient id="dots_svg__b" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="rotate(-136.519 627.256 92.637) scale(929.292 677.674)"><stop stop-color="#D9D9D9"></stop><stop offset="0.574" stop-color="#D9D9D9" stop-opacity="0"></stop></radialGradient><clipPath id="dots_svg__a"><path fill="#fff" transform="rotate(90 306 306)" d="M0 0h297v612H0z"></path></clipPath></defs></svg></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"site":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ke4xTi5OD9gnn2YtRXyCV","type":"Entry","createdAt":"2022-10-14T08:31:23.161Z","updatedAt":"2023-06-06T20:01:24.167Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"site"}},"locale":"en-US"},"fields":{"title":"Anyscale","description":"From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab.","identifier":"site-master-new","footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"16eb9I0yhaIALNOPR8WKTY","type":"Entry","createdAt":"2020-09-27T16:03:59.581Z","updatedAt":"2022-10-27T01:20:31.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer","body":"Â© Anyscale, Inc 2022","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"C7AXPFHtleWPOZ9BuT73K","type":"Asset","createdAt":"2020-09-28T14:40:50.925Z","updatedAt":"2020-09-28T14:40:50.925Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"anyscale_logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/C7AXPFHtleWPOZ9BuT73K/927f5130df1357db041d7a7908ab572e/Graphic_only__transparent_background_.png","details":{"size":1415,"image":{"width":331,"height":331}},"fileName":"Graphic only (transparent background).png","contentType":"image/png"}}}],"subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1cDdKq2eUfklc6w52dU6dg","type":"Entry","createdAt":"2020-09-27T16:05:42.307Z","updatedAt":"2022-10-27T01:20:26.714Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-1","header":"Products","body":"- [Anyscale Compute Platform](/platform)\n- [Ray Open Source](/ray-open-source)\n"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5tpXhcAJwSdhX8Wqw9HYTt","type":"Entry","createdAt":"2020-09-27T16:06:37.409Z","updatedAt":"2022-10-27T01:20:21.887Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":10,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-2","header":"Events","body":"- [Webinars](/events)\n- [Meetups](/events)\n- [Summits](/events)\n"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3c3LEomScM93hIg3i2zItU","type":"Entry","createdAt":"2020-09-27T16:07:42.856Z","updatedAt":"2022-10-27T01:20:18.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-3","header":"Company","body":"- [About Us](/about)\n- [News](/press)\n- [Careers](/careers)\n- [Community](/community)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6du4SSXPTxDZKoIVNHVVnT","type":"Entry","createdAt":"2022-10-27T01:20:13.320Z","updatedAt":"2022-10-27T01:20:13.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-4","header":"Learn","body":"- [Blog](/blog)\n- [Demos \u0026 Webinars](/events)\n- [Anyscale Academy](/events)\n- [Anyscale Docs](https://docs.anyscale.com)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1hovCXhGcWlgJmrqNzJ79k","type":"Entry","createdAt":"2022-10-27T01:20:09.895Z","updatedAt":"2022-10-27T01:20:09.895Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-5","header":"Scalable AI and Python","body":"- [Data Ingestion](/data-ingestion)\n- [Reinforcement Learning](/reinforcement-learning)\n- [Ray AIR](/ray-air)\n- [Model Serving](/model-serving)\n- [Hyperparameter Tuning](/hyperparameter-tuning)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"YFy5fUqwOD5ShdMaYEvzB","type":"Entry","createdAt":"2022-10-27T01:20:06.631Z","updatedAt":"2022-10-27T01:20:06.631Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-6","header":"Use Cases","body":"- [Demand Forecasting/Pricing](/demand-forecasting)\n- [Industrial Automation](/industrial-automation)\n- [Scalable ML Platforms](/machine-learning)\n- [NLP](/natural-language-processing)\n- [Recommendation System](/recommendation-system)"}}]}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"KBAZ5OavkdZfNTVjdJkp3","type":"Entry","createdAt":"2022-10-14T10:47:29.682Z","updatedAt":"2022-10-14T10:47:29.682Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Products","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KNN1Kmzo7jxSlxX1k74XQ","type":"Entry","createdAt":"2022-10-07T13:14:07.892Z","updatedAt":"2022-10-07T13:14:07.892Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"52Mmru1brS6ZkmItPeIW86","type":"Entry","createdAt":"2022-10-07T13:13:22.198Z","updatedAt":"2022-11-03T01:33:41.860Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Anyscale Platform","link":"/platform"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6cN6ZwdiFNiPcqIhd7EiQY","type":"Entry","createdAt":"2022-10-07T13:13:49.673Z","updatedAt":"2022-10-07T13:13:49.673Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Open Source","link":"/ray-open-source"}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5R1OTMfuLnPqont0Fjn9Xb","type":"Entry","createdAt":"2022-10-07T13:20:19.578Z","updatedAt":"2022-10-14T10:43:21.550Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Solutions","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1mp3IF3ZJE5JAViu3L9E95","type":"Entry","createdAt":"2022-10-07T13:16:00.346Z","updatedAt":"2022-10-07T13:21:01.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Scalable AI and Python","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4VUyKorAkgQP2V295h8eW3","type":"Entry","createdAt":"2022-10-07T13:15:35.949Z","updatedAt":"2022-10-07T13:15:35.949Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Data Ingestion","link":"/data-ingestion"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"Ouf5WgAzYmLovCZFNWqYm","type":"Entry","createdAt":"2022-10-07T13:17:09.728Z","updatedAt":"2022-10-07T13:17:09.728Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Reinforcement Learning","link":"/reinforcement-learning"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5kVPKqUbXBbKmrkvsImcIi","type":"Entry","createdAt":"2022-10-07T13:17:50.307Z","updatedAt":"2022-10-07T13:17:50.307Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray AIR","link":"/ray-air"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5uUidEvWw1oqjWrRcskEW","type":"Entry","createdAt":"2022-10-07T13:18:21.246Z","updatedAt":"2022-10-07T13:18:21.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Model Serving","link":"/model-serving"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Rbw0CrBN1KecPafCPvF4H","type":"Entry","createdAt":"2022-10-07T13:18:45.601Z","updatedAt":"2022-10-07T13:18:45.601Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Hyperparameter Tuning","link":"/hyperparameter-tuning"}}],"card":"##### Scalable AI and Python\n\nLeverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more."}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2RgWCuIiZe2TI6Nx5Ns7sB","type":"Entry","createdAt":"2022-10-07T13:25:03.101Z","updatedAt":"2023-05-31T12:47:25.274Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Use Cases","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3XMbpU1mPdsU7gktltlAzX","type":"Entry","createdAt":"2023-05-31T12:43:54.011Z","updatedAt":"2023-05-31T12:43:54.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Large Language Models","link":"/large-language-models"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6MAYw6HTSmH5JHcOoYjCzC","type":"Entry","createdAt":"2022-10-07T13:22:29.165Z","updatedAt":"2022-10-07T13:22:29.165Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Demand Forecasting/Pricing","link":"/demand-forecasting"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"30BlKhv6h4neaZwjA2jlxO","type":"Entry","createdAt":"2022-10-07T13:23:09.471Z","updatedAt":"2022-10-07T13:23:09.471Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Industrial Automation","link":"/industrial-automation"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2gGwUVxDuBeUso1uQH2gEW","type":"Entry","createdAt":"2022-10-07T13:23:32.145Z","updatedAt":"2022-10-07T13:23:32.145Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Scalable ML Platforms","link":"/machine-learning"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1cbwbmIvP5ZkV9iNEraQGP","type":"Entry","createdAt":"2022-10-07T13:23:59.732Z","updatedAt":"2022-10-07T13:23:59.732Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"NLP","link":"/natural-language-processing"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2A7FMbckpN5bgW1wp5ZuSh","type":"Entry","createdAt":"2022-10-07T13:24:41.861Z","updatedAt":"2022-10-07T13:24:41.861Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Recommendation System","link":"/recommendation-system"}}],"card":"##### Use Cases\n\nLeverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more."}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QNdrsmqz82oiBJvFTQ26n","type":"Entry","createdAt":"2023-04-13T11:37:45.354Z","updatedAt":"2023-04-13T11:37:45.354Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ecosystem","link":"/ray-ecosystem"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i0nHjF3pmjzGAdEE4V5MG","type":"Entry","createdAt":"2021-12-05T02:08:18.719Z","updatedAt":"2022-03-03T02:01:08.894Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Success Stories","link":"/user-stories"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5mFZvzY56aQCY4xvlBDi9W","type":"Entry","createdAt":"2022-10-14T10:46:35.218Z","updatedAt":"2022-10-14T10:46:35.218Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Learn","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"32tQWjMRAI5qci97W3cuie","type":"Entry","createdAt":"2022-10-07T13:28:46.057Z","updatedAt":"2023-03-22T20:31:55.243Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JQbOCOkxHfH2ZPGNNQVbt","type":"Entry","createdAt":"2021-12-05T14:57:28.664Z","updatedAt":"2021-12-05T14:57:28.664Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Blog","link":"/blog"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4qEkJgYkXZmiDq3lsxg0xW","type":"Entry","createdAt":"2023-03-22T20:31:21.666Z","updatedAt":"2023-03-22T20:31:21.666Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Summit 2023","link":"https://raysummit.anyscale.com/?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=home_nav-pulldown"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48sdeWXptwt3UgEZzeTt3k","type":"Entry","createdAt":"2022-10-07T13:26:28.331Z","updatedAt":"2022-10-07T13:26:28.331Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Videos \u0026 Webinars","link":"/events"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6hMfRHFDZUXcJH1tIq0xhY","type":"Entry","createdAt":"2021-10-21T20:59:36.918Z","updatedAt":"2023-03-06T20:24:26.811Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Training","link":"https://github.com/ray-project/ray-educational-materials"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cVWy7hmqbAzJCW6d7cLUx","type":"Entry","createdAt":"2022-10-07T13:28:33.537Z","updatedAt":"2022-10-07T13:28:33.537Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Anyscale Docs","link":"https://docs.anyscale.com"}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Qjs7zNfhrinHTNnzMpVIO","type":"Entry","createdAt":"2022-10-14T10:44:42.403Z","updatedAt":"2022-10-14T10:44:42.403Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Company","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ZFDrG8YGgU0oyNgBOId7X","type":"Entry","createdAt":"2022-10-07T13:32:38.746Z","updatedAt":"2022-10-07T13:32:38.746Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2RD65wmpn4gmL5Bvxu5VOD","type":"Entry","createdAt":"2020-09-08T17:53:32.413Z","updatedAt":"2021-06-20T15:04:11.482Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"About us","link":"/about"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71EHE1t3QrjnQb8hD9O8Ww","type":"Entry","createdAt":"2022-10-07T13:32:07.593Z","updatedAt":"2022-10-07T13:32:07.593Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"News","link":"/press"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3TNUPaz7ViQ6hbiZciBOQV","type":"Entry","createdAt":"2020-09-28T23:19:39.838Z","updatedAt":"2022-05-12T19:18:18.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Careers","link":"/careers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ubc5oYY5moS7HexIp7AjO","type":"Entry","createdAt":"2022-07-22T20:40:53.600Z","updatedAt":"2022-07-22T20:40:53.600Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Community","link":"/community"}}]}}]}}],"bannerText":"Are you passionate about Ray, scalable AI and distributed computing? Join us at Ray Summit!","bannerLink":"https://raysummit.anyscale.com/?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=homepage_ticker","bannerCta":" Find out more! "}},"title":"Blog","slug":"blog","sections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6nWVrKik3UzKQc0m6QjMQ7","type":"Entry","createdAt":"2020-09-01T18:35:40.585Z","updatedAt":"2023-06-20T17:29:52.368Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":59,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"featured-posts","header":"Blog","subheader":"Featured Posts and News","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7xkHJWpQ3mRxrgAQw3oYFk","type":"Entry","createdAt":"2023-06-15T13:47:45.558Z","updatedAt":"2023-06-16T13:24:48.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Introducing the Anyscale Databricks Connector","seoTitle":"Integrating Anyscale Ray with Databricks","slug":"introducing-the-anyscale-databricks-connector","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3e6JquwKuIz6QvCWO0BAz","type":"Entry","createdAt":"2023-06-15T13:16:27.453Z","updatedAt":"2023-06-15T13:16:27.453Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Greene","slug":"eric-greene","link":"https://www.linkedin.com/in/greene/"}}],"publishedDate":"2023-06-15","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Introducing the Anyscale Databricks connector. Simple Data Transfer and Efficient AI Workflows with Anyscale and Databricks.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The Anyscale Databricks Connector is a new Ray Datasets capability that facilitates easy data transfer between Databricks clusters and Anyscale hosted Ray clusters. The connector makes it easy for Data Scientists using Anyscale to leverage their Databricks data lake during machine learning and discovery.Â  It also enables a simpler way for machine learning engineers to create end-to-end workloads by allowing the entire ML pipeline to be executed within a single Python script. By taking advantage of the highly-scalable nature of Ray and Ray Datasets, machine learning workloads such as training, tuning and batch serving jobs can be executed more quickly and at lower cost, all while taking advantage of the latest advancements in AI through other Ray integrations with Hugging Face, XGBoost, LightGBM and many AI frameworks and libraries.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Key Benefits","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simple access to data","nodeType":"text"},{"data":{},"marks":[],"value":" - Access Databricks data through SQL queries within Anyscale Workspaces Visual Code or JupyterHub environments, empowering data science discovery and development.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Improved data security and governance","nodeType":"text"},{"data":{},"marks":[],"value":" - Ensure data security and governance by directly copying data from Databricks data lakes to Ray clusters, eliminating the need for intermediate steps and maintaining separate controls over sensitive data. All data is encrypted in transit and at rest.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Highly-scalable data exchange","nodeType":"text"},{"data":{},"marks":[],"value":" - Take advantage of the parallel read and write capabilities of Ray Datasets, enabling the exchange of terabytes of data in minutes. This capability significantly speeds up data-intensive operations and enhances scalability while reducing job run times and overall costs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simplified workload development","nodeType":"text"},{"data":{},"marks":[],"value":" - Simplify machine learning workflows by consolidating all the necessary logic into a single script. This script can query features, train and tune models, and score and materialize results back into the data lake, streamlining the entire process.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Unlock the latest AI Innovations","nodeType":"text"},{"data":{},"marks":[],"value":" - Leverage the power of Databricks for querying and joining data, while benefiting from the scalability and simplicity of Ray AIR for machine learning and AI development. Ray AIR provides integration to the latest AI innovations such as pretrained Hugging Face language models.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Simple, Secure and Scalable Data Access","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing the Anyscale Databricks connector, large datasets can be queried from a Databricks SQL warehouse, and quickly transferred into a Ray Dataset distributed across the Ray cluster. The Databricks connector reads query results in chunks of data, in parallel across the Ray cluster. The size of the dataset and the speed at which it can be transferred scales based on the size of your Ray cluster and the number of simultaneous requests the underlying Databricks SQL warehouse supports.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6taTlZK9ffeivO5RCUJiQ4","type":"Asset","createdAt":"2023-06-15T13:27:43.116Z","updatedAt":"2023-06-15T13:27:43.116Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"1 - Anyscale Databricks Architecture","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6taTlZK9ffeivO5RCUJiQ4/c5fc0bfef768f0fb6e8785bf71ded1e2/1_-_Anyscale_Databricks_Arch.jpg","details":{"size":95989,"image":{"width":761,"height":478}},"fileName":"1 - Anyscale Databricks Arch.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Anyscale Databricks connectors, parallel reads and writes and overall speed will scale with the size of the Ray cluster. The benchmarks for reading and writing demonstrate how the cluster scales out to support larger data sets.  ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2KimYEe845rk6fb2oYaCmz","type":"Asset","createdAt":"2023-06-15T13:35:36.597Z","updatedAt":"2023-06-15T13:35:36.597Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2 - Anyscale Databricks Table Example","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2KimYEe845rk6fb2oYaCmz/8302017d6535b5d676d3dfb50dc12cbd/2_-_Anyscale_Databricks_Table_Example.jpg","details":{"size":218942,"image":{"width":2178,"height":1002}},"fileName":"2 - Anyscale Databricks Table Example.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Simplified ML Development","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing the Databricks connector from within an Anyscale Workspaces, Data Scientists and Machine Learning Engineers have a unified experience while developing workloads. The data query, feature engineering, training, tuning and inference can all be executed within a single Python script that scales across a Ray cluster. Ray integrates with the latest AI and Machine learning libraries, enabling the most advanced ML workloads that work with 3rd party libs  like Hugging Face, XGBoost, LightGBM, and most PyTorch and TensorFlow model architectures.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7c7IX8TpJKbaXdnLIwoD88","type":"Asset","createdAt":"2023-06-15T13:39:22.298Z","updatedAt":"2023-06-15T13:39:22.298Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"3 - Anyscale Databricks UI","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7c7IX8TpJKbaXdnLIwoD88/f17fb8c8884ce197c9ad35a8ce3f6b4e/3_-_Anyscale_Databricks_Product_UI.jpg","details":{"size":242920,"image":{"width":2226,"height":1230}},"fileName":"3 - Anyscale Databricks Product UI.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2muX02EO70X2MXN8wBRyUJ","type":"Asset","createdAt":"2023-06-15T13:41:08.580Z","updatedAt":"2023-06-15T13:41:08.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"4 - Anyscale Databricks Product UI 2","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2muX02EO70X2MXN8wBRyUJ/cc5301c192c66e632cb3141a9e5bde55/4_-_Anyscale_Databricks_Product_UI_2.jpg","details":{"size":383181,"image":{"width":1860,"height":1282}},"fileName":"4 - Anyscale Databricks Product UI 2.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlock new use cases with AI and ML ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nAnyscale and Ray integrate with most open source AI and ML libraries, enabling the latest innovation in AI to be applied to Databricks data. Ray makes working with Hugging face, XGBoost, LightGBM, TensorFlow and PyTorch and SciKit Learn a unified experience, with the added benefits of scaling with distributed training, tuning and serving.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical ML workload to train a LightGBM can be implemented in 20 lines of code.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6rUeSQ88fvbTd68jkhrii0","type":"Asset","createdAt":"2023-06-15T13:42:58.984Z","updatedAt":"2023-06-15T13:42:58.984Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"5 - Anyscale Databricks deployment","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6rUeSQ88fvbTd68jkhrii0/43de8dc64eaf8792ad73ac6faf8b4551/5_-_Anyscale_Databricks_deployment.jpg","details":{"size":272363,"image":{"width":1470,"height":1402}},"fileName":"5 - Anyscale Databricks deployment.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Want to see for yourself? ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/signup"},"content":[{"data":{},"marks":[],"value":"Request a Trial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" today!\n","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71IpYQvsxplh7okIL7RJVx","type":"Asset","createdAt":"2023-06-15T13:50:54.646Z","updatedAt":"2023-06-16T13:19:09.178Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"locale":"en-US"},"fields":{"title":"0 -Anyscale Databricks headline image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/71IpYQvsxplh7okIL7RJVx/b193e65fbc2d6500e3ccb2162d323080/0_-_Updated_Databricks_Anyscale_Image.png","details":{"size":90723,"image":{"width":1500,"height":1000}},"fileName":"0 - Updated Databricks Anyscale Image.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2GdxAQ2xXKoNlzKc7DXvLS","type":"Entry","createdAt":"2023-05-31T19:04:09.560Z","updatedAt":"2023-06-01T03:29:01.792Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Announcing Aviary: Open Source Multi-LLM Serving","slug":"announcing-aviary-open-source-multi-llm-serving-solution","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-05-31","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Exec Summary","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": Open source LLMs are getting better every day, so today weâre open sourcing Aviary to make testing, evaluating and deploying Open Source LLMs easier â we found it harder than we thought it should be so we used Ray Serve to fix it.Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Try it out with Aviary Explorer ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://aviary.anyscale.com/"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Github Repo ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/aviary"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Video demo and explanation ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.youtube.com/watch?v=WmqPfQOXJ-4"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Slack support ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Forum support ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/c/llms-generative-ai/27"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Want a managed version of Aviary? Sign up ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://bit.ly/run-aviary"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". \n\nWeâre excited to announce the release of Aviary: a new open source project that simplifies and enables easy self-hosted serving of multiple LLM models efficiently.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Weâre big fans of open source LLMs here at Anyscale. The rate of improvement of open source LLMs has been ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.semianalysis.com/p/google-we-have-no-moat-and-neither"},"content":[{"nodeType":"text","value":"nothing short of phenomenal","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". This has given the AI community many options beyond the big âclosedâ players like OpenAI, Anthropic, Cohere and more.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Why are companies exploring self hosted open source LLMs? There are a few reasons: ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Cost: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"LLMs are very expensive to operate. A single query can cost tens of cents. Using open source can be considerably cheaper.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Latency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"By colocating LLMs and business logic, sometimes even on the same machine, latency â one of the biggest issues with deploying LLMs â can be kept low.Â Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Transparency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Organizations want to understand exactly what is happening in their models. Open source allows them to understand whatâs happening inside them.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Deployment flexibility: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Open Source models can be deployed on premise, in the cloud using the userâs cloud resources, or as part of a SaaS offering.Â Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Data Control: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Data governance issues are much easier to guarantee if the data does not leave your control.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Customization:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" Customizing/fine-tuning on proprietary data to enable domain specific answers at high quality.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We were writing some tools for ourselves to really understand the answers to the questions above, as well as the advantages of closed models, mainly focused around quality. A funny thing happened on the way, however: we discovered that while the open source models have improved, the infrastructure for serving LLMs has not kept up. We started building our own library for loading the models, autoscaling them efficiently, etc on top of Ray Serve, dealing with variations between the models on things like stop tokens etc.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It was then that we realized, actually the rest of the LLM community might benefit from this library as well. So, today we are releasing Aviary as open source.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary helps leverage the advantages of open source models by building on the solid foundation of ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io"},"content":[{"nodeType":"text","value":"Ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", the popular framework for scalable AI. In particular, it takes advantage of ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"nodeType":"text","value":"Ray Serve","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", the highly flexible serving framework that is part of Ray.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary does this by:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Providing an extensive suite of pre-configured open source LLMs, with reasonable defaults that work out of the box.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Integrating acceleration approaches like DeepSpeed with the packaged LLMs.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Simplifying the deployment of multiple LLMs within a single unified framework.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Simplifying the addition of new LLMs to within minutes in most cases","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Offering unique autoscaling support, including scale-to-zero (a first in open source).Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Weâve also included a demo Gradio frontend that shows off whatâs possible with these capabilities, as well as some command line tools to address the evaluation questions that originally motivated this project.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"You can see these features in the video below:Â ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jx26kCBQB1GRKEulIDxoi","type":"Entry","createdAt":"2023-05-31T02:10:20.974Z","updatedAt":"2023-05-31T02:10:20.974Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Aviary Announcement Video","videoUrl":"https://www.youtube.com/watch?v=WmqPfQOXJ-4"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"While existing solutions have some of these features individually and we are grateful to build on their foundations (such as Hugging Faceâs text-generation-inference), none of the existing solutions brings these capabilities together in a way that is convenient for users. We are also planning to continue to expand the feature set of Aviary, adding support for features like streaming, continuous batching and others.Â Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary is also open to community contributions â especially for adding new LLMs. Weâll then redeploy these in production so the rest of the world can use those LLMs too. We will also be supporting Aviary through Slack, Discourse, GitHub issues and an LLM-based RayBot.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"At the same time we understand that not everyone wants to take on the additional challenges of maintaining their own aviary of models. For that reason, we will also be offering a hosted version of Aviary that builds on top of our Anyscale managed Ray platform and offers additional features around deployment including:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Using spot instances with on-demand fallback for large potential savings","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Zero downtime upgrades with no dropped requests","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Faster deployment and autoscaling","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Improved observability tools.Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We are also announcing that Aviary will be available at no additional charge for existing Anyscale customers via our workspaces solution, and weâre actively onboarding new Aviary customers now. If youâd like to deploy Aviary, please reach out to us ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://bit.ly/run-aviary"},"content":[{"nodeType":"text","value":"here","marks":[],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1c2aG5pfLWU6YenPBZIHvI","type":"Asset","createdAt":"2023-05-31T19:00:13.845Z","updatedAt":"2023-05-31T19:00:13.845Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Aviary Light Mode","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1c2aG5pfLWU6YenPBZIHvI/742f647b979b4eb4b682e37fc35c71a7/aviary-light-mode.png","details":{"size":712180,"image":{"width":3030,"height":1750}},"fileName":"aviary-light-mode.png","contentType":"image/png"}}},"mainImageFit":"contain","hideIntro":true,"showMainImage":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UsGSYssf1ebf8N5mRbNxT","type":"Entry","createdAt":"2023-05-17T17:26:50.771Z","updatedAt":"2023-05-24T20:17:43.464Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Numbers every LLM Developer should know","slug":"num-every-llm-developer-should-know","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-05-17","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"When I was at Google, there was a document put together by Jeff Dean, the legendary engineer, called ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://brenocon.com/dean_perf.html"},"content":[{"nodeType":"text","value":"Numbers every Engineer should know","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Itâs really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Prompts","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"40-90%: Amount saved by appending âBe Conciseâ to your prompt","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Itâs important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money [1]. This can be broadened beyond simply appending âbe conciseâ to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"1.3: Average tokens per word","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"LLMs operate on tokens. Tokens are words or sub-parts of words, so âeatingâ might be broken into two tokens âeatâ and âingâ.Â A 750 word document will be about 1000 tokens.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Knowing this ratio is important because most billing is done in tokens, and the LLMâs context window size is also defined in tokens.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Prices","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Prices [2] are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~50: Cost Ratio of GPT-4 to GPT-3.5 Turbo","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"What this means is that for many practical applications, itâs much better to use GPT-4 for things like generation and then use that data to fine tune a smaller model. It is roughly 50 times cheaper to use GPT-3.5-Turbo [3] than GPT-4 (the âroughlyâ is because GPT-4 charges differently for the prompt and the generated output)Â  â so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"5: Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embeddingÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. âWhat is the capital of Delaware?â when looked up in an neural information retrieval system costs about 5x less [4] than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x!Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"10: Cost Ratio of OpenAI embedding to Self-Hosted embeddingÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using HuggingFaceâs SentenceTransformers (which are pretty much as good as OpenAIâs embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees).Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"6: Cost Ratio of OpenAI base vs fine tuned model queries","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"1: Cost Ratio of Self-Hosted base vs fine-tuned model queriesÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If youâre self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Training and Fine Tuning\n","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://arxiv.org/abs/2302.13971"},"content":[{"nodeType":"text","value":"LLaMa paper","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. Thatâs not something most companies can do (shameless plug time: of course, we at Anyscale can â thatâs our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray"},"content":[{"nodeType":"text","value":"bread and butter","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"! Contact us if youâd like to learn more). The point is that training your own LLM is possible, but itâs not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"\u003c 0.001: Cost ratio of fine tuning vs training from scratch","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using"},"content":[{"nodeType":"text","value":"6B parameter model for about $7","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Even at OpenAIâs rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), youâre looking at $40. However, fine tuning is one thing and training from scratch is another [5].\n","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"GPU Memory","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If youâre self-hosting a model, itâs really important to understand GPU memory because LLMs push your GPUâs memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It may seem strange, but itâs important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"2x number of parameters: Typical GPU memory requirements of an LLM for serving","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. Thereâs usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy there is too much loss of resolution. Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but thatâs atypical.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~1GB: Typical GPU memory requirements of an embedding model","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/"},"content":[{"nodeType":"text","value":"sentence transformers","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". OpenAI also has its own embeddings that they provide commercially.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"You typically donât have to worry about how much memory embeddings take on the GPU, theyâre fairly small. Weâve even had the embedding and the LLM on the same GPU.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"\u003e10x: Throughput improvement from batching LLM requestsÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second.Â  The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~1 MB: GPU Memory required for 1 token of output with a 13B parameter model","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say â I have 24GB to spare, whatâs 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but itâs still a real issue.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"embedded-entry-inline","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1ACoDuIrrm71E0BWViBO4Y","type":"Entry","createdAt":"2023-05-17T17:26:34.593Z","updatedAt":"2023-05-17T17:26:34.593Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"inlineImage"}},"locale":"en-US"},"fields":{"name":"numbers-cheatsheet","image":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1AkaPcJlWoSpqixtqeKcD1","type":"Asset","createdAt":"2023-05-17T16:48:22.989Z","updatedAt":"2023-05-17T20:48:45.978Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"numbers-cheatsheet","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1AkaPcJlWoSpqixtqeKcD1/9505980d855c36120b4818980745fd00/Screenshot_2023-05-17_at_1.46.09_PM.png","details":{"size":550573,"image":{"width":2194,"height":1734}},"fileName":"Screenshot 2023-05-17 at 1.46.09 PM.png","contentType":"image/png"}}},"isRetina2x":false}}},"content":[]},{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Next Steps","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Review the up-to-date metrics referenced in this blog in the following ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/llm-numbers"},"content":[{"nodeType":"text","value":"Github repo","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". \n\nSee our earlier ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"nodeType":"text","value":"blog series on solving Generative AI infrastructure","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" ","marks":[{"type":"underline"}],"data":{}},{"nodeType":"text","value":"and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"nodeType":"text","value":"using LangChain with Ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in learning more about Ray, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io/"},"content":[{"nodeType":"text","value":"Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://docs.ray.io/"},"content":[{"nodeType":"text","value":"Docs.Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nTo connect with the Ray community join #LLM on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"Ray Slack","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" or our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"Discuss forum","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://www.anyscale.com/platform"},"content":[{"nodeType":"text","value":"Anyscale.com/Platform ","marks":[],"data":{}}]},{"nodeType":"text","value":"and click the 'Try it now' button","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Summit 2023:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"nodeType":"text","value":"Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"nodeType":"text","value":"practical training focused on LLMs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"hr","data":{},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[1] Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2022-05-14.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[2] Retrieved from ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://openai.com/pricing"},"content":[{"nodeType":"text","value":"http://openai.com/pricing","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on 2022-05-14.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[3] ","marks":[],"data":{}},{"nodeType":"text","value":"GPT-4","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). ","marks":[],"data":{}},{"nodeType":"text","value":"GPT-3.5 Turbo","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": 0.2c/1k tokens.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[4] This assumes the vector lookup is âfree.â Itâs not, but it uses CPUs (much cheaper) and is fairly fast.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[5] 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3ZnD84ZZAfH60U5ncBebaO","type":"Asset","createdAt":"2023-05-17T15:56:36.431Z","updatedAt":"2023-05-17T18:04:30.622Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"numbers-cover-image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3ZnD84ZZAfH60U5ncBebaO/227f68207ec7731e789f342e7ec320e8/Screenshot_2023-05-17_at_10.12.01_AM.png","details":{"size":445922,"image":{"width":2348,"height":1616}},"fileName":"Screenshot 2023-05-17 at 10.12.01 AM.png","contentType":"image/png"}}},"mainImageFit":"contain","hideIntro":true,"showMainImage":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3vUo7K4aHmhARGamZX7Ujj","type":"Entry","createdAt":"2023-05-11T15:17:24.618Z","updatedAt":"2023-06-16T21:13:49.520Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Streaming distributed execution across CPUs and GPUs","seoTitle":"Streaming distributed execution across CPUs and GPUs","slug":"streaming-distributed-execution-across-cpus-and-gpus","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2aJRcdEj9iAktAnqn0wucE","type":"Entry","createdAt":"2021-02-16T07:30:38.421Z","updatedAt":"2021-02-16T07:30:38.421Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Stephanie Wang","slug":"stephanie-wang","link":"https://github.com/stephanie-wang"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2edothjNBpyQEu2aX55KRl","type":"Entry","createdAt":"2023-05-10T15:30:16.699Z","updatedAt":"2023-05-10T15:30:16.699Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Cheng Su","slug":"cheng-su"}}],"publishedDate":"2023-05-11","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"In a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/offline-batch-inference-comparing-ray-apache-spark-and-sagemaker"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"previous blog,","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" we showed how Ray Data can provide speedups over SageMaker and frameworks like Apache Spark for large-scale batch inference workloads. This blog post delves further into how Ray Data streaming works and how to use it for your own ML pipelines.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Some of the most demanding machine learning (ML) use cases we have encountered involve pipelines that span both CPU and GPU devices in distributed environments. These situations arise in various workloads, including:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Batch inference, which involves a CPU-intensive preprocessing stage (e.g., video decoding or image resizing) before utilizing a GPU-intensive model to make predictions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed training, where similar CPU-heavy transformations are required to prepare or augment the dataset prior to GPU training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Interestingly, in many of these workloads, the preprocessing steps often prove to be the bottleneck. This can happen when preprocessing requires ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"parallelism across multiple nodes","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"significant memory to buffer results","nodeType":"text"},{"data":{},"marks":[],"value":" between the CPU and GPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For instance, consider the decoding of compressed images in memory. A typical JPEG decompression ratio is 10:1, which implies that the output memory size may be ten times that of the input, resulting in substantial memory pressure in addition to CPU load. The challenges become more complex with other data modalities, such as video; for example, H264 can decompress at a 2000:1 ratio, producing 200GB of frame outputs for a 100MB file input. This means that practitioners trying to offload CPU-heavy preprocessing onto multiple CPUs or machines have to deal with intermediate results that are far larger than their already sizable source datasets:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7ntuAY4zsUXtrtIxSofnyg","type":"Asset","createdAt":"2023-05-10T15:33:44.406Z","updatedAt":"2023-05-10T18:55:03.562Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"streaming_figure_1","description":"Figure 1. Decoding of compressed images with large images spilled to disk","file":{"url":"//images.ctfassets.net/xjan103pcp94/7ntuAY4zsUXtrtIxSofnyg/1fdb9631162d6b66777d205d1d3a7e1c/Screenshot_from_2023-05-10_11-00-43.png","details":{"size":64895,"image":{"width":1493,"height":436}},"fileName":"Screenshot from 2023-05-10 11-00-43.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Executed naively, these compute and memory-intensive workloads fail to fully utilize expensive GPU hardware. For example, we could buffer 200GB of frame outputs on disk, but that could add minutes of overheads. Even if this is overlapped with GPU computation, we can end up with long pauses on the GPU. This motivates fully streamed execution in the cluster setting, which avoids such delays by streaming intermediate data through cluster memory:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"40VpN8OKxEM7wHohV802gE","type":"Asset","createdAt":"2023-05-10T15:36:35.326Z","updatedAt":"2023-05-10T18:56:06.767Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_2","description":"Figure 2. Decoding intermediate data streamed through memory","file":{"url":"//images.ctfassets.net/xjan103pcp94/40VpN8OKxEM7wHohV802gE/c07086bfe677dc5e3109496f5a4ec083/Screenshot_from_2023-05-10_11-01-00.png","details":{"size":35974,"image":{"width":1199,"height":268}},"fileName":"Screenshot from 2023-05-10 11-01-00.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we guide you through our development of a versatile streaming backend for Ray Data, and show examples of how to implement the demanding use cases mentioned earlier. We discuss the performance benefits of implementing pipelined execution across CPUs and GPUs compared to bulk processing, and discuss its broader applicability to various workloads, including distributed training.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pipelined execution in a cluster","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To understand the necessity and advantages of pipelined (or streaming) execution across CPU and GPU devices, let's first examine how a ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Bulk_synchronous_parallel"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"bulk synchronous parallel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (BSP) framework might handle batch inference. Because of its simplicity and generality, BSP is a common way frameworks (e.g., MapReduce, Apache Spark) parallelize distributed computations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Note that pipelining within a single machine is already commonly used in data loading libraries such as ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Torch DataLoader","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://petastorm.readthedocs.io/en/latest/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Petastorm.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" Here, we discuss pipelining computations across an entire cluster of machines.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical batch inference job consists of the following operations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6mkS4HlXNovPW6Dl387jYe","type":"Asset","createdAt":"2023-05-10T15:40:10.724Z","updatedAt":"2023-05-10T19:28:57.091Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"locale":"en-US"},"fields":{"title":"streaming_figure_3","description":"Figure 3. A typical batch inference job sequential operations","file":{"url":"//images.ctfassets.net/xjan103pcp94/6mkS4HlXNovPW6Dl387jYe/1a707558e51ac00b776ae6f5d808b74e/Screenshot_from_2023-05-10_12-23-58.png","details":{"size":33844,"image":{"width":1526,"height":244}},"fileName":"Screenshot from 2023-05-10 12-23-58.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To implement such an inference job, you'd write the code using Ray Data (or another similar framework):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1kYVOacE6epR4czS5YoOHX","type":"Entry","createdAt":"2023-05-10T15:41:24.239Z","updatedAt":"2023-05-10T15:41:24.239Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_1","body":"import ray\n\n# Define model and preprocessor.\nmodel = ...\npreprocess_fn = ...\n\n# Define the inference function.\ndef model_fn(batch: Dict[str, np.ndarray]):\n    return {\"results\": model(batch)}\n\n# The following snippet implements the pipeline.\nray.data.read_parquet(...) \\           # 1. Load\n    .map_batches(preprocess_fn) \\      # 2. Preprocess\n    .map_batches(model_fn) \\           # 3. Inference\n    .write_parquet(...)                # 4. Save\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a BSP framework, each of these operations can be modeled as a 1:1 transformation, which can be fused together into a single stage. Each task within the stage will execute the mentioned operations locally in sequence:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4oQ40RlSK1AF7Y4ZnVGV8J","type":"Asset","createdAt":"2023-05-10T15:44:10.631Z","updatedAt":"2023-05-10T18:59:16.654Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_4","description":"Figure 4. BSP framework operations modeled as a 1:1 transformation","file":{"url":"//images.ctfassets.net/xjan103pcp94/4oQ40RlSK1AF7Y4ZnVGV8J/a0413e90a55ec69d7d0ba49554a7f2a0/Screenshot_from_2023-05-10_11-07-39.png","details":{"size":103326,"image":{"width":1511,"height":583}},"fileName":"Screenshot from 2023-05-10 11-07-39.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This execution strategy is ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"memory optimal","nodeType":"text"},{"data":{},"marks":[],"value":", as there is no intermediate data and execution is inherently pipelined (since there is only a single stage). However, challenges emerge in a heterogeneous setting, where certain pipeline steps may prefer to run on CPUs or GPUs independently. In Ray Data, such inference pipelines are expressed with transformations that launch actors scheduled onto GPUs ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"(num_gpus=1 and compute=ActorPoolStrategy)","nodeType":"text"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3nLf8c4wMq9KInsI4ca7HB","type":"Entry","createdAt":"2023-05-10T15:46:05.506Z","updatedAt":"2023-05-10T16:00:09.475Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_2","body":"import ray\nfrom ray.data import ActorPoolStrategy\n\nModel = ...\npreprocess_fn = ...\n\n# Define the model as a stateful class with cached setup.\nclass MyModelCallableCls:\n    def __init__(self):\n        self.model = Model(...)\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e\n        return {\"results\": self.model(batch)}\n\n# Modify the pipeline to use GPU actors.\nray.data.read_parquet(...) \\\n    .map_batches(preprocess_fn) \\\n    .map_batches(\n          MyModelCallableCls,\n          num_gpus=1,\n          compute=ActorPoolStrategy(size=N)) \\\n    .write_parquet(...)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the above code snippet, the load and preprocessing steps run on CPU (Stage 1), inference runs on GPUs (Stage 2), and then the result saving runs on CPU again (Stage 3). This configuration leads to spilling of data to remote storage when intermediate results (e.g., decoded video frames) exceed cluster memory sizes. Hence, we see that ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"BSP is not memory optimal for heterogeneous workloads","nodeType":"text"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"66FIu8nzl2kZRB1Ht5xZFL","type":"Asset","createdAt":"2023-05-10T15:49:26.155Z","updatedAt":"2023-05-10T19:00:24.815Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_5","description":"Figure 5. BSP is not memory optimal for heterogeneous workloads","file":{"url":"//images.ctfassets.net/xjan103pcp94/66FIu8nzl2kZRB1Ht5xZFL/295ab9176fab5d04ee47ac53c5e1b418/Screenshot_from_2023-05-10_11-02-18.png","details":{"size":162611,"image":{"width":1750,"height":730}},"fileName":"Screenshot from 2023-05-10 11-02-18.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"These unnecessary overheads can be avoided with end-to-end pipelining (i.e., streaming) across the cluster, which we present in the next section.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data streaming","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In Ray 2.4, we've changed the default execution strategy of Ray Data to streaming from bulk synchronous. These streaming Datasets are fully backwards compatible with the existing API, i.e., they can be transformed lazily with map operations, support shuffle operations, and also caching / materialization in memory:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5CSJWRWgN6pQwxcFMBhMSr","type":"Entry","createdAt":"2023-05-10T15:50:53.654Z","updatedAt":"2023-05-10T15:59:44.870Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_3","body":"# Create a dataset over parquet files\nds: ray.data.Dataset = ray.data.read_parquet(...)\n\n# Transform the dataset\nds = ds.map_batches(my_preprocess_fn)\nds = ds.map_batches(my_model_fn)\n\n# Iterate over dataset batches in streaming fashion\nfor batch in ds.iter_batches():\n   print(batch)\n\n# Materialize all contents of the dataset in memory\nds = ds.materialize()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In other words, the Ray Data API now leverages streaming execution for improved performance on large datasets, with the same simple transformation API as in previous Ray versions.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Video processing example","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To more fully understand the capabilities of this API, let's walk through what this looks like in an example video processing pipeline. In the pipeline, we'll first decode video frames, annotate each frame with an ML model, apply a classification model to the annotated frames, and then save the results to JSON:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"s1kmF3kQraHomGuDsvOQt","type":"Asset","createdAt":"2023-05-10T15:55:25.682Z","updatedAt":"2023-05-10T19:30:26.377Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"streaming_figure_6","description":"Figure 6. A pipeline decoding video frames, annotating frames, applying classification, and saving results to JSON.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/s1kmF3kQraHomGuDsvOQt/9385b71522afe76eebc6342f54387ffa/Screenshot_from_2023-05-10_12-24-36.png","details":{"size":38452,"image":{"width":1478,"height":230}},"fileName":"Screenshot from 2023-05-10 12-24-36.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's start by declaring the components of our pipeline. For brevity, we'll just provide stub classes here, but you can find the ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/ericl/ac086f0a9bdf41b6101dca665045dfec"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"full skeleton here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (and the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference docs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") that can be adapted to your own workloads. Data between these steps is kept in dicts of numpy arrays:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3DoR7DQ01JPe4k0MjPdubR","type":"Entry","createdAt":"2023-05-10T15:57:40.210Z","updatedAt":"2023-05-10T15:59:24.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_5","body":"# Declare a function that decodes frames.\ndef decode_frames(batch: Dict[str, np.ndarray]) -\u003e Dict:\n    video_data = batch[\"bytes\"]\n    ...\n    return {\"frames\": decoded_frames}\n\n# Declare a model that annotates decoded frame data.\nclass FrameAnnotator:\n    def __init__(self):\n        self.model = ...\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e Dict:\n        frames = batch[\"frames\"]\n        return {\"annotated_frames\": self.model(frames)}\n\n# Declare a model that classifies annotated frame data.\nclass FrameClassifier:\n    def __init__(self):\n        self.model = ...\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e Dict:\n        frames = batch[\"annotated_frames\"]\n        return {\"results\": self.model(frames)}","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next, we'll use Ray Data to connect these steps together. This takes just a few lines of code, and we can customize the resource requirements per step to configure CPUs and GPUs:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1U2wSU4PEADDsQpVQqJJAa","type":"Entry","createdAt":"2023-05-10T15:59:06.340Z","updatedAt":"2023-05-10T15:59:06.340Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_6","body":"# Create a dataset from video binary data.\nds = ray.data.read_binary(...)\n\n# Apply the decode step. We can customize the resources per\n# task. Here each decode task requests 4 CPUs from Ray.\nds = ds.map_batches(decode_frames, num_cpus=4)\n\n# Apply the annotation step, using an actor pool of size 5.\n# Each actor runs on a CPU.\nds = ds.map_batches(\n    FrameAnnotator,\n    compute=ActorPoolStrategy(size=5))\n\n# Apply the classification step, using a pool of 2 GPU actors,\n# and a fixed data batch size of 64 for the actor calls.\nds = ds.map_batches(\n    FrameClassifier,\n    num_gpus=1,\n    batch_size=64,\n    compute=ActorPoolStrategy(size=2))\n\n# Trigger execution and write outputs to json.\nds.write_json(\"/tmp/output\")\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The configured streaming topology from above looks like this logically. Since each step has a unique resource requirement / task vs actor strategy, here every step becomes its own stage (Ray Data fuses steps with equivalent resource requirements):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2XaeSiV04QKLILBLAlD0t5","type":"Asset","createdAt":"2023-05-11T15:44:03.608Z","updatedAt":"2023-05-11T15:44:03.608Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"streaming_figure_7","description":"Figure 7: Streaming execution across CPUs and GPUs","file":{"url":"//images.ctfassets.net/xjan103pcp94/2XaeSiV04QKLILBLAlD0t5/d75fceb54323c7587312028b40a4b37c/Screenshot_from_2023-05-10_11-02-51.png","details":{"size":68124,"image":{"width":1445,"height":299}},"fileName":"Screenshot from 2023-05-10 11-02-51.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We run the above script (","nodeType":"text"},{"data":{"uri":"https://gist.github.com/ericl/ac086f0a9bdf41b6101dca665045dfec"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GitHub gist","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") in an Anyscale workspace with 2 GPU nodes and 5 CPU nodes (you can use the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/getting-started.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray cluster launcher","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to also launch a cluster for this):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3hsRFIiW2RGkFM5Igr2jVT","type":"Entry","createdAt":"2023-05-10T16:04:25.585Z","updatedAt":"2023-05-10T16:06:19.918Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_7","body":"$ python workload.py\n\n2023-05-02 15:10:01,105 INFO streaming_executor.py:91 -- Executing DAG\nInputDataBuffer[Input] -\u003e TaskPoolMapOperator[MapBatches(decode_frames)] -\u003e \nActorPoolMapOperator[MapBatches(FrameAnnotator)] -\u003e\nActorPoolMapOperator[MapBatches(FrameClassifier)] -\u003e \nTaskPoolMapOperator[Write]\n2023-05-02 15:10:01,128 INFO actor_pool_map_operator.py:114 -- MapBatches(FrameAnnotator): Waiting for 5 pool actors to start...\n2023-05-02 15:10:02,691 INFO actor_pool_map_operator.py:114 -- MapBatches(FrameClassifier): Waiting for 2 pool actors to start...\n\nRunning: 25.0/112.0 CPU, 2.0/2.0 GPU, 33.19 GiB/32.27 GiB object_store_memory:  28%|ââââ        | 285/1000 [01:40\u003c03:12,  3.71it/s]","language":"Shell"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"While the workload is running, we can view the observability graphs in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-core/ray-dashboard.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Dashboard","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". We can see from the actor metrics that our 7 worker actors were active for the entire run (RUNNING_TASK), and that Ray Data kept them busy with MapBatches(FrameClassifier) as well as MapBatches(FrameAnnotator) tasks. Because of the streaming execution strategy, tasks for all stages run concurrently:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5MDgPW8470uVoy6MFBqowq","type":"Asset","createdAt":"2023-05-10T16:09:18.740Z","updatedAt":"2023-05-10T16:09:27.512Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_8","description":"Figure 8. View active tasks and actors for this job in the Ray Dashboard","file":{"url":"//images.ctfassets.net/xjan103pcp94/5MDgPW8470uVoy6MFBqowq/39f6300387d895145313c0caee8149cf/image3.png","details":{"size":77415,"image":{"width":1216,"height":516}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can also inspect hardware utilization in the Ray dashboard. We'll focus on the network activity. We can see two nodes receiving ~500MiB/s (these are the two GPU nodes hosting the FrameClassifier actors), and a number of other nodes driving significant traffic from 100-200MiB/s, likely sending data to the GPU nodes or between the decode and FrameAnnotator steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6QEOgHncDDrSl3SKl9YzWF","type":"Asset","createdAt":"2023-05-10T16:12:30.030Z","updatedAt":"2023-05-10T19:54:24.559Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"streaming_figure_9","description":"Figure 9. Ray Dashboard showing network activity","file":{"url":"//images.ctfassets.net/xjan103pcp94/6QEOgHncDDrSl3SKl9YzWF/bae6e33fce2749f2ade8afd73681e16b/Screenshot_from_2023-05-10_12-42-27.png","details":{"size":107840,"image":{"width":1635,"height":707}},"fileName":"Screenshot from 2023-05-10 12-42-27.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The overall pipeline finishes in ~5 minutes, successfully processing ~350GB of video frame data total on a heterogeneous cluster of CPUs and GPUs. We see that Ray Data was able to pipeline the entire execution across the cluster to make the best use of memory.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Discussion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Optimizations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"In order to seamlessly execute such streaming topologies, Ray Data provides a number of optimizations under the hood. This includes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Memory stability","nodeType":"text"},{"data":{},"marks":[],"value":": Ray Data relies on the underlying Ray scheduler to schedule tasks and actors, but still needs to manage back-pressure across the streaming topology to bound memory usage and avoid object store spilling. It does this by only scheduling new tasks if it would keep the streaming execution under ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/data-internals.html#streaming-execution"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"configured resources limits","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Intuitively, enforcing a cap on intermediate result memory usage is needed to avoid degrading to bulk execution.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data locality","nodeType":"text"},{"data":{},"marks":[],"value":": While Ray will already place tasks on nodes where their input arguments are local, Ray Data's streaming backend extends this to optimize the scheduling of actor tasks. For example, in the above example, a lot of network traffic is avoided between the `decode_frames` and `FrameAnnotator` steps by routing decoded frames to actors that are on the same node.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Fault tolerance","nodeType":"text"},{"data":{},"marks":[],"value":": Ray Data leverages Ray's built-in ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-core/fault-tolerance.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"fault tolerance","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in order to handle object loss in large jobs. When objects are lost, they are recomputed based on their task lineage tracked by Ray. If actors were needed to produce these objects, Ray restarts these actors prior to re-submitting the task.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scalability","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"To give an idea of the upper scalability envelope of Ray Data streaming, we ran a number of synthetic benchmarks that stressed Ray's object manager and Ray Data's ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/execution/streaming_executor.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"streaming executor","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" processing a 20TiB array dataset. Under the hood, Ray Data is using ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.remote() / ray.wait() ","nodeType":"text"},{"data":{},"marks":[],"value":"to orchestrate the streaming topology, which means it is similarly scalable as Ray itself:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2vKUP0VZYl0yPNspQmfU05","type":"Asset","createdAt":"2023-05-10T16:17:37.701Z","updatedAt":"2023-05-10T16:17:37.701Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"streaming_figure_10","description":"Figure 10. Synthetic benchmarks stressing Ray's object manager and Ray Data's streaming executor processing a 20TiB array dataset","file":{"url":"//images.ctfassets.net/xjan103pcp94/2vKUP0VZYl0yPNspQmfU05/0566c92beb5ea743d9ecc3404a551705/image9.png","details":{"size":44857,"image":{"width":1636,"height":640}},"fileName":"image9.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"These benchmarks were run on a cluster of 500 machines, and test pipelines with between one to four stages similar to the video streaming example. We see that for the simplest pipeline on this cluster, Ray Data can process input data at a rate exceeding 1TiB/s. For more complex multi-stage pipelines, Ray Data can still sustain about 100-200GiB/s of end-to-end throughput on a large cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed Training","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"A significant portion of ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" users today are also using Ray Data as a distributed data backend for ML training. Ray Data streaming allows these users to efficiently work with datasets that are larger than cluster memory for training, leveraging both CPU and GPU nodes to speed up their jobs. We are working on enhancing Ray Train's API to natively work with Ray Data streaming.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What's next?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Today, early users are taking advantage of the Ray Data streaming backend to create efficient large-scale inference pipelines over unstructured data, including video and audio data. To learn more about how to build your own pipeline, check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference tutorial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in our docs. We'll also be following up with a blog on more unstructured application examples. To learn more generally about streaming in Ray Data, check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/data.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"library docs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and let us know what ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/issues"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"improvements","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" you'd like to see to Ray Data.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Also, we will be presenting at the ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Register now","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â â early bird registration is open until","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Â June 30, 2023.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5qTVayH4YnKM7pGL8CiSwc","type":"Asset","createdAt":"2023-05-11T15:46:46.691Z","updatedAt":"2023-05-11T15:46:46.691Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_streaming_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5qTVayH4YnKM7pGL8CiSwc/99806986301b595308c6c89a8bad6249/images_and_diagrams__13_.png","details":{"size":83586,"image":{"width":960,"height":540}},"fileName":"images and diagrams (13).png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1n9cmtmuJ3wQPGW1TtXZ4t","type":"Entry","createdAt":"2023-05-08T15:57:32.674Z","updatedAt":"2023-05-09T00:38:09.625Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Building a Self Hosted Question Answering Service using LangChain + Ray in 20 minutes","slug":"building-a-self-hosted-question-answering-service-using-langchain-ray","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-05-08","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 3 of a blog series. In this blog, weâll show you how to build an LLM question and answering service. In future parts, we will optimize the code and measure performance: cost, latency and throughput.","body":{"nodeType":"document","data":{},"content":[{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4laoMTyctlD4QnuM9KzTI6","type":"Entry","createdAt":"2023-05-08T20:47:41.452Z","updatedAt":"2023-05-08T20:47:41.452Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Building a Question Answering Chatbot","videoUrl":"https://youtu.be/Sy-Xp-sdlh0"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\nThis blog post builds on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"nodeType":"text","value":"Part 1","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" of our LangChain series, where we built a semantic search service in about 100 lines. Still, search is so â¦ 2022. What if we wanted to build a question answering service?Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"One option is we could just ask the LLM directly without any background at all. Unfortunately one of the biggest problems with LLMs is not just ignorance (âI donât knowâ) but hallucination (âI think I know but I actually donât ","marks":[],"data":{}},{"nodeType":"text","value":"at all","marks":[{"type":"italic"}],"data":{}},{"nodeType":"text","value":".â) This is perhaps the biggest issue facing LLMs at the current time. The way we overcome that is by combining factual information from our search engine and the capabilities of an LLM together.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Again, as we demonstrated before, there is a powerful combination in Ray + LangChain. LangChain provides a chain that is well suited to this (Retrieval QA). To give a fuller picture of how the pieces come together, weâre going to implement some parts that could usually just as easily be wrapped.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The Question Answering Service we will build will query the results from our Search Engine, and then use an LLM to summarize the results of the search.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Previously we had shown this diagram for how to serve semantic search results: ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1yspRkXhEt1xIRdiuztbyh","type":"Asset","createdAt":"2023-05-08T15:43:10.951Z","updatedAt":"2023-05-08T15:43:10.951Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"qna-save-search-queries","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1yspRkXhEt1xIRdiuztbyh/f5a50d1046b085b95cd18742e51d5393/qna-save-search-queries.png","details":{"size":243832,"image":{"width":1600,"height":794}},"fileName":"qna-save-search-queries.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We are going to augment that now by creating a chain that consists of the above stage, then generating a prompt, and feeding that to an LLM to generate the answer.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Hence, the resulting system we are trying to build looks like this:Â ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mD1pRX9Do1SDRCUor1Yex","type":"Asset","createdAt":"2023-05-08T15:44:48.379Z","updatedAt":"2023-05-08T15:44:48.379Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"qna-answering-questions-with","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2mD1pRX9Do1SDRCUor1Yex/d4041f6d0d296c73edd77de26e076a17/qna-answering-questions-with.png","details":{"size":170666,"image":{"width":1556,"height":1024}},"fileName":"qna-answering-questions-with.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In other words, we take the vector search results, we take a prompt template, generate the prompt and then pass that to the LLM. Today we will use ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/Stability-AI/StableLM"},"content":[{"nodeType":"text","value":"StableLM","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" but you can easily swap in whatever model you want to.Â \n\nBefore we get started, itâs worth noting that you can find the source code to this project in our LangChain Ray examples repo at: ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/langchain-ray"},"content":[{"nodeType":"text","value":"https://github.com/ray-project/langchain-ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 1: The Prompt Template","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The prompt template is derived from the suggested one from StableLM, but modified for our use case.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In serve.py, we declare the following template:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2MvraLhg4SD47Hq41r5Jd7","type":"Entry","createdAt":"2023-05-08T15:46:38.818Z","updatedAt":"2023-05-08T15:46:38.818Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-1","body":"TEMPLATE = \"\"\"\n\u003c|SYSTEM|\u003e# StableLM Tuned (Alpha version)\n- You are a helpful, polite, fact-based agent for answering questions about Ray. \n- Your answers include enough detail for someone to follow through on your suggestions. \n\u003c|USER|\u003e\n\nPlease answer the following question using the context provided. If you don't know the answer, just say that you don't know. Base your answer on the context below. Say \"I don't know\" if the answer does not appear to be in the context below. \n\nQUESTION: {question} \nCONTEXT: \n{context}\n\nANSWER: \u003c|ASSISTANT|\u003e\n\"\"\"","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Letâs go through the template. The first part is the â\u003c|SYSTEM|\u003eâ tag. You can think of this as setting the âpersonalityâ of the LLM. LLMs are trained to treat the system tag differently. Not all LLMs support this, but OpenAI and StableLM do.Â  The second part is the â\u003c|USER|\u003eâ tag which is the question we want to ask. Note that the question and context are âtemplatizedâ â we will provide them from another source. Finally, since LLMs generate outputs by continuing on from the input, we say to the LLM âOK, hereâs where you take over.âÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The question will come from the userâs query. The context will use what we built last time: the search results from our semantic search.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 2: Setting up the embeddings and the LLM","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now letâs have a look at the __init__ method for our deployment. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3A4WRFfgvE3dNjwHACGYRB","type":"Entry","createdAt":"2023-05-08T15:47:56.437Z","updatedAt":"2023-05-08T15:47:56.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-2","body":"def __init__(self):\n       #... the same code from Part 1 .. \n       self.llm = StableLMPipeline.from_model_id(model_id=\"stabilityai/stablelm-tuned-alpha-7b\",\n                                                    task=\"text-generation\", model_kwargs=\n                                                    {\"device_map\":\"auto\", \"torch_dtype\": torch.float16})\n       WandbTracer.init({\"project\": \"wandb_prompts_2\"})\n       self.chain = load_qa_chain(\n           llm=self.llm,\n           chain_type=\"stuff\",\n           prompt=PROMPT)","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"As you can see, weâve added just 3 lines.Â \n\nThe first line creates a new StableLM LLM. In this case we had to write a little bit of glue code because we wanted to specify using ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://en.wikipedia.org/wiki/Half-precision_floating-point_format"},"content":[{"nodeType":"text","value":"float16","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" (halving the memory consumption of the model). We are working with the authors of Langchain to make this unnecessary. The key line from that file is this one: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"lthNpTGxWTEV31PX65mYb","type":"Entry","createdAt":"2023-05-08T15:50:17.006Z","updatedAt":"2023-05-08T15:50:17.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-3","body":" response = self.pipeline(prompt, temperature=0.1, max_new_tokens=256, do_sample=True)\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Here we specify the maximum number of tokens, and that we want it to pretty much answer the question the same way every time, and that we want to do one word at a time.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The second line sets up our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.wandb.ai/ref/python/integrations/wandbtracer"},"content":[{"nodeType":"text","value":"tracing with Weights and Biases","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". This is completely optional, but will allow us to visualize the input. You can find out more about Weights and Biases ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://wandb.ai"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The third thing we do is create a new chain that is specifically designed for answering questions. We specify the LLM to use, the prompt to use and finally the âchain typeâ â for now we set this to âstuffâ but there are other options like âmap_reduceâ, and also pass in the prompt.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 3: Respond to questions","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Weâve now got our Langchain ready, now all we have to do is write the code that uses the chain to answer questions!Â ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4Z72OU7xq4tPFzV8zLks6","type":"Entry","createdAt":"2023-05-08T15:51:41.475Z","updatedAt":"2023-05-08T15:51:41.475Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-4","body":"   def answer_question(self, query):\n       search_results = self.db.similarity_search(query)\n       print(f'Results from db are: {search_results}')\n       result = self.chain({\"input_documents\": search_results, \"question\":query})\n\n       print(f'Result is: {result}')\n       return result[\"output_text\"]","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Youâll notice the first line is identical to our previous version. Now we execute the chain with both our search results and the question being fed into the template.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 4: Go!","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now letâs get this started. If youâre using Weights and Biases, donât forget to log in using wandb login. To start, letâs do serve run serve:deployment.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now itâs started, letâs use a simple query script to test it.","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4XXeXIabNrMpB1kEND5KBK","type":"Entry","createdAt":"2023-05-08T15:53:45.904Z","updatedAt":"2023-05-08T15:53:45.904Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-5","body":"$ python query.py 'What are placement groups?'\nPlacement groups are a way for users to group resources together and schedule tasks or actors on those resources. They allow users to reserve resources across multiple nodes and can be used for gang-scheduling actors or for spreading resources apart. Placement groups are represented by a list of bundles, which are used to group resources together and schedule tasks or actors. After the placement group is created, tasks or actors can be scheduled according to the placement group and even on individual bundles.\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Yay! It works (well, mostly, that part at the end is a bit weird)! Letâs also check that it doesnât make stuff up: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12iqa2l4xNWuK18GLTu9Ly","type":"Entry","createdAt":"2023-05-08T15:54:22.876Z","updatedAt":"2023-05-08T15:54:22.876Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-6","body":"$ python query.py 'How do I make fried rice?'\nI don't know.\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Letâs just check the prompt that was sent to StableLM. This is where Weights and Biases comes in. Pulling up our interface we can find the prompt that was sent:Â ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"64oHWc7Rnz0fgCIBOR8gEF","type":"Entry","createdAt":"2023-05-08T15:55:06.135Z","updatedAt":"2023-05-08T15:55:06.135Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-7","body":"\u003c|SYSTEM|\u003e# StableLM Tuned (Alpha version)\n- You are a helpful, polite, fact-based agent for answering questions about Ray. \n- Your answers include enough detail for someone to follow through on your suggestions. \n\u003c|USER|\u003e\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\n\nPlease answer the following question using the context provided. \nQUESTION: What are placement groups? \nCONTEXT: \nPlacement Groups#\nPlacement groups allow users to atomically reserve groups of resources across multiple nodes (i.e., gang scheduling).\nThey can be then used to schedule Ray tasks and actors packed as close as possible for locality (PACK), or spread apart\n(SPREAD). Placement groups are generally used for gang-scheduling actors, but also support tasks.\nHere are some real-world use cases:\n\nray list placement-groups provides the metadata and the scheduling state of the placement group.\nray list placement-groups --detail provides statistics and scheduling state in a greater detail.\nNote\nState API is only available when you install Ray is with pip install \"ray[default]\"\nInspect Placement Group Scheduling State#\nWith the above tools, you can see the state of the placement group. The definition of states are specified in the following files:\nHigh level state\nDetails\n\nPlacement groups are represented by a list of bundles. For example, {\"CPU\": 1} * 4 means youâd like to reserve 4 bundles of 1 CPU (i.e., it reserves 4 CPUs).\nBundles are then placed according to the placement strategies across nodes on the cluster.\nAfter the placement group is created, tasks or actors can be then scheduled according to the placement group and even on individual bundles.\nCreate a Placement Group (Reserve Resources)#\n\nSee the User Guide for Objects.\nPlacement Groups#\nPlacement groups allow users to atomically reserve groups of resources across multiple nodes (i.e., gang scheduling). They can be then used to schedule Ray tasks and actors packed as close as possible for locality (PACK), or spread apart (SPREAD). Placement groups are generally used for gang-scheduling actors, but also support tasks.\nSee the User Guide for Placement Groups.\nEnvironment Dependencies#\n\nANSWER: \u003c|ASSISTANT|\u003e\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We can see the search results that we made in are being included. StableLM is then using this to synthesize its answer.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Conclusion","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this blog post we showed how we could build on the simple search engine we built in the previous blog in this series and make a retrieval-based question answering service. It didnât need us to do much: we needed to bring up a new LLM (StableLM),Â  we needed to generate a prompt with the search results in it, and then feed that result to the LLM asking it to derive an answer from it. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Next Steps","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Review the code and data used in this blog in the following ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/langchain-ray/tree/main/open_source_LLM_retrieval_qa"},"content":[{"nodeType":"text","value":"Github repo","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". \n\nIf you are interested in learning more about Ray, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io/"},"content":[{"nodeType":"text","value":"Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://docs.ray.io/"},"content":[{"nodeType":"text","value":"Docs.Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\n","marks":[],"data":{}},{"nodeType":"text","value":"See our earlierÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"nodeType":"text","value":"blog series on solving Generative AI infrastructure","marks":[],"data":{}}]},{"nodeType":"text","value":"Â with Ray.","marks":[],"data":{}},{"nodeType":"text","value":"\n\nTo connect with the Ray community join #LLM on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"Ray Slack","marks":[],"data":{}}]},{"nodeType":"text","value":" or our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"Discuss forum","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://www.anyscale.com/platform"},"content":[{"nodeType":"text","value":"Anyscale.com/Platform ","marks":[],"data":{}}]},{"nodeType":"text","value":"and click the 'Try it now' button","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Summit 2023:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"nodeType":"text","value":"Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"nodeType":"text","value":"practical training focused on LLMs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mD1pRX9Do1SDRCUor1Yex","type":"Asset","createdAt":"2023-05-08T15:44:48.379Z","updatedAt":"2023-05-08T15:44:48.379Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"qna-answering-questions-with","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2mD1pRX9Do1SDRCUor1Yex/d4041f6d0d296c73edd77de26e076a17/qna-answering-questions-with.png","details":{"size":170666,"image":{"width":1556,"height":1024}},"fileName":"qna-answering-questions-with.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2C7QtMnASHfVdag9xYamXh","type":"Entry","createdAt":"2023-05-04T15:03:36.052Z","updatedAt":"2023-06-06T14:20:09.992Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker","seoTitle":"Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker","slug":"offline-batch-inference-comparing-ray-apache-spark-and-sagemaker","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DwU0wh6AmJoGv8JOy6n0Q","type":"Entry","createdAt":"2020-11-11T23:34:15.165Z","updatedAt":"2021-02-05T22:22:08.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","slug":"amog-kamsetty","link":"https://www.linkedin.com/in/amogkamsetty/","bio":"Amog is a software engineer at Anyscale. He helps maintain and develop two machine learning libraries: Ray Tune for distributed hyperparameter tuning and RaySGD for distributed deep learning training.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J134jX2ybZRLPfcAIGLTI","type":"Asset","createdAt":"2020-11-13T00:05:05.533Z","updatedAt":"2020-11-13T00:05:05.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J134jX2ybZRLPfcAIGLTI/12589bb0c6792051050ebc6e1e83af25/Amog_Kamsetty_.jpg","details":{"size":57660,"image":{"width":480,"height":640}},"fileName":"Amog Kamsetty .jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}}],"publishedDate":"2023-05-04","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Update 5/10: ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"Thanks to the Databricks Apache Spark developers for pointing out the prefetching flag ","nodeType":"text"},{"data":{},"marks":[{"type":"code"},{"type":"italic"}],"value":"spark.databricks.execution.pandasUDF.prefetch.maxBatches ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"available in Databricks Runtime that can be used with the Iterator API. It is also possible to implement this manually in Spark open source with background threads. With prefetching set to 4 batches, Spark reaches ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"},{"type":"italic"}],"value":"159.86 images/sec","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":". The code can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/blob/main/spark/code/torch-batch-inference-s3-10G-standard-iterator-databricks-prefetch.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"italic"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"TL;DR","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As more companies use large scale machine learning (ML) models for training and evaluation, offline batch inference becomes an essential workload. A number of challenges come with it: managing compute infrastructure; optimizing use of all heterogeneous resources; and transferring data from storage to hardware accelerators. Addressing these challenges, Ray performs significantly better as it can coordinate clusters of diverse resources, allowing for better utilization of the specific resource requirements of the workload.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we conduct a comparison of three different solutions for offline batch inference: AWS ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker Batch Transform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Our experiments demonstrate that Ray Data achieves speeds up to ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"17x ","nodeType":"text"},{"data":{},"marks":[],"value":"faster than SageMaker Batch Transform and","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" 2x","nodeType":"text"},{"data":{},"marks":[],"value":" faster than Spark for offline image classification. Ray Data also scales effectively to terabyte-sized datasets. The code for these benchmarks is publicly available and can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introduction","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46bNG3QTa1atN02rsemfe8","type":"Asset","createdAt":"2023-05-03T22:20:44.214Z","updatedAt":"2023-05-03T22:20:44.214Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_1_batch_offline_inference","description":"Figure 1. General parallel batch processing architecture\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/46bNG3QTa1atN02rsemfe8/120bf3aac26ae94257b41ac674f42e94/image3.png","details":{"size":48540,"image":{"width":960,"height":540}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Offline batch inference is a critical workload for many AI products, especially with the growing usage of pre-trained foundation models. At its core, it seems like a simple problem: given a trained model and a dataset, get model predictions for each data point. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, there are many challenges to doing this at scale:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 1: ","nodeType":"text"},{"data":{},"marks":[],"value":"Managing compute infrastructure and cloud clusters, especially when needing to use ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"heterogeneous clusters","nodeType":"text"},{"data":{},"marks":[],"value":", consisting of different instance types to maximize throughput.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 2: ","nodeType":"text"},{"data":{},"marks":[],"value":"Parallelizing data processing and utilizing all resources (CPUs \u0026 GPUs) in the cluster at any given point in time.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 3: ","nodeType":"text"},{"data":{},"marks":[],"value":"Efficiently transferring data between cloud storage, CPU RAM for preprocessing, and GPU RAM for model inference, especially for unstructured data like images and text. This has ramifications for both performance and developer velocity.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 4:","nodeType":"text"},{"data":{},"marks":[],"value":" A user experience that makes it easy to iterate and develop while working at scale.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What does the industry recommend for offline batch inference?","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The industry hasnât converged on a standard solution. ","nodeType":"text"},{"data":{"uri":"https://twitter.com/benhamner/status/1605730034450169857"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"It seems like every ML practitioner has their own favorite approach.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5hRMwuEGUS4nJ7eV5riIK9","type":"Asset","createdAt":"2023-05-03T22:23:14.590Z","updatedAt":"2023-05-03T22:23:14.590Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"tweet_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5hRMwuEGUS4nJ7eV5riIK9/05b76abcbe4ea58ae3839fd319858594/image4.png","details":{"size":177304,"image":{"width":1798,"height":588}},"fileName":"image4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We see that there are three categories of solutions that attempt to address the above challenges. However, only category three addresses all the challenges.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Batch Services: ","nodeType":"text"},{"data":{},"marks":[],"value":"Cloud providers such as AWS, GCP, and Azure provide batch services to manage compute infrastructure for you. Some newer products like ","nodeType":"text"},{"data":{"uri":"https://modal.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Modal Labs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provide a user experience layer around cloud providers to abstract away even more complexity. Regardless of which service you choose, the process is the same: you provide your code, and the service runs your code on each node in a cluster. However, while infrastructure management is necessary (Challenge #1), it is not enough. These services have limitations, such as a lack of software libraries to address Challenges #2, #3, and #4 which makes them suitable only for experienced users who can write their own optimized batch inference code.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Online Inference solutions: ","nodeType":"text"},{"data":{},"marks":[],"value":"Â Solutions like ","nodeType":"text"},{"data":{"uri":"https://www.bentoml.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Bento ML","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provide APIs to make it easy to write performant inference code and can abstract away infrastructure complexities. But they are designed for online inference rather than offline batch inferenceâtwo different problems with different sets of requirements. These solutions often donât perform well in the offline case, leading inference service providers like ","nodeType":"text"},{"data":{"uri":"https://modelserving.com/blog/unifying-real-time-and-batch-inference-with-bentoml-and-spark"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Bento ML to integrating with Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for offline inference.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed data systems:","nodeType":"text"},{"data":{},"marks":[],"value":" These solutions are designed to handle large amounts of data and are better suited to handle all three challenges listed above. They break large datasets into reasonable batches for processing and can map a given function (i.e., the model) over the dataset efficiently, effectively handling the challenges of offline batch inference at scale. Examples include ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ultimately, we have observed that Ray Data is the best practical solution for offline batch inference for modern deep learning applications, both in terms of performance and user experience.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We ran the following comparisons and experiments to corroborate the above observations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing AWS SageMaker Batch Transform, Apache Spark, and Ray Data performance and UX on an image classification use case","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling up Ray Data batch inference to 10 TB, showing 90%+ GPU utilization","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Image Classification: SageMaker Batch Transform vs. Apache Spark vs. Ray Data","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We ran an image classification task from the ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1911.02549"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLPerf Inference Benchmark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" suite in the offline setting. This benchmark uses images from the ","nodeType":"text"},{"data":{"uri":"https://image-net.org/challenges/LSVRC/2012/2012-downloads.php#Images"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"ImageNet 2012 Dataset","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and a pre-trained ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch ResNet50 model.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We timed a full end-to-end batch inference workload. With images stored in the parquet format in a public S3 bucket, the workload involved the following three steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Reading images from S3","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Simple CPU preprocessing (resizing, cropping, normalization)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Model inference on GPU","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"While all Ray experiments were run on Anyscale, Spark experiments were run on Databricks runtime v12.0, with Apache Spark 3.3.1. More details on the full configurations can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"in the repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In particular for Spark, we used two different setups:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Single-cluster","nodeType":"text"},{"data":{},"marks":[],"value":": Both the CPU preprocessing and GPU inference were done on the same cluster. We tried both versions of the PySpark Pandas UDF API: ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.pandas_udf.html#pyspark.sql.functions.pandas_udf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Series-based and Iterator-based","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Multi-cluster","nodeType":"text"},{"data":{},"marks":[],"value":": CPU preprocessing is done on one cluster, the preprocessed data is saved to a shared file system, and GPU inference is done on a different cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"All the source code for the experiments conducted above is available ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", including additional details on various other configurations that we attempted.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Â Results of throughput from experiments","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"When running with a 10 GB dataset on a single g4dn.12xlarge instance (48 CPUs, 4 GPUs), we observe the following throughput (images/sec) results for ResNet-50 batch inference:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Urc9cqOTvtI8d4w9DjiEm","type":"Asset","createdAt":"2023-05-04T00:09:19.405Z","updatedAt":"2023-05-04T00:09:19.405Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_5_table","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Urc9cqOTvtI8d4w9DjiEm/42718e584dcc4d0d16592749b0782b4f/Screen_Shot_2023-05-03_at_5.01.12_PM.png","details":{"size":91393,"image":{"width":1211,"height":541}},"fileName":"Screen Shot 2023-05-03 at 5.01.12 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The above throughput differences translate directly into cost savings as the batch inference job with Ray can run much fasterâso the faster it finishes the fewer resources are consumed and the cheaper it gets. Additionally, Ray is fully open source.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2dNPqjBTFiDZn2gcXkdir9","type":"Asset","createdAt":"2023-05-03T22:50:03.518Z","updatedAt":"2023-05-03T22:50:03.518Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 1","description":"Figure 1. Throughput difference among three different solutions for 10GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/2dNPqjBTFiDZn2gcXkdir9/f07a675dcb4f124e29aeb2f6618dbf15/image1.png","details":{"size":163062,"image":{"width":1334,"height":948}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"SageMaker Batch Transform for batch inference","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As we can see, SageMaker Batch Transform is not well suited for batch inference, both for throughput and cost.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While SageMaker handles","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" Challenge #1 ","nodeType":"text"},{"data":{},"marks":[],"value":"to some degree and abstracts away compute infrastructure management, it doesnât fully address the remaining challenges","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The low-throughput results are primarily because while SageMaker Batch Transform claims to be a ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"solution for batch inference on large datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", under the hood, it uses the same architecture as for online serving systems. It starts an HTTP server, and deploys your model as an endpoint. And then for each image, a separate request is sent to the server.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"SageMaker also does not provide support for batching across ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"multiple files","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and even if you batched your data beforehand, the max payload per request is 100 MB, nowhere near enough to fully saturate your GPUs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In terms of developer velocity, when running these benchmarks, we ran into a few additional challenges with SageMaker. In particular, the need to spin up a new cluster every time we ran a script and the difficult to parse stack traces slowed down developer iterations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While online inference solutions have their place, the additional overhead they have, as well as the inability to fully utilize cluster resources and saturate GPUs by maximizing batch sizes and effectively pipelining data make them highly inefficient for offline batch inference.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing Spark and Ray for batch inference","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Clearly, as we observed above, shoehorning online inference solutions for offline use cases is not the right approach. Letâs compare the two distributed data system approaches: Spark and Ray Data.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We see that Ray Data has","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" 2x ","nodeType":"text"},{"data":{},"marks":[],"value":"faster throughput than Spark. Letâs break down how Ray Data and Spark fare for the challenges mentioned earlier:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #2: Hybrid CPU and GPU workload","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deep learning workloads involve ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"both CPU and GPU computation","nodeType":"text"},{"data":{},"marks":[],"value":". For optimal parallelization, all the CPUs and GPUs in the cluster should be fully utilized at any given time. However, this is a limitation of Spark scheduling as all the stages are fused together, regardless of their resource requirements. This means that in the single cluster setup, the total parallelism is limited by the number of GPUs, leading to underutilization of CPUs. This also means that GPUs are completely idle during the reading and preprocessing steps.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"When using a separate CPU and GPU cluster, data needs to be persisted and orchestrated between stages. Overall, for these CPU+GPU workloads, Spark does not fully address ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"challenge #2","nodeType":"text"},{"data":{},"marks":[],"value":" and cannot utilize all resources in the cluster concurrently. We attempted to use Sparkâs stage level scheduling feature to address this issue, but were ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/tree/main/spark#stage-level-scheduling"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"unsuccessful","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6m5c6zQiMVCithTLEIazb8","type":"Asset","createdAt":"2023-05-03T23:02:01.263Z","updatedAt":"2023-05-03T23:02:01.263Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 2","description":"Figure 2. How Ray data efficiently pipelines and optimizes hardware: CPUs/GPUs","file":{"url":"//images.ctfassets.net/xjan103pcp94/6m5c6zQiMVCithTLEIazb8/1be9788a7aa54707a42f38023597ae2f/5mb.gif","details":{"size":4991804,"image":{"width":3260,"height":1182}},"fileName":"5mb.gif","contentType":"image/gif"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data, on the other hand, can independently scale the CPU preprocessing and GPU inference steps. It also streams data ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"through these stages","nodeType":"text"},{"data":{},"marks":[],"value":", increasing CPUs and GPUs utilization and reducing out-of-memory errors (OOMs), and fine-grained resource allocation for ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html#scaling-and-resource-allocation"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"autoscaling and management","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #3: Large, multi-dimensional tensors","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deep learning workloads often involve dealing with complex data types like images, audio, or video, which are represented by multi-dimensional ","nodeType":"text"},{"data":{"uri":"https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"arrays","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Having first-class support for these data types is necessary for better performance and developer experience. Ray Data has Numpy batch format as a first-class citizen, and does not require Pandas, which is suited for deep learning and leads to higher performance and more memory efficiency, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-2-2-improved-developer-experience-performance-and-stability"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"as shown in previous benchmarks","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #4: Ray is Python native","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another fundamental difference is that Ray is Python native whereas Spark is not, which also impacts performance. We ran a microbenchmark that applies a ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"time.sleep(1)","nodeType":"text"},{"data":{},"marks":[],"value":" UDF. Surprisingly, running the UDF takes ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/tree/main/spark#microbenchmark"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"80 seconds","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"; we believe due to JVM\u003c\u003ePyarrow overhead! On Ray Data, the same microbenchmark takes a little ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/blob/main/ray/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"under 5 seconds","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Not to mention, this makes Spark development more difficult. In particular when debugging, the recommended way is to include debug information in the ","nodeType":"text"},{"data":{"uri":"https://stackoverflow.com/a/57175768"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"output pandas dataframe rather than using print statements","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Even as we scale to a larger 300 GB dataset using 4 nodes and 16 GPUs, we see that these performance differences still hold:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5wDKGXZNHB2Hlt6JEMMKOt","type":"Asset","createdAt":"2023-05-03T23:04:40.956Z","updatedAt":"2023-05-03T23:04:40.956Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_3_batch_inference","description":"Figure 3. Throughput difference between Spark and Ray solution for 300 GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/5wDKGXZNHB2Hlt6JEMMKOt/da27ee6ea7e5773abef42eb291dd1a97/image5.png","details":{"size":92846,"image":{"width":1102,"height":830}},"fileName":"image5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"While big data processing systems are the best solutions for offline inference, for deep learning workloads, Ray Data outperforms Spark.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling up Ray Data and maximizing performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we test the scalability of Ray Data and attempt to maximize performance by using a ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"heterogeneous cluster","nodeType":"text"},{"data":{},"marks":[],"value":" consisting of a mix of GPU nodes and CPU-only nodes. Deep learning workloads involving complex data types are often memory-bound, so by using additional CPU-only nodes, we add more total memory to our cluster to better maximize GPU utilization.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlike SageMaker or Spark, Ray works natively with heterogeneous clusters consisting of different instance types, fully addressing ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"challenge #1.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We run the same benchmark, except with 10 TB worth of images, using a single cluster of 10 g4dn.12xlarge instances and 10 m5.16xlarge instances.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Throughput: 11,580.958 img/sec","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"As we can see below, Ray Data achieves over 90%+ GPU utilization throughout the workload run:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"16XJHcynegHxIdae1hupAF","type":"Asset","createdAt":"2023-05-03T23:07:22.875Z","updatedAt":"2023-05-03T23:07:22.875Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"batch_inference_figure_4","description":"Figure 4. GPU utilization of all 40 GPUs in the cluster. We are roughly at 90%+ GPU utilization during the entire duration of the workload.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/16XJHcynegHxIdae1hupAF/8eab9567ac78a8d558bcd04218f7e1ca/image2.png","details":{"size":466601,"image":{"width":1999,"height":621}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nThe ability to scale to many GPUs and large data sizes without needing to rewrite code and not sacrificing on throughput is what sets Ray Data apart compared to other solutions for deep learning workloads.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog, we identified four technical challenges for doing batch inference at scale and investigated three commonly cited industry solutions for offline batch inferences. Using the cited approaches, we compared Ray, Spark, and Amazon SageMaker Batch Transform.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We concluded that Ray Data best meets all four challenges:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data abstracts away compute infrastructure management, and can maximize performance for memory-bound workloads as it natively supports heterogeneous clusters.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"streams","nodeType":"text"},{"data":{},"marks":[],"value":" data from cloud storage -\u003e CPU -\u003e GPU, ensuring that all resources in the cluster are utilized at any given point in time. This improves throughput and reduces overall cost.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data has native support for multi-dimensional tensors, which is vital for deep learning workloads.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data is Python native and programmatic, making it easy to develop and debug.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Overall, Ray Data is the best option for deep learning offline batch inference. In our image classification benchmarks, as shown in the figures above, Ray Data significantly outperforms SageMaker Batch Transform (by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"17x)","nodeType":"text"},{"data":{},"marks":[],"value":" and Spark (by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"2x ","nodeType":"text"},{"data":{},"marks":[],"value":"and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"3x","nodeType":"text"},{"data":{},"marks":[],"value":") while linearly scaling to TB level data sizes. All code for these benchmarks can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Lastly, as a result of higher throughput and Ray as an open source solution, Ray lowers the overall costs on top of the base EC2 cost.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Whatâs Next","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stay tuned for a follow up post where we dive into Ray Data streaming's design and how to use it for your own ML pipelines. Meanwhile, you can get started with Ray Data for batch inference ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Alternatively, ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfC6rmBeb9pLPSKRf1e7VCy7BtkjSGu20gh0xjQhdo6rcUflQ/viewform"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"fill out this form","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â to connect with us if you are interested in working on a proof of concept (POC) for your offline batch inference workload with Ray.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have our ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit 2023 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"early-bird registration open. Secure your spot, and save some money.\n\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n\n\n\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2dNPqjBTFiDZn2gcXkdir9","type":"Asset","createdAt":"2023-05-03T22:50:03.518Z","updatedAt":"2023-05-03T22:50:03.518Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 1","description":"Figure 1. Throughput difference among three different solutions for 10GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/2dNPqjBTFiDZn2gcXkdir9/f07a675dcb4f124e29aeb2f6618dbf15/image1.png","details":{"size":163062,"image":{"width":1334,"height":948}},"fileName":"image1.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"51oIQOHymWExFWIRYQRXge","type":"Entry","createdAt":"2023-05-02T17:47:40.421Z","updatedAt":"2023-05-12T07:23:26.789Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Turbocharge LangChain: guide to 20x faster embedding","slug":"turbocharge-langchain-now-guide-to-20x-faster-embedding","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DwU0wh6AmJoGv8JOy6n0Q","type":"Entry","createdAt":"2020-11-11T23:34:15.165Z","updatedAt":"2021-02-05T22:22:08.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","slug":"amog-kamsetty","link":"https://www.linkedin.com/in/amogkamsetty/","bio":"Amog is a software engineer at Anyscale. He helps maintain and develop two machine learning libraries: Ray Tune for distributed hyperparameter tuning and RaySGD for distributed deep learning training.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J134jX2ybZRLPfcAIGLTI","type":"Asset","createdAt":"2020-11-13T00:05:05.533Z","updatedAt":"2020-11-13T00:05:05.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J134jX2ybZRLPfcAIGLTI/12589bb0c6792051050ebc6e1e83af25/Amog_Kamsetty_.jpg","details":{"size":57660,"image":{"width":480,"height":640}},"fileName":"Amog Kamsetty .jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6G9xTnF76ZUKKvgcVOqhtm","type":"Entry","createdAt":"2022-08-23T02:58:28.957Z","updatedAt":"2023-05-02T04:18:43.884Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Philipp Moritz","slug":"philipp-moritz","link":"https://www.linkedin.com/in/philipp-moritz-61419682/","bio":"Co-founder and CTO at Anyscale"}}],"publishedDate":"2023-05-03","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 2([part 1 here](https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray)) of a blog series. In this blog, weâll show you how to turbocharge embeddings. In future parts, we will show you how to combine a vector database and an LLM to create a fact-based question answering service. Additionally, we will optimize the code and measure performance: cost, latency and throughput.","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7nxbgQNlTcYZECfJ0vAabS","type":"Entry","createdAt":"2023-05-02T17:16:29.953Z","updatedAt":"2023-05-02T17:16:29.953Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"LangChain + Ray Tutorial: How to Generate Embeddings For 33,000 Pages for $1","videoUrl":"https://youtu.be/hGnZajytlac"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Generating embeddings from documents is a critical step for LLM workflows. Many LLM apps are being built today through retrieval based similarity search:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Documents are embedded and stored in a vector database.Â Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Incoming queries are used to pull semantically relevant passages from the vector database, and these passages are used as context for LLMs to answer the query.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"previous blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we showed how to use ","nodeType":"text"},{"data":{"uri":"https://github.com/hwchase17/langchain"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to do step 1, and we also showed how to parallelize this step by using Ray tasks for faster embedding creation.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we take it one step further, scaling out to many more documents. Continue reading to see how to use ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a distributed data processing system thatâs a part of the Ray framework, to generate and store embeddings for 2,000 PDF documents from cloud storage, parallelizing across 20 GPUs, all in under 4 minutes and in less than 100 lines of code.\n\nWhile in this walkthrough we use ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to read PDF files from S3 cloud storage, it also supports a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/api/input_output.html#input-output"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"wide number of other data formats","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" like text data, parquet, images, and can read data from a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/api/input_output.html#input-output"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"variety of sources","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" like MongoDB and SQL Databases.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Why do I need to parallelize this?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2CHfkngLbr34XsDZ7IKBAW","type":"Asset","createdAt":"2023-05-02T02:38:14.040Z","updatedAt":"2023-05-02T02:38:14.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Why do I need to parallelize this?","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2CHfkngLbr34XsDZ7IKBAW/af3491b43c8afcdb307890881b2adf5c/embedding-why-do-I-need-to-parallelize-this.png","details":{"size":153788,"image":{"width":1600,"height":1004}},"fileName":"embedding-why-do-I-need-to-parallelize-this.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"LangChain provides all the tools and the integrations for building LLM applications, including loading, embedding, and storing documents. While LangChain works great for quickly getting started with a handful of documents, when you want to scale your corpus up to thousands or more documents, this can quickly become unwieldy.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Naively using a for loop to do this for each document within a corpus of a 2,000 documents takes 75 minutes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4eHSySPomCQud1Y6m1yiyB","type":"Entry","createdAt":"2023-05-02T02:45:13.859Z","updatedAt":"2023-05-02T04:15:57.187Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Embedded-Code-Snippet-1","body":"import os\nfrom tqdm import tqdm\n\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=100,\n    length_function=len,\n)\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nhf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n\n# Put your directory containing PDFs here\ndirectory = '/tmp/data/'\npdf_documents = [os.path.join(directory, filename) for filename in os.listdir(directory)]\n\nlangchain_documents = []\nfor document in tqdm(pdf_documents):\n    try:\n        loader = PyPDFLoader(document)\n        data = loader.load()\n        langchain_documents.extend(data)\n    except Exception:\n        continue\n\nprint(\"Num pages: \", len(langchain_documents))\nprint(\"Splitting all documents\")\nsplit_docs = text_splitter.split_documents(langchain_documents)\n\nprint(\"Embed and create vector index\")\ndb = FAISS.from_documents(split_docs, embedding=hf)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Clearly, if you want to iterate quickly and try out different multiple document corpuses, splitting techniques, chunk sizes, or embedding models, just doing this in a for loop wonât cut it.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Instead, for faster development, you need to horizontally scale, and for this you need a framework to make this parallelization very easy.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"By using Ray Data, we can define our embedding generation pipeline and execute it in a few lines of code, and it will automatically scale out, leveraging the compute capabilities of all the CPUs and GPUs in our cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stages of our Data Pipeline","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this example, we want to generate embeddings for our document corpus consisting of the top 2,000 arxiv papers on âlarge language modelsâ. There are over 30,000 pages in all these documents. The code for generating this dataset can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/blob/main/embedding_pdf_documents/arxiv_dataset_generation.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J2BxxlXHAsmjVdrjCqLdn","type":"Asset","createdAt":"2023-05-02T02:52:04.314Z","updatedAt":"2023-05-02T02:52:04.314Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Stages of our Data Pipeline","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J2BxxlXHAsmjVdrjCqLdn/2ee02bfb6d2d96c7be6ab848c757d45e/embedded-stages-of-our-pipeline.png","details":{"size":127047,"image":{"width":1512,"height":556}},"fileName":"embedded-stages-of-our-pipeline.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs take a look at the stages of our data pipeline:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Load the PDF documents from our S3 bucket as raw bytes","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use PyPDF to convert those bytes into string text","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use LangChainâs text splitter to split the text into chunks","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use a pre-trained sentence-transformers model to embed each chunk","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Store the embeddings and the original text into a FAISS vector store","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The full data pipeline was run on 5 g4dn.12xlarge instances on AWS EC2, consisting of 20 GPUs in total. The code for the full data pipeline can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/blob/main/embedding_pdf_documents/embedding_ray.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Starting the Ray Cluster","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Follow the steps ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/blob/main/embedding_pdf_documents/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to set up a multi-node Ray cluster on AWS.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray clusters can also be started on GCP, Azure, or other cloud providers. See the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/getting-started.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Cluster documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for full info.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Alternatively, you can use ","nodeType":"text"},{"data":{"uri":"https://docs.anyscale.com/develop/workspaces/get-started"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Workspaces on Anyscale","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to manage your Ray clusters.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now that we have the cluster setup, letâs go through the steps in our script.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Installing Dependencies","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"First, we need to install the necessary dependencies on all the nodes in our Ray cluster. We can do this via Rayâs ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#id1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"runtime environment","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" feature.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4EbdJbi2f8hfsYWczaTcqZ","type":"Entry","createdAt":"2023-05-02T03:46:04.461Z","updatedAt":"2023-05-02T03:46:04.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Embedding-code-snippet-2","body":"import ray\n\nray.init(runtime_env={\"pip\": [\"langchain\", \"pypdf\", \"sentence_transformers\", \"transformers\"]})","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 1","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Load the 2,143 documents from our S3 bucket as raw bytes.The S3 bucket contains unmodified PDF files that have been downloaded from arxiv.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can easily do this via Ray Dataâs read APIs, which creates a Ray Dataset object. Ray Datasets are lazy. Further operations can be chained and the stages are run only when execution is triggered.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Cuzf28zxzpUhECAe0PxyR","type":"Entry","createdAt":"2023-05-02T03:47:03.011Z","updatedAt":"2023-05-02T17:11:29.904Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-3","body":"from ray.data.datasource import FileExtensionFilter\n\n# Filter out non-PDF files.\nds = ray.data.read_binary_files(\"s3://ray-llm-batch-inference/\", partition_filter=FileExtensionFilter(\"pdf\"))","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 2","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Use PyPDF to load in the raw bytes and parse them as string text. We also skip over any documents or pages that are unparseable. Even after skipping these, we still have over 33,642 pages in our dataset.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We use the pypdf library directly to read PDFs directly from bytes rather than file paths. Once ","nodeType":"text"},{"data":{"uri":"https://github.com/hwchase17/langchain/pull/3915"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://github.com/hwchase17/langchain/pull/3915","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is merged, LangChainâs PyPdfLoader can be used directly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"60msecWkdwI269c9fYmRih","type":"Entry","createdAt":"2023-05-02T03:49:06.119Z","updatedAt":"2023-05-02T04:15:18.317Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-4","body":"def convert_to_text(pdf_bytes: bytes):\n    pdf_bytes_io = io.BytesIO(pdf_bytes)\n\n    try:\n        pdf_doc = PdfReader(pdf_bytes_io)\n    except pypdf.errors.PdfStreamError:\n        # Skip pdfs that are not readable.\n        # We still have over 30,000 pages after skipping these.\n        return []\n\n    text = []\n    for page in pdf_doc.pages:\n        try:\n            text.append(page.extract_text())\n        except binascii.Error:\n            # Skip all pages that are not parseable due to malformed characters.\n            print(\"parsing failed\")\n    return text\n\n# We use `flat_map` as `convert_to_text` has a 1-\u003eN relationship.\n# It produces N strings for each PDF (one string per page).\n# Use `map` for 1-\u003e1 relationship.\nds = ds.flat_map(convert_to_text)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 3","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Split the text into chunks using LangChainâs TextSplitter abstraction. After applying this transformation, the 33,642 pages are split into 144,411 chunks. Each chunk will then be encoded into an embedding in Step 4.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3S1Tcod9rihoVKoyg9RMMz","type":"Entry","createdAt":"2023-05-02T03:50:32.629Z","updatedAt":"2023-05-02T04:14:39.402Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-5","body":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ndef split_text(page_text: str):\n    # Use chunk_size of 1000.\n    # We felt that the answer we would be looking for would be \n    # around 200 words, or around 1000 characters.\n    # This parameter can be modified based on your documents and use case.\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000, chunk_overlap=100, length_function=len\n    )\n    split_text: List[str] = text_splitter.split_text(page_text)\n\n    split_text = [text.replace(\"\\n\", \" \") for text in split_text]\n    return split_text\n\n# We use `flat_map` as `split_text` has a 1-\u003eN relationship.\n# It produces N output chunks for each input string.\n# Use `map` for 1-\u003e1 relationship.\nds = ds.flat_map(split_text)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 4","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Then, we can embed each of our chunks using a pre-trained sentence transformer model on GPUs. Here, we leverage Ray Actors for stateful computation, allowing us to initialize a model only once per GPU, rather than for every single batch.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"At the end of this stage, we have 144,411 encodings by running 20 model replicas across 20 GPUs, each processing a batch of 100 chunks at a time to maximize GPU utilization.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Dp9uXgJznvx0VgoNwZt0e","type":"Entry","createdAt":"2023-05-02T03:53:27.990Z","updatedAt":"2023-05-02T04:13:54.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-6","body":"class Embed:\n    def __init__(self):\n        # Specify \"cuda\" to move the model to GPU.\n        self.transformer = SentenceTransformer(model_name, device=\"cuda\")\n\n    def __call__(self, text_batch: List[str]):\n        # We manually encode using sentence_transformer since LangChain\n        # HuggingfaceEmbeddings does not support specifying a batch size yet.\n        embeddings = self.transformer.encode(\n            text_batch,\n            batch_size=100,  # Large batch size to maximize GPU utilization.\n            device=\"cuda\",\n        ).tolist()\n\n        return list(zip(text_batch, embeddings))\n\n# Use `map_batches` since we want to specify a batch size to maximize GPU utilization.\nds = ds.map_batches(\n    Embed,\n    # Large batch size to maximize GPU utilization.\n    # Too large a batch size may result in GPU running out of memory.\n    # If the chunk size is increased, then decrease batch size.\n    # If the chunk size is decreased, then increase batch size.\n    batch_size=100,  # Large batch size to maximize GPU utilization.\n    compute=ray.data.ActorPoolStrategy(min_size=20, max_size=20),  # I have 20 GPUs in my cluster\n    num_gpus=1,  # 1 GPU for each actor.\n)\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We use the `sentence_transformers` library directly so that we can provide a specific batch size. Once ","nodeType":"text"},{"data":{"uri":"https://github.com/hwchase17/langchain/pull/3914"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://github.com/hwchase17/langchain/pull/3914","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is merged, LangChainâs `HuggingfaceEmbeddings` can be used instead.\n\n","nodeType":"text"},{"data":{},"marks":[{"type":"underline"}],"value":"Stage 5","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we can execute this Data Pipeline by iterating through it, and we store the results in a persisted FAISS vector database for future querying.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3WstnFWdDedO9wxSKYDGR8","type":"Entry","createdAt":"2023-05-02T03:54:47.846Z","updatedAt":"2023-05-02T04:17:51.737Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-7","body":"from langchain import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\ntext_and_embeddings = []\nfor output in ds.iter_rows():\n    text_and_embeddings.append(output)\n\nvectore_store = FAISS.from_embeddings(\n    text_and_embeddings,\n    # Provide the embedding model to embed the query.\n    # The documents are already embedded.\n    embedding=HuggingFaceEmbeddings(model_name=model_name)\n)\n\n# Persist the vector store.\nvectore_store.save_local(\"faiss_index\")","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Execution","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Executing this code, we see that all 20 GPUs are utilized at near 100% utilization. And what would normally take over an hour to run, can now be done in under 4 minutes! If you use AWS spot instances, this would only cost $0.95 total.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5rAh6B2lrImLTTXvpBMeHG","type":"Asset","createdAt":"2023-05-02T04:00:42.271Z","updatedAt":"2023-05-02T04:00:42.271Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Execution-1","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5rAh6B2lrImLTTXvpBMeHG/70baf2935bfad28c70d5fb310fe15ef1/embedding-execution-1.png","details":{"size":9146,"image":{"width":352,"height":130}},"fileName":"embedding-execution-1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1yk9oF8K2IIuK88xWBTfGr","type":"Asset","createdAt":"2023-05-02T04:01:19.132Z","updatedAt":"2023-05-02T04:43:16.984Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"locale":"en-US"},"fields":{"title":"Embedding - Execution 2","file":{"url":"//images.ctfassets.net/xjan103pcp94/1yk9oF8K2IIuK88xWBTfGr/ee185a15ff08445d14f3e47ed7f0f2b9/embedding-execution-2.jpg","details":{"size":127225,"image":{"width":1097,"height":780}},"fileName":"embedding-execution-2.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Querying the Vector Database","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can now load in our persisted FAISS database, and query it for similarity search. Letâs see the top document thatâs most relevant to the âprompt engineeringâ query:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3DulKBBh4pLynITCBlacTf","type":"Entry","createdAt":"2023-05-02T04:04:27.586Z","updatedAt":"2023-05-02T04:04:27.586Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-8","body":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain import FAISS\n\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nquery_embedding = HuggingFaceEmbeddings(model_name=model_name)\ndb = FAISS.load_local(\"faiss_index\", query_embedding)\ndocuments = db.similarity_search(query=\"prompt engineering\", k=1)\n[doc.page_content for doc in documents]","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"SMSzMi5hzq9x7y2pUs2Ja","type":"Entry","createdAt":"2023-05-02T04:05:20.246Z","updatedAt":"2023-05-02T04:05:35.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-9","body":"['Prompt Engineering for Job Classiï¬cation 7 5 LLMs \u0026 Prompt Engineering Table '\n '3. Overview of the various prompt modiï¬cations explored in thi s study. '\n 'Short name Description Baseline Provide a a job posting and asking if it is '\n 'ï¬t for a graduate. CoT Give a few examples of accurate classiï¬cation before '\n 'queryi ng. Zero-CoT Ask the model to reason step-by-step before providing '\n 'its an swer. rawinst Give instructions about its role and the task by adding '\n 'to the user msg. sysinst Give instructions about its role and the task as a '\n 'system msg. bothinst Split instructions with role as a system msg and task '\n 'as a user msg. mock Give task instructions by mocking a discussion where it '\n 'ackn owledges them. reit Reinforce key elements in the instructions by '\n 'repeating the m. strict Ask the model to answer by strictly following a '\n 'given templat e. loose Ask for just the ï¬nal answer to be given following a '\n 'given temp late. right Asking the model to reach the right conclusion.']\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next Steps","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"See part 3 ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/building-a-self-hosted-question-answering-service-using-langchain-ray"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Review the code and data used in this blog in the following ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/tree/main/embedding_pdf_documents"},"content":[{"data":{},"marks":[],"value":"Github repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\n","nodeType":"text"},{"data":{},"marks":[],"value":"See our earlierÂ ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[],"value":"blog series on solving Generative AI infrastructure","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â with Ray.","nodeType":"text"},{"data":{},"marks":[],"value":"\n\nIf you are interested in learning more about Ray, see ","nodeType":"text"},{"data":{"uri":"http://ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"http://docs.ray.io/"},"content":[{"data":{},"marks":[],"value":"Docs.Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nTo connect with the Ray community join #LLM on the ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"data":{},"marks":[],"value":"Ray Slack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or our ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[],"value":"Discuss forum","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","nodeType":"text"},{"data":{"uri":"http://www.anyscale.com/platform"},"content":[{"data":{},"marks":[],"value":"Anyscale.com/Platform ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"and click the 'Try it now' button","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray Summit 2023:","nodeType":"text"},{"data":{},"marks":[],"value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"practical training focused on LLMs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J2BxxlXHAsmjVdrjCqLdn","type":"Asset","createdAt":"2023-05-02T02:52:04.314Z","updatedAt":"2023-05-02T02:52:04.314Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Stages of our Data Pipeline","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J2BxxlXHAsmjVdrjCqLdn/2ee02bfb6d2d96c7be6ab848c757d45e/embedded-stages-of-our-pipeline.png","details":{"size":127047,"image":{"width":1512,"height":556}},"fileName":"embedded-stages-of-our-pipeline.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2rfQivy2LoE7OTc9bGVGvm","type":"Entry","createdAt":"2023-04-27T13:59:53.933Z","updatedAt":"2023-06-06T14:28:19.779Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Announcing Ray 2.4.0: Infrastructure for LLM training, tuning, inference, and serving","seoTitle":"Announcing Ray 2.4.0: Infrastructure for LLM training, tuning, inference, and serving","slug":"announcing-ray-2-4-0-infrastructure-for-llm-training-tuning-inference-and","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6TqWiyUibvC9ETfyYljP8b","type":"Entry","createdAt":"2022-02-11T15:42:52.165Z","updatedAt":"2022-02-11T15:42:52.165Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jiajun Yao","slug":"jiajun-yao"}}],"publishedDate":"2023-04-27","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.4.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2.4 release ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"features exciting improvements across the Ray ecosystem. In this blog post, we highlight new features and examples for Generative AI workloads and share four overall additions:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements to Ray data for ease of use, stability, and observability","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Improved Serve observability","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Introduce RLlibâs module for custom reinforcement learning","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Improved Ray scalability for large clusters","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Generative AI model examples and new features using Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Over the past few months, we have seen a flurry of innovative activities around ","nodeType":"text"},{"data":{"uri":"https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-generative-ai"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"generative AI models","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Large_language_model"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"large language models (LLMs)","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". To continue our steadfast commitment to make Ray a pivotal compute substrate for ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"generative AI workloads","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and address the challenges (as explained in our ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"blog series","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"), we have added significant enhancements in this release to ensure that these open source LLM models and workloads are accessible to the open source community and performant with Ray.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QDe7dun9hKhLCoGFSXKh9","type":"Asset","createdAt":"2023-04-26T19:23:25.596Z","updatedAt":"2023-04-26T19:23:25.596Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_image","file":{"url":"//images.ctfassets.net/xjan103pcp94/1QDe7dun9hKhLCoGFSXKh9/d9f93233f6ee72608e34735eac545a74/image1.png","details":{"size":154908,"image":{"width":960,"height":540}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Generative AI Workloads:","nodeType":"text"},{"data":{},"marks":[],"value":" First, with 2.4 we are releasing a set of new working examples showing how to use Ray with ","nodeType":"text"},{"data":{"uri":"https://stablediffusionweb.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stable Diffusion","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and LLMs like ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/transformers/model_doc/gptj"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GPT-J","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Below we tabulate the workload of each of these models and their respective links. More are underway with respect to Rayâs integration with ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"49AVYyPR2gbRD9P3NOH7My","type":"Entry","createdAt":"2023-04-26T19:44:22.407Z","updatedAt":"2023-04-26T22:30:40.568Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"generative_ai_features_and_examples","header":"Table 1. New LLM features, examples, and tutorials","body":"| Generative AI Workload| Description| Links|\n| ---------- | ---------- | ---------- |\n|Fine-tuning GPT-J|In this example, we showcase how to use [Ray Train](https://docs.ray.io/en/releases-2.4.0/train/train.html) for fine-tuning [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) using [DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed) and Hugging Face. GPT-J is a GPT-2-like causal language mode, with 6B parameters, trained on the PILE dataset.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html)|\n|Fine-tuning Dreambooth|This example shows how to fine-tune a [DreamBooth](https://dreambooth.github.io/) model using [Ray Train](https://docs.ray.io/en/releases-2.4.0/train/train.html). Dreambooth is a technique to fine-tune diffusion models (like Stable Diffusion) by injecting a custom subject to the model.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/dreambooth_finetuning.html)|\n|Scalable Batch Inference  with GPT-J|This example showcases how to use[ Ray Data](https://docs.ray.io/en/releases-2.4.0/data/dataset.html) for scalable batch inference with  GPT-J.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_batch_prediction.html)| Scalable Batch Inference  with Stable Diffusion|In this example, showcase how to use [Ray Data](https://docs.ray.io/en/releases-2.4.0/data/dataset.html) for scalable batch inference with [Stable Diffusion](https://stablediffusionweb.com/). Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from CompVis, Stability AI and LAION.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/stablediffusion_batch_prediction.html)|\n|Serving GPT-J|In this example, we showcase how to use [Ray Serve](https://docs.ray.io/en/releases-2.4.0/serve/index.html) for serving GPT-J.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_serving.html)|\n|Serving Stable Diffusion|In this example, we showcase how to use [Ray Serve](https://docs.ray.io/en/releases-2.4.0/serve/index.html) for serving [Stable Diffusion.](https://stablediffusionweb.com/)|[Source and documentation](https://docs.ray.io/en/master/serve/tutorials/stable-diffusion.html)|\n|Ray integration with LangChain|In this tutorial, we showcase how to build an open source search engine with Ray and [LangChain](https://python.langchain.com/en/latest/index.html)|[Source and blog](https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray)|"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray AIR new trainers:","nodeType":"text"},{"data":{},"marks":[],"value":" Second, to further enable additional large model workloads on Ray, weâre also releasing an ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"AccelerateTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", allowing you to run ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/accelerate"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"HuggingFace Accelerate","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/accelerate/usage_guides/deepspeed"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"DeepSpeed","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on Ray with minimal code changes. This Trainer integrates with the rest of the Ray ecosystemâincluding the ability to run distributed ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"hyperparameter tuning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with each trial dispatched as a distributed training job.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"See how ","nodeType":"text"},{"data":{"uri":"https://github.com/CarperAI/trlx/blob/9bc08369ca9ec83342c4d7755205dab1a7723006/trlx/sweep.py#L325-L340"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"CarperAI/trlx","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" uses this for RLHF hyperparameter sweeps, and take a peek at the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html#ray-train-huggingface-accelerate-acceleratetrainer"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" too.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4CHwg0j8NDYgH399cu2hGC","type":"Entry","createdAt":"2023-04-26T21:23:30.012Z","updatedAt":"2023-04-26T21:23:30.012Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"accelerate_trainer_code_snippet","body":"from ray.train.huggingface.accelerate import AccelerateTrainer\n\ndef train_func():\n    ...\n\ntrainer = AccelerateTrainer(\n    train_func,\n    accelerate_config={}  # you can pass in a config filepath here as well\n    scaling_config=ScalingConfig(\n        trainer_resources={\"CPU\": 0},\n        num_workers=4,\n        use_gpu=True,\n        resources_per_worker={\"CPU\": 2},\n    ),\n)\n\nresult = trainer.fit()\n\n# Integration with Ray Tune\ntuner = tune.Tuner(\n    trainer,\n    param_space={\"lr\": tune.grid_search([0.1, 0.2, 0.3]),\n)\n\nresults = tuner.fit()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can see the full example for AccelerateTrainer ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html#ray-train-huggingface-accelerate-acceleratetrainer"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Updated LightningTrainer:","nodeType":"text"},{"data":{},"marks":[],"value":" Third, in the broader deep learning space, weâre introducing the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.lightning.LightningTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LightningTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", allowing you to scale your ","nodeType":"text"},{"data":{"uri":"https://lightning.ai/docs/pytorch/stable//index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch Lightning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on Ray. As part of our continued effort for seamless integration and ease of use, we have enhanced and replaced our existing ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray_lightning","nodeType":"text"},{"data":{},"marks":[],"value":" integration, which was widely adopted, with the latest changes to PyTorch Lighting.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The new ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"LightningTrainer","nodeType":"text"},{"data":{},"marks":[],"value":" provides better compatibility with the other Ray libraries: Ray Tune or Ray Data or Ray Serve directly with Ray connecting APIs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Check out ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.lightning.LightningTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"the documentation for this component","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or take a look ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/examples/lightning/lightning_mnist_example.html#lightning-mnist-example"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"at an example","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1p8EMLSFTFDI0RuBceMrPe","type":"Entry","createdAt":"2023-04-26T21:27:50.764Z","updatedAt":"2023-04-26T21:27:50.764Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"updated_lightning_trainder_code_snippet","body":"from pytorch_lightning.loggers.csv_logs import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\nfrom ray.train.lightning import (\n    LightningTrainer,\n    LightningConfigBuilder,\n    LightningCheckpoint,\n)\n\nclass MNISTClassifier:\n\n    â¦\n\nclass MNISTDataModule:\n    â¦\n\nlightning_config = (\n    LightningConfigBuilder()\n    .module(MNISTClassifier, lr=1e-3, feature_dim=128)\n    .trainer(\n        max_epochs=10,\n        accelerator=\"cpu\",\n        log_every_n_steps=100,\n        logger=CSVLogger(\"logs\"),\n    )\n    .fit_params(datamodule=datamodule)\n    .checkpointing(monitor=\"val_accuracy\", mode=\"max\", save_top_k=3)\n    .build()\n)\nscaling_config = ScalingConfig(\n    num_workers=4, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n)\n\nrun_config = RunConfig(\n    name=\"ptl-mnist-example\",\n    local_dir=\"/tmp/ray_results\",\n    checkpoint_config=CheckpointConfig(\n        num_to_keep=3,\n        checkpoint_score_attribute=\"val_accuracy\",\n        checkpoint_score_order=\"max\",\n    ),\n)\ntrainer = LightningTrainer(\n    lightning_config=lightning_config,\n    scaling_config=scaling_config,\n    run_config=run_config,\n)\n\nresult = trainer.fit()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"See the full source code ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/examples/lightning/lightning_mnist_example.html#lightning-mnist-example"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements in Ray DataÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The enhancements and features in the release cut across a few dimensions: stability, ease of use, and observability.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data and ease of use: ","nodeType":"text"},{"data":{},"marks":[],"value":"Data ingestion and preprocessing for machine learning (ML) at scale are central to any end-to-end ML.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To enable ease of use, the default backend execution model for Ray Data is streaming-based under the hood. As such developers need not worry about instructing Ray Data to use the streaming execution backend; itâs the default execution mode. Second, the default streaming backend lends itself well to large scale workloads such as setting up distributed training data streaming and batch inference or preprocessing. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4fRyCVJVYH8NAjkcFhTgDX","type":"Entry","createdAt":"2023-04-26T21:30:40.030Z","updatedAt":"2023-04-26T21:30:40.030Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_data_code_snippet","body":"import ray\nfrom ray.data import ActorPoolStrategy\n\nimport time\n\ndef preprocess_func(x):\n   # Do some work with the data x\n   x = ...\n   time.sleep(0.1)\n   return x\n\nfor _ in (\n   ray.data.range_tensor(5000, shape=(80, 80, 3), parallelism=200)\n   .map_batches(preprocess_func, compute=ActorPoolStrategy(size=4))\n   .map_batches(preprocess_func, compute=ActorPoolStrategy(size=3))\n   .map_batches(preprocess_func, num_cpus=1)\n   .iter_batches()\n\n):\n\n   pass","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This launches a simple 4-stage pipeline. We use different compute args for each stage, which forces them to be run as separate operators instead of getting fused together. You should see a log message indicating streaming execution is being used.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While the above example is using dummy tasks, these sort of operator pipelines are a common pattern Ray users desire to scale:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6T6VTlZroKCZ6eyyII0HC1","type":"Asset","createdAt":"2023-04-26T21:34:22.865Z","updatedAt":"2023-04-26T21:34:22.865Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_data_distributed_image","description":"Figure 1. Ray data streaming data with ActorPool ","file":{"url":"//images.ctfassets.net/xjan103pcp94/6T6VTlZroKCZ6eyyII0HC1/d7202c977c1a0c25eeb4563ab5c50319/image3.png","details":{"size":106561,"image":{"width":1249,"height":515}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray data and SQL","nodeType":"text"},{"data":{},"marks":[],"value":": Additionally, we have extended Ray Dataâs ability to fetch data from a common ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/creating-datastreams.html#reading-from-sql-databases"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SQL data source","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", with ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"read_sql(...)","nodeType":"text"},{"data":{},"marks":[],"value":", enabling you to connect to a common SQL data store.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3imkshawnZw7DglAUBNTN3","type":"Entry","createdAt":"2023-04-26T21:36:16.889Z","updatedAt":"2023-04-26T21:36:16.889Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_data_sql_connector_code_snippet","body":"import mysql.connector\n\nimport ray\n\ndef create_connection():\n    return mysql.connector.connect(\n        user=\"admin\",\n        password=...,\nhost=\"example-mysql-database.c2c2k1yfll7o.us-west-2.rds.amazonaws.com\",\n      connection_timeout=30,\n      database=\"example\",\n    )\n\n# Get all movies\ndatastream = ray.data.read_sql(\"SELECT * FROM movie\", create_connection)\n# Get movies after the year 1980\ndatastream = ray.data.read_sql(\n    \"SELECT title, score FROM movie WHERE year \u003e= 1980\", create_connection\n)\n# Get the number of movies per year\ndatastream = ray.data.read_sql(\n    \"SELECT year, COUNT(*) FROM movie GROUP BY year\", create_connection\n)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements to Serve observability","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In 2.4, we are adding new functionality for observability and monitoring for Ray Serve applications. You can use the Ray dashboard to get a high-level overview of your Ray cluster and Ray Serve applicationâs states. This includes details such as:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"the number of deployment replicas currently running","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"logs for your Serve controller, deployment replicas, and HTTP proxies","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"the Ray nodes (i.e., machines) running in your Ray cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can access the Ray dashboard at port 8265 at your clusterâs URI. For example, if youâre running Ray Serve locally, use ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"http://localhost:8265","nodeType":"text"},{"data":{},"marks":[],"value":" in your browser.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3CtBAZRebvQVRJsbp63rD2","type":"Asset","createdAt":"2023-04-26T21:39:28.459Z","updatedAt":"2023-04-26T21:39:28.459Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_serve_observebility","description":"Figure 1. Serve observesability metrics in the Ray Dashboard.","file":{"url":"//images.ctfassets.net/xjan103pcp94/3CtBAZRebvQVRJsbp63rD2/0951a7ad1f4d4ec1ae2a3ba60a3af218/image2.png","details":{"size":146211,"image":{"width":1999,"height":1040}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"For more information on the dashboard, read the documentation for the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/ray-core/ray-dashboard.html#dash-serve-view"},"content":[{"data":{},"marks":[],"value":"Serve page","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"RLlibâs new module for custom reinforcement","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In RLlib, we are introducing a new ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/rllib-rlmodule.html#rl-modules-alpha"},"content":[{"data":{},"marks":[],"value":"RLModule","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" abstraction (alpha). RLModule API provides a unified way to define custom reinforcement learning models in RLlib. This API enables you to design and implement your own models to suit specific needs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To maintain consistency and usability, RLlib offers a standardized approach for defining module objects for both single-agent and multi-agent reinforcement learning environments through the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/package_ref/rl_modules.html"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"SingleAgentRLModuleSpec","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/package_ref/doc/ray.rllib.core.rl_module.marl_module.MultiAgentRLModuleSpec.html#ray.rllib.core.rl_module.marl_module.MultiAgentRLModuleSpec"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"MultiAgentRLModuleSpec","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" classes. The built-in RLModules in RLlib follow this consistent design pattern, making it easier for you to understand and utilize these modules.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1w6lK0sBUAVl4BmOGBQcQ9","type":"Entry","createdAt":"2023-04-26T21:48:25.122Z","updatedAt":"2023-04-26T21:48:25.122Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"rllib_multiagent_code_snippet","body":"import gymnasium as gym\nfrom ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec\nfrom ray.rllib.core.testing.torch.bc_module import DiscreteBCTorchModule\n\nenv = gym.make(\"CartPole-v1\")\n\nspec = SingleAgentRLModuleSpec(\n    module_class=DiscreteBCTorchModule,\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    model_config_dict={\"fcnet_hiddens\": [64]},\n)\n\nmodule = spec.build()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is an experimental module that serves as a general replacement for ModelV2, and is subject to change. It will eventually match the functionality of the previous stack. If you only use high-level RLlib APIs such as ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html#ray.rllib.algorithms.algorithm.Algorithm"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Algorithm","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", you should not experience significant changes, except for a few new parameters to the configuration object. Read more about it in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/rllib/rllib-rlmodule.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Core scaling to 2000 nodes","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Weâve also invested time in making sure Ray can support larger scale workloads. With this release, we are announcing official support for Ray clusters to up to 2000 nodes. See","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/release/benchmarks/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":" scalability envelope","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for more details.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"First and foremost, we want to thank all contributors for their valuable contributions to this new release of ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.4.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2.4","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Your enduring support continues to foster the wider use of Ray adoption.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This release is a milestone in our efforts to offer Ray as a performant compute infrastructure for LLM training, inference, and serving. We shared a number of examples that speak to these efforts.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Additionally, we enhanced Ray Data, improved Serve observability, introduced a new RLlib customization module, and scaled Ray core to support large workloads, up to 2000 nodes. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Have a go at the latest release with pip install âray[default]â and let us know of your feedback. Weâre always interested to hear from you â feel free to reach out to us on ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Github","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discuss","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Whatâs Next?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stay tuned for more Generative AI how-to blog series as we continue our efforts to make Ray the platform choice and compute substrate for all your Generative AI workloads. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you missed our four-part blog series on Generative AI, in which lay out our vision, position, and solution on how Ray addresses the challenges associated with OSS Generative AI workloads, peruse the links below. Much of the above Generative AI examples and features stem from this series.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-ray-solves-common-production-challenges-for-generative-ai"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"How Ray solves common production challenges for generative AI infrastructure","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Training 175B Parameter Language Models at 1000 GPU scale with Alpa and Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/faster-stable-diffusion-fine-tuning-with-ray-air"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Faster stable diffusion fine-tuning with Ray AIR","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"How to fine tune and serve LLMs simply, quickly and cost effectively using Ray + DeepSpeed + HuggingFace","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Building an LLM open source search engine in 100 lines using LangChain and Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Join our ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/community"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Community","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the ","nodeType":"text"},{"data":{"uri":"https://forms.gle/9TSdDYUgxYs8SA9e8"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray #LLM slack channel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have our ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit 2023 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"early-bird registration open. Secure your spot, and save some money.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QDe7dun9hKhLCoGFSXKh9","type":"Asset","createdAt":"2023-04-26T19:23:25.596Z","updatedAt":"2023-04-26T19:23:25.596Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_image","file":{"url":"//images.ctfassets.net/xjan103pcp94/1QDe7dun9hKhLCoGFSXKh9/d9f93233f6ee72608e34735eac545a74/image1.png","details":{"size":154908,"image":{"width":960,"height":540}},"fileName":"image1.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"574o0Bh7HzHlCEc6AXphfF","type":"Entry","createdAt":"2023-04-19T16:00:08.661Z","updatedAt":"2023-05-31T18:34:15.880Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":16,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Building an LLM open source search engine in 100 lines using LangChain and Ray","seoTitle":"Building an LLM Open-Source Search Engine in 100 Lines","slug":"llm-open-source-search-engine-langchain-ray","description":"In part 1 of a new blog series, we show how to build a search engine in 100 lines using LLM embeddings and a vector database.","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-04-18","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 1 of a blog series. In this blog, weâll introduce you to LangChain and Ray Serve and how to use them to build a search engine using LLM embeddings and a vector database. In future parts, we will show you how to turbocharge embeddings and how to combine a vector database and an LLM to create a fact-based question answering service. Additionally, we will optimize the code and measure performance: cost, latency and throughput.\n\n\u003cscript type=\"application/ld+json\"\u003e{\n\"@context\": \"http://schema.org\",\n\"@type\": \"VideoObject\",\n\"name\": \"Open Source LLM Search Engine with LangChain on Ray\",\n\"description\": \"Waleed, Head of Engineering at Anyscale, explains how to use LangChain and Ray Serve to build a search engine using LLM embeddings and a vector database. Blog: https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray\",\n\"thumbnailUrl\": \"https://i.ytimg.com/vi/v7a8SR-sZpI/default.jpg\",\n\"uploadDate\": \"2023-04-19T16:00:41Z\",\n\"duration\": \"PT7M36S\",\n\"embedUrl\": \"https://www.youtube.com/embed/v7a8SR-sZpI\",\n\"interactionCount\": \"4540\"\n}\u003c/script\u003e","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"b9V0HkkRoYEqCnIrMO9kC","type":"Entry","createdAt":"2023-04-19T17:16:15.326Z","updatedAt":"2023-04-19T17:16:15.326Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"LLM Search Engine with Langchain","videoUrl":"https://www.youtube.com/watch?v=v7a8SR-sZpI"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog, we'll cover:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"An introduction to ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and show why itâs awesome.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"An explanation of how Ray complements ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" by:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Showing how with a few minor changes, we can speed parts of the process up by a factor of 4x or more","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Making ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"âs capabilities available in the cloud using Ray Serve","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Using self-hosted models by running ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the model all in the same Ray cluster without having to worry about maintaining individual machines.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introduction","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a very powerful framework for ML orchestration, but with great power comes voluminous documentation. 120 megabytes in fact. How can we make that documentation more accessible? ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Answer: make it searchable! It used to be that creating your own high quality search results was hard. But by using ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we can build it in about 100 lines of code.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is where ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" comes in. ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provides an amazing suite of tools for everything around LLMs. Itâs kind of like ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/"},"content":[{"data":{},"marks":[],"value":"HuggingFace","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" but specialized for LLMs. There are tools (chains) for prompting, indexing, generating and summarizing text. While an amazing tool, using Ray with it can make ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" even more powerful. In particular, it can:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Simply and quickly help you deploy a ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" service.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Rather than relying on remote API calls, allow Chains to run co-located and auto-scalable with the LLMs itself. This brings all the advantages we ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"discussed in a previous blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": lower cost, lower latency, and control over your data.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Building the index","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4OzISThpksdKgjZ0gVJUiB","type":"Asset","createdAt":"2023-04-19T16:59:49.253Z","updatedAt":"2023-05-01T09:40:14.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"doc-index-starter-langchain","description":"Build a document index easily with Ray and Langchain","file":{"url":"//images.ctfassets.net/xjan103pcp94/4OzISThpksdKgjZ0gVJUiB/85bb7fccdfef1df3d061c57e9af1062a/index-langchain.jpg","details":{"size":28918,"image":{"width":900,"height":381}},"fileName":"index-langchain.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"First we will build the index via the following steps.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Download the content we want to index locally.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Read the content and cut it into tiny little pieces (about a sentence each). This is because it is easier to match queries against pieces of a page rather than the whole page.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use the ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/sentence-transformers"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Sentence Transformers","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" library from HuggingFace to generate a vector representation of each sentence.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Embed those vectors in a Vector database (we use ","nodeType":"text"},{"data":{"uri":"https://github.com/facebookresearch/faiss"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FAISS","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", but you could use whatever you like).Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The amazing thing about this code is how simple it is - ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/d06097768abbea54d59e5d3ed4f045f3"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"See Here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". As you will see, thanks to LangChain, all the heavy lifting is done for us. Letâs pick a few excerpts.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Assuming weâve downloaded the Ray docs, this is all we have to do to read all the docs in: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5XcvTPG6LASLRu3arklXJ0","type":"Entry","createdAt":"2023-04-17T08:22:33.719Z","updatedAt":"2023-04-17T08:22:33.719Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-1","body":"loader = ReadTheDocsLoader(\"docs.ray.io/en/master/\")\ndocs = loader.load() ","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The next step is to break each document into little chunks. LangChain uses splitters to do this. So all we have to do is this: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"72vZr3kpoioEza3WTS1vfT","type":"Entry","createdAt":"2023-04-17T08:23:19.379Z","updatedAt":"2023-04-17T08:23:19.379Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-2","body":"chunks = text_splitter.create_documents(\n    [doc.page_content for doc in docs], \n    metadatas=[doc.metadata for doc in docs])","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We want to preserve the metadata of what the original URL was, so we make sure to retain the metadata along with these documents.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now we have the chunks we can embed them as vectors. LLM providers do offer APIs for doing this remotely (and this is how most people use LangChain). But, with just a little bit of glue we can download Sentence Transformers from HuggingFace and run them locally (inspired by LangChainâs support for llama.cpp). ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/aea1d312d68c9431949442cc562d5f2c"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Hereâs the glue code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"By doing so, we reduce latency, stay on open source technologies, and donât need a HuggingFace key or to pay for API usage.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have the embeddings, now we can use a vector database â in this case FAISS â to store the embeddings. Vector databases are optimized for doing quick searches in high dimensional spaces. Again, LangChain makes this effortless. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4YljOvEjiKrfhFRmpA1PDG","type":"Entry","createdAt":"2023-04-17T08:23:35.223Z","updatedAt":"2023-04-17T08:23:35.223Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-3","body":"from langchain.vectorstores import FAISS\n\ndb = FAISS.from_documents(chunks, embeddings)\ndb.save_local(FAISS_INDEX_PATH)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"And thatâs it. The code for this is ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/d0915e52cbe56dff328f5c00ded21107"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Now we can build the store.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"39FzcaaXnaSmciROgXSD0x","type":"Entry","createdAt":"2023-04-17T08:29:45.440Z","updatedAt":"2023-04-17T08:29:45.440Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-python-build-snippet","body":"% python build_vector_store.py"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This takes about 8 minutes to execute. Most of that time is spent doing the embeddings. Of course, itâs not a big deal in this case, but imagine if you were indexing hundreds of gigabytes instead of hundreds of megabytes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Accelerating indexing using Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"[Note: This is a slightly more advanced topic and can be skipped on first reading. It just shows how we can do it more quickly â 4x to 8x times more quickly]","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How can we speed up indexing? The great thing is that embedding is easy to parallelize. What if we:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Sliced the list of chunks into 8 shards.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Embedded each of the 8 shards separately.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Merge the shards.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7tDpD5Q7nxtRyX9lgDvbkI","type":"Asset","createdAt":"2023-04-17T08:04:57.120Z","updatedAt":"2023-04-17T08:04:57.120Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"langchain-ray-accelerated-indexing","description":"Build a document index 4-8x faster with Ray","file":{"url":"//images.ctfassets.net/xjan103pcp94/7tDpD5Q7nxtRyX9lgDvbkI/6209fbd875c5cd379c2289ef6f6554f0/Screen_Shot_2023-04-16_at_6.20.10_PM.png","details":{"size":277144,"image":{"width":2284,"height":936}},"fileName":"Screen Shot 2023-04-16 at 6.20.10 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"One key thing to realize is that embedding is GPU accelerated, so if we want to do this, we need 8 GPUs. Thanks to Ray, those 8 GPUS donât have to be on the same machine. But even on a single machine, there are significant advantages to using Ray. And one does not have to go to the complexity of setting up a Ray cluster, all you need to do is ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"pip install ray[default]","nodeType":"text"},{"data":{},"marks":[],"value":" and then ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"import ray","nodeType":"text"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This requires some minor surgery to the code. Hereâs what we have to do.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"First, create a task that creates the embedding and then uses it to index a shard. Note the Ray annotation and us telling us each task will need a whole GPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3NmJuC8SstZJpoHLrzrLgg","type":"Entry","createdAt":"2023-04-17T08:23:48.410Z","updatedAt":"2023-04-17T08:23:48.410Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-4","body":"@ray.remote(num_gpus=1)\ndef process_shard(shard): \n    embeddings = LocalHuggingFaceEmbeddings('multi-qa-mpnet-base-dot-v1')\n    result = FAISS.from_documents(shard, embeddings)\n    return result\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next, split the workload in the shards. NumPy to the rescue! This is a single line:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7lotIjqpm0Sm03BPHE4t2n","type":"Entry","createdAt":"2023-04-17T08:24:01.572Z","updatedAt":"2023-04-17T08:24:01.572Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-5","body":"shards = np.array_split(chunks, db_shards)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Then, create one task for each shard and wait forÂ the results.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7m8iFk9DvJOBZc3r4maPTY","type":"Entry","createdAt":"2023-04-17T08:24:13.248Z","updatedAt":"2023-04-17T08:24:13.248Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-6","body":"futures = [process_shard.remote(shards[i]) for i in range(db_shards)]\nresults = ray.get(futures)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, letâs merge the shards together. We do this using simple linear merging.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"21KuagZ35WVzVVJ7Xq4qHy","type":"Entry","createdAt":"2023-04-17T08:24:27.839Z","updatedAt":"2023-04-17T08:29:23.908Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-7","body":"db = results[0]\nfor i in range(1,db_shards):\n    db.merge_from(results[i])","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/4c41f3ee66040f57d34c6a40e42b5969"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Hereâs what the sped up code looks like.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"You might be wondering, does this actually work? We ran some tests on a g4dn.metal instance with 8 GPUs. The original code took 313 seconds to create the embeddings, the new code took 70 seconds, thatâs about a 4.5x improvement. Thereâs still some one-time overheads to creating tasks, setting up the GPUs etc. This reduces as the data increases. For example, we did a simple test with 4 times the data, and it was around 80% of the theoretical maximum performance (ie. 6.5x faster vs theoretical maximum of 8x faster from the 8 GPUs).Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can use the Ray Dashboard to see how hard those GPUs are working. Sure enough theyâre all close to 100% running the process_shard method we just wrote. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"14IoasmHxwEeQdAEGJqlyO","type":"Asset","createdAt":"2023-04-17T08:10:19.509Z","updatedAt":"2023-04-17T21:38:58.348Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"accelerated-index-langchain-dashboard","description":"Dashboard shows that GPU utilization is maxed out across all instances","file":{"url":"//images.ctfassets.net/xjan103pcp94/14IoasmHxwEeQdAEGJqlyO/5ae6e6739e258252a78c889ca7959683/raydash.png","details":{"size":278628,"image":{"width":2442,"height":842}},"fileName":"raydash.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"It turns out merging vector databasesÂ  is pretty fast, taking only 0.3 seconds for all 8 shards to be merged.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Serving","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1g6zBePU72Rmz5MBH2reaB","type":"Asset","createdAt":"2023-04-17T08:08:50.696Z","updatedAt":"2023-04-17T08:08:50.696Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Serving-Queries-Ray-Langchain","description":"Serve search queries with Ray and Langchain","file":{"url":"//images.ctfassets.net/xjan103pcp94/1g6zBePU72Rmz5MBH2reaB/db400e9bbbc445d7214d45658f81992f/Screen_Shot_2023-04-16_at_9.42.46_PM.png","details":{"size":380753,"image":{"width":2718,"height":1348}},"fileName":"Screen Shot 2023-04-16 at 9.42.46 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Serving is another area where the combination of LangChain and Ray Serve shows its power. This is just scratching the surface: weâll explore amazing capabilities like independent auto scaling and request batching in our next blog post in the series. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The steps required to do this are: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Load the FAISS database we created and the instantiate the embedding","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Start using FAISS to do similarity searches. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve makes this magically easy. Ray uses a âdeploymentâ to wrap a simple python class. The __init__ method does the loading and then __call__ is what actually does the work. Ray takes care of connecting it to the internet, bringing up a service, http and so on. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hereâs a simplified version of the code: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7GwywaSkEi9NnwUhk3wMMD","type":"Entry","createdAt":"2023-04-17T08:24:42.996Z","updatedAt":"2023-04-17T08:24:42.996Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-8","body":"@serve.deployment\nclass VectorSearchDeployment:\n    def __init__(self):\n        self.embeddings = â¦ \n        self.db = FAISS.load_local(FAISS_INDEX_PATH, self.embeddings)\n\n    def search(self,query): \n        results = self.db.max_marginal_relevance_search(query)\n        retval = \u003csome string processing of the results\u003e\n        return retval\n\n    async def __call__(self, request: Request) -\u003e List[str]:\n        return self.search(request.query_params[\"query\"])\n\ndeployment = VectorSearchDeployment.bind()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Thatâs it! ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs now start this service with the command line (of course Serve has more deployment options than this, but this is an easy way):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"o0cCaJMv4yeC2ikcQuJhf","type":"Entry","createdAt":"2023-04-17T08:29:00.307Z","updatedAt":"2023-04-17T08:29:00.307Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-serve-snippet","body":"% serve run serve_vector_store:deployment"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now we can write a simple python script to query the service to get relevant vectors(itâs just a web server running on port 8000). ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3E0la6cbL8n7UPpvve36nJ","type":"Entry","createdAt":"2023-04-17T08:24:56.873Z","updatedAt":"2023-04-17T08:24:56.873Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-9","body":"import requests\nimport sys\nquery = sys.argv[1]\nresponse = requests.post(f'http://localhost:8000/?query={query}')\nprint(response.content.decode())","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"And now letâs try it out: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SL7auZuJzHEyaq8fVIifv","type":"Entry","createdAt":"2023-04-17T08:25:16.327Z","updatedAt":"2023-04-17T21:14:41.421Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-10","body":"$ python query.py 'Does Ray Serve support batching?'\nFrom http://docs.ray.io/en/master/serve/performance.html\n\nYou can check out our microbenchmark instructions\nto benchmark Ray Serve on your hardware.\nRequest Batching#\n====\n\nFrom http://docs.ray.io/en/master/serve/performance.html\n\nYou can enable batching by using the ray.serve.batch decorator. Letâs take a look at a simple example by modifying the MyModel class to accept a batch.\nfrom ray import serve\nimport ray\n@serve.deployment\nclass Model:\n    def __call__(self, single_sample: int) -\u003e int:\n        return single_sample * 2\n====\n\nFrom http://docs.ray.io/en/master/ray-air/api/doc/ray.train.lightgbm.LightGBMPredictor.preferred_batch_format.html\n\nnative batch format.\nDeveloperAPI: This API may change across minor Ray releases.\n====\n\nFrom http://docs.ray.io/en/master/serve/performance.html\n\nMachine Learning (ML) frameworks such as Tensorflow, PyTorch, and Scikit-Learn support evaluating multiple samples at the same time.\nRay Serve allows you to take advantage of this feature via dynamic request batching.\n===="}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We showed in the above code just how easy it is to build key components of an LLM-based search engine and serve its responses to the entire world by combining the power of LangChain and Ray Serve. And we didnât have to deal with a single pesky API key!Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Tune in for Part 2, where we will show how to turn this into a chatgpt-like answering system. Weâll use open source LLMs like Dolly 2.0 to do that.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"And finally weâll share Part 3 where weâll talk about scalability and cost. The above is fine for a few hundred queries per second, but what if you need to scale to a lot more? And are the claims about latency correct? ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next Steps","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"See part 2 ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/turbocharge-langchain-now-guide-to-20x-faster-embedding"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nReview the code and data used in this blog in the following ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/tree/main/open_source_LLM_search_engine"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Github repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". \n\nSee our earlier ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[],"value":"blog series on solving Generative AI infrastructure","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with Ray.\n\nIf you are interested in learning more about Ray, see ","nodeType":"text"},{"data":{"uri":"http://ray.io"},"content":[{"data":{},"marks":[],"value":"Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"http://docs.ray.io"},"content":[{"data":{},"marks":[],"value":"Docs.Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To connect with the Ray community join #LLM on the ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"data":{},"marks":[],"value":"Ray Slack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or our ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[],"value":"Discuss forum","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","nodeType":"text"},{"data":{"uri":"http://www.anyscale.com/platform"},"content":[{"data":{},"marks":[],"value":"Anyscale.com/Platform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and click the 'Try it now' button","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray Summit 2023:","nodeType":"text"},{"data":{},"marks":[],"value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"practical training focused on LLMs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4OzISThpksdKgjZ0gVJUiB","type":"Asset","createdAt":"2023-04-19T16:59:49.253Z","updatedAt":"2023-05-01T09:40:14.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"doc-index-starter-langchain","description":"Build a document index easily with Ray and Langchain","file":{"url":"//images.ctfassets.net/xjan103pcp94/4OzISThpksdKgjZ0gVJUiB/85bb7fccdfef1df3d061c57e9af1062a/index-langchain.jpg","details":{"size":28918,"image":{"width":900,"height":381}},"fileName":"index-langchain.jpg","contentType":"image/jpeg"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7bptc6mEv7bhFZvq5AOXqc","type":"Entry","createdAt":"2023-04-18T20:36:43.249Z","updatedAt":"2023-04-19T16:00:08.646Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Why I Joined Anyscale: Solving Cutting-Edge Problems in a Time of Enormous Change","seoTitle":"why i joined anyscale by sidney rabsatt","slug":"why-i-joined-anyscale-solving-cutting-edge-problems-in-a-time-of-enormous","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"461P1q4TML68SzYxOG9sxm","type":"Entry","createdAt":"2023-04-18T20:05:20.525Z","updatedAt":"2023-04-18T20:05:20.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Sidney Rabsatt","slug":"sidney-rabsatt","link":"https://www.linkedin.com/in/sidney-rabsatt/"}}],"publishedDate":"2023-04-19","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42F9iHs5a05rRrOuqEQwvB","type":"Entry","createdAt":"2022-06-15T01:30:47.322Z","updatedAt":"2022-06-15T01:30:47.322Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Culture","slug":"culture","tintColor":"#39C8E5"}},"intro":"Why Sidney Rabsatt joined Anyscale as Head of Product.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Throughout my career, I've had the opportunity to work on cutting-edge problems across some of the most impactful technological transitions of our time. From the early days of the World Wide Web to the rise of Mobile and Cloud computing, Iâve worked on numerous commercial products across Networking, Observability, and App/Cloud Infrastructure and on some of the most widely-adopted open-source projects including Wireshark, Nginx, and now Ray. I've been deeply involved in resolving the complexities and, at times, unexpected infrastructure obstacles that technologists encounter, while empowering companies to fully benefit from these advancements.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7uPFFDyMQg5GZ5FU5kM8rw","type":"Asset","createdAt":"2023-04-18T20:34:23.318Z","updatedAt":"2023-04-18T20:34:23.318Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Sidney Rabsatt Headshot","description":"Picture of Sidney Rabsatt, Anyscale Head of Product","file":{"url":"//images.ctfassets.net/xjan103pcp94/7uPFFDyMQg5GZ5FU5kM8rw/225783b86665705f9c198768754f8722/Sidney_pic.jpeg","details":{"size":501555,"image":{"width":3540,"height":2832}},"fileName":"Sidney pic.jpeg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"AI","nodeType":"text"},{"data":{},"marks":[],"value":" promises to be the most complex transition across the most dimensions that affect our lives. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"I joined Anyscale to embrace and be part of solving those challenges. I joined to make AI available to all organizations, to give people better tools, and do it responsibly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray and Anyscale work at the core of the best known and most advanced AI applications such as Generative AI with OpenAIâs ChatGPT and Prediction for Uber rides / ETAs, not to mention what Spotify, Instacart and others are doing.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The team is one of the strongest in the industry, comprised of AI/ML PhDâs and experienced AI experts from top schools like UC Berkeley and companies like Uber, Google, Amazon, Meta, and more!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Our technology is proven and continues to evolve rapidly with strong community involvement. And the opportunity is vast to define how best to develop and run AI.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"AI will touch and shape our lives in many ways and joining Anyscale to lead Product is how Iâm getting involved.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7uPFFDyMQg5GZ5FU5kM8rw","type":"Asset","createdAt":"2023-04-18T20:34:23.318Z","updatedAt":"2023-04-18T20:34:23.318Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Sidney Rabsatt Headshot","description":"Picture of Sidney Rabsatt, Anyscale Head of Product","file":{"url":"//images.ctfassets.net/xjan103pcp94/7uPFFDyMQg5GZ5FU5kM8rw/225783b86665705f9c198768754f8722/Sidney_pic.jpeg","details":{"size":501555,"image":{"width":3540,"height":2832}},"fileName":"Sidney pic.jpeg","contentType":"image/jpeg"}}},"mainImageFit":"contain","hideIntro":true,"showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7e5A9XUzg0NfneWobPOKxf","type":"Entry","createdAt":"2023-04-10T23:01:28.472Z","updatedAt":"2023-05-31T18:31:32.938Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":12,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"How to fine tune and serve LLMs simply, quickly and cost effectively using Ray + DeepSpeed + HuggingFace","seoTitle":"How to Fine-Tune a 6 Billion Parameter LLM for Less Than $7","slug":"how-to-fine-tune-and-serve-llms","description":"In part 4 of our Generative AI series, we share how to build a system for fine-tuning \u0026 serving LLMs in 40 minutes or less.","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2yKvtARlwNkkIYOYLFgkhB","type":"Entry","createdAt":"2022-06-22T19:54:47.912Z","updatedAt":"2022-06-22T19:54:47.912Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jun Gong","slug":"jun-gong"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BGRfsYFKUgIpGjaXTUYml","type":"Entry","createdAt":"2021-08-09T15:38:43.390Z","updatedAt":"2021-08-09T15:38:43.390Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Antoni Baum","slug":"antoni-baum","link":"https://www.linkedin.com/in/yard1/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}}],"publishedDate":"2023-04-10","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 4 of our blog series on Generative AI. In the previous blog posts we explained:\n1.[Why Ray is a sound platform for Generative AI](https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure)\n2.[we showed how it can push the performance limits](https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray)\n3.[how you can use Ray for stable diffusion](https://www.anyscale.com/blog/faster-stable-diffusion-fine-tuning-with-ray-air). \n","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this blog, we share aÂ  practical approach on how you can use the combination of HuggingFace, DeepSpeed, and Ray to build a system for fine-tuning and serving LLMs, in 40 minutes for less than $7 for a 6 billion parameter model. In particular, we illustrate the following:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Using these three components, you can simply and quickly put together an open-source LLM fine-tuning and serving system.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"By taking advantage of Rayâs distributed capabilities, we show how this can be both more cost-effective and faster than using a single large (and often ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://aiascendant.substack.com/p/taiwan-is-pandora-gpus-are-unobtainium"},"content":[{"nodeType":"text","value":"unobtainable","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":") machine.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Hereâs what weâll be doing:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Discussing why you might want to run your own LLM instead of using one of the new API providers.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing you the evolving tech stack we are seeing for cost-effective LLM fine-tuning and serving, combining HuggingFace, DeepSpeed, Pytorch, and Ray.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing you 40 lines of Python code that can enable you to serve a 6 billion parameter GPT-J model.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing you, for less than $7, how you can fine-tune the model to sound more medieval using the works of Shakespeare by doing it in a distributed fashion on low-cost machines, which is considerably more cost-effective than using a single large powerful machine.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing how you can serve the fine-tuned 6B LLM compiled model binary.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing how the fine-tuned model compares to a prompt engineering approach with large systems. ","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Why would I want to run my own LLM?Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"There are ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://openai.com"},"content":[{"nodeType":"text","value":"many","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://anthropic.com/"},"content":[{"nodeType":"text","value":"many","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://cohere.ai/"},"content":[{"nodeType":"text","value":"providers","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" of LLM APIs online. Why would you want to run your own? There are a few reasons:Â ","marks":[],"data":{}}]},{"nodeType":"ordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Cost, especially for fine-tuned inference","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": For example, OpenAI charges 12c per 1000 tokens (about 700 words) for a fine-tuned model on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://openai.com/pricing"},"content":[{"nodeType":"text","value":"Davinci","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Itâs important to remember that many user interactions require multiple backend calls (e.g. one to help with the prompt generation, post-generation moderation, etc), so itâs very possible that a single interaction with an end user could cost a few dollars. For many applications, this is cost prohibitive.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Latency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"using these LLMs is especially slow. A GPT-3.5 query for example can take up to 30 seconds. Combine a few round trips from your data center to theirs and it is possible for a query to take minutes. Again, this makes many applications impossible. Bringing the processing in-house allows you to optimize the stack for your application, e.g. by using low-resolution models, tightly packing queries to GPUs, and so on. We have heard from users that optimizing their workflow has often resulted in a 5x or more latency improvement.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Data Security \u0026 Privacy: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"In order to get the response from these APIs, you have to send them a lot of data for many applications (e.g. send a few snippets of internal documents and ask the system to summarize them). Many of the API providers reserve the right to use those instances for retraining. Given the sensitivity of organizational data and also frequent legal constraints like data residency, this is especially limiting. One, particularly concerning recent development, is the ability to ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://arxiv.org/pdf/2301.13188.pdf"},"content":[{"nodeType":"text","value":"regenerate training data from learned models","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", and people ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.techradar.com/news/samsung-workers-leaked-company-secrets-by-using-chatgpt"},"content":[{"nodeType":"text","value":"unintentionally disclosing secret information","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"How to create and run your own LLMÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The LLM space is an incredibly fast-moving space, and it is currently evolving very rapidly. What we are seeing is a particular technology stack that combines multiple technologies: ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70GhsrIVrh0Qz0fNDUp4A8","type":"Asset","createdAt":"2023-04-07T14:17:45.223Z","updatedAt":"2023-04-07T14:17:45.223Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"llm-stack","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/70GhsrIVrh0Qz0fNDUp4A8/458e076a02c4745369c683851c378536/Screenshot_2023-04-07_at_10.17.23_AM.png","details":{"size":144021,"image":{"width":1073,"height":663}},"fileName":"Screenshot 2023-04-07 at 10.17.23 AM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"What weâve also seen is a reluctance to go beyond a single machine for training. In part, because there is a perception that moving to multiple machines is seen as complicated. The good news is this is where ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io"},"content":[{"nodeType":"text","value":"Ray.io","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" shines (ba-dum-tish). It simplifies cross-machine coordination and orchestration aspects using not much more than Python and Ray decorators, but also is a great framework for composing this entire stack together.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\nRecent results on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html"},"content":[{"nodeType":"text","value":"Dolly","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/lm-sys/FastChat"},"content":[{"nodeType":"text","value":"Vicuna","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" (both trained on Ray or trained on models built with Ray like ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://huggingface.co/EleutherAI/gpt-j-6b"},"content":[{"nodeType":"text","value":"GPT-J","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":") are small LLMs (relatively speaking â say the open source model GPT-J-6B with 6 billion parameters) that can be incredibly powerful when fine-tuned on the right data. The key is fine-tuning and the right data parts. So you do not always need to use the latest and greatest model with 150 billion-plus parameters to get useful results. Letâs get started! \n","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Serving a pre-existing model with Ray for text generation","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The detailed steps on how to serve the GPT-J model with Ray can be found ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/gptj_serving.html"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", so letâs highlight some of the aspects of how we do that. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"56imF50uP87DrWXWfjxNqV","type":"Entry","createdAt":"2023-04-07T18:37:36.999Z","updatedAt":"2023-04-07T18:47:21.234Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"serve-llm","body":"Â Â Â @serve.deployment(ray_actor_options={\"num_gpus\":1})\nÂ Â Â classPredictDeployment:\nÂ Â Â Â Â def__init__(self, model_id:str, revision:str=None):\nÂ Â Â Â Â Â Â Â from transformers import AutoModelForCausalLM, AutoTokenizer\nÂ Â Â Â Â Â Â Â import torch\nÂ Â Â Â Â Â Â Â self.model = AutoModelForCausalLM.from_pretrained(\nÂ Â Â Â Â Â Â Â Â Â Â Â \"EleutherAI/gpt-j-6B\",\nÂ Â Â Â Â Â Â Â Â Â Â Â revision=revision,\nÂ Â Â Â Â Â Â Â Â Â Â Â torch_dtype=torch.float16,\nÂ Â Â Â Â Â Â Â Â Â Â Â low_cpu_mem_usage=True,\nÂ Â Â Â Â Â Â Â Â Â Â Â device_map=\"auto\",Â  # automatically makes use of all GPUs available to the Actor\nÂ Â Â Â Â Â Â Â )\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Serving in Ray happens in actors, in this case, one called PredictDeployment. This code shows the __init__ method of the action that downloads the model from Hugging Face. To launch the model on the current node, we simply do: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7jOn5zhnrYvHQZg3fqmwjE","type":"Entry","createdAt":"2023-04-07T18:39:26.501Z","updatedAt":"2023-04-07T18:47:32.494Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"serve-cmd","body":"deployment = PredictDeployment.bind(model_id=model_id, revision=revision)\nserve.run(deployment)\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"That starts a service on port 8000 of the local machine.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We can now query that service using a few lines of Python","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6HQg2dtvaXrNsKuUvSIX3S","type":"Entry","createdAt":"2023-04-07T18:41:24.773Z","updatedAt":"2023-04-07T18:47:42.594Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"serve-query","body":"import requests\nprompt = (\n    âOnce upon a time, there was a horse. â\n)\nsample_input = {\"text\": prompt}\noutput = requests.post(\"http://localhost:8000/\", json=[sample_input]).json()\nprint(output)","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"And sure enough, this prints out a continuation of the above opening. Each time it runs, there is something slightly different.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\"Once upon a time, there was a horse.","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"But this particular horse was too big to be put into a normal stall. Instead, the animal was moved into an indoor pasture, where it could take a few hours at a time out of the stall. The problem was that this pasture was so roomy that the horse would often get a little bored being stuck inside. The pasture also didnât have a roof, and so it was a good place for snow to accumulate.\"","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is certainly an interesting direction and story â¦ but now we want to set it in the medieval era. What can we do? ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Fine Tuning Your LLM","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now that weâve shown how to serve a model, how do we fine-tune it to be more medieval? What about if we train it on 2500 lines from Shakespeare?Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is where ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.deepspeed.ai/"},"content":[{"nodeType":"text","value":"DeepSpeed","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" comes in. DeepSpeed is a set of optimized algorithms for training and fine-tuning networks. The problem is that DeepSpeed doesnât have an orchestration layer. This is not so much of a problem on a single machine, but if you want to use multiple machines, this typically involves a bunch of bespoke ssh commands, complex managed keys, and so on.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is where Ray can help.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html"},"content":[{"nodeType":"text","value":"page","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" in the Ray documentation discusses how to fine-tune it to sound more like something from the 15th century with a bit of flair.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Letâs go through the key parts. First, we load the data from hugging face","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12gknw4CuwTUwzyVwR5MEH","type":"Entry","createdAt":"2023-04-07T18:42:15.242Z","updatedAt":"2023-04-07T18:47:51.606Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hf-load-data","body":"from datasets import load_dataset\nprint(\"Loading tiny_shakespeare dataset\")\ncurrent_dataset = load_dataset(\"tiny_shakespeare\")\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Skipping the tokenization code, hereâs the heart of the code that we will run for each worker.Â ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5633wVi5PVmd43ciiJRLrv","type":"Entry","createdAt":"2023-04-07T18:43:13.431Z","updatedAt":"2023-04-07T18:48:00.199Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hf-trainer","body":"def trainer_init_per_worker(train_dataset, eval_dataset=None,**config):\nÂ Â Â Â # Use the actual number of CPUs assigned by Ray\nÂ Â Â Â model = GPTJForCausalLM.from_pretrained(model_name, use_cache=False)\nÂ Â Â Â model.resize_token_embeddings(len(tokenizer))\nÂ Â Â Â enable_progress_bar()\nÂ Â Â Â metric = evaluate.load(\"accuracy\")\nÂ Â Â Â trainer = Trainer(\nÂ Â Â Â Â Â Â Â model=model,\nÂ Â Â Â Â Â Â Â args=training_args,\nÂ Â Â Â Â Â Â Â train_dataset=train_dataset,\nÂ Â Â Â Â Â Â Â eval_dataset=eval_dataset,\nÂ Â Â Â Â Â Â Â compute_metrics=compute_metrics,\nÂ Â Â Â Â Â Â Â tokenizer=tokenizer,\nÂ Â Â Â Â Â Â Â data_collator=default_data_collator,\nÂ Â Â Â )\nÂ Â Â Â return trainer\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\nAnd now we create a Ray AIR HuggingFaceTrainer that orchestrates the distributed run and wraps around multiple copies of the training loop above: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"Qo7c20UkHRTIAgy2zvQ4G","type":"Entry","createdAt":"2023-04-07T18:43:58.073Z","updatedAt":"2023-04-07T18:48:09.302Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hf-trainer2","body":"trainer = HuggingFaceTrainer(\nÂ Â Â Â trainer_init_per_worker=trainer_init_per_worker,\nÂ Â Â Â trainer_init_config={\nÂ Â Â Â Â Â Â Â \"batch_size\":16,Â  # per device\nÂ Â Â Â Â Â Â Â \"epochs\":1,\nÂ Â Â Â },\nÂ Â Â Â scaling_config=ScalingConfig(\nÂ Â Â Â Â Â Â Â num_workers=num_workers,\nÂ Â Â Â Â Â Â Â use_gpu=use_gpu,\nÂ Â Â Â Â Â Â Â resources_per_worker={\"GPU\":1,\"CPU\": cpus_per_worker},\nÂ Â Â Â ),\nÂ Â Â Â datasets={\"train\": ray_datasets[\"train\"],\"evaluation\": ray_datasets[\"validation\"]},\nÂ Â Â Â preprocessor=Chain(splitter, tokenizer),\n)\nresults = trainer.fit()\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"While there is some complexity here, it is not much more complex than the code to get it to run on a simple machine. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Fine-tuning and Performance","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"One of the most important topics related to LLMs is the question of cost. In this particular case, the costs are small (in part because we ran only one epoch of fine-tuning, depending on the problem 1-10 epochs of fine-tuning are used, and also in part because this dataset is not so large). But running the tests on different configurations shows us that understanding performance is not always easy. The below shows some benchmarking results with different configurations of machines on AWS. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3k8l58AKMayxIzIhrO7Btr","type":"Entry","createdAt":"2023-04-07T14:39:44.461Z","updatedAt":"2023-04-07T14:39:44.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"fine-tune-performance","body":"\n| Configuration| #instances| Time (mins)| Total Cost (on-demand)|Total Cost (spot)| Cost Ratio|\n| ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n| 16 x g4dn.4xlarge (1 x T4 16GB GPU)|16|48|$15.41|__$6.17__|100%|\n| 32 x g4dn.4xlarge (1 x T4 16GB GPU)|32|__30__|$19.26|$7.71|125%|\n| 1 x p3.16xlarge (8 x V100 16GB GPU)|1|44|$17.95|$9.27|150%|\n| 1 x g5.48xlarge (8 x A10G 24GB GPU)|1|84|$22.81|$10.98|178%|\n"}}},"content":[]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3C7MbWRZ8oYJoE00IW4mIi","type":"Asset","createdAt":"2023-04-11T16:45:53.578Z","updatedAt":"2023-04-11T16:45:53.578Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"llm-graph","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3C7MbWRZ8oYJoE00IW4mIi/93f686b238a84d2ef64abe3aa7670791/Screenshot_2023-04-11_at_12.44.46_PM.png","details":{"size":76511,"image":{"width":1047,"height":644}},"fileName":"Screenshot 2023-04-11 at 12.44.46 PM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Note:","marks":[{"type":"underline"}],"data":{}},{"nodeType":"text","value":" we tried to run the same test with A100s, but we were unable to obtain the p4d machines to do so.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Looking at these numbers, we see some surprises:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Perhaps the most obvious machine to use â the g5.48xlarge â the machine with the highest on-paper performance â is both the most expensive and the slowest at almost twice the price when using spot instances.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The p3.16xlarge with its use of NVLink between the GPUs is a considerably better option.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Most surprising of all, using multiple machines is both the ","marks":[],"data":{}},{"nodeType":"text","value":"cheapest ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"and the ","marks":[],"data":{}},{"nodeType":"text","value":"fastest ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"option.Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The exact same code is running on all the machines, and aside from tweaking the number of GPU workers, nothing else was changed. Using multiple machines gave us the cheapest (16 machines) and the fastest (32 machines) option of the ones we benchmarked.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is the beauty and power of Ray. The code itself was simple enough, and in fact, was able to use a standard library âÂ  DeepSpeed â with no modifications. So it was no more complex in this case than a single machine. Simultaneously, it gave more options and flexibility to optimize to be both cheaper and faster than a single machine.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Closing the loop: Serving the fine-tuned model","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now that we have a fine-tuned model, letâs try to serve it. The only change we need to make is to (a) copy the model to s3 from the fine-tuning process and (b) load it from there. In other words, the only change from the previous code we started with originally is: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"202WfToEkr4LafSdvnynbx","type":"Entry","createdAt":"2023-04-07T18:45:09.318Z","updatedAt":"2023-04-07T18:48:20.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"load-model","body":"        checkpoint = Checkpoint.from_uri(\n             \"s3://demo-pretrained-model/gpt-j-6b-shakespeare\"\n        )\n        with checkpoint.as_directory() as dir:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                dir,\n                torch_dtype=torch.float16,\n                low_cpu_mem_usage=True,\n                device_map=\"auto\")\n            self.tokenizer = AutoTokenizer.from_pretrained(dir)\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"And now letâs try querying it again:Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Once upon a time there was a horse. This horse was in my youth, a little unruly, but yet the best of all. I have, sir; I know every horse in the field, and the best that I have known is the dead. And now I thank the gods, and take my leave.","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"As you can see, it definitely has more of a Shakespearean flavor.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Conclusion","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We have shown a new tech stack that combines Ray, HuggingFace, DeepSpeed, and PyTorch to make a system that:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Makes it simple and quick to deploy as a service.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Can be used to cost-effectively fine-tune and is actually most cost-effective when using multiple machines without the complexity.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"How fine-tuning â even a single epoch â can change the output of a trained model.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Deploying a fine-tuned model is only marginally harder than deploying a standard one.Â ","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Next Steps","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If you want to use LangChain + Ray to serve LLM's, see our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"nodeType":"text","value":"LangChain blog series","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in learning more about Ray, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io"},"content":[{"nodeType":"text","value":"Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://docs.ray.io"},"content":[{"nodeType":"text","value":"Docs.Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"To connect with the Ray community join #LLM on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"Ray Slack","marks":[],"data":{}}]},{"nodeType":"text","value":" or our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"Discuss forum","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in our Ray hosted service for ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/model-serving"},"content":[{"nodeType":"text","value":"ML Training and Serving","marks":[],"data":{}}]},{"nodeType":"text","value":", see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://www.anyscale.com/platform"},"content":[{"nodeType":"text","value":"Anyscale.com/Platform","marks":[],"data":{}}]},{"nodeType":"text","value":" and click the 'Try it now' button","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Summit 2023:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"nodeType":"text","value":"Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"nodeType":"text","value":"practical training focused on LLMs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"19zr72hDLSKFt8vQMz3hb6","type":"Asset","createdAt":"2023-04-11T00:38:37.097Z","updatedAt":"2023-04-11T00:38:37.097Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"fine-tune-stack","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/19zr72hDLSKFt8vQMz3hb6/fd9f6b83a9fe5b66456ae54ecf9bb04d/fine-tune-stack.png","details":{"size":344489,"image":{"width":1716,"height":1180}},"fileName":"fine-tune-stack.png","contentType":"image/png"}}}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70ZthWUkgA42DqmZ1GVmuM","type":"Entry","createdAt":"2022-06-15T16:41:59.066Z","updatedAt":"2022-06-15T16:43:47.038Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"blog-types-tags","body":"This section is used to order the \"Types\" and \"Tags\" that show up for filters on the Blog Index","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"56lORqEsxxZXgpuGzAhJBC","type":"Entry","createdAt":"2022-06-15T16:42:23.797Z","updatedAt":"2022-06-15T16:44:24.730Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"list"}},"locale":"en-US"},"fields":{"title":"Types","identifier":"blog-type-filters","items":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42F9iHs5a05rRrOuqEQwvB","type":"Entry","createdAt":"2022-06-15T01:30:47.322Z","updatedAt":"2022-06-15T01:30:47.322Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Culture","slug":"culture","tintColor":"#39C8E5"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KeH0JzrIEpuheazhMAhKf","type":"Entry","createdAt":"2022-06-15T01:31:23.950Z","updatedAt":"2022-06-15T01:31:23.950Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"User Story","slug":"user-story","tintColor":"#6D59C6"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1fDHWgr5HgjPURy6aaDlnB","type":"Entry","createdAt":"2022-06-15T16:42:41.243Z","updatedAt":"2022-06-22T15:37:31.744Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"list"}},"locale":"en-US"},"fields":{"title":"Products / Libraries","identifier":"blog-tag-filters","items":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i6fp6Gfdy5wFeDDOaKcfh","type":"Entry","createdAt":"2021-12-03T22:33:42.248Z","updatedAt":"2021-12-03T22:33:42.248Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Anyscale","identifier":"anyscale"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}}]}}]}}],"recommendations":[],"articles":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4fUXNiWIPmVK9q6xW4YWV0","type":"Entry","createdAt":"2023-06-13T14:22:00.442Z","updatedAt":"2023-06-13T14:22:32.590Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Ray 2.5 features training and serving for LLMs, Multi-GPU training in RLlib, and enhanced Ray Data support","seoTitle":"Ray 2.5 features training and serving for LLMs, Multi-GPU training in RLlib, and enhanced Ray Data support","slug":"ray-2-5-features-training-and-serving-for-llms-multi-gpu-training-in-rllib","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}}],"publishedDate":"2023-06-13","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.5.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2. 5 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"release features focus on a number of enhancements and improvements across the Ray ecosystem. In this blog, we expound on a few key features, including:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Support for training LLMs with Ray Train","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ability to serve LLMs with Ray Serve","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Multi-GPU learner stack in RLlib for cost efficiency and scalable RL-agent trainingÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Performant and improved approach to batch inference at scale","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2ty9UoIKnnNnrvsudSmAvH","type":"Asset","createdAt":"2023-06-12T22:29:43.226Z","updatedAt":"2023-06-12T22:29:43.226Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray_2.5_main_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2ty9UoIKnnNnrvsudSmAvH/d42b6c80596baf3b1cd13103bdc97a55/image1.png","details":{"size":158300,"image":{"width":960,"height":540}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Improved support for LLMs in Ray Train","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"This release comes with a couple key features for improving LLM support in ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/train.html"},"content":[{"data":{},"marks":[],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed Checkpointing for distributed models: ","nodeType":"text"},{"data":{},"marks":[],"value":"With the recent influx of LLMs, weâve noticed that there has been a lack of support across different frameworks for managing large model checkpoints.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"One common workaround is to gather the entire model checkpoint onto a single worker, before uploading it to some cloud storage. This introduces two problems (see Figure 1): ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"An extra step of communication bottleneck by the bandwidth of a single node.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Can lead to out of memory (OOM) issues for sufficiently large models during gathering of model states.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this release, weâre introducing a new experimental feature for supporting large model checkpoints that resolves these problems.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6HXGTDK9EABlzG19g3DmIX","type":"Asset","createdAt":"2023-06-12T22:34:17.334Z","updatedAt":"2023-06-12T22:34:17.334Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_2.5_release_fig_1","description":"Figure 1. Single node uploading the full checkpoint after gathering from all workers","file":{"url":"//images.ctfassets.net/xjan103pcp94/6HXGTDK9EABlzG19g3DmIX/9ec4291d7df025c2220da44789dcaeb3/image2.png","details":{"size":185446,"image":{"width":1999,"height":1017}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In model parallel training workloads, different partitions of a model are held by different workers, in contrast to data parallel training workloads, where the same model is replicated across different workers. To support proper checkpointing of distributed models, Ray Train can now be configured to save different partitions of the model held by each worker and upload its respective partitions directly to cloud storage.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6WdvLOMIcwsoDsro3P9362","type":"Asset","createdAt":"2023-06-12T22:36:11.421Z","updatedAt":"2023-06-12T22:36:11.421Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_2.5_figure_2","description":"Figure 2. Individual worker nodes uploading their respective checkpoints\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/6WdvLOMIcwsoDsro3P9362/eefd86d37bfb5779946e56f9dbbba6bb/image3.png","details":{"size":225462,"image":{"width":1999,"height":1042}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To use this feature, enable cloud storage, then include ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"_checkpoint_keep_all_ranks","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"_checkpoint_upload_from_workers","nodeType":"text"},{"data":{},"marks":[],"value":" as part of ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/ray-air/api/doc/ray.air.RunConfig.html#ray-air-runconfig"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RunConfig","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". This feature will work for the following trainer APIs:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.TorchTrainer.html"},"content":[{"data":{},"marks":[],"value":"TorchTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/api/doc/ray.train.huggingface.AccelerateTrainer.html#ray.train.huggingface.AccelerateTrainer"},"content":[{"data":{},"marks":[],"value":"AccelerateTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (with deepspeed and FSDP integrations)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/api/doc/ray.train.huggingface.TransformersTrainer.html#ray.train.huggingface.TransformersTrainer"},"content":[{"data":{},"marks":[],"value":"TransformersTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (with deepspeed and FSDP integrations)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Note","nodeType":"text"},{"data":{},"marks":[],"value":": This feature should only be turned on if your training loop is configured to save the sharded model state per worker. For example, when using ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"TransformersTrainer","nodeType":"text"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"AccelerateTrainer","nodeType":"text"},{"data":{},"marks":[],"value":" with deepspeed, the ","nodeType":"text"},{"data":{"uri":"https://deepspeed.readthedocs.io/en/latest/zero3.html#deepspeed.runtime.zero.config.DeepSpeedZeroConfig.gather_16bit_weights_on_model_save"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"gather_16bit_weights_on_model_save deepspeed configuration","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" should be set to False. See the example below for a skeleton of what your training script should look like:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UnXTMYsWcyYWDzEfohDzo","type":"Entry","createdAt":"2023-06-12T22:40:59.358Z","updatedAt":"2023-06-12T22:40:59.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_2.5_release_code_snippet_1","body":"def trainer_init_per_worker(train_dataset, eval_dataset=None, **config):\n    deepspeed = {\n        ...,\n        \"zero_optimization\": {\n            # Configure deepspeed to save checkpoint shards.\n            \"gather_16bit_weights_on_model_save\": False,\n            ...\n        }\n    }\n    training_args = TrainingArguments(\n        ...,\n        deepspeed=deepspeed,\n    )\n    trainer = Trainer(..., args=training_args)\n    return trainer\n\ntrainer = TransformersTrainer(\n    trainer_init_per_worker=trainer_init_per_worker,\n    scaling_config=ScalingConfig(num_workers=4),\n    run_config=RunConfig(\n        # Requirement: Use cloud storage\n        # Your checkpoints will be found within \"s3://your-s3-bucket/example\"\n        storage_path=\"s3://your-s3-bucket\",\n        name=\"example\",\n        checkpoint_config=CheckpointConfig(\n            _checkpoint_keep_all_ranks=True,\n            _checkpoint_upload_from_workers=True,\n        ),\n    )\n    datasets=...\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"For other supported trainers, we plan to write full-fledged examples showing their distributed checkpoint configuration in the documentation shortly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"LightningTrainer FSDP support: ","nodeType":"text"},{"data":{},"marks":[],"value":"In ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/announcing-ray-2-4-0-infrastructure-for-llm-training-tuning-inference-and"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2.4,","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" we released alpha support for the LightningTrainer. After user feedback, weâve introduced support for ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/dolly_lightning_fsdp_finetuning.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FSDP","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"LightningTrainer","nodeType":"text"},{"data":{},"marks":[],"value":", and an example can be found ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/dolly_lightning_fsdp_finetuning.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"HuggingFace Trainer renaming: ","nodeType":"text"},{"data":{},"marks":[],"value":"In this release, for naming consistency and logical modularity, we are also renaming the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"HuggingFaceTrainer","nodeType":"text"},{"data":{},"marks":[],"value":" to ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"TransformersTrainer","nodeType":"text"},{"data":{},"marks":[],"value":", and we are also moving the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"AccelerateTrainer","nodeType":"text"},{"data":{},"marks":[],"value":" into the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"HuggingFace","nodeType":"text"},{"data":{},"marks":[],"value":" package, so that we can have a more intuitive organization of these integrations. For example,","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DMVT7rhes70fG1fDO9XT3","type":"Entry","createdAt":"2023-06-12T22:42:51.134Z","updatedAt":"2023-06-12T22:42:51.134Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_2.5_release_code_snippet_2","body":"from ray.train.huggingface import AccelerateTrainer, TransformersTrainer","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve for serving LLMs","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We have added two experimental features that augment the use of Ray Serve for online batch inference for streaming responses and model multiplexing for load balancing and serving multiple models across multiple replicas.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Streaming Response","nodeType":"text"},{"data":{},"marks":[],"value":": Some applications, in particular text generation in large language models (LLMs) or video processing, require return of incremental results to the caller. For instance, in the case of LLMs or large neural networks, a full forward pass could take multiple seconds, so providing incremental results offers a better user experience.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can achieve returning a ","nodeType":"text"},{"data":{"uri":"https://www.starlette.io/responses/#streamingresponse"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"StreamingResponse","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" from your HTTP request by wrapping a Python generator in your HTTP handler. Supported in basic HTTP ingress deployments in ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/serve/http-guide.html#serve-fastapi-http"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FastAPI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", the code snippet below shows how to.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7z6Hpu3CoSVOQudOmSNPU2","type":"Entry","createdAt":"2023-06-12T22:47:47.404Z","updatedAt":"2023-06-12T22:47:47.404Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_2.5_release_code_snippet_3","body":"import time\nfrom typing import Generator\n\nimport requests\nfrom starlette.responses import StreamingResponse\nfrom starlette.requests import Request\n\nfrom ray import serve\n@serve.deployment\nclass StreamingResponder:\n    def generate_numbers(self, max: int) -\u003e Generator[str, None, None]:\n        for i in range(max):\n            yield str(i)\n            time.sleep(0.1)\n\n    def __call__(self, request: Request) -\u003e StreamingResponse:\n        max = request.query_params.get(\"max\", \"25\")\n        gen = self.generate_numbers(int(max))\n        return StreamingResponse(gen, status_code=200, media_type=\"text/plain\")\n\nserve.run(StreamingResponder.bind())\n\nr = requests.get(\"http://localhost:8000?max=10\", stream=True)\nstart = time.time()\nr.raise_for_status()\nfor chunk in r.iter_content(chunk_size=None, decode_unicode=True):\n    print(f\"Got result {round(time.time()-start, 1)}s after start: '{chunk}'\")","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This short snippet yields the following streaming response:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38k4c6oQO3N8jvh1k3VScN","type":"Entry","createdAt":"2023-06-12T22:48:47.801Z","updatedAt":"2023-06-12T22:48:47.801Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_2.5_release_code_snippet_4","body":"â¦\nGot result 0.0s after start: '0'\nGot result 0.1s after start: '1'\nGot result 0.2s after start: '2'\nGot result 0.3s after start: '3'\nGot result 0.4s after start: '4'\nGot result 0.5s after start: '5'\nGot result 0.6s after start: '6'\nGot result 0.7s after start: '7'\nGot result 0.8s after start: '8'\nGot result 0.9s after start: '9'\n(ServeReplica:default_StreamingResponder pid=41052) INFO 2023-05-25 10:49:52,230 default_StreamingResponder default_StreamingResponder#qlZFCa yomKnJifNJ / default replica.py:634 - __CALL__ OK 1017.6ms\n","language":"shell"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Model Multiplexing: ","nodeType":"text"},{"data":{},"marks":[],"value":"A common use case we observe among ML practitioners is deploying multiple models that have dissimilar model shapes. For example, a different network architecture is trained for a particular SKU, user_id, or geo-location but takes similar inputs and produces a respective output. The multiple models are deployed across a pool of replicas among which requests are load balanced. When a request arrives, depending on the request header that contains model id such as SKU, user_id, or zip_code, the request is routed to the right and respective model replica.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For brevity we refer you to an example in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/serve/model-multiplexing.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" of how to write a multiplexed deployment for the above mentioned use case.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Multi-GPU stack for cost efficient, scalable, Multi-GPU RL agents training","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The training of reinforcement learning (RL) agents is hindered by the sampling process, which acts as the main bottleneck. While sampling can be distributed across multiple compute nodes as RolloutWorkers and simulators, the training phase is restricted to a single node. Consequently, the number of GPUs available for training is limited to a single GPU. This again creates another bottleneck on the batch size that can be effectively trained due to the memory constraints of a single GPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1erBs3bcBldQrmMcIGp0rK","type":"Asset","createdAt":"2023-06-12T22:52:56.639Z","updatedAt":"2023-06-12T22:52:56.639Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_2.5_release_figure_3","description":"Figure 3. Challenges and solutions for RLlib data collection and training ","file":{"url":"//images.ctfassets.net/xjan103pcp94/1erBs3bcBldQrmMcIGp0rK/327dd22c43747c4566c57762d2ba06ee/image4.png","details":{"size":106554,"image":{"width":1204,"height":900}},"fileName":"image4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In RLlib we introduce a multi-node, multi-gpu training stack that addresses both the challengesÂ  and bottlenecks shown in Figure 3. With this new stack we can combine different types of GPUs to reduce costs by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"1.7x.","nodeType":"text"},{"data":{},"marks":[],"value":" ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In an upcoming blog, we detail implementation and experimentation showing RLlib's Proximal Policy Optimization (PPO) implementation on the ","nodeType":"text"},{"data":{"uri":"https://gymnasium.farama.org/environments/atari/breakout/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"ALE/Breakout-V5","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" environment on the new multi GPU training stack, using an increasing number of GPUs and larger batch sizes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performant and improved batch inference","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"One common and imperative workload that requires efficiency and optimized usage of hardware acceleratorsâboth CPUs and GPUsâis batch inference. In the 2.4 Ray release, we introduced ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/streaming-distributed-execution-across-cpus-and-gpus"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data streaming execution","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" mode, which allows saturation of CPUs and GPUs for workloads such as offline ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/offline-batch-inference-comparing-ray-apache-spark-and-sagemaker"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Further improving Ray Data in this release, Ray Data provides additional enhancements. For instance, a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"strict mode ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"is enabled by default. This means that schemas are required for all Datasets, and standalone Python objects are no longer supported. Together with benefits from simplification, this also aligns the Ray Data API closer to industry-standard distributed data APIs like Apache Spark and emerging standards for machine learning datasets like HuggingFace.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Also, the default batch format is fixed to ","nodeType":"text"},{"data":{"uri":"https://numpy.org/"},"content":[{"data":{},"marks":[],"value":"NumPy","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", giving better performance for ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", along with the support of concurrent actors for ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ActorPool ","nodeType":"text"},{"data":{},"marks":[],"value":"helps too.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2SGa5CwRAvIGWOCpBR8Ha6","type":"Entry","createdAt":"2023-06-12T22:58:25.253Z","updatedAt":"2023-06-12T22:59:30.124Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_2.5_release_code_snippet_5","body":"from typing import Dict\nimport numpy as np\n\nimport ray\n\n# Step 1: Create a Ray Dataset from in-memory Numpy arrays.\n# You can also create a Ray Dataset from many other sources and file\n# formats.\nds = ray.data.from_numpy(np.asarray([\"Complete this\", \"for me\"]))\n\n# Step 2: Define a Predictor class for inference.\n# Use a class to initialize the model just once in `__init__`\n# and re-use it for inference across multiple batches.\nclass HuggingFacePredictor:\n    def __init__(self):\n        from transformers import pipeline\n        # Initialize a pre-trained GPT2 Huggingface pipeline.\n        self.model = pipeline(\"text-generation\", model=\"gpt2\")\n\n    # Logic for inference on 1 batch of data.\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e Dict[str, list]:\n        # Get the predictions from the input batch.\n        predictions = self.model(list(batch[\"data\"]), max_length=20, num_return_sequences=1)\n        # `predictions` is a list of length-one lists. For example:\n        # [[{'generated_text': 'output_1'}], ..., [{'generated_text': 'output_2'}]]\n        # Modify the output to get it into the following format instead:\n        # ['output_1', 'output_2']\n        batch[\"output\"] = [sequences[0][\"generated_text\"] for sequences in predictions]\n        return batch\n\n# Use 2 parallel actors for inference. Each actor predicts on a\n# different partition of data.\nscale = ray.data.ActorPoolStrategy(size=2)\n# Step 3: Map the Predictor over the Dataset to get predictions.\npredictions = ds.map_batches(HuggingFacePredictor, compute=scale)\n# Step 4: Show one prediction output.\npredictions.show(limit=1)\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"With each release of Ray, we strive toward ease of use, performance, and stability.Â This release marched towards that end by:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"extending Ray Train functionality to support distributed checkpointing for large language models","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"enhancing user experience in Ray Serve by returning HTTP streaming response to HTTP input requestsÂ Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"extending Ray Serve functionality for multi-model serving by multiplexing among replicas of dissimilar shaped model architectures but similar input data types","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"solving bottlenecks and challenges in RLlib agent training by introducing a new multi-gpu, multi-node training stack for RLlib","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"improving easy use of Ray Data for batch inferenceÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"We want to thank all contributors for their valuable contributions to this new release of ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.5.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2.5","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Your enduring support continues to foster the wider use of Ray adoption.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Have a go at the latest release with pip install âray[default]â and let us know of your feedback. Weâre always delighted to share new Ray releases with you and equally interested to hear your feedback â feel free to reach out to us on ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Github","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discuss","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Join our ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/community"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Community","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the ","nodeType":"text"},{"data":{"uri":"https://forms.gle/9TSdDYUgxYs8SA9e8"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray #LLM slack channel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have our ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit 2023 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"early-bird registration open until 6/30. Secure your spot, save some money, savor the community camaraderie at the summit.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Whatâs Next?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stay tuned for additional Ray 2.5 related blogs on RLlib, meanwhile take a peek at the following blogs:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/announcing-aviary-open-source-multi-llm-serving-solution"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Announcing Aviary: Open Source Multi-LLM Serving","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/streaming-distributed-execution-across-cpus-and-gpus"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Streaming distributed execution across CPUs and GPUs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/offline-batch-inference-comparing-ray-apache-spark-and-sagemaker"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2ty9UoIKnnNnrvsudSmAvH","type":"Asset","createdAt":"2023-06-12T22:29:43.226Z","updatedAt":"2023-06-12T22:29:43.226Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray_2.5_main_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2ty9UoIKnnNnrvsudSmAvH/d42b6c80596baf3b1cd13103bdc97a55/image1.png","details":{"size":158300,"image":{"width":960,"height":540}},"fileName":"image1.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}}],"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1rOzWuYiNWHf0G13J8y8mS","type":"Entry","createdAt":"2023-05-10T14:58:16.356Z","updatedAt":"2023-05-11T15:12:46.057Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"10 must-attend Ray Summit sessions: Generative AI, scalable ML workloads, and more","seoTitle":"10 must-attend Ray Summit sessions: Generative AI, Reinforcement learning, scalable ML workloads, and more","slug":"10-must-attend-ray-summit-sessions-generative-ai-scalable-ml-workloads-and","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2ZQcscC4cFydZNkAjQf0Cp","type":"Entry","createdAt":"2020-09-14T19:20:26.098Z","updatedAt":"2020-09-14T19:20:26.098Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Ben Lorica","slug":"ben-lorica","link":"https://twitter.com/bigdata"}}],"publishedDate":"2023-05-10","intro":"*This yearâs Ray Summit has it all: developer deep dives, Ray use cases across industries, Ray in production, scalable AI/ML workloads, Generative AI and more. Below are just a few of the sessions weâre looking forward to attending this year.*\n","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The Ray Summit brings together the Ray community to delve into all aspects of AI and beyond on the Ray platform, while providing opportunities to create, learn, and discover. This year's event marks our second in-person gathering, featuring an impressive lineup of keynote speakers, breakout sessions, and lightning talks.Â ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5YwMumAO7WAOwBjX6VgFzW","type":"Asset","createdAt":"2023-05-09T20:36:06.506Z","updatedAt":"2023-05-09T20:36:06.506Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray_summit_2023_main_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5YwMumAO7WAOwBjX6VgFzW/b982aaf769deb7b4ef98639a67d058e4/Screen_Shot_2023-05-09_at_1.35.20_PM.png","details":{"size":2759902,"image":{"width":1940,"height":1680}},"fileName":"Screen Shot 2023-05-09 at 1.35.20 PM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The program encompasses a wide range of topics, such as developer-focused Ray-related sessions, Ray applications across different industries, practical Ray deployment strategies for constructing scalable machine learning platforms, and Generative AI sessions, all designed to cater to the diverse needs of AI/ML practitioners.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/agenda"},"content":[{"nodeType":"text","value":"full agenda","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" ð is posted, and here are just a few of the sessions that Ben and I are looking forward to attending this year:","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"ð£ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/agenda/sessions/282"},"content":[{"nodeType":"text","value":"Solving Generative AI challenges with Ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":": Anyscale senior staff software engineer Jun Gong will focus on Ray's suitability for large-scale generative AI model training and the infrastructural challenges it solves. He will cover recent improvements to the platform that make it easier and more effective for the community to train and serve large generative models. If you want to gain insight into Ray's capabilities for powering large language models that are vital to the Generative AI field, you may want to attend this session.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"ð£ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/agenda/sessions/173"},"content":[{"nodeType":"text","value":"How Spotify Built a Robust Ray Platform with a Frictionless Developer Experience","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":":Â  Spotify ML Platform engineers Keshi Dai and David Xia will present how their team improved the developer experience by simplifying access to computational resources and streamlining the coding process. Their presentation will highlight improvements to Spotify's platform, such as reliability, scalability, performance, and cost-efficiency.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"ð£ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/agenda/sessions/220"},"content":[{"nodeType":"text","value":"Forecasting Covid Infections for the UK's National Health Service using Ray and Kubernetes","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":": During the COVID pandemic, the UK's National Health Service (NHS) used AI to address the risk of hospitals not having enough capacity. They re-architected the system and employed Ray Core and Kuberay to create a highly stable architecture. The talk aims to give an overview of this integration, which complemented the AWS Karpenter autoscaler to make a powerful platform for Bayesian modeling. Alex Remedios, a software engineer, will share how they did it with Ray.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"ð£ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/agenda/sessions/158"},"content":[{"nodeType":"text","value":"Scaling Computer Vision Models with Ray: A Cost-Effective and Efficient Distributed Training Framework","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":": David Scott of Instacart will discuss the implementation of a computer vision model using the Ray open-source framework with Kubeflow, another popular distributed training framework. He evaluates the cost effectiveness, training speed, GPU utilization, and throughput of training the machine learning model using both frameworks.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"ð£ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/agenda/sessions/188"},"content":[{"nodeType":"text","value":"Ray breaks the $1/TB barrier as the worldâs most cost-efficient sorting system","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":": Frank Luan, PhD at UC Berkeley will share how The Sky Computing Lab at UC Berkeley developed Exoshuffle, a new architecture for building distributed shuffle that achieved high performance. Building upon this architecture and using Ray, the team set a new world record on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://sortbenchmark.org/2014_06_CloudSort_v_0_4.pdf"},"content":[{"nodeType":"text","value":"CloudSort benchmark","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", sorting 100TB of data on the public cloud for only $97 worth of cloud resources or $0.97 per terabyte. This is 33% more cost-efficient than the previous record set by Apache Spark in 2016, and 15% cheaper when factoring in decreasing hardware costs.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"ð£ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/agenda/sessions/247"},"content":[{"nodeType":"text","value":"From Apache Spark to Ray: An Exabyte-Scale Production Migration Case Study","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":": Patrick Ames, principal software engineer, will discuss a case study about Amazon's successful transition of exabyte-scale data catalog management jobs from Apache Spark to Ray. Patrick will revisit key milestones, challenges faced, and concessions made during the transition, including future plans to incorporate Ray more deeply into critical batch and streaming business intelligence pipelines at Amazon.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"ð£ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/agenda/sessions/135"},"content":[{"nodeType":"text","value":"Serving Large Language Models with KubeRay on TPUs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":": Messrs Richard Liu and Winston Chiang, of Google, will discuss the growing popularity of large language models (LLMs) and the challenges in serving them due to their massive computational resources. They introduce ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/kuberay"},"content":[{"nodeType":"text","value":"KubeRay ","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"on TPUs as a solution to improve the performance of LLMs.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"ð£ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/agenda/sessions/154"},"content":[{"nodeType":"text","value":"Heterogeneous Training Cluster with Ray at Netflix","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":": In this talk, ML engineers Pablo Delgado andÂ  Lingyi Lui, will discuss the increased computational resources required to train complex deep learning models and explores the benefits of using Ray for building a training cluster with a mix of CPU and GPU instances. TheyâllÂ  cover the steps involved in setting up such a cluster and demonstrate how to run distributed training jobs using Ray's automatic resource allocation and management for scheduling different types of workers.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"ð£ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/agenda/sessions/172"},"content":[{"nodeType":"text","value":"Supercharging self-driving algorithms development with Ray: scaling simulation workloads and democratizing autotuning at Zoox:","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" Messrs Yunpeng Pan and Ritwik Bera will share how Zoox has created an autotuning platform that speeds up algorithm development through distributed simulation and metrics evaluation. The Zook ML team utilized Ray to scale simulation and metrics workloads and enable developers to improve autonomous driving without modifying the code. Their talk will cover their autotuning process and demonstrate how Ray helped scale simulation and metrics workloads at Zoox.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"ð£ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/agenda/sessions/146"},"content":[{"nodeType":"text","value":"Modernizing DoorDash Model Serving Platform with Ray Serve:","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" The DoorDash ML platform team has implemented Ray for model training and inference, and has built a new model serving platform that prioritizes flexibility and self-service. DoorDash software engineers Siddharth Kodwani","marks":[],"data":{}},{"nodeType":"text","value":" ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"and Kornel Csernai","marks":[],"data":{}},{"nodeType":"text","value":" will ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"explain how they evaluated various frameworks and found that Ray Serve's user-friendly approach was the best fit for their needs. They will detail their experience of transitioning prediction services from their previous generation to the Ray Serve ecosystem.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Thatâs just the beginning, by no means the only picks of the sessions, of what we have in store at this yearâs Ray Summit. Over the two days of the conference, youâll learn how companies such as Amazon, Ant Group, Cruise, DoorDash, IBM, Instacart, Google, Microsoft, Netflix, Spotify, Uber, Verizon, and more are building cutting-edge AI/ML platforms and applications with Ray.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Also, youâll ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/speakers"},"content":[{"nodeType":"text","value":"hear keynotes ","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"from Generative AI and ML luminaries such as Aidan Gomez, Co-founder and CEO, Cohere; Albert Greenberg, VP of Engineering, Uber; Brian McClendon, Senior Vice President of Engineering, Niantic, Inc; Robert Nishihara, Co-founder and CEO, Anyscale; John Schulman","marks":[],"data":{}},{"nodeType":"text","value":" ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Co-founder, OpenAI; Ion Stoica, Co-founder, Executive Chairman and President, Anyscale and Professor, UC Berkeley; and Ya Xu","marks":[],"data":{}},{"nodeType":"text","value":", ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"VP of Engineering, Head of Data and AI, LinkedIn.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Finally at the Raydiate Bar, we have planned enjoyable community activities and a happy hour filled with fun. ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/?utm_source=Anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=blog-jules"},"content":[{"nodeType":"text","value":"Register now","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" â early bird registration is open until","marks":[],"data":{}},{"nodeType":"text","value":"Â May 31, 2023.","marks":[{"type":"bold"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\n\n\n","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5YwMumAO7WAOwBjX6VgFzW","type":"Asset","createdAt":"2023-05-09T20:36:06.506Z","updatedAt":"2023-05-09T20:36:06.506Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray_summit_2023_main_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5YwMumAO7WAOwBjX6VgFzW/b982aaf769deb7b4ef98639a67d058e4/Screen_Shot_2023-05-09_at_1.35.20_PM.png","details":{"size":2759902,"image":{"width":1940,"height":1680}},"fileName":"Screen Shot 2023-05-09 at 1.35.20 PM.png","contentType":"image/png"}}},"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2C7QtMnASHfVdag9xYamXh","type":"Entry","createdAt":"2023-05-04T15:03:36.052Z","updatedAt":"2023-06-06T14:20:09.992Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker","seoTitle":"Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker","slug":"offline-batch-inference-comparing-ray-apache-spark-and-sagemaker","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DwU0wh6AmJoGv8JOy6n0Q","type":"Entry","createdAt":"2020-11-11T23:34:15.165Z","updatedAt":"2021-02-05T22:22:08.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","slug":"amog-kamsetty","link":"https://www.linkedin.com/in/amogkamsetty/","bio":"Amog is a software engineer at Anyscale. He helps maintain and develop two machine learning libraries: Ray Tune for distributed hyperparameter tuning and RaySGD for distributed deep learning training.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J134jX2ybZRLPfcAIGLTI","type":"Asset","createdAt":"2020-11-13T00:05:05.533Z","updatedAt":"2020-11-13T00:05:05.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J134jX2ybZRLPfcAIGLTI/12589bb0c6792051050ebc6e1e83af25/Amog_Kamsetty_.jpg","details":{"size":57660,"image":{"width":480,"height":640}},"fileName":"Amog Kamsetty .jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}}],"publishedDate":"2023-05-04","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Update 5/10: ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"Thanks to the Databricks Apache Spark developers for pointing out the prefetching flag ","nodeType":"text"},{"data":{},"marks":[{"type":"code"},{"type":"italic"}],"value":"spark.databricks.execution.pandasUDF.prefetch.maxBatches ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"available in Databricks Runtime that can be used with the Iterator API. It is also possible to implement this manually in Spark open source with background threads. With prefetching set to 4 batches, Spark reaches ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"},{"type":"italic"}],"value":"159.86 images/sec","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":". The code can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/blob/main/spark/code/torch-batch-inference-s3-10G-standard-iterator-databricks-prefetch.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"italic"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"TL;DR","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As more companies use large scale machine learning (ML) models for training and evaluation, offline batch inference becomes an essential workload. A number of challenges come with it: managing compute infrastructure; optimizing use of all heterogeneous resources; and transferring data from storage to hardware accelerators. Addressing these challenges, Ray performs significantly better as it can coordinate clusters of diverse resources, allowing for better utilization of the specific resource requirements of the workload.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we conduct a comparison of three different solutions for offline batch inference: AWS ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker Batch Transform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Our experiments demonstrate that Ray Data achieves speeds up to ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"17x ","nodeType":"text"},{"data":{},"marks":[],"value":"faster than SageMaker Batch Transform and","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" 2x","nodeType":"text"},{"data":{},"marks":[],"value":" faster than Spark for offline image classification. Ray Data also scales effectively to terabyte-sized datasets. The code for these benchmarks is publicly available and can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introduction","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46bNG3QTa1atN02rsemfe8","type":"Asset","createdAt":"2023-05-03T22:20:44.214Z","updatedAt":"2023-05-03T22:20:44.214Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_1_batch_offline_inference","description":"Figure 1. General parallel batch processing architecture\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/46bNG3QTa1atN02rsemfe8/120bf3aac26ae94257b41ac674f42e94/image3.png","details":{"size":48540,"image":{"width":960,"height":540}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Offline batch inference is a critical workload for many AI products, especially with the growing usage of pre-trained foundation models. At its core, it seems like a simple problem: given a trained model and a dataset, get model predictions for each data point. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, there are many challenges to doing this at scale:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 1: ","nodeType":"text"},{"data":{},"marks":[],"value":"Managing compute infrastructure and cloud clusters, especially when needing to use ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"heterogeneous clusters","nodeType":"text"},{"data":{},"marks":[],"value":", consisting of different instance types to maximize throughput.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 2: ","nodeType":"text"},{"data":{},"marks":[],"value":"Parallelizing data processing and utilizing all resources (CPUs \u0026 GPUs) in the cluster at any given point in time.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 3: ","nodeType":"text"},{"data":{},"marks":[],"value":"Efficiently transferring data between cloud storage, CPU RAM for preprocessing, and GPU RAM for model inference, especially for unstructured data like images and text. This has ramifications for both performance and developer velocity.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 4:","nodeType":"text"},{"data":{},"marks":[],"value":" A user experience that makes it easy to iterate and develop while working at scale.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What does the industry recommend for offline batch inference?","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The industry hasnât converged on a standard solution. ","nodeType":"text"},{"data":{"uri":"https://twitter.com/benhamner/status/1605730034450169857"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"It seems like every ML practitioner has their own favorite approach.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5hRMwuEGUS4nJ7eV5riIK9","type":"Asset","createdAt":"2023-05-03T22:23:14.590Z","updatedAt":"2023-05-03T22:23:14.590Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"tweet_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5hRMwuEGUS4nJ7eV5riIK9/05b76abcbe4ea58ae3839fd319858594/image4.png","details":{"size":177304,"image":{"width":1798,"height":588}},"fileName":"image4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We see that there are three categories of solutions that attempt to address the above challenges. However, only category three addresses all the challenges.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Batch Services: ","nodeType":"text"},{"data":{},"marks":[],"value":"Cloud providers such as AWS, GCP, and Azure provide batch services to manage compute infrastructure for you. Some newer products like ","nodeType":"text"},{"data":{"uri":"https://modal.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Modal Labs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provide a user experience layer around cloud providers to abstract away even more complexity. Regardless of which service you choose, the process is the same: you provide your code, and the service runs your code on each node in a cluster. However, while infrastructure management is necessary (Challenge #1), it is not enough. These services have limitations, such as a lack of software libraries to address Challenges #2, #3, and #4 which makes them suitable only for experienced users who can write their own optimized batch inference code.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Online Inference solutions: ","nodeType":"text"},{"data":{},"marks":[],"value":"Â Solutions like ","nodeType":"text"},{"data":{"uri":"https://www.bentoml.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Bento ML","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provide APIs to make it easy to write performant inference code and can abstract away infrastructure complexities. But they are designed for online inference rather than offline batch inferenceâtwo different problems with different sets of requirements. These solutions often donât perform well in the offline case, leading inference service providers like ","nodeType":"text"},{"data":{"uri":"https://modelserving.com/blog/unifying-real-time-and-batch-inference-with-bentoml-and-spark"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Bento ML to integrating with Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for offline inference.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed data systems:","nodeType":"text"},{"data":{},"marks":[],"value":" These solutions are designed to handle large amounts of data and are better suited to handle all three challenges listed above. They break large datasets into reasonable batches for processing and can map a given function (i.e., the model) over the dataset efficiently, effectively handling the challenges of offline batch inference at scale. Examples include ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ultimately, we have observed that Ray Data is the best practical solution for offline batch inference for modern deep learning applications, both in terms of performance and user experience.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We ran the following comparisons and experiments to corroborate the above observations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing AWS SageMaker Batch Transform, Apache Spark, and Ray Data performance and UX on an image classification use case","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling up Ray Data batch inference to 10 TB, showing 90%+ GPU utilization","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Image Classification: SageMaker Batch Transform vs. Apache Spark vs. Ray Data","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We ran an image classification task from the ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1911.02549"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLPerf Inference Benchmark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" suite in the offline setting. This benchmark uses images from the ","nodeType":"text"},{"data":{"uri":"https://image-net.org/challenges/LSVRC/2012/2012-downloads.php#Images"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"ImageNet 2012 Dataset","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and a pre-trained ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch ResNet50 model.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We timed a full end-to-end batch inference workload. With images stored in the parquet format in a public S3 bucket, the workload involved the following three steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Reading images from S3","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Simple CPU preprocessing (resizing, cropping, normalization)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Model inference on GPU","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"While all Ray experiments were run on Anyscale, Spark experiments were run on Databricks runtime v12.0, with Apache Spark 3.3.1. More details on the full configurations can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"in the repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In particular for Spark, we used two different setups:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Single-cluster","nodeType":"text"},{"data":{},"marks":[],"value":": Both the CPU preprocessing and GPU inference were done on the same cluster. We tried both versions of the PySpark Pandas UDF API: ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.pandas_udf.html#pyspark.sql.functions.pandas_udf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Series-based and Iterator-based","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Multi-cluster","nodeType":"text"},{"data":{},"marks":[],"value":": CPU preprocessing is done on one cluster, the preprocessed data is saved to a shared file system, and GPU inference is done on a different cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"All the source code for the experiments conducted above is available ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", including additional details on various other configurations that we attempted.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Â Results of throughput from experiments","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"When running with a 10 GB dataset on a single g4dn.12xlarge instance (48 CPUs, 4 GPUs), we observe the following throughput (images/sec) results for ResNet-50 batch inference:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Urc9cqOTvtI8d4w9DjiEm","type":"Asset","createdAt":"2023-05-04T00:09:19.405Z","updatedAt":"2023-05-04T00:09:19.405Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_5_table","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Urc9cqOTvtI8d4w9DjiEm/42718e584dcc4d0d16592749b0782b4f/Screen_Shot_2023-05-03_at_5.01.12_PM.png","details":{"size":91393,"image":{"width":1211,"height":541}},"fileName":"Screen Shot 2023-05-03 at 5.01.12 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The above throughput differences translate directly into cost savings as the batch inference job with Ray can run much fasterâso the faster it finishes the fewer resources are consumed and the cheaper it gets. Additionally, Ray is fully open source.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2dNPqjBTFiDZn2gcXkdir9","type":"Asset","createdAt":"2023-05-03T22:50:03.518Z","updatedAt":"2023-05-03T22:50:03.518Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 1","description":"Figure 1. Throughput difference among three different solutions for 10GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/2dNPqjBTFiDZn2gcXkdir9/f07a675dcb4f124e29aeb2f6618dbf15/image1.png","details":{"size":163062,"image":{"width":1334,"height":948}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"SageMaker Batch Transform for batch inference","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As we can see, SageMaker Batch Transform is not well suited for batch inference, both for throughput and cost.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While SageMaker handles","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" Challenge #1 ","nodeType":"text"},{"data":{},"marks":[],"value":"to some degree and abstracts away compute infrastructure management, it doesnât fully address the remaining challenges","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The low-throughput results are primarily because while SageMaker Batch Transform claims to be a ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"solution for batch inference on large datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", under the hood, it uses the same architecture as for online serving systems. It starts an HTTP server, and deploys your model as an endpoint. And then for each image, a separate request is sent to the server.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"SageMaker also does not provide support for batching across ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"multiple files","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and even if you batched your data beforehand, the max payload per request is 100 MB, nowhere near enough to fully saturate your GPUs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In terms of developer velocity, when running these benchmarks, we ran into a few additional challenges with SageMaker. In particular, the need to spin up a new cluster every time we ran a script and the difficult to parse stack traces slowed down developer iterations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While online inference solutions have their place, the additional overhead they have, as well as the inability to fully utilize cluster resources and saturate GPUs by maximizing batch sizes and effectively pipelining data make them highly inefficient for offline batch inference.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing Spark and Ray for batch inference","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Clearly, as we observed above, shoehorning online inference solutions for offline use cases is not the right approach. Letâs compare the two distributed data system approaches: Spark and Ray Data.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We see that Ray Data has","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" 2x ","nodeType":"text"},{"data":{},"marks":[],"value":"faster throughput than Spark. Letâs break down how Ray Data and Spark fare for the challenges mentioned earlier:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #2: Hybrid CPU and GPU workload","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deep learning workloads involve ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"both CPU and GPU computation","nodeType":"text"},{"data":{},"marks":[],"value":". For optimal parallelization, all the CPUs and GPUs in the cluster should be fully utilized at any given time. However, this is a limitation of Spark scheduling as all the stages are fused together, regardless of their resource requirements. This means that in the single cluster setup, the total parallelism is limited by the number of GPUs, leading to underutilization of CPUs. This also means that GPUs are completely idle during the reading and preprocessing steps.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"When using a separate CPU and GPU cluster, data needs to be persisted and orchestrated between stages. Overall, for these CPU+GPU workloads, Spark does not fully address ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"challenge #2","nodeType":"text"},{"data":{},"marks":[],"value":" and cannot utilize all resources in the cluster concurrently. We attempted to use Sparkâs stage level scheduling feature to address this issue, but were ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/tree/main/spark#stage-level-scheduling"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"unsuccessful","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6m5c6zQiMVCithTLEIazb8","type":"Asset","createdAt":"2023-05-03T23:02:01.263Z","updatedAt":"2023-05-03T23:02:01.263Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 2","description":"Figure 2. How Ray data efficiently pipelines and optimizes hardware: CPUs/GPUs","file":{"url":"//images.ctfassets.net/xjan103pcp94/6m5c6zQiMVCithTLEIazb8/1be9788a7aa54707a42f38023597ae2f/5mb.gif","details":{"size":4991804,"image":{"width":3260,"height":1182}},"fileName":"5mb.gif","contentType":"image/gif"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data, on the other hand, can independently scale the CPU preprocessing and GPU inference steps. It also streams data ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"through these stages","nodeType":"text"},{"data":{},"marks":[],"value":", increasing CPUs and GPUs utilization and reducing out-of-memory errors (OOMs), and fine-grained resource allocation for ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html#scaling-and-resource-allocation"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"autoscaling and management","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #3: Large, multi-dimensional tensors","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deep learning workloads often involve dealing with complex data types like images, audio, or video, which are represented by multi-dimensional ","nodeType":"text"},{"data":{"uri":"https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"arrays","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Having first-class support for these data types is necessary for better performance and developer experience. Ray Data has Numpy batch format as a first-class citizen, and does not require Pandas, which is suited for deep learning and leads to higher performance and more memory efficiency, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-2-2-improved-developer-experience-performance-and-stability"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"as shown in previous benchmarks","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #4: Ray is Python native","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another fundamental difference is that Ray is Python native whereas Spark is not, which also impacts performance. We ran a microbenchmark that applies a ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"time.sleep(1)","nodeType":"text"},{"data":{},"marks":[],"value":" UDF. Surprisingly, running the UDF takes ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/tree/main/spark#microbenchmark"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"80 seconds","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"; we believe due to JVM\u003c\u003ePyarrow overhead! On Ray Data, the same microbenchmark takes a little ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/blob/main/ray/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"under 5 seconds","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Not to mention, this makes Spark development more difficult. In particular when debugging, the recommended way is to include debug information in the ","nodeType":"text"},{"data":{"uri":"https://stackoverflow.com/a/57175768"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"output pandas dataframe rather than using print statements","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Even as we scale to a larger 300 GB dataset using 4 nodes and 16 GPUs, we see that these performance differences still hold:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5wDKGXZNHB2Hlt6JEMMKOt","type":"Asset","createdAt":"2023-05-03T23:04:40.956Z","updatedAt":"2023-05-03T23:04:40.956Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_3_batch_inference","description":"Figure 3. Throughput difference between Spark and Ray solution for 300 GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/5wDKGXZNHB2Hlt6JEMMKOt/da27ee6ea7e5773abef42eb291dd1a97/image5.png","details":{"size":92846,"image":{"width":1102,"height":830}},"fileName":"image5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"While big data processing systems are the best solutions for offline inference, for deep learning workloads, Ray Data outperforms Spark.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling up Ray Data and maximizing performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we test the scalability of Ray Data and attempt to maximize performance by using a ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"heterogeneous cluster","nodeType":"text"},{"data":{},"marks":[],"value":" consisting of a mix of GPU nodes and CPU-only nodes. Deep learning workloads involving complex data types are often memory-bound, so by using additional CPU-only nodes, we add more total memory to our cluster to better maximize GPU utilization.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlike SageMaker or Spark, Ray works natively with heterogeneous clusters consisting of different instance types, fully addressing ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"challenge #1.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We run the same benchmark, except with 10 TB worth of images, using a single cluster of 10 g4dn.12xlarge instances and 10 m5.16xlarge instances.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Throughput: 11,580.958 img/sec","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"As we can see below, Ray Data achieves over 90%+ GPU utilization throughout the workload run:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"16XJHcynegHxIdae1hupAF","type":"Asset","createdAt":"2023-05-03T23:07:22.875Z","updatedAt":"2023-05-03T23:07:22.875Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"batch_inference_figure_4","description":"Figure 4. GPU utilization of all 40 GPUs in the cluster. We are roughly at 90%+ GPU utilization during the entire duration of the workload.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/16XJHcynegHxIdae1hupAF/8eab9567ac78a8d558bcd04218f7e1ca/image2.png","details":{"size":466601,"image":{"width":1999,"height":621}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nThe ability to scale to many GPUs and large data sizes without needing to rewrite code and not sacrificing on throughput is what sets Ray Data apart compared to other solutions for deep learning workloads.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog, we identified four technical challenges for doing batch inference at scale and investigated three commonly cited industry solutions for offline batch inferences. Using the cited approaches, we compared Ray, Spark, and Amazon SageMaker Batch Transform.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We concluded that Ray Data best meets all four challenges:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data abstracts away compute infrastructure management, and can maximize performance for memory-bound workloads as it natively supports heterogeneous clusters.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"streams","nodeType":"text"},{"data":{},"marks":[],"value":" data from cloud storage -\u003e CPU -\u003e GPU, ensuring that all resources in the cluster are utilized at any given point in time. This improves throughput and reduces overall cost.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data has native support for multi-dimensional tensors, which is vital for deep learning workloads.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data is Python native and programmatic, making it easy to develop and debug.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Overall, Ray Data is the best option for deep learning offline batch inference. In our image classification benchmarks, as shown in the figures above, Ray Data significantly outperforms SageMaker Batch Transform (by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"17x)","nodeType":"text"},{"data":{},"marks":[],"value":" and Spark (by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"2x ","nodeType":"text"},{"data":{},"marks":[],"value":"and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"3x","nodeType":"text"},{"data":{},"marks":[],"value":") while linearly scaling to TB level data sizes. All code for these benchmarks can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Lastly, as a result of higher throughput and Ray as an open source solution, Ray lowers the overall costs on top of the base EC2 cost.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Whatâs Next","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stay tuned for a follow up post where we dive into Ray Data streaming's design and how to use it for your own ML pipelines. Meanwhile, you can get started with Ray Data for batch inference ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Alternatively, ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfC6rmBeb9pLPSKRf1e7VCy7BtkjSGu20gh0xjQhdo6rcUflQ/viewform"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"fill out this form","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â to connect with us if you are interested in working on a proof of concept (POC) for your offline batch inference workload with Ray.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have our ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit 2023 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"early-bird registration open. Secure your spot, and save some money.\n\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n\n\n\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2dNPqjBTFiDZn2gcXkdir9","type":"Asset","createdAt":"2023-05-03T22:50:03.518Z","updatedAt":"2023-05-03T22:50:03.518Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 1","description":"Figure 1. Throughput difference among three different solutions for 10GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/2dNPqjBTFiDZn2gcXkdir9/f07a675dcb4f124e29aeb2f6618dbf15/image1.png","details":{"size":163062,"image":{"width":1334,"height":948}},"fileName":"image1.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}}],"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2rfQivy2LoE7OTc9bGVGvm","type":"Entry","createdAt":"2023-04-27T13:59:53.933Z","updatedAt":"2023-06-06T14:28:19.779Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Announcing Ray 2.4.0: Infrastructure for LLM training, tuning, inference, and serving","seoTitle":"Announcing Ray 2.4.0: Infrastructure for LLM training, tuning, inference, and serving","slug":"announcing-ray-2-4-0-infrastructure-for-llm-training-tuning-inference-and","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6TqWiyUibvC9ETfyYljP8b","type":"Entry","createdAt":"2022-02-11T15:42:52.165Z","updatedAt":"2022-02-11T15:42:52.165Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jiajun Yao","slug":"jiajun-yao"}}],"publishedDate":"2023-04-27","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.4.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2.4 release ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"features exciting improvements across the Ray ecosystem. In this blog post, we highlight new features and examples for Generative AI workloads and share four overall additions:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements to Ray data for ease of use, stability, and observability","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Improved Serve observability","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Introduce RLlibâs module for custom reinforcement learning","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Improved Ray scalability for large clusters","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Generative AI model examples and new features using Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Over the past few months, we have seen a flurry of innovative activities around ","nodeType":"text"},{"data":{"uri":"https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-generative-ai"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"generative AI models","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Large_language_model"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"large language models (LLMs)","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". To continue our steadfast commitment to make Ray a pivotal compute substrate for ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"generative AI workloads","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and address the challenges (as explained in our ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"blog series","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"), we have added significant enhancements in this release to ensure that these open source LLM models and workloads are accessible to the open source community and performant with Ray.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QDe7dun9hKhLCoGFSXKh9","type":"Asset","createdAt":"2023-04-26T19:23:25.596Z","updatedAt":"2023-04-26T19:23:25.596Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_image","file":{"url":"//images.ctfassets.net/xjan103pcp94/1QDe7dun9hKhLCoGFSXKh9/d9f93233f6ee72608e34735eac545a74/image1.png","details":{"size":154908,"image":{"width":960,"height":540}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Generative AI Workloads:","nodeType":"text"},{"data":{},"marks":[],"value":" First, with 2.4 we are releasing a set of new working examples showing how to use Ray with ","nodeType":"text"},{"data":{"uri":"https://stablediffusionweb.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stable Diffusion","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and LLMs like ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/transformers/model_doc/gptj"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GPT-J","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Below we tabulate the workload of each of these models and their respective links. More are underway with respect to Rayâs integration with ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"49AVYyPR2gbRD9P3NOH7My","type":"Entry","createdAt":"2023-04-26T19:44:22.407Z","updatedAt":"2023-04-26T22:30:40.568Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"generative_ai_features_and_examples","header":"Table 1. New LLM features, examples, and tutorials","body":"| Generative AI Workload| Description| Links|\n| ---------- | ---------- | ---------- |\n|Fine-tuning GPT-J|In this example, we showcase how to use [Ray Train](https://docs.ray.io/en/releases-2.4.0/train/train.html) for fine-tuning [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) using [DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed) and Hugging Face. GPT-J is a GPT-2-like causal language mode, with 6B parameters, trained on the PILE dataset.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html)|\n|Fine-tuning Dreambooth|This example shows how to fine-tune a [DreamBooth](https://dreambooth.github.io/) model using [Ray Train](https://docs.ray.io/en/releases-2.4.0/train/train.html). Dreambooth is a technique to fine-tune diffusion models (like Stable Diffusion) by injecting a custom subject to the model.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/dreambooth_finetuning.html)|\n|Scalable Batch Inference  with GPT-J|This example showcases how to use[ Ray Data](https://docs.ray.io/en/releases-2.4.0/data/dataset.html) for scalable batch inference with  GPT-J.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_batch_prediction.html)| Scalable Batch Inference  with Stable Diffusion|In this example, showcase how to use [Ray Data](https://docs.ray.io/en/releases-2.4.0/data/dataset.html) for scalable batch inference with [Stable Diffusion](https://stablediffusionweb.com/). Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from CompVis, Stability AI and LAION.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/stablediffusion_batch_prediction.html)|\n|Serving GPT-J|In this example, we showcase how to use [Ray Serve](https://docs.ray.io/en/releases-2.4.0/serve/index.html) for serving GPT-J.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_serving.html)|\n|Serving Stable Diffusion|In this example, we showcase how to use [Ray Serve](https://docs.ray.io/en/releases-2.4.0/serve/index.html) for serving [Stable Diffusion.](https://stablediffusionweb.com/)|[Source and documentation](https://docs.ray.io/en/master/serve/tutorials/stable-diffusion.html)|\n|Ray integration with LangChain|In this tutorial, we showcase how to build an open source search engine with Ray and [LangChain](https://python.langchain.com/en/latest/index.html)|[Source and blog](https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray)|"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray AIR new trainers:","nodeType":"text"},{"data":{},"marks":[],"value":" Second, to further enable additional large model workloads on Ray, weâre also releasing an ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"AccelerateTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", allowing you to run ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/accelerate"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"HuggingFace Accelerate","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/accelerate/usage_guides/deepspeed"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"DeepSpeed","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on Ray with minimal code changes. This Trainer integrates with the rest of the Ray ecosystemâincluding the ability to run distributed ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"hyperparameter tuning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with each trial dispatched as a distributed training job.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"See how ","nodeType":"text"},{"data":{"uri":"https://github.com/CarperAI/trlx/blob/9bc08369ca9ec83342c4d7755205dab1a7723006/trlx/sweep.py#L325-L340"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"CarperAI/trlx","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" uses this for RLHF hyperparameter sweeps, and take a peek at the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html#ray-train-huggingface-accelerate-acceleratetrainer"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" too.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4CHwg0j8NDYgH399cu2hGC","type":"Entry","createdAt":"2023-04-26T21:23:30.012Z","updatedAt":"2023-04-26T21:23:30.012Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"accelerate_trainer_code_snippet","body":"from ray.train.huggingface.accelerate import AccelerateTrainer\n\ndef train_func():\n    ...\n\ntrainer = AccelerateTrainer(\n    train_func,\n    accelerate_config={}  # you can pass in a config filepath here as well\n    scaling_config=ScalingConfig(\n        trainer_resources={\"CPU\": 0},\n        num_workers=4,\n        use_gpu=True,\n        resources_per_worker={\"CPU\": 2},\n    ),\n)\n\nresult = trainer.fit()\n\n# Integration with Ray Tune\ntuner = tune.Tuner(\n    trainer,\n    param_space={\"lr\": tune.grid_search([0.1, 0.2, 0.3]),\n)\n\nresults = tuner.fit()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can see the full example for AccelerateTrainer ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html#ray-train-huggingface-accelerate-acceleratetrainer"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Updated LightningTrainer:","nodeType":"text"},{"data":{},"marks":[],"value":" Third, in the broader deep learning space, weâre introducing the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.lightning.LightningTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LightningTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", allowing you to scale your ","nodeType":"text"},{"data":{"uri":"https://lightning.ai/docs/pytorch/stable//index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch Lightning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on Ray. As part of our continued effort for seamless integration and ease of use, we have enhanced and replaced our existing ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray_lightning","nodeType":"text"},{"data":{},"marks":[],"value":" integration, which was widely adopted, with the latest changes to PyTorch Lighting.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The new ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"LightningTrainer","nodeType":"text"},{"data":{},"marks":[],"value":" provides better compatibility with the other Ray libraries: Ray Tune or Ray Data or Ray Serve directly with Ray connecting APIs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Check out ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.lightning.LightningTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"the documentation for this component","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or take a look ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/examples/lightning/lightning_mnist_example.html#lightning-mnist-example"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"at an example","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1p8EMLSFTFDI0RuBceMrPe","type":"Entry","createdAt":"2023-04-26T21:27:50.764Z","updatedAt":"2023-04-26T21:27:50.764Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"updated_lightning_trainder_code_snippet","body":"from pytorch_lightning.loggers.csv_logs import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\nfrom ray.train.lightning import (\n    LightningTrainer,\n    LightningConfigBuilder,\n    LightningCheckpoint,\n)\n\nclass MNISTClassifier:\n\n    â¦\n\nclass MNISTDataModule:\n    â¦\n\nlightning_config = (\n    LightningConfigBuilder()\n    .module(MNISTClassifier, lr=1e-3, feature_dim=128)\n    .trainer(\n        max_epochs=10,\n        accelerator=\"cpu\",\n        log_every_n_steps=100,\n        logger=CSVLogger(\"logs\"),\n    )\n    .fit_params(datamodule=datamodule)\n    .checkpointing(monitor=\"val_accuracy\", mode=\"max\", save_top_k=3)\n    .build()\n)\nscaling_config = ScalingConfig(\n    num_workers=4, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n)\n\nrun_config = RunConfig(\n    name=\"ptl-mnist-example\",\n    local_dir=\"/tmp/ray_results\",\n    checkpoint_config=CheckpointConfig(\n        num_to_keep=3,\n        checkpoint_score_attribute=\"val_accuracy\",\n        checkpoint_score_order=\"max\",\n    ),\n)\ntrainer = LightningTrainer(\n    lightning_config=lightning_config,\n    scaling_config=scaling_config,\n    run_config=run_config,\n)\n\nresult = trainer.fit()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"See the full source code ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/examples/lightning/lightning_mnist_example.html#lightning-mnist-example"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements in Ray DataÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The enhancements and features in the release cut across a few dimensions: stability, ease of use, and observability.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data and ease of use: ","nodeType":"text"},{"data":{},"marks":[],"value":"Data ingestion and preprocessing for machine learning (ML) at scale are central to any end-to-end ML.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To enable ease of use, the default backend execution model for Ray Data is streaming-based under the hood. As such developers need not worry about instructing Ray Data to use the streaming execution backend; itâs the default execution mode. Second, the default streaming backend lends itself well to large scale workloads such as setting up distributed training data streaming and batch inference or preprocessing. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4fRyCVJVYH8NAjkcFhTgDX","type":"Entry","createdAt":"2023-04-26T21:30:40.030Z","updatedAt":"2023-04-26T21:30:40.030Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_data_code_snippet","body":"import ray\nfrom ray.data import ActorPoolStrategy\n\nimport time\n\ndef preprocess_func(x):\n   # Do some work with the data x\n   x = ...\n   time.sleep(0.1)\n   return x\n\nfor _ in (\n   ray.data.range_tensor(5000, shape=(80, 80, 3), parallelism=200)\n   .map_batches(preprocess_func, compute=ActorPoolStrategy(size=4))\n   .map_batches(preprocess_func, compute=ActorPoolStrategy(size=3))\n   .map_batches(preprocess_func, num_cpus=1)\n   .iter_batches()\n\n):\n\n   pass","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This launches a simple 4-stage pipeline. We use different compute args for each stage, which forces them to be run as separate operators instead of getting fused together. You should see a log message indicating streaming execution is being used.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While the above example is using dummy tasks, these sort of operator pipelines are a common pattern Ray users desire to scale:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6T6VTlZroKCZ6eyyII0HC1","type":"Asset","createdAt":"2023-04-26T21:34:22.865Z","updatedAt":"2023-04-26T21:34:22.865Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_data_distributed_image","description":"Figure 1. Ray data streaming data with ActorPool ","file":{"url":"//images.ctfassets.net/xjan103pcp94/6T6VTlZroKCZ6eyyII0HC1/d7202c977c1a0c25eeb4563ab5c50319/image3.png","details":{"size":106561,"image":{"width":1249,"height":515}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray data and SQL","nodeType":"text"},{"data":{},"marks":[],"value":": Additionally, we have extended Ray Dataâs ability to fetch data from a common ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/creating-datastreams.html#reading-from-sql-databases"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SQL data source","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", with ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"read_sql(...)","nodeType":"text"},{"data":{},"marks":[],"value":", enabling you to connect to a common SQL data store.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3imkshawnZw7DglAUBNTN3","type":"Entry","createdAt":"2023-04-26T21:36:16.889Z","updatedAt":"2023-04-26T21:36:16.889Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_data_sql_connector_code_snippet","body":"import mysql.connector\n\nimport ray\n\ndef create_connection():\n    return mysql.connector.connect(\n        user=\"admin\",\n        password=...,\nhost=\"example-mysql-database.c2c2k1yfll7o.us-west-2.rds.amazonaws.com\",\n      connection_timeout=30,\n      database=\"example\",\n    )\n\n# Get all movies\ndatastream = ray.data.read_sql(\"SELECT * FROM movie\", create_connection)\n# Get movies after the year 1980\ndatastream = ray.data.read_sql(\n    \"SELECT title, score FROM movie WHERE year \u003e= 1980\", create_connection\n)\n# Get the number of movies per year\ndatastream = ray.data.read_sql(\n    \"SELECT year, COUNT(*) FROM movie GROUP BY year\", create_connection\n)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements to Serve observability","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In 2.4, we are adding new functionality for observability and monitoring for Ray Serve applications. You can use the Ray dashboard to get a high-level overview of your Ray cluster and Ray Serve applicationâs states. This includes details such as:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"the number of deployment replicas currently running","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"logs for your Serve controller, deployment replicas, and HTTP proxies","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"the Ray nodes (i.e., machines) running in your Ray cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can access the Ray dashboard at port 8265 at your clusterâs URI. For example, if youâre running Ray Serve locally, use ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"http://localhost:8265","nodeType":"text"},{"data":{},"marks":[],"value":" in your browser.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3CtBAZRebvQVRJsbp63rD2","type":"Asset","createdAt":"2023-04-26T21:39:28.459Z","updatedAt":"2023-04-26T21:39:28.459Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_serve_observebility","description":"Figure 1. Serve observesability metrics in the Ray Dashboard.","file":{"url":"//images.ctfassets.net/xjan103pcp94/3CtBAZRebvQVRJsbp63rD2/0951a7ad1f4d4ec1ae2a3ba60a3af218/image2.png","details":{"size":146211,"image":{"width":1999,"height":1040}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"For more information on the dashboard, read the documentation for the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/ray-core/ray-dashboard.html#dash-serve-view"},"content":[{"data":{},"marks":[],"value":"Serve page","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"RLlibâs new module for custom reinforcement","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In RLlib, we are introducing a new ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/rllib-rlmodule.html#rl-modules-alpha"},"content":[{"data":{},"marks":[],"value":"RLModule","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" abstraction (alpha). RLModule API provides a unified way to define custom reinforcement learning models in RLlib. This API enables you to design and implement your own models to suit specific needs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To maintain consistency and usability, RLlib offers a standardized approach for defining module objects for both single-agent and multi-agent reinforcement learning environments through the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/package_ref/rl_modules.html"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"SingleAgentRLModuleSpec","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/package_ref/doc/ray.rllib.core.rl_module.marl_module.MultiAgentRLModuleSpec.html#ray.rllib.core.rl_module.marl_module.MultiAgentRLModuleSpec"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"MultiAgentRLModuleSpec","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" classes. The built-in RLModules in RLlib follow this consistent design pattern, making it easier for you to understand and utilize these modules.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1w6lK0sBUAVl4BmOGBQcQ9","type":"Entry","createdAt":"2023-04-26T21:48:25.122Z","updatedAt":"2023-04-26T21:48:25.122Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"rllib_multiagent_code_snippet","body":"import gymnasium as gym\nfrom ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec\nfrom ray.rllib.core.testing.torch.bc_module import DiscreteBCTorchModule\n\nenv = gym.make(\"CartPole-v1\")\n\nspec = SingleAgentRLModuleSpec(\n    module_class=DiscreteBCTorchModule,\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    model_config_dict={\"fcnet_hiddens\": [64]},\n)\n\nmodule = spec.build()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is an experimental module that serves as a general replacement for ModelV2, and is subject to change. It will eventually match the functionality of the previous stack. If you only use high-level RLlib APIs such as ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html#ray.rllib.algorithms.algorithm.Algorithm"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Algorithm","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", you should not experience significant changes, except for a few new parameters to the configuration object. Read more about it in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/rllib/rllib-rlmodule.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Core scaling to 2000 nodes","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Weâve also invested time in making sure Ray can support larger scale workloads. With this release, we are announcing official support for Ray clusters to up to 2000 nodes. See","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/release/benchmarks/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":" scalability envelope","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for more details.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"First and foremost, we want to thank all contributors for their valuable contributions to this new release of ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.4.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2.4","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Your enduring support continues to foster the wider use of Ray adoption.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This release is a milestone in our efforts to offer Ray as a performant compute infrastructure for LLM training, inference, and serving. We shared a number of examples that speak to these efforts.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Additionally, we enhanced Ray Data, improved Serve observability, introduced a new RLlib customization module, and scaled Ray core to support large workloads, up to 2000 nodes. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Have a go at the latest release with pip install âray[default]â and let us know of your feedback. Weâre always interested to hear from you â feel free to reach out to us on ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Github","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discuss","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Whatâs Next?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stay tuned for more Generative AI how-to blog series as we continue our efforts to make Ray the platform choice and compute substrate for all your Generative AI workloads. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you missed our four-part blog series on Generative AI, in which lay out our vision, position, and solution on how Ray addresses the challenges associated with OSS Generative AI workloads, peruse the links below. Much of the above Generative AI examples and features stem from this series.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-ray-solves-common-production-challenges-for-generative-ai"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"How Ray solves common production challenges for generative AI infrastructure","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Training 175B Parameter Language Models at 1000 GPU scale with Alpa and Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/faster-stable-diffusion-fine-tuning-with-ray-air"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Faster stable diffusion fine-tuning with Ray AIR","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"How to fine tune and serve LLMs simply, quickly and cost effectively using Ray + DeepSpeed + HuggingFace","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Building an LLM open source search engine in 100 lines using LangChain and Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Join our ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/community"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Community","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the ","nodeType":"text"},{"data":{"uri":"https://forms.gle/9TSdDYUgxYs8SA9e8"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray #LLM slack channel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have our ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit 2023 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"early-bird registration open. Secure your spot, and save some money.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QDe7dun9hKhLCoGFSXKh9","type":"Asset","createdAt":"2023-04-26T19:23:25.596Z","updatedAt":"2023-04-26T19:23:25.596Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_image","file":{"url":"//images.ctfassets.net/xjan103pcp94/1QDe7dun9hKhLCoGFSXKh9/d9f93233f6ee72608e34735eac545a74/image1.png","details":{"size":154908,"image":{"width":960,"height":540}},"fileName":"image1.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}}],"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6dbaF024PqltIV7jyXkovs","type":"Entry","createdAt":"2023-03-22T18:00:09.083Z","updatedAt":"2023-06-06T14:30:20.586Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":10,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Training 175B Parameter Language Models at 1000 GPU scale with Alpa and Ray","seoTitle":"High-Performance LLM Training at 1000 GPU Scale With Alpa \u0026 Ray","slug":"training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray","description":"In part 2 of our generative AI blog series, we cover how Ray empowers large language models (LLM) frameworks such as Alpa.","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3pIMa06o2RZXbtCwhydJM8","type":"Entry","createdAt":"2022-05-18T16:01:41.609Z","updatedAt":"2022-05-18T16:01:41.609Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jiao Dong","slug":"jiao-dong"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3VBVM2RrXhGxvgznZWGTaW","type":"Entry","createdAt":"2021-05-25T01:19:46.837Z","updatedAt":"2021-05-25T01:19:46.837Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Hao Zhang","slug":"hao-zhang","link":"https://www.cs.cmu.edu/~hzhang2/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5kUeUW2Pf5XSHoK5DqNs3e","type":"Entry","createdAt":"2023-03-21T13:45:50.879Z","updatedAt":"2023-03-21T13:45:50.879Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Lianmin Zheng","slug":"lianmin-zheng","link":"https://www.linkedin.com/in/lianmin-zheng-6266a8114/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2yKvtARlwNkkIYOYLFgkhB","type":"Entry","createdAt":"2022-06-22T19:54:47.912Z","updatedAt":"2022-06-22T19:54:47.912Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jun Gong","slug":"jun-gong"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6qqwM6jH2g1gEzSeQdBHJc","type":"Entry","createdAt":"2022-07-11T13:55:01.009Z","updatedAt":"2022-07-11T13:55:01.009Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Phi Nguyen","slug":"phi-nguyen"}}],"publishedDate":"2023-03-22","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 2 of our generative AI blog series. Here we cover how Ray empowers large language models (LLM) frameworks such as Alpa. To learn how to use Ray to productionize generative model workloads, see [part 1.](https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure)\n","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"This blog post presents how two open-source frameworks, ","nodeType":"text"},{"data":{"uri":"https://alpa.ai/opt"},"content":[{"data":{},"marks":[],"value":"Alpa","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", closely integrate to achieve the scale to train a 175B parameters OPT-175B model (equivalent to GPT-3)Â with pipeline parallelism up to 1024 A100 GPUs in collaboration with Nvidia. With this integration, our benchmarks show three scaling results:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"1. Alpa can scale beyond 1000 GPUs for 175 billion parameter scale LLMs.\n2. Alpa can achieve SOTA peak GPU utilization (57.5%) and HW FLOPs per GPU (179 TFLOPs), about 21%~42% higher compared with published LLM benchmarks from Meta, Google, and Nvidia in 2022.\n3. All LLM parallelization and partitioning are done automatically with one line decorator.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Alpa and Ray are open source projects that work together to offer a scalable and efficient solution for training large language models at scale. In this blog, we examine how these two integrated frameworks, their combined stack's architecture, developer friendly API, scalability, and performance in detail.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Background of large language models (LLM)","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Because of rapid research in academia and industries, a growing trend in the release of an array of models, with an exponential number of training parameters in billions, ensued in a short span of time, as shown in the figure below. This new wave of machine learning is spearheaded by ","nodeType":"text"},{"data":{"uri":"https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-generative-ai"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Generative AI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" models, including ","nodeType":"text"},{"data":{"uri":"https://openai.com/blog/chatgpt/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"ChatGPT","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/2209.00796"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Diffusion","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1907.11692"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RoBERT","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"underline"}],"value":"a","nodeType":"text"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://cdn.openai.com/papers/dall-e-2.pdf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"DALL-E","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which allow users to feed as input into the model different modalitiesâtext, video, audio, and imageâto analyze, synthesize, and generate new content as simple sequence-to-sequence tasks. ","nodeType":"text"},{"data":{"uri":"https://txt.cohere.com/generative-ai-future-or-present/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Generative AI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is the next era in natural language processing (NLP).\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"RnNRNwPnLNhKqvcD0m2NP","type":"Asset","createdAt":"2023-03-21T13:53:00.731Z","updatedAt":"2023-03-21T13:53:00.731Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 0","description":"Trend showing LLM models growing in size over the years","file":{"url":"//images.ctfassets.net/xjan103pcp94/RnNRNwPnLNhKqvcD0m2NP/11f05969afde0883b1cddeac6adb2f65/image12.png","details":{"size":380192,"image":{"width":1956,"height":1262}},"fileName":"image12.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"But training these LLM models with billions of parameters from scratch or fine tuning with new data has its set of challenges. Training and evaluating demand massive distributed computing power, clusters of accelerated-based hardware and memory, reliable and scalable machine learning frameworks and fault-tolerant systems. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the next two sections, we discuss some of the challenges, followed by our approach to address them.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Machine learning system challenges of LLM","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The parameter size of a modern LLM is at the magnitude of hundreds of billions that exceeds the GPU memory of a single device or host â we call it âthe memory wall.â For an ","nodeType":"text"},{"data":{"uri":"https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"OPT-175B","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" model, it requires 350 GB GPU memory to just fit the parameters, not to mention GPU memory needed for gradients and optimizer states during training that can further push memory requirements beyond 1 TB. Meanwhile, commodity GPUs only have 16GB / 24GB GPU memory, even the most advanced A100 and H100 GPUs only have 40GB / 80GB GPU memory per device.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Tools and algorithms such as ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/pdf/2104.07857.pdf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"ZeRO Infinity","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" can help with addressing this âmemory wallâ problem by allowing you to train much larger models with limited memory, but it often comes at the cost of hardware utilization and efficiency.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the same spirit of making LLM more accessible, we explored scaling LLM training and inference with all parameters remaining on GPU for best efficiency without sacrificing usability.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In order to efficiently run training and inference for LLMs, we need to ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"partition the model ","nodeType":"text"},{"data":{},"marks":[],"value":"across its","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" ","nodeType":"text"},{"data":{},"marks":[],"value":"computation graph, parameters, and optimizer states such that each partition fits nicely within the memory limit of a single GPU. Based on the GPU cluster available, ML researchers need to devise a strategy to optimize across different parallelization dimensions in order to train efficiently.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Today, however, optimizing training across different parallelization dimensions is manual and difficult. These dimensional partition strategies of an LLM can be categorized as:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Inter-operator parallelism:","nodeType":"text"},{"data":{},"marks":[],"value":" Partition the full computation graph to disjoint subgraphs. Each device computes its assigned subgraph and communicates with other devices upon finishing.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Intra-operator parallelism:","nodeType":"text"},{"data":{},"marks":[],"value":" Partition matrices participating in the operator to sub-matrices. Each device computes its assigned submatrices and communicates with other devices when multiplication or addition takes place.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Both strategies can be applied to the same computation graph.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"bold"}],"value":"Note","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":": Some research work categorizes model parallelism as â3D-parallelismâ that represents data, tensor and pipeline parallelism respectively. In Alpaâs terminology, data is simply the outer dimension of tensor parallelism that maps to intra-operator parallelism, and pipeline parallelism is the result of inter-operator parallelism that partitions graph into separate stages with pipelining orchestration. They are equivalent in power, and we will keep the partitioning terminology simple and consistent to only use inter and intra-op parallelism from here. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"79V3r4hVsQmTbngefUcNHQ","type":"Asset","createdAt":"2023-03-21T13:57:44.776Z","updatedAt":"2023-03-21T13:57:44.776Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure-1","description":"Figure 1: Partition strategies for inter and intra operator of a computation graph\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/79V3r4hVsQmTbngefUcNHQ/2154cd52f8a363fde487340e56d95a4f/image9.png","details":{"size":113967,"image":{"width":1336,"height":589}},"fileName":"image9.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Exploring the possible strategies of inter and intra operator parallelism is a challenging combinatorial problem with tradeoffs. With reasonable computation graph partitioning of inter-operator parallelism, the communication cost can be small between subgraphs but introduces data dependency. Even though pipelining can help alleviate the problem, device idle time is still inevitable.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"On the other hand, intra-operator parallelism can parallelize the operator computation among multiple GPU devices with less idle time, but higher communication cost when the next operator cannot preserve matrix partition from the previous one.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In addition to partitioning of matrices and computation graphs, we need the ability to map partitions to GPU devices with awareness of the heterogeneous network topology. GPU connections inside a node (","nodeType":"text"},{"data":{"uri":"https://www.nvidia.com/en-us/data-center/nvlink/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"NVLink","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") are orders of magnitude faster than inter-host networking (","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/InfiniBand"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"InfiniBand","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", EFA, ethernet), and will lead to significant performance differences among different partition strategies.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6VZ2y3co7ITPJz3htkNoNi","type":"Asset","createdAt":"2023-03-21T14:00:39.396Z","updatedAt":"2023-03-21T14:00:39.396Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure-2","description":"Figure 2:  Network topology of GPU clusters\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/6VZ2y3co7ITPJz3htkNoNi/7e272d729720828a5ad8ea409392fd96/image14.png","details":{"size":93996,"image":{"width":1334,"height":567}},"fileName":"image14.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Current state of LLM parallelization","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are a number of prior works in the model parallelism domain that achieve different parallelism techniques mentioned above, as shown in the following Figure 3. As illustrated earlier, finding and executing optimal model partitioning strategy is very manual and difficult that requires substantially deep domain expertise.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Alpa handles inter and intra operator parallelism ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"automatically with one line decorator","nodeType":"text"},{"data":{},"marks":[],"value":" that seamlessly devises a partition strategy for data, tensor and pipeline parallelism for LLM at scale, and is capable of generalizing to a wide range of model architectures that greatly simplifies model parallelism to make LLM more accessible to everyone.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3DdXyhSjac5PUDj5SGwbSy","type":"Asset","createdAt":"2023-03-21T14:02:30.572Z","updatedAt":"2023-03-21T14:02:30.572Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure-3","description":"Figure 3: Alpaâs positioning with automatic inter and intra operator parallelism","file":{"url":"//images.ctfassets.net/xjan103pcp94/3DdXyhSjac5PUDj5SGwbSy/48bc3ee758c135bfe54cd28b3c3190b1/image18.png","details":{"size":262936,"image":{"width":1999,"height":1120}},"fileName":"image18.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scalability issues of JAX on GPU clusters","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The impact of network topology further manifests into the difference of scaling LLMs for TPU and GPU clusters with their unique hardware level connections.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"On a TPU cluster (see Figure 4), the network fabric is specifically designed with a ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Torus_interconnect"},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"torus topology","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", so it can scale to thousands of chips with intra-op parallelism only with APIs provided by ","nodeType":"text"},{"data":{"uri":"https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"jax.pjit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4EHBK0MuNg03vJFpAviZhD","type":"Asset","createdAt":"2023-03-21T14:04:15.366Z","updatedAt":"2023-03-21T14:04:15.366Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure -4 ","description":"Figure 4: Network topology comparison between TPU and GPU cluster\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/4EHBK0MuNg03vJFpAviZhD/c9739e6aa172a4162c2b50caf667454d/image10.png","details":{"size":185787,"image":{"width":1332,"height":496}},"fileName":"image10.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"On a GPU cluster, the network fabric is typically a ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Fat_tree"},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"fat-tree topology","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with limited inter-host network bandwidth and more challenging to scale up in larger scale clusters, which calls for parallelization plans that are more communication-efficient, such as pipeline parallelism.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, pipeline parallelism is not provided in JAX currently, thus it limits the scalability of JAX LLM on GPU clusters.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Architecture overview","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Before we dive into how we addressed these challenges with our layered technical stack, it is important to provide an architectural overview of its critical components.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1PMwieM1tr0GeWtEbHdrra","type":"Asset","createdAt":"2023-03-21T14:05:47.855Z","updatedAt":"2023-03-21T14:05:47.855Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure-5","description":"Figure 5: Technical integration layered stack for LLM\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/1PMwieM1tr0GeWtEbHdrra/d172167ebcdc35815b3ad5bc85f5bfc1/image8.png","details":{"size":201210,"image":{"width":1900,"height":1104}},"fileName":"image8.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introduction to Alpa","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{"uri":"https://alpa.ai/opt"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Alpa","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a unified compiler that automatically discovers and executes the best ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Inter-op","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Intra-op","nodeType":"text"},{"data":{},"marks":[],"value":" parallelism for large","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" ","nodeType":"text"},{"data":{},"marks":[],"value":"deep learning models.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Alpaâs key API is a simple ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"@alpa.parallelize","nodeType":"text"},{"data":{},"marks":[],"value":" decorator that parallelizes and optimizes for the best model parallelism strategy automatically. Given JAXâs nature of static graph definition with known size and shapes, a simple tracing on the train_step with sample batch is sufficient for us to capture all information we need for automatic partitioning and parallelization. Consider the simple code below.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1efxbs4cOFPY5Og62IWRCy","type":"Entry","createdAt":"2023-03-21T14:09:20.163Z","updatedAt":"2023-03-21T14:09:20.163Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Alpa-decorator-code","body":"@alpa.parallelize\ndef train_step(model_state, batch):\n    def loss_func(params):\n        out = model_state.forward(params, batch[\"x\"])\n        return np.mean((out - batch[\"y\"]) ** 2)\n\n    grads = grad(loss_func)(state.params)\n    new_model_state = model_state.apply_gradient(grads)\n    return new_model_state\n\n# A typical JAX training loop\nmodel_state = create_train_state()\nfor batch in data_loader:\n    model_state = train_step(model_state, batch)\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Automatic parallelization passes in Alpa","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Alpa introduces a unique approach to tackle the complex parallel strategy search space of a two-level hierarchical system. Traditional methods have struggled to find a unified algorithm to derive an optimal parallel strategy from the vast space of inter- and intra-operator options. Alpa addresses this challenge by decoupling and reorganizing the search space at different levels.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"At the first level, Alpa searches for the most effective ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"inter-operator parallel","nodeType":"text"},{"data":{},"marks":[],"value":" plan. Then, at the second level, the best ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"intra-operator","nodeType":"text"},{"data":{},"marks":[],"value":" ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"parallel","nodeType":"text"},{"data":{},"marks":[],"value":" plan for each stage of the inter-operator parallel plan is derived. This approach works well and the problem is solvable by simplifying the search space with hierarchy and focusing on algorithms optimized for each stageâs cost function for its computational characteristics respectively.Â Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"39aSvQqT6aLrlfdFv99TZd","type":"Asset","createdAt":"2023-03-21T14:13:02.062Z","updatedAt":"2023-03-21T14:13:02.062Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure-6","description":"Figure 6: Alpaâs hierarchical search space for partitioning strategy","file":{"url":"//images.ctfassets.net/xjan103pcp94/39aSvQqT6aLrlfdFv99TZd/aed0ca9d28d968e49660855d17d580fa/image17.png","details":{"size":141013,"image":{"width":1336,"height":751}},"fileName":"image17.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The Alpa compiler is built around the search space decomposition approach we introduced. Its input consists of a computational graph and a cluster specification. To optimize the parallel strategy, Alpa conducts two compiler passes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"The first pass","nodeType":"text"},{"data":{},"marks":[],"value":": inter-operator utilizes dynamic programming to identify the most suitable inter-operator parallelism strategy.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"The second pass","nodeType":"text"},{"data":{},"marks":[],"value":": intra-operator uses integer linear programming to find the best intra-operator parallelism strategy.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The optimization process is hierarchical, where the higher-level inter-operator pass calls the lower-level intra-operator pass multiple times, making decisions based on the feedback from the intra-operator pass. Finally, the runtime orchestration pass executes the parallel plan and brings the strategy to life.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5HICTarBg2dSOs8P7lHvjq","type":"Asset","createdAt":"2023-03-21T14:15:04.154Z","updatedAt":"2023-03-21T14:15:04.154Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure-7","description":"Figure 7: Alpaâs automatic partitioning passes at different levels\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/5HICTarBg2dSOs8P7lHvjq/e108428b401b63dffc8e86aae08d8956/image5.png","details":{"size":68221,"image":{"width":1335,"height":565}},"fileName":"image5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the next section, letâs dive into Ray, a distributed programming framework that Alpa is built on top of to comprehend how GPU cluster virtualization and pipeline parallelism runtime orchestration are enabled to empower LLM at scale.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introduction to Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray is an open-source unified framework for scaling AI and Python applications like machine learning.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We will defer to the Ray documentation for ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/ray-overview/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"an extended overview of Ray.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing Ray patterns and primitives as advanced abstractions","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray tasks and actors, we can formulate a few simple patterns of using Ray. In the following parts, weâll uncover how they can be used to build advanced abstractions such as a DeviceMesh, GPU Buffer, and Ray Collective, to empower LLM at scale.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"wmxvXNV4w8UFjdBBtvdut","type":"Asset","createdAt":"2023-03-21T14:27:04.708Z","updatedAt":"2023-03-21T14:27:04.708Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure-10","description":"Figure 10: Ray patterns with Tasks and Actors","file":{"url":"//images.ctfassets.net/xjan103pcp94/wmxvXNV4w8UFjdBBtvdut/699acfe47de19f936a7ca31cd787d4ca/image2.png","details":{"size":190090,"image":{"width":1999,"height":930}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Advanced pattern: DeviceMesh\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Earlier in the blog, we explained that in order to efficiently scale an LLM, we must partition model weights and computations on multiple GPU devices. Alpa utilizes Ray Actors to create more advanced device management abstractions such as a DeviceMesh: a two-dimensional mesh of GPU devices (see Figure 11).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A logical mesh can span multiple physical hosts, including all their GPU devices, with each mesh acquiring a slice of all GPUs on the same host. Multiple meshes can reside on the same host, and a mesh can even encompass an entire host. Ray Actors offer tremendous flexibility to manage GPU devices within a cluster.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For example, you can choose to have one actor per host, one per mesh, or even one per device depending on the level of orchestration control you require.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5kx4pEPNsZYSb8apgaPQ92","type":"Asset","createdAt":"2023-03-21T14:33:40.819Z","updatedAt":"2023-03-21T14:33:40.819Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure-11","description":"Figure 11: DeviceMesh for GPU cluster virtualization and management\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/5kx4pEPNsZYSb8apgaPQ92/4fc76332c681fd8216a6004f4809ef34/image13.png","details":{"size":312866,"image":{"width":1999,"height":830}},"fileName":"image13.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Advanced pattern: GPU Buffers","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The second advanced pattern in Alpa is GPU buffer management across DeviceMeshes. During GPU computations, we often end up with GPU tensors that represent tiles of a larger matrix. Alpa has an application-level GPU buffer management system that assigns a UUID for each GPU buffer and provides basic primitives, such as Send/Recv/Delete, to enable cross-mesh tensor movement and lifecycle management.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Using Ray Actors and DeviceMesh abstractions, buffers can be managed and transferred by invoking corresponding methods on the host to facilitate advanced model training paradigms.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5yMYLAinbpOcVhLu5evawr","type":"Asset","createdAt":"2023-03-21T14:35:21.391Z","updatedAt":"2023-03-21T14:35:21.391Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure-12","description":"Figure 12: GPU buffer management with Ray Actor","file":{"url":"//images.ctfassets.net/xjan103pcp94/5yMYLAinbpOcVhLu5evawr/c93ba8eef921f288c561a7f8213255c5/image1.png","details":{"size":232133,"image":{"width":1999,"height":853}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Advanced pattern: Ray Collective","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The third advanced pattern is ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/ray-more-libs/ray-collective.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Collective","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a collection of communication primitives thatÂ  enables efficient and flexible tensor movement across different CPUs, GPUs and DeviceMesh(s). It is an essential communication layer for pipeline parallelism.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The simple intra-host case is depicted on the left side of figure 13 (Host 1), where GPU devices are interconnected with NVlink. The right side of figure 13 (Host 2 and 3) shows the multi-mesh, multi-host scenario, where communication occurs in a potentially more heterogeneous setup with a mix of intra-host NVLink and inter-host networking, such as InfiniBand, EFA, or Ethernet.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nWith Ray Collective, we can move and reshard tensors freely across DeviceMeshes via high-performance networking with ","nodeType":"text"},{"data":{"uri":"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html#:~:text=The%20NVIDIA%20Collective%20Communications%20Library,%2Dpoint%20send%2Freceive%20primitives."},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"NCCL","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", Nvidiaâs Collective Communication Library for GPUs. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2KNKQv1Wa0Jj07X4xiWZVl","type":"Asset","createdAt":"2023-03-21T14:37:13.388Z","updatedAt":"2023-03-21T14:37:13.388Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure-13","description":"Figure 13:  Ray Collective for cross mesh tensor movement via NCCL\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/2KNKQv1Wa0Jj07X4xiWZVl/d4609a9ff9329f731fbbf4aed8ec72ba/image4.png","details":{"size":236541,"image":{"width":1999,"height":724}},"fileName":"image4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pipeline parallelism runtime orchestration\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In JAX and Alpa, computations, communication, and instructions are often created to be static. The static artifact is an important property, because in JAX, a user program can be compiled to intermediate representations (IR) and then fed to ","nodeType":"text"},{"data":{"uri":"https://www.tensorflow.org/xla"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"XLA","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" as a self-contained executable. Users can pass inputs into the executable and expect results as outputs, where all tensors are known in size and shape, just like a function for tensors.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The functional aspect of JAX and its lower level Intermediate Representation (IR) play nicely with Ray. If we revisit the Ray Task, where we decorate a function and let it execute in a cluster, the decorated function is the âexecutable.â In Ray, the executable is always produced by serializing the decorated Python function or class that wraps arbitrary code.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"With JAX, however, the executable is a powerful unit of computation with clean mathematical properties. ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"With good dispatching and orchestration of executables, we can represent complex and powerful neural networks, training paradigms such as transformers and pipeline parallelism, which is the essential technique imperative to scale LLM to GPU clusters.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Figure 14 is an end-to-end LLM pipeline parallelism example with Alpa on Ray. The end to end flow can be roughly divided into the following stages:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Inter-operator parallelism pass:","nodeType":"text"},{"data":{},"marks":[],"value":" Alpa optimally splits transformer blocks into separate pipeline stages and assigns them to respective DeviceMesh(es).Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Intra-operator parallelism pass","nodeType":"text"},{"data":{},"marks":[],"value":": Alpa partitions operator input and output matrices across GPU devices living on the same host along with ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/pdf/2105.04663.pdf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GSPMD","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Generate static instructions for mesh workers","nodeType":"text"},{"data":{},"marks":[],"value":": Compile a static executable for each DeviceMesh with respect to user configs such as pipeline schedule (","nodeType":"text"},{"data":{"uri":"https://arxiv.org/pdf/1806.03377.pdf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"1F1B","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/pdf/1811.06965.pdf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GPipe","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"), micro batching, gradient accumulation, etc.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Each instruction can be {RUN, SEND, RECV, FREE} that handles running a self-contained JAX HLO/XLA program, allocate/transfer/free GPU buffer across DeviceMesh(es).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"With static instructions, we greatly reduced scheduling frequency and overhead at Rayâs single controller level for better performance and scalability.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Put compiled executables into corresponding host Ray actors for later invocation.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7jgyGzwDEJxdDjWZJHrAjs","type":"Asset","createdAt":"2023-03-21T14:45:19.457Z","updatedAt":"2023-03-21T14:45:19.457Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure-14: ","description":"Figure 14: Example static instruction for two-layer pipeline parallelism\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/7jgyGzwDEJxdDjWZJHrAjs/386ce11fcc13f36fbed71e3125bc710d/alpa_static_instructions.gif","details":{"size":5158204,"image":{"width":3576,"height":1198}},"fileName":"alpa_static_instructions.gif","contentType":"image/gif"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"4. Driver calls and orchestrates compiled executables on each host worker to kick off end to end pipelined transformer training.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6mzhu4NaEOZo3VCp5ETS6","type":"Asset","createdAt":"2023-03-21T14:47:28.375Z","updatedAt":"2023-03-21T14:47:28.375Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure-15 ","description":" Figure 15:  End to end pipeline parallelism runtime orchestration with Alpa on Ray","file":{"url":"//images.ctfassets.net/xjan103pcp94/6mzhu4NaEOZo3VCp5ETS6/c7d3e40de114e64de580cb7aa780d12c/image3.png","details":{"size":334111,"image":{"width":1999,"height":893}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray on Alpa benchmark results","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We closely collaborated with Nvidia to benchmark this effort for accurate performance results as well as scalability. For scalability and performance, the charts below, verified on an Nvidiaâs Selene cluster, demonstrated total HW FLOPs throughput of ","nodeType":"text"},{"data":{"uri":"https://alpa.ai/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"OPT-175B","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with various GPU cluster sizes with ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"peak HW FLOPs utilization of ~57.5%","nodeType":"text"},{"data":{},"marks":[],"value":" at ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"~179 TFLOPs/GPU.Â  ","nodeType":"text"},{"data":{},"marks":[],"value":"Model parallelization and partitioning are done ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"automatically with a one-liner decorator","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Meta's","nodeType":"text"},{"data":{"uri":"https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":" original training of OPT-175B","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with PyTorch FSDP and manual Megatron-LM policy achieved ~147 TFLOPs/GPU in 2022.Â  By contrast, Alpa on Ray achieved ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"~21.8% higher HW FLOPs","nodeType":"text"},{"data":{},"marks":[],"value":" without the requirement of implementing manual partitioning.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"With respect to Benchmarks published by ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/pdf/2201.11990.pdf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"NVIDIA researchers","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on similar hardware, we achieved ~126 TFLOPs/GPU on NLG-530B in 2022, whereas Alpa on Ray achieved ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"~42% higher HW FLOPs","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Compared to Googleâs internal ","nodeType":"text"},{"data":{"uri":"https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PaLM-540B with Pathways","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which achieved ~57.8% HW FLOPs utilization on TPUs in 2022, Alpa on Ray is very close to the efficiency of their internal implementation. However, we cannot make objective comparisons for benchmarks on different hardware â this is more of a reference.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"These benchmark results strongly suggest that Alpa on Ray is one of the most performant and scalable frameworks for training LLM models in JAX, even at 175B scale. Furthermore, itâs capable of finding and executing optimal parallelization strategies automatically.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"44QLClLXgmVEYzlrMOA2LU","type":"Asset","createdAt":"2023-03-21T14:50:30.045Z","updatedAt":"2023-03-21T14:50:30.045Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure -6","description":"Figure 16: OPT-175B training throughput with Alpa on Ray, HW FLOPS\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/44QLClLXgmVEYzlrMOA2LU/b1f18aa959d7ca7940574995da71ffe2/image7.png","details":{"size":155719,"image":{"width":1999,"height":895}},"fileName":"image7.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The above chart includes more details about the model definition and other configurations used to achieve the results. Refer to annotations at the bottom of figure for explanations.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6L6N4Zqp9jKL5uzQjsjd6j","type":"Asset","createdAt":"2023-03-21T14:51:45.330Z","updatedAt":"2023-03-21T14:51:45.330Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure-17","description":"Figure 17:  OPT-175B detailed config and metrics\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/6L6N4Zqp9jKL5uzQjsjd6j/088b9cfe0d3a4138a5ec30e678bf2cf0/image16.png","details":{"size":284335,"image":{"width":1999,"height":873}},"fileName":"image16.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Future improvements and considerations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"A number of future improvements include:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Support for T5 with bf16 + pipeline parallelism at larger scale (Weâve enabled and benchmarked at 4 hosts scale within capacity constraint.)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ease of use and production readiness improvements for the ML community","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Further simplify LLM accessibility on commodity GPUsÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Acknowledgements","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"We (Alpa and Ray team) would like to thank ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"AWS","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://www.coreweave.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"CoreWeave","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for their generous support and sponsorship of working on A100 GPUs to facilitate our interactive development. Thank ","nodeType":"text"},{"data":{"uri":"https://www.nvidia.com/en-us/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Nvidia","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for internal ","nodeType":"text"},{"data":{"uri":"https://www.nvidia.com/en-us/on-demand/session/supercomputing2020-sc2019/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Selene cluster ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"access for benchmarking at scale, as well as tremendous help for their partnership and support in this collaboration. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next Steps","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"For further exploration of Ray, Ray AIR, and Ray on Alpa:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Read why we opine Ray as scalable compute is the right choice for LLM","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Checkout the sources on GitHub","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Checkout ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Join our Ray monthly ","nodeType":"text"},{"data":{"uri":"https://www.meetup.com/bay-area-ray-meetup/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Meetup","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", where we discuss all things Ray","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Connect with the Ray community via forums: ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/community"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"slack and discuss","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Early-bird registration of","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":" Ray Summit 2023 is open","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Grab your spot early","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"For further information of Alpa:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Checkout and star ","nodeType":"text"},{"data":{"uri":"https://github.com/alpa-projects/alpa"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Alpaâs github","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for latest examples of LLM training and inference","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Connect with the Alpa community via ","nodeType":"text"},{"data":{"uri":"https://forms.gle/YEZTCrtZD6EAVNBQ7"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"slack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1PMwieM1tr0GeWtEbHdrra","type":"Asset","createdAt":"2023-03-21T14:05:47.855Z","updatedAt":"2023-03-21T14:05:47.855Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure-5","description":"Figure 5: Technical integration layered stack for LLM\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/1PMwieM1tr0GeWtEbHdrra/d172167ebcdc35815b3ad5bc85f5bfc1/image8.png","details":{"size":201210,"image":{"width":1900,"height":1104}},"fileName":"image8.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4oRe6xaRdSknLX1wjzYzEN","type":"Entry","createdAt":"2023-03-02T16:29:08.413Z","updatedAt":"2023-04-24T17:51:14.283Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Automatic and optimistic memory scheduling for ML workloads in Ray","seoTitle":"Automatic Memory Scheduling for ML Workloads in Ray","slug":"automatic-and-optimistic-memory-scheduling-for-ml-workloads-in-ray","description":"Learn about Ray's new out of memory (OOM) monitor and detection feature â all part of our efforts to make Ray easy to observe \u0026 debug for ML engineers.","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6v1ky00dfp5N272WgP68Fl","type":"Entry","createdAt":"2023-03-02T02:54:59.641Z","updatedAt":"2023-03-02T02:54:59.641Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Clarence Ng","slug":"clarence-ng","link":"https://www.linkedin.com/in/clarng/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}}],"publishedDate":"2023-03-02","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Since the release of Ray 2.0, with our goals to make distributed computing scalable, unified,Â  open, and observable, we have continued this course with subsequent Ray releases. Guided by these goals to increase observability and ability to prevent memory-intensive Ray Tasks and Actors resources that affect cluster-wide resource degradation, this blog introduces an out of memory (OOM) monitor and detection feature â all part of our efforts to make Ray easy to observe and debug for machine learning engineers. Currently in beta, this monitor is available in ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html#out-of-memory-prevention"},"content":[{"nodeType":"text","value":"Ray release 2.2","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" and","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/announcing-ray-2-3-performance-improvements-new-features-and-new-platforms"},"content":[{"nodeType":"text","value":" 2.3","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", and will continue to enhance it in future releases.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Why do you need OOM monitoring?","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"An out of memory error is a common fatal occurrence in Python libraries. There are a number of motivational reasons why you need an OOM memory monitor. First, some common Python libraries and frameworks, including ones that support distributed compute, do not provide a policy-based monitor that can preempt a memory-hungry Python application, especially during processing of large amounts of unstructured data. When a node runs out of memory, the offending process or the node on which it runs could crash. On linux, a rudimentary prevention is performed by the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.kernel.org/doc/gorman/html/understand/understand016.html"},"content":[{"nodeType":"text","value":"out of memory manager","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Worst case, without any intervention, OOMs could degrade the cluster or fail the application. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"One common example in ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/machine-learning"},"content":[{"nodeType":"text","value":"machine learning (ML) workloads","marks":[],"data":{}}]},{"nodeType":"text","value":" is to preprocess huge amounts of data, in order of tens of gigabytes. A user defined function (UDF) preprocessing this volume per core could result in an OOM if the batch size is too big to fit into the heap space. Another example is a slow Ray actor or task with a gradual memory leak during distributed training will eventually make the node inaccessible.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Second, while Python is the favorite, preferable, and easy-to-use programming language for data scientists and ML practitioners, out of the box, Python offers little built-in support to control policy-based memory usage and detection mechanism to forestall or foresee a possible runaway Python memory-hungry application.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Third, none of the common distributed compute frameworks such as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://spark.apache.org/docs/latest/tuning.html"},"content":[{"nodeType":"text","value":"Apache Spark","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" provide a policy-based scheduling mechanism to prevent OOM events. This is a crucial feature out of the box to handle ML workloads for a diverse set of use cases.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"And finally, in a Rayâs cluster environment where scaling your ML workflow and workloads are essential, itâs likely that a long running Ray task or an actor, either unwittingly because of a programming flaw you introduced or because of processing large amounts of unstructured data while using a third-party Python library, will consume large amounts of memory off the heap. This could result in an OOM error and disturb your application.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Worse, this avarice of memory consumption could stall fetching metrics, disrupt the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-core/ray-dashboard.html"},"content":[{"nodeType":"text","value":"Ray Dashboard","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" display, and terminate Rayâs controlling processes, making the cluster unusable.Â ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2rpL4AtLbfOqPrxHKlmJNf","type":"Asset","createdAt":"2023-03-02T03:10:54.634Z","updatedAt":"2023-03-02T03:10:54.634Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 1 a \u0026 b","description":"Figure 1a: Metric shows disruption before the use of the OOM monitor","file":{"url":"//images.ctfassets.net/xjan103pcp94/2rpL4AtLbfOqPrxHKlmJNf/1ac5a191a2464bcc9a7a6704551953f0/image11.png","details":{"size":53576,"image":{"width":936,"height":417}},"fileName":"image11.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4WMNtgJZZAtZqdmzbL3NEE","type":"Asset","createdAt":"2023-03-02T03:12:31.929Z","updatedAt":"2023-03-02T03:28:05.058Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Figure 1b","description":" Figure 1b: Shows smooth operation and no disruption with the OOM monitor","file":{"url":"//images.ctfassets.net/xjan103pcp94/4WMNtgJZZAtZqdmzbL3NEE/72d0f5596de86fdfac5da084a9397c07/image5.png","details":{"size":48571,"image":{"width":925,"height":422}},"fileName":"image5.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"To ensure better Python support in detection of memory usage mechanisms while using Ray native libraries or third-party Python libraries with Ray, we were motivated to offer an OOM monitor as a novel feature that achieve three things:","marks":[],"data":{}}]},{"nodeType":"ordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Observe and detect possible bad actors or tasks to mitigate worse cases scenarios","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Offer to act with policy-based preventive and preemptive measures based on default  configurations","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Enable many embarrassingly parallel ML and python compute workloads to âjust workâ with automatic and policy-based memory detection and prevention, without adjusting any memory specific dials or manual intervention, which the common distributed compute frameworks such as Apache Spark lack.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"What are those preventive or preemptive measures and how does an OOM monitor work?","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"How does the OOM monitor work?Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Embedded within the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-contribute/whitepaper.html#whitepaper"},"content":[{"nodeType":"text","value":"Raylet process","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on each Ray cluster node, the monitor periodically inspects ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-core/scheduling/memory-management.html#memory"},"content":[{"nodeType":"text","value":"collective memory usage","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"âheap space and object storeâfor each worker on the cluster node, fetching it from the underlying operating system, as depicted in ","marks":[],"data":{}},{"nodeType":"text","value":"Figure 2","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"At any point during these inspections, if the collective usage exceeds a configurable threshold (see below for those thresholds), the Raylet process will terminate a task or an actor as a preventive or preemptive measure before an OOM event occurs, and reshedule it later.","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"HXsljnbL3DCWGkqud413u","type":"Asset","createdAt":"2023-03-02T03:16:33.643Z","updatedAt":"2023-03-02T04:14:18.712Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Figure 2","description":"Figure 2: OOM monitor high-level architecture and data flow","file":{"url":"//images.ctfassets.net/xjan103pcp94/HXsljnbL3DCWGkqud413u/a46c5174e409ae64f5594c4b8d23b463/image6.png","details":{"size":45709,"image":{"width":960,"height":540}},"fileName":"image6.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"By default, the monitor is enabled, and you can further fine tune it, based on your use case and memory demands, with a minimum set of configurable environmental variables described in the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html#how-do-i-configure-the-memory-monitor"},"content":[{"nodeType":"text","value":"documentation","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". If the application requires additional memory, you can increase the threshold as described in the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html#how-do-i-configure-the-memory-monitor"},"content":[{"nodeType":"text","value":"docs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".  And to disable the monitor, follow the instructions in the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html#how-do-i-configure-the-memory-monitor"},"content":[{"nodeType":"text","value":"documentation","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Policy for terminating memory-intensive tasksÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"When the memory usage exceeds the threshold, the raylet will apply a policy to decide which task to free up for memory. The raylet will apply the policy as needed to bring down the usage below the threshold. The policy is a multi-step process to filter down to the worker that should be killed:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It first filters for tasks that are retriable.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It groups the tasks by the caller that submitted it, and picks one of the groups.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Within that group, it picks one task to kill.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The policy first prioritizes tasks that are retriable, i.e., when ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html#worker-killing-policy"},"content":[{"nodeType":"text","value":"max_retries or max_restarts","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" is \u003e 0. This is done to minimize workload failure. Actors by default are not retriable since max_restarts defaults to 0. Therefore, by default, tasks are preferred to actors when it comes to what gets killed first.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"When there are multiple callers that have submitted tasks, the policy will pick a task from the caller with the most number of running tasks. If two callers have the same number of tasks it picks the caller whose earliest task has a later start time. This is done to ensure fairness and allow each caller to make progress. A caller could be the driver process or another task or actor.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Among the tasks that share the same caller, the latest started task will be killed first.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Below is a program to demonstrate the policy in action. In this example, we create two tasks, which in turn creates four more tasks each:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"28CvK9xzWk4D3TA0kjCnuc","type":"Entry","createdAt":"2023-03-02T03:26:34.669Z","updatedAt":"2023-03-02T03:26:34.669Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"first_code_oom_example","body":"import ray\n\n@ray.remote\ndef task_submitted_by_driver():\n  futures = [leaf_task.remote() for _in range(4)]\n  ray.get(futures)\n\n@ray.remote\ndef leaf_task():\n  print(\"running leaf task\")\n  pass\n\ntasks = [task)submitted_by_driver.remote() for _ in range(2)]\nray.get(tasks)","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://gist.github.com/clarng/e4b7f538e518aa95a867903f5da09b3b"},"content":[{"nodeType":"text","value":"source","marks":[],"data":{}}]},{"nodeType":"text","value":"\n","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Below is the task execution graph, where the tasks are colored such that each color forms a group of tasks where they belong to the same task submitter:","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5nxHfYlUkFFXLYKnCyG5h3","type":"Asset","createdAt":"2023-03-02T03:30:14.306Z","updatedAt":"2023-03-02T03:30:14.306Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 3","description":"Figure 3: Grouping of candidate tasks for applying the policy","file":{"url":"//images.ctfassets.net/xjan103pcp94/5nxHfYlUkFFXLYKnCyG5h3/f16bb0235d04caf611d21da0c53effbb/image3.png","details":{"size":138483,"image":{"width":2000,"height":878}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this example, the tasks are divided into 3 groups:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The first group of two tasks are submitted by the driver.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The second group that contains four tasks are submitted by the blue task on the left.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The third group that contains four tasks are submitted by the blue task on the right.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Note the driver, which runs the main program, is not retriable and does not belong to any group.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If, at this point, the memory monitor sees the node memory usage is above the threshold, it will pick a task from the submitter with the most number of tasks, and kill its task which started the last. In the example, we assume the second groupâs tasks were started later than the third groupâs.","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"PPW41yWFvIKayDiyZnL6o","type":"Asset","createdAt":"2023-03-02T03:33:22.300Z","updatedAt":"2023-03-02T03:35:12.712Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Figure 4","description":"Figure 4: Applying the policy - the second groupâs last task is terminated\n\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/PPW41yWFvIKayDiyZnL6o/398197a6deb53bd3f051bda7979052a9/image1.png","details":{"size":142856,"image":{"width":1999,"height":834}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\nAfter the termination of the last task, if, at this point, the memory monitor sees again the node memory usage is still aboveÂ the threshold, it will repeat the process, and pick a task from the submitter with the most number of tasks:","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"8ScOxuIu2ivYsvE5P44BT","type":"Asset","createdAt":"2023-03-02T03:36:47.593Z","updatedAt":"2023-03-02T03:36:47.593Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 5","description":"Figure 5: Applying the policy the second time, when the memory usage is still above the threshold\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/8ScOxuIu2ivYsvE5P44BT/0fffeb644ace882df846c8017a14e9e2/image8.png","details":{"size":165002,"image":{"width":1984,"height":840}},"fileName":"image8.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3u42MzsLhwtzYMTL9rPRr9","type":"Asset","createdAt":"2023-03-02T03:37:57.547Z","updatedAt":"2023-03-02T03:37:57.547Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 6","description":"Figure 6: The third groupâs last task is terminated by the policy","file":{"url":"//images.ctfassets.net/xjan103pcp94/3u42MzsLhwtzYMTL9rPRr9/546acd2c83386843475e3c5607632b7f/image2.png","details":{"size":125279,"image":{"width":1976,"height":862}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The memory monitor avoids infinite loops of task retries by ensuring at least one task is able to run for each submitter on each node. If it is unable to ensure this, the workload will fail with an ","marks":[],"data":{}},{"nodeType":"text","value":"OutOfMemory","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" error. Note that this is only an issue for tasks, since the memory monitor will not indefinitely retry actors.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Below is a program where the memory-leaking task will fail immediately, since it is the last and only task submitted by the driver:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3A9Qhi3QHxLZyVGiWm2XNk","type":"Entry","createdAt":"2023-03-02T03:47:48.199Z","updatedAt":"2023-03-02T03:47:48.199Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"oom_code_exmaple_2","body":"import ray\n\n@ray_remote(max_tries=-1):\ndef leaks_memory():\n  chunks=[]\n  bits_to_allocate = 8 * 100 * 1024 * 1024    #100 MiB\n  while True:\n    chuncks.append([0] * bits_to_allocate)\n\ntry:\n  ray.get(leaks_memory.remote()\nexcept ray.exceptions.OutOfMemoryError as ex:\n  print(\"This task will throw an OutOfMemory error without retyring\")","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If the workload fails due to OutOfMemoryError, refer to the Ray ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-core/scheduling/ray-oom-prevention.html#addressing-memory-issues"},"content":[{"nodeType":"text","value":"documentation","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on how to address the issue.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"How to investigate OOM problems with the monitor and Ray Dashboard","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Imagine we have a program that runs two tasks in parallel and leaks memory constantly:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"175f1PGsxXqQrTCSk4fRaw","type":"Entry","createdAt":"2023-03-02T03:49:57.702Z","updatedAt":"2023-03-02T03:49:57.702Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"oom_code_example_3","body":"import ray\n\n@ray.remote(max_retries=-1)\ndef leaks_memory():\n    chunks = []\n    bits_to_allocate = 8 * 100 * 1024 * 1024  # ~100 MiB\n    while True:\n        chunks.append([0] * bits_to_allocate)\n\nray.get([leaks_memory.remote() for _ in range(2)]","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"With the Ray memory monitor turned on (which is the default since Ray 2.2), the driver will print the following message when the raylet has killed the workers due to the memory usage going above the threshold:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71NDrmSSU6gAQsYwGVQ2zF","type":"Entry","createdAt":"2023-03-02T03:51:31.685Z","updatedAt":"2023-03-02T03:53:25.690Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"oom_logout_put_1","body":"1 Workers (tasks / actors) killed due to memory pressure (OOM), \n0 Workers crashed due to other reasons at node \n(ID: 2c82620270df6b9dd7ae2791ef51ee4b5a9d5df9f795986c10dd219c, IP: \n172.31.183.172) over the last time period. To see more information about \nthe Workers killed on this node, \n\nuse `ray logs raylet.out -ip 172.31.183.172`\n\nRefer to the documentation on how to address the out of memory issue:\nhttps://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html.\n\nConsider provisioning more memory on this node or reducing task parallelism\nby requesting more CPUs per task. To adjust the kill threshold, set the \nenvironment variable `RAY_memory_usage_threshold` when starting Ray. \nTo disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n\ntask failed with OutOfMemoryError, which is expected\n","language":"txt"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This shows the raylet has killed 1 worker in the past 1 min (the default reporting interval). We can get details of the nodeâs memory usage from the raylet logs at the time the worker was killed. One way to quickly fetch the raylet logs is to issue âray logsâ from the command line on the head node, by copy-pasting the command provided in the message above.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"ray logs raylet.out -ip 172.31.183.172","marks":[{"type":"code"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If we browse the logs we will see the details of the memory usage at the time the worker was killed:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7qRdhCD2Ua28CPrI6B2FVf","type":"Entry","createdAt":"2023-03-02T03:59:21.729Z","updatedAt":"2023-03-02T03:59:21.729Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"oom_log_output_2","body":"Top 10 memory users:\nPID     MEM(GB) COMMAND\n2161    15.18   ray::leaks_memory\n2211    11.90   ray::leaks_memory\n1550    0.11    /home/ray/.vscode-hosted-server/vscode-reh-web-linux-x64/node --max-old-space-size=3072 /mnt/cluster...\n339     0.11    /home/ray/anaconda3/bin/python /home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/dashboa...\n1366    0.09    /home/ray/.vscode-hosted-server/vscode-reh-web-linux-x64/node /home/ray/.vscode-hosted-server/vscode...\n56      0.09    /home/ray/anaconda3/bin/python /home/ray/anaconda3/bin/anyscale session web_terminal_server --deploy...\n1583    0.08    python blog.py\n51      0.07    /home/ray/anaconda3/bin/python -m anyscale.snapshot_util autosnapshot\n246     0.06    /home/ray/anaconda3/lib/python3.7/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...\n1449    0.06    /home/ray/anaconda3/bin/python /efs/workspaces/expwrk_lsnsr1z7bflh4xlga32le7lj91/cluster_storage/vsc...\n","language":"txt"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Here we can see the two tasks consuming the majority of the memory on the node. Given the amount of memory consumed, it is likely due to a memory leak from the tasks.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"To double check, we can go to the Ray dashboard, and look at the node memory usage under the metrics tab:","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"40oprLMyYFPDO1lfIygisw","type":"Asset","createdAt":"2023-03-02T04:01:49.448Z","updatedAt":"2023-03-02T04:15:38.814Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Figure 7","description":"Figure 7: Memory usage spikes by tasks at it grows fast\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/40oprLMyYFPDO1lfIygisw/b82ee6c1c6734f3a89fb94099fb508f4/image9.png","details":{"size":126580,"image":{"width":922,"height":822}},"fileName":"image9.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Note sometimes the memory usage may grow too fast due to the number of tasks running in parallel. When that happens, the memory monitor may not be able to keep up with the memory growth, and the OS OOM killer will kick in as a fall back, when some process fails to allocate memory.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":" The OS OOM killer will kill a process that has high memory usage via SIGKILL. Ray also sets the oom score for the workers to reduce the likelihood that the OS will kill critical ray processes like the raylet. When this happens, tasks or actors will fail without a clear error message, and the driver will print an error message that looks like the following:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4ItqRjok4qEmT2Bp8lLFZ6","type":"Entry","createdAt":"2023-03-02T04:04:14.039Z","updatedAt":"2023-03-02T04:04:14.039Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"oom _log_output_3","body":"The actor is dead because its worker process has died. Worker exit type:\nUNEXPECTED_SYSTEM_EXIT Worker exit detail: Worker unexpectedly exits with a\nconnection error code 2. End of file. There are some potential root causes.\n\n(1) The process is killed by SIGKILL by OOM killer due to high memory usage.\n(2) ray stop --force is called. (3) The worker is crashed unexpectedly due \nto SIGSEGV or other unexpected errors.\n","language":"txt"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"To verify if this is caused by OOM, check the Ray dashboard to see if the node memory usage is close to the limit. Furthermore, depending on the OS, check the kernel logs to see if the OS OOM killer triggered and killed the worker. To find the logs on Ubuntu use the following command (may require sudo):","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"dmesg -T","marks":[{"type":"code"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Browsing the logs we can see the OS OOM killer has killed the task:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3TGo2jCkjWeHZNusWjZQfq","type":"Entry","createdAt":"2023-03-02T04:06:25.882Z","updatedAt":"2023-03-02T04:06:25.882Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"oom_log_output_4","body":"oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=431955d77fd24ac80cfdae518bcb1902c00eda5d733b5b8d8cfe364789ebb843,mems_allowed=0,oom_memcg=/docker/4\n31955d77fd24ac80cfdae518bcb1902c00eda5d733b5b8d8cfe364789ebb843,task_memcg=/docker/431955d77fd24ac80cfdae518bcb1902c00eda5d733b5b8d8cfe364789ebb843,task=ray::leaks_memo,pid=117644,uid=10\n00\n\nMemory cgroup out of memory: Killed process 117644 (ray::leaks_memo) total-vm:44400568kB, anon-rss:28903420kB, file-rss:32412kB, shmem-rss:0kB, UID:1000 pgtabl\nes:57140kB oom_score_adj:1000\n\noom_reaper: reaped process 117644 (ray::leaks_memo), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB\n\nray::leaks_memo invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=1000\n","language":"txt"}}},"content":[]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Conclusion","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"To sum up, OOM errors are pernicious and can degrade cluster usage if not detected and mitigated in a timely manner. We described why you need an OOM monitor and what were the primary motivations for it:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Common Python libraries and frameworks do not provide a policy-based monitor to preempt runaway-hungry memory code.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Out of the box, Python offers little support for easy detection and mitigation of memory-hungry applications. Nor do other distributed compute systems offer this policy-based detection and prevention novel feature.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"For Ray to scale ML workloads, itâs imperative that it can continue to run your ML application without distributing the Ray cluster while ensuring, monitoring, and preempting any memory leaks as part of your workload by preempting those tasks without disrupting the entire ML workload because of an OOM error.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"To achieve all this, the OOM monitor applies grouping policies in a hierarchical grouping to ascertain and detect the most likely candidate for preempting. This policy applies fairness to ensure arbitration of which candidate is selected based on a multi-step process that filters down to the selected worker for termination.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Enabled by default in Ray 2.2 and 2.3, you can further visually examine the OOM monitorâs effects and actions in the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-core/ray-dashboard.html"},"content":[{"nodeType":"text","value":"Ray Dashboard ","marks":[],"data":{}}]},{"nodeType":"text","value":"via the Metrics tab. Furthermore, all the OOM actions are logged for further perusal.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"As a memory metric lens into your Ray application and an ability to prevent and preempt any OOM errors are huge benefits for Ray observability and transparency of your ML workloads.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Whatâs next?","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"For any new open source feature, having a community have a go at it and provide feedback, either via slack or filing issues, is important for iterating and improving. So try it out and let us know should you run into issues.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Also, we gave a talk on the OOM memory monitor at our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/events/2023/01/26/ray-meetup-community-talks-january-2023"},"content":[{"nodeType":"text","value":"Ray Meetup","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". If you missed our previous Ray talk on observability and debugging Ray, you can view the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/events/2022/11/30/ray-community-meetup-talks-november-2022"},"content":[{"nodeType":"text","value":"meetup talk here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Join our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://forms.gle/9TSdDYUgxYs8SA9e8"},"content":[{"nodeType":"text","value":"Ray slack","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" for suggestions or questions on the ","marks":[],"data":{}},{"nodeType":"text","value":"#observability","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" channel.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Finally, if you are a Ray user and would like to share your Ray journey or use cases with the Ray global community, our","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.myeventi.events/raysummit23/cfp/"},"content":[{"nodeType":"text","value":" call for presentations","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" to the Ray Summit 2023 are open until","marks":[],"data":{}},{"nodeType":"text","value":" March 6.","marks":[{"type":"bold"}],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"HXsljnbL3DCWGkqud413u","type":"Asset","createdAt":"2023-03-02T03:16:33.643Z","updatedAt":"2023-03-02T04:14:18.712Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Figure 2","description":"Figure 2: OOM monitor high-level architecture and data flow","file":{"url":"//images.ctfassets.net/xjan103pcp94/HXsljnbL3DCWGkqud413u/a46c5174e409ae64f5594c4b8d23b463/image6.png","details":{"size":45709,"image":{"width":960,"height":540}},"fileName":"image6.png","contentType":"image/png"}}},"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7IvEvpd9fyWgDU7bprW9Oz","type":"Entry","createdAt":"2023-02-24T14:50:53.136Z","updatedAt":"2023-02-27T18:20:54.316Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Announcing Ray 2.3: performance improvements, new features and new platforms","seoTitle":"Announcing Ray 2.3: performance improvements, new features and new platforms","slug":"announcing-ray-2-3-performance-improvements-new-features-and-new-platforms","description":"The Ray 2.3 release features exciting improvements across the Ray ecosystem. In this blog post, we will highlight new features, performance enhancements, and support for new platforms. ","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3LSqSgEjlmC7vbiqMz8HsP","type":"Entry","createdAt":"2023-02-22T00:59:35.251Z","updatedAt":"2023-02-22T00:59:35.251Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Cade Daniel","slug":"cade-daniel","link":"https://www.linkedin.com/in/cade-daniel/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6xEJbq1IfaJwNFeM0tPFEy","type":"Entry","createdAt":"2021-01-13T18:21:16.252Z","updatedAt":"2021-01-13T18:32:11.251Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Zhe Zhang","slug":"zhe-zhang","link":"https://www.linkedin.com/in/zhezhang-zhz/"}}],"publishedDate":"2023-02-24","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"The Ray 2.3 release features exciting improvements across the Ray ecosystem. In this blog post, we will highlight new features, performance enhancements, and support for new platforms.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The Ray 2.3 release features exciting improvements across the Ray ecosystem. In this blog post, we will highlight new features, performance enhancements, and support for new platforms. In particular we want to highlight six overall additions in this release:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Observability enhancements to the Ray Dashboard","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Dataset Streaming (developer preview)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Boost in Ray core scheduling performance","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Additions of Gym/Gymnasium library to RLlib","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Support for ARM and Python 3.11","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Support for multiple applications for Ray Serve (developer preview)","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"SoJppwrNhisvy62kz3Kjn","type":"Asset","createdAt":"2023-02-22T01:03:21.254Z","updatedAt":"2023-02-24T18:20:24.072Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"main_release_image","description":"Ray 2.3 ","file":{"url":"//images.ctfassets.net/xjan103pcp94/SoJppwrNhisvy62kz3Kjn/5f587fdaeecb258f9955800b88d089f7/image6.png","details":{"size":158872,"image":{"width":960,"height":540}},"fileName":"image6.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For specific details, see the ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.3.0"},"content":[{"data":{},"marks":[],"value":"release notes","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on all the various improvements made across the Ray ecosystem. Letâs start with observability improvements.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Observability improvements to the Ray Dashboard","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"In 2.3, we restructured the Ray dashboard UI layout to improve the information hierarchy and usability. By taking a user-journey driven approach of organizing the dashboard, we organized the dashboard by top level concepts like jobs, cluster (nodes and autoscaler) and logs; provide better navigability so you can quickly click to go to the information you need; and added visualizations and content so that you can double click into details of your application.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3F2fRnrUZ221bN9ZlGUFOm","type":"Asset","createdAt":"2023-02-22T01:08:44.550Z","updatedAt":"2023-02-22T01:08:44.550Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"dashboard_1","description":"Figure 1. Overview of high-level lens into Ray resources and metrics","file":{"url":"//images.ctfassets.net/xjan103pcp94/3F2fRnrUZ221bN9ZlGUFOm/954c6ff6bd6398938768291b1706993a/image5.png","details":{"size":181406,"image":{"width":1839,"height":965}},"fileName":"image5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Additionally, we added a new ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"timeline view, ","nodeType":"text"},{"data":{},"marks":[],"value":"which is a higher level view that lets you optimize or debug errors in your job. As a result, you can quickly see how long tasks are taking to run in your application and how well the workload is distributed across all the workers in your cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6WBzJXXfyAsSRHl3s0SYvn","type":"Asset","createdAt":"2023-02-22T01:10:40.686Z","updatedAt":"2023-02-22T01:10:40.686Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"dashboard_2","description":"Figure 2: Task timeline view","file":{"url":"//images.ctfassets.net/xjan103pcp94/6WBzJXXfyAsSRHl3s0SYvn/2f07bd61772e13cfa66e92dc9fdc904a/image2.png","details":{"size":64662,"image":{"width":1472,"height":322}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ZJfwRtyVMzjR7JH3PMGz6","type":"Asset","createdAt":"2023-02-22T01:12:10.310Z","updatedAt":"2023-02-22T01:12:10.310Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"dashboard-3","description":"Figure 3: Granular task time lines ","file":{"url":"//images.ctfassets.net/xjan103pcp94/ZJfwRtyVMzjR7JH3PMGz6/855999e84f19ed35f7fd4bc9d1112935/image4.png","details":{"size":313776,"image":{"width":1999,"height":1106}},"fileName":"image4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we added improvements to the progress bar, which makes it easier to view tasks from a higher level grouping and to determine if errors occurred within the task itself or because a downstream dependency errored.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42vhIA8UN33G98Eiusb5j2","type":"Asset","createdAt":"2023-02-22T01:14:32.189Z","updatedAt":"2023-02-22T01:14:32.189Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"dashboard-4","description":"Figure 4: Additional drilled down into individual activities within a task","file":{"url":"//images.ctfassets.net/xjan103pcp94/42vhIA8UN33G98Eiusb5j2/2108e1b3247f28e999bcfcafe1b86157/image7.png","details":{"size":127069,"image":{"width":1676,"height":1280}},"fileName":"image7.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Dataset Streaming (developer preview)","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are two key machine learning (ML) workloads common during any ML pipeline. First is ingesting data, and second is doing batch inference; both demand high throughput and scale.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"training data ingest","nodeType":"text"},{"data":{},"marks":[],"value":": where distributed trainer processes (e.g., PyTorch workers), read, preprocess, and ingest data","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"batch inference","nodeType":"text"},{"data":{},"marks":[],"value":": where a pretrained model across a large dataset generates predictions for each batchÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Both workloads are extremely performance sensitive, for they require maximizing GPU utilization.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Prior to 2.3, Ray Dataset users have been generally successful when operating on small to medium datasets (e.g., 1-100GB) that fit in the Ray object store memory. However, users often struggle to use and work with the old Ray's pipelined data API to load and operate efficiently on larger-than-memory datasets. In particular, three operational issues surface immediately:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Data batch size and parallelism are challenging to tune, resulting in poor performance and frustration.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Performance is subpar due to suboptimal or unnecessary data conversions, copy, and materialization to object store.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Certain aspects of its execution model, such as recreating actor pools for each data batch/window, add significant overheads.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the 2.3 release, we introduce major performance developer preview changes to Datasets to address these above issues:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Introducing a ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Datasets streaming execution backend","nodeType":"text"},{"data":{},"marks":[],"value":" that improves efficiency and removes the need to carefully tune the configuration of ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/api/dataset_pipeline.html"},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"code"}],"value":"DatasetPipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"making","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" Datasets lazy by default","nodeType":"text"},{"data":{},"marks":[],"value":" -- meaning that each operation is not executed immediately, but is added to the execution plan (i.e., lazily). When a user calls a consumption/action API (e.g.,","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"fully_executed()","nodeType":"text"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"iter_batches()","nodeType":"text"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"take()","nodeType":"text"},{"data":{},"marks":[],"value":" etc), all operations within the execution plan are executed togetherÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"introducing a","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" dataset Iterator ","nodeType":"text"},{"data":{},"marks":[],"value":"that replaces ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Dataset","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"DatasetPipeline","nodeType":"text"},{"data":{},"marks":[],"value":" as the default data iterator interface in AIR trainersÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Below is a code example of how to use the default streaming execution backend.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7eofm8rOV4Ml4s9I8YKCN9","type":"Entry","createdAt":"2023-02-22T01:17:40.746Z","updatedAt":"2023-02-22T01:17:40.746Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"dataset streaming code","body":"import ray\nimport time\nray.data.context.DatasetContext.get_current().use_streaming_executor = True\n\ndef sleep(x):\n    time.sleep(0.1)\n    return x\n\nfor _ in (\n    ray.data.range_tensor(5000, shape=(80, 80, 3), parallelism=200)\n    .map_batches(sleep, num_cpus=2)\n    .map_batches(sleep, compute=ray.data.ActorPoolStrategy(2, 4))\n    .map_batches(sleep, num_cpus=1)\n    .iter_batches()\n):\n    pass\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This launches a simple 4-stage pipeline. We use different compute args for each stage, which forces them to be run as separate operators instead of getting fused together. You should see a log message indicating streaming execution is being used:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1HRLZ3YQ9hZNH53G308YPE","type":"Entry","createdAt":"2023-02-22T01:21:12.864Z","updatedAt":"2023-02-22T01:23:40.871Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_data_console_output","body":"2023-01-2414:59:06,327\nINFO streaming_executor.py:57 -- Executing DAG InputDataBuffer[Input]\n\n-\u003e MapOperator[read] -\u003e MapOperator[map_batches] \n-\u003e MapOperator[map_batches] -\u003e MapOperator[map_batches]\n","language":"text"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Currently, the Ray dataset streaming execution backend is in ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/document/d/1BXd1cGexDnqHAIVoxTnV3BV0sklO9UXqPwSdHukExhY/edit#heading=h.dkdm0ffungpx"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"developer preview","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and not yet ready for production, while the other two (lazy execution and dataset iterator) are enabled by default. You can find ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/document/d/1BXd1cGexDnqHAIVoxTnV3BV0sklO9UXqPwSdHukExhY/edit#heading=h.dkdm0ffungpx"},"content":[{"data":{},"marks":[],"value":"the developer guide here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scheduling speed improvement for parallel workloads","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Worker startup is one of the primary overheads when running jobs in Ray. It is very common that the first set of runs (e.g., starting a bunch of actors or starting many tasks) is way slower than later runs because of the worker startup overhead.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We have worked on performance enhancements for launch times of actors or tasks to improve this user experience. As the chart below shows, a ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"8X ","nodeType":"text"},{"data":{},"marks":[],"value":"improvement.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3fRPOdap7LCwiWOc3O5auT","type":"Asset","createdAt":"2023-02-22T01:27:29.309Z","updatedAt":"2023-02-24T14:44:40.468Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"startup_improvements","description":"Figure 5: Worker startup benchmark with 8X improvements","file":{"url":"//images.ctfassets.net/xjan103pcp94/3fRPOdap7LCwiWOc3O5auT/bf3db531bd8167db548206084798e263/image1.png","details":{"size":56940,"image":{"width":1200,"height":742}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"All these improvements require no API code changes. They are done under the hood, so you can take advantage of these performance improvements.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Gym/Gymnasium migration for RLlib","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In releases prior to Ray 2.3, RLlib supported ","nodeType":"text"},{"data":{"uri":"https://github.com/openai/gym"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"gym","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" version 0.23.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"gym","nodeType":"text"},{"data":{},"marks":[],"value":" was updated to 0.26.x some time ago. The upgrade resulted in breaking API changes that RLlib did not support. Furthermore, ","nodeType":"text"},{"data":{"uri":"https://www.gymlibrary.dev/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"OpenAI has dropped the support for their original gym library","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in favor of a new ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"gymnasium","nodeType":"text"},{"data":{},"marks":[],"value":", which is a drop in replacement maintained by the ","nodeType":"text"},{"data":{"uri":"https://github.com/farama-foundation"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Farama Foundation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In 2.3, RLlib now supports the new gymnasium and ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"gym\u003e=0.26.0","nodeType":"text"},{"data":{},"marks":[],"value":" APIs to ensure that the library remains up to date with the latest developments in the field of RL.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Options to auto-convert existing ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"(gym \u003c 0.26.0)","nodeType":"text"},{"data":{},"marks":[],"value":" Env subclasses have been added such that a transition from Ray 2.2 to Ray 2.3 should be completely seamless.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For more details about the migration path, please consult the ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/document/d/1lxYK1dI5s0Wo_jmB6V6XiP-_aEBsXDykXkD1AXRase4/edit#"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"migration guide","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Support for ARM and Python 3.11","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We are excited to announce support for:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/issues/21786"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"ARM64","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" â Ray now builds Docker images and wheels for Linux ARM64.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/issues/27881"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Python 3.11","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" â Ray 2.3 ships with 3.11 Linux wheels. Mac and Windows wheels are planned for a later Ray release.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"This addresses some of the top questions and requests from the community since mid-2022, bringing Ray support up to date with emerging platforms.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Multi-application support for Ray Serve (developer preview)","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the 2.0 release, the Ray Serve project introduced a new major experimental API centered around ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/package-ref.html"},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"code"}],"value":"serve.run","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", as a way to deploy and run your Serve deployment. However, after the 2.0 release and its subsequent use by users, we heard from them that they need to manage multiple applications on a single cluster and would want to extend the functionality to support it. This includes separate individual models, deployment graphs, and/or FastAPI apps.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To that end, in the 2.3 release, Ray Serve offers experimental support for multiple applications on the same Serve cluster with 2.x API, with the ability to deploy and delete applications separately. Note that this API is experimental, and we would love your feedback.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7Ls7OUPo91Iqepchb9j9fm","type":"Entry","createdAt":"2023-02-22T01:43:58.165Z","updatedAt":"2023-02-22T01:47:19.972Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"multi-app-launch","body":"from ray import serve\nimport requests\nimport startlette.requests\n\n@serve.deployment\nclass Model:\n  def __init__(self, model_name):\n    # some model loading logic\n    self.model_name = model_name\n\n  async def __call__(self, req: startlette.requests.Request):\n    return self.model\n\nmodel1 = Model.bind(\"Model1\")\nserve.run(model1, app=\"app1\", route_prefix=\"/app1\")\n\nmodel2 = Model.bind(\"Model2\")\nserve.run(model2, app=\"app2\", route_prefix=\"/app2\")\n\nrequests.post(\"http://127.0.0.1:8000/app1\", json={\"text\": \"hello\"}\n#Model1\n\nrequests.post(\"http://127.0.0.1:8000/app2\", json={\"text\": \"world\"}\n#Model2","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Concluding comments","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"First and foremost, we want to thank all contributors for their valuable contributions to this new release of Ray 2.3. Your enduring support continues to foster the wider use of Ray adoption.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We have a ton of exciting improvements planned in subsequent Ray releases ahead with focus on bolstering stability, improving performance, extending integration with larger Python and ML ecosystem, and offering observability into Ray jobs and clusters, so please let us know if you have any questions or feedback.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Have a go at the latest release with ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"pip install -U \"ray[default]\"","nodeType":"text"},{"data":{},"marks":[],"value":" and let us know of your feedback. Weâre always interested to hear from you â feel free to reach out to us on ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Github","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discourse","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have our Ray Summit 2023 coming up later in the year. If you have a Ray story or use case to share with the global Ray Community, we are ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-summit-2023-call-for-proposals-is-now-open"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"accepting proposals for speakers","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"!\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"SoJppwrNhisvy62kz3Kjn","type":"Asset","createdAt":"2023-02-22T01:03:21.254Z","updatedAt":"2023-02-24T18:20:24.072Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"main_release_image","description":"Ray 2.3 ","file":{"url":"//images.ctfassets.net/xjan103pcp94/SoJppwrNhisvy62kz3Kjn/5f587fdaeecb258f9955800b88d089f7/image6.png","details":{"size":158872,"image":{"width":960,"height":540}},"fileName":"image6.png","contentType":"image/png"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"mL2pcw31fmyQcsj7INsPN","type":"Entry","createdAt":"2022-04-25T21:41:59.997Z","updatedAt":"2022-04-25T21:41:59.997Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Ray Train: Production-ready distributed deep learning","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"daYk6TnKli7zugQmS8OY1","type":"Entry","createdAt":"2022-01-31T21:06:54.819Z","updatedAt":"2022-06-28T16:49:15.161Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":17,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"event"}},"locale":"en-US"},"fields":{"title":"Ray Train: Production-ready distributed deep learning","slug":"ray-train-production-ready-distributed-deep-learning","startDate":"2022-02-09T09:00-08:00","endDate":"2022-02-09T10:00-08:00","category":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3IY1nTIqpsQSkyGN72wgym","type":"Entry","createdAt":"2022-06-28T16:20:55.714Z","updatedAt":"2022-06-28T17:42:17.187Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"eventCategory"}},"locale":"en-US"},"fields":{"title":"Webinar","slug":"webinars"}},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7kS5Mc1LeZRPqVlItSGMNI","type":"Asset","createdAt":"2022-01-25T05:17:10.580Z","updatedAt":"2022-06-06T23:16:27.147Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"locale":"en-US"},"fields":{"title":"Ray Train","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7kS5Mc1LeZRPqVlItSGMNI/28c234fac2d658dbca32e136b2030396/1382642_BlogImageIllustration-11_2_060322.jpg","details":{"size":545837,"image":{"width":1500,"height":1000}},"fileName":"1382642_BlogImageIllustration-11_2_060322.jpg","contentType":"image/jpeg"}}},"videoLink":"https://youtu.be/oQ0N-G7MsTw","summary":"Today, most frameworks for deep learning prototyping, training, and distributing to a cluster are either powerful and inflexible, or nimble and toy-like. Data scientists are forced to choose between a great developer experience and a production-ready framework. \n\nTo fix this gap, the Ray ML team has developed Ray Train. ","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train is a library built on top of the Ray ecosystem that simplifies distributed deep learning. Currently in stable beta in Ray 1.9, Ray Train offers the following features: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scales to multi-GPU and multi-node training with zero code changes ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Runs seamlessly on any cloud (AWS, GCP, Azure, Kubernetes, or on-prem) ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Supports PyTorch, TensorFlow, and Horovod ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed data shuffling and loading with Ray Datasets ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed hyperparameter tuning with Ray Tune","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Built-in loggers for TensorBoard and MLflow ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this webinar, we'll talk through some of the challenges in large-scale computer vision ML training, and show a demo of Ray Train in action.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Resources","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://speakerdeck.com/anyscale/ray-train-production-ready-distributed-deep-learning"},"content":[{"data":{},"marks":[],"value":"Slides \u003e\u003e","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-5"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.google.com/document/d/1zRgwyabrffjy9sffw_XNfu8GkrNrv3bLOodgbRRTnsk/edit"},"content":[{"data":{},"marks":[],"value":"Webinar Q\u0026A \u003e\u003e","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-5"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/matthewdeng/ray-train-demos"},"content":[{"data":{},"marks":[],"value":"GitHub: Ray Train demos \u003e\u003e","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-5"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/train.html"},"content":[{"data":{},"marks":[],"value":"Ray Train Docs \u003e\u003e","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-5"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/architecture.html"},"content":[{"data":{},"marks":[],"value":"How Ray Train parallelizes data and distributes computation \u003e\u003e","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-5"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"speakers":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3XZBgZdQF0b0hXsYTk9ymf","type":"Entry","createdAt":"2021-10-06T16:35:26.676Z","updatedAt":"2021-10-19T16:04:46.969Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Will Drevo","position":"Product Manager, Anyscale","affiliation":"Anyscale","bio":"Will is a Product Manager for ML at Anyscale. Previously, he was the first ML Engineer at Coinbase, and ran a couple of ML-related startups, one in the data labeling space and the other in the pharmaceutical space. He has a BS in CS and Music Composition from MIT, and did his master's thesis at MIT in machine learning systems. In his spare time, he produces electronic music, travels, and tries to find the best Ethiopian food in the Bay Area.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7nlMFH6QxymR43jMVfhLpp","type":"Asset","createdAt":"2021-10-06T16:35:20.496Z","updatedAt":"2021-10-06T16:35:20.496Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Will Drevo","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7nlMFH6QxymR43jMVfhLpp/e0d43d13afa5117a8ed6e814638b671a/Will_Drevo.jpeg","details":{"size":20940,"image":{"width":361,"height":361}},"fileName":"Will_Drevo.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6xUgY2DL1lQTVYZi9BwOts","type":"Entry","createdAt":"2022-01-28T23:25:48.801Z","updatedAt":"2022-01-31T22:27:32.120Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Matthew Deng","position":"Software Engineer ","affiliation":"Software Engineer, Anyscale","bio":"Matthew Deng is a software engineer at Anyscale where he works on distributed machine learning libraries built on top of Ray. Before that, he was a software engineer at LinkedIn. He holds a BS in Electrical Engineering and Computer Science from UC Berkeley.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1qWWz3bTjcePpvnRxihDzU","type":"Asset","createdAt":"2022-01-31T22:27:26.678Z","updatedAt":"2022-01-31T22:27:26.678Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Matthew Deng","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1qWWz3bTjcePpvnRxihDzU/5872139b24bb33b347b74c7a585d0e26/Matt_Deng.jpg","details":{"size":39301,"image":{"width":435,"height":435}},"fileName":"Matt_Deng.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3SvWPYblmudEPyDCFJPf6g","type":"Entry","createdAt":"2021-04-23T22:29:26.083Z","updatedAt":"2021-05-14T16:07:26.109Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","position":"Software Engineer, Anyscale","affiliation":"Anyscale","bio":"Amog Kamsetty is a software engineer at Anyscale where he works on building distributed training libraries and integrations on top of Ray. He previously completed his MS degree at UC Berkeley working with Ion Stoica on machine learning for database systems.\n","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ai9DjjcGVGsx5wCIQgNrE","type":"Asset","createdAt":"2021-04-23T22:27:16.833Z","updatedAt":"2021-04-23T22:27:16.833Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/6ai9DjjcGVGsx5wCIQgNrE/e0339f546995da6cb3f8a408d3fd76a7/Amog_Kamsetty.jpg","details":{"size":847883,"image":{"width":2125,"height":2125}},"fileName":"Amog_Kamsetty.jpg","contentType":"image/jpeg"}}}}}],"form":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7C3GkiBGQ7PWOCkfQD6XKF","type":"Entry","createdAt":"2022-01-28T23:11:30.765Z","updatedAt":"2022-02-10T19:52:02.867Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"form"}},"locale":"en-US"},"fields":{"identifier":"Ray-train-(part 1)-Jan-2022","title":"View Presentation","formId":"377fc345-40b8-42e7-bc20-eaa745cf6233","submitText":"Watch on-demand"}},"gatedType":"On-demand","gatedDescription":"Register below to access video recording","tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}}],"recommendations":[]}},"ctaText":"Watch on demand","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4pvt4qBgpargy3hSa4cICE","type":"Asset","createdAt":"2022-03-24T19:32:14.959Z","updatedAt":"2022-03-24T19:32:14.959Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-neural-net-light","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4pvt4qBgpargy3hSa4cICE/f573f93ebc1283f5e78948e9a6bf0ff3/blog-recommended-content-neural-net-light.jpg","details":{"size":42583,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-neural-net-light.jpg","contentType":"image/jpeg"}}},"recommendations":null}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}}],"hideIntro":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"23osxhLC6e4DesLLFltpAl","type":"Entry","createdAt":"2023-01-19T20:18:39.796Z","updatedAt":"2023-02-16T15:24:21.123Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Many Models Batch Training at Scale with Ray Core","seoTitle":"How to Conduct Many-Model Batch Training at Scale with Ray","slug":"many-models-batch-training-at-scale-with-ray-core","description":"Learn how to conduct batch training using only Ray Core and stateless Ray tasks. We take an in-depth look at two approaches to employ Ray tasks to scale.","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BGRfsYFKUgIpGjaXTUYml","type":"Entry","createdAt":"2021-08-09T15:38:43.390Z","updatedAt":"2021-08-09T15:38:43.390Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Antoni Baum","slug":"antoni-baum","link":"https://www.linkedin.com/in/yard1/"}}],"publishedDate":"2023-01-19","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Batch processing is often employed when grappling with large amounts of data.Â Historically, this scheme has been pervasive in data engineering tasks: building scalable data pipelines; extracting, transforming, loading (ETL) data from myriad data sources into a central or common data store.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"More recently, batch processing has become equally pervasive and increasingly common for machine learning training, tuning or model scoring. Simple common use cases include time-series forecasting; training hundreds or thousands of models, each model for a unique product SKU or a geographical zone, such as zip code or a pick up location, each trained on its specific batch of data; or even a unique customer id for personalized model product recommendations, each batch holding data pertinent to the unique customer id.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Two Approaches to Many-Model Batch Training","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In a previous blog, we ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/training-one-million-machine-learning-models-in-record-time-with-ray"},"content":[{"nodeType":"text","value":"explained why train hundreds of thousands of models","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", why Ray is being used for these many models, and how to use ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.linkedin.com/posts/dmatrix_llm-foundational-ray-activity-7013545452478365696-1QKx?utm_source=share\u0026utm_medium=member_desktop"},"content":[{"nodeType":"text","value":"different approaches with Ray Core APIs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" to accomplish this endeavor at scale.","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11LiFRlfWQUsx8mHP9Hnad","type":"Asset","createdAt":"2023-01-18T21:05:50.950Z","updatedAt":"2023-01-18T21:05:50.950Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"NYC taxi dirver data sete","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/11LiFRlfWQUsx8mHP9Hnad/8d8428f9096a5d3b4e1aaab33da2e820/image16.png","details":{"size":3914440,"image":{"width":2000,"height":1000}},"fileName":"image16.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"source: ","marks":[{"type":"italic"}],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/justin-hj-kim/NYCtaxi_data_science"},"content":[{"nodeType":"text","value":"https://github.com/justin-hj-kim/NYCtaxi_data_science","marks":[{"type":"underline"},{"type":"italic"}],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this blog, we continue to demonstrate yet another example of how to conduct batch training on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"},"content":[{"nodeType":"text","value":"NYC Taxi Dataset ","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"using only Ray Core and stateless Ray tasks. Because Ray tasks are asynchronous and can be embarrassingly parallelized, we will examine two approaches to employ Ray tasks to scale:","marks":[],"data":{}}]},{"nodeType":"ordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Distributed data loading","marks":[{"type":"bold"}],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Centralized data loading","marks":[{"type":"bold"}],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The first approach is distributed data loading. That is, delegate each independent task to read its respective batch into memory, ensuring that the desired data fits into memory. The second approach is centralized data loading. We preload each data partition once into the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-core/objects.html#objects-in-ray"},"content":[{"nodeType":"text","value":"Ray object store","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", and extract each batch per location_id, store it into the Ray object store, from which each task fetches its batch data via object references, albeit at a higher cost of memory to reduce reading and training times.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"But first, let's peek at our data, understand what relationship we seek to establish among features, what transformation or projection we will need, and what features we want to train on.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Glimpse at the NYC taxi data","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The NYC data set contains many columns that are not of much interest to us for our task at hand, so we can discard or filter out, and only focus on columns of interest. For our use case, we want to establish a relationship between the pickup location and drop off location, and the trip duration. Because each drop off and pickup location relationship will vary at different times of the day, we need to train a separate model, each for a combination of pickup location-month combination as a batch, as shown in the Figure 1.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It turns out that the data is already partitioned into each month and year, so we can use the ","marks":[],"data":{}},{"nodeType":"text","value":"pickup_location_id ","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":"column in the dataset to project and group it into respective data batches. Using these features, we can fit three distinct scikit-learn linear models for each batch and choose the best MAE score.","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6I0bQiDiJZL2iKGIwYXarM","type":"Asset","createdAt":"2023-01-18T21:06:11.784Z","updatedAt":"2023-01-18T21:33:15.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"NYC data set by location id","description":"DataFrame with relevant features per pickup location id for training","file":{"url":"//images.ctfassets.net/xjan103pcp94/6I0bQiDiJZL2iKGIwYXarM/dc0537d5eca617f03267f2eee61b0435/image7.png","details":{"size":157764,"image":{"width":1999,"height":504}},"fileName":"image7.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Anyscale Ray cluster configurations, dataset sizes, and models","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We used ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/"},"content":[{"nodeType":"text","value":"Anyscale","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" Ray clusters with the configurations described below for this machine learning workload. Anyscale provides a seamlessly easy way to configure, provision, launch, autoscale, and manage Ray clusters, along with insightful ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-core/ray-dashboard.html"},"content":[{"nodeType":"text","value":"Ray Dashboard ","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"metrics to gauge and observe your jobsâ progress. For example, see all the Ray dashboard figures below.","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7uNuYer59Oqjp6M7EkQVHV","type":"Asset","createdAt":"2023-01-18T21:06:32.726Z","updatedAt":"2023-01-18T21:06:32.726Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Anyscale home page","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7uNuYer59Oqjp6M7EkQVHV/903ae39f885a517cef680a18e3805f60/image9.png","details":{"size":260734,"image":{"width":1999,"height":1117}},"fileName":"image9.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Data: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"In all, we use 18 months of data (the year 2018 and six months of the year 2019), collectively giving us 18 files, each file withÂ  ~7M rows, so a total of 126M rows.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Anyscale Ray cluster configuration","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":":Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"64 CPU cores","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"4-5 m5.2xlarge AWS EC2 instances","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"124 GiB RAM + 51.38 GiB Node memory","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Models:","marks":[{"type":"bold"}],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Three scikit-learn model linear estimators trained and fitted for each unique pickup_locations_id","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"LinearRegression,","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" ","marks":[{"type":"italic"}],"data":{}},{"nodeType":"text","value":"DecisionTreeRegressor","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":", ","marks":[{"type":"italic"}],"data":{}},{"nodeType":"text","value":"DecisionTreeRegressor","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":", with random ","marks":[],"data":{}},{"nodeType":"text","value":"splitter","marks":[{"type":"code"}],"data":{}}]}]}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Total models trained ","marks":[],"data":{}},{"nodeType":"text","value":"14100","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" for a total of ","marks":[],"data":{}},{"nodeType":"text","value":"4700","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" unique","marks":[],"data":{}},{"nodeType":"text","value":" ","marks":[{"type":"italic"}],"data":{}},{"nodeType":"text","value":"pickup_locations_id","marks":[{"type":"code"}],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Approach 1 (Distributed): A task per batch loading data into memory","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this approach, we divide our training of models into three modular and functional work:","marks":[],"data":{}}]},{"nodeType":"ordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Reading the parquet data","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Creating Ray tasks to preprocess, train, and evaluate data batches","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Dividing data into batches to spawn a Ray task for each batchÂ ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Each of these units of functional work is modularized as a Python function, defined in ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/dmatrix/misc-code/blob/master/py/ray/tasks/mmd_utils.py"},"content":[{"nodeType":"text","value":"mmd_utils.py","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"1. Reading the parquet data","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Using the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://arrow.apache.org/docs/python/index.html"},"content":[{"nodeType":"text","value":"PyArrow","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://parquet.apache.org/"},"content":[{"nodeType":"text","value":"Parquet","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" push-down predicate, each task reads a parquet file, projects and extracts the batch we want to train and fit the model on, providing all the rows associated with a ","marks":[],"data":{}},{"nodeType":"text","value":"pickup_location_id","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":". Achieved through the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/dmatrix/misc-code/blob/master/py/ray/tasks/mmd_utils.py"},"content":[{"nodeType":"text","value":" read_data()","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" function in each Ray task, the function reads data and extracts batches separately. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pdK4Tgh7QxbKoK8avqnK","type":"Entry","createdAt":"2023-01-18T21:08:22.888Z","updatedAt":"2023-01-18T21:08:22.888Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"read_data+code","body":"def read_data(file: str, pickup_location_id: int) -\u003e pd.DataFrame:\n   \"\"\"\n   Read a file into a PyArrow table, convert to pandas and return \n   as Pandas DataFrame. Use push-down predicates since PyArrow\n   supports it and only extract the needed fields, filtered\n   on pickup_location_id\n   Args:\n       file: str path to the parquet file containing data\n       pickup_location_id: int id to filter out\n   Returns:\n       Pandas DataFrame filtered by pickup_location_id and respective\n       columns\n   \"\"\"\n   return pq.read_table(\n       file,\n       filters=[(\"pickup_location_id\", \"=\", pickup_location_id)],\n       columns=[\n           \"pickup_at\",\n           \"dropoff_at\",\n           \"pickup_location_id\",\n           \"dropoff_location_id\",\n       ],\n   ).to_pandas()","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"By breaking into batches specific to a ","marks":[],"data":{}},{"nodeType":"text","value":"pickup_location_id","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":",","marks":[{"type":"italic"}],"data":{}},{"nodeType":"text","value":" we avoid loading the entire partition into memory, preventing OOM errors, and extracting the desired data per ","marks":[],"data":{}},{"nodeType":"text","value":"pickup_location_id","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":". ","marks":[{"type":"italic"}],"data":{}},{"nodeType":"text","value":"Converting it to pandas allows us to train with scikit-learn estimators.","marks":[],"data":{}}]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"2. Creating Ray tasks to preprocess, train and evaluate data batches","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In order to compute a trip duration, we transform our batch data pick and drop off times in standard date format to compute our duration, the time we want to predict. As part of this transformation, we fill in any missing entries. This transformation is done per task per its respective batch.","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4TQ0JC0ocTwaOi121ONJZR","type":"Entry","createdAt":"2023-01-18T21:08:32.206Z","updatedAt":"2023-01-18T21:08:32.206Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"transformation_code","body":"def transform_batch(df: pd.DataFrame) -\u003e pd.DataFrame:\n    \"\"\"\n    Given a Pandas DataFrame as an argument, tranform time format \n    for the pickup and drop times, and return the transformed Pandas\n    DataFrame. Having the duration in only seconds helps for easy\n    math operations.\n    Args:\n        df: Pandas DataFrame to be transformed\n    Returns:\n        a transformed Pandas DataFrame with time formats and duration\n        in seconds as an additonal column\n    \"\"\"\n    df[\"pickup_at\"] = pd.to_datetime(\n        df[\"pickup_at\"], format=\"%Y-%m-%d %H:%M:%S\"\n    )\n    df[\"dropoff_at\"] = pd.to_datetime(\n        df[\"dropoff_at\"], format=\"%Y-%m-%d %H:%M:%S\"\n    )\n    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds\n    df[\"pickup_location_id\"] = df[\"pickup_location_id\"].fillna(-1)\n    df[\"dropoff_location_id\"] = df[\"dropoff_location_id\"].fillna(-1)\n    return df","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Once transformed, we use the transformed DataFrame to fit and score the scikit-learn model and calculate the mean absolute error (MAE) on the validation set, giving us a simple regression model to predict the relationship between the pick up and drop-off location and the trip duration.","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1PaThZxvsTnqj9x3onCyiy","type":"Entry","createdAt":"2023-01-18T21:08:42.279Z","updatedAt":"2023-01-18T21:08:42.279Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"fit_and_score_code","body":"@ray.remote\ndef fit_and_score_sklearn(\n    train: pd.DataFrame, test: pd.DataFrame, model: BaseEstimator\n) -\u003e Tuple[BaseEstimator, float]:\n    \"\"\"\n    A Ray remote task that fits and scores a sklearn model base estimator with the train and test \n    data set supplied. Each Ray task will train on its respective batch of dataframe comprised of\n    a pickup_location_id.The model will establish a linear relationship between the dropoff location\n    and the trip duration.\n    Args:\n        train: Pandas DataFrame training data\n        test: Pandas DataFrame test data\n        model: sklearn BaseEstimator\n    Returns: \n        a Tuple of a fitted model and its corrosponding mean absolute error (MAE)\n    \"\"\"\n    train_X = train[[\"dropoff_location_id\"]]\n    train_y = train[\"trip_duration\"]\n    test_X = test[[\"dropoff_location_id\"]]\n    test_y = test[\"trip_duration\"]\n\n    # Start training.\n    model = model.fit(train_X, train_y)\n    pred_y = model.predict(test_X)\n    error = round(mean_absolute_error(test_y, pred_y), 3)\n    return model, error","language":"Python"}}},"content":[]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"3. Dividing data into batches to spawn a Ray task for each batch","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Finally, we define a ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/dmatrix/misc-code/blob/master/py/ray/tasks/mmd_utils.py"},"content":[{"nodeType":"text","value":"train_and_evaluate()","marks":[{"type":"underline"},{"type":"code"}],"data":{}}]},{"nodeType":"text","value":" Ray task that embodies all necessary logic to load a data batch, transform it, split it into train and test, and fit and evaluate models on it. Returning a tuple to the file path and location id used for training can map the fitted models back to","marks":[],"data":{}},{"nodeType":"text","value":" pick_location_id","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" for experimental tracking, say with ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/tune/examples/tune-mlflow.html"},"content":[{"nodeType":"text","value":"MLflow","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" or ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/tune/examples/tune-wandb.html"},"content":[{"nodeType":"text","value":"Weight \u0026 Biases","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"To load and transform data, we use the ","marks":[],"data":{}},{"nodeType":"text","value":"read_data()","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"text","value":"transform_batch()","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" ","marks":[{"type":"italic"}],"data":{}},{"nodeType":"text","value":"functions. For blog brevity, see the code details in ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/dmatrix/misc-code/blob/master/py/ray/tasks/mmd_utils.py"},"content":[{"nodeType":"text","value":"mmd_utils.py","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". The driver notebook ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/dmatrix/misc-code/blob/master/py/ray/tasks/mmd_tasks.ipynb"},"content":[{"nodeType":"text","value":"mmd_tasks.ipynb","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" runs and trains the models in incremental batches of three files, culminating to a total of 18 files, prints the cumulative stats, and plots training times. There is an equivalent Python command line driver ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/dmatrix/misc-code/blob/master/py/ray/tasks/mmd_tasks.py"},"content":[{"nodeType":"text","value":"mmd_tasks.py","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" that produces the same results. (See the figures below.)","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now, let's consider an optimized approach, working with the same data and workload but with lesser training times by using ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-core/objects.html"},"content":[{"nodeType":"text","value":"Rayâs object store","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Approach 2 (Centralized): A task per batch loading data from Ray object store","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This approach assumes two things: 1) youâve sufficient memory for the object store and enough memory in each node in the cluster and 2) you donât mind a higher memory cost with the merits and benefits of lower execution and training times.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We optimize by loading and processing each partition into memory, extract all the relevant batches for the ","marks":[],"data":{}},{"nodeType":"text","value":"pick_location_id","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":", and store the batchesâ references into Rayâs object store.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"At the heart of this optimization is the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/dmatrix/misc-code/blob/master/py/ray/tasks/mmo_utils.py"},"content":[{"nodeType":"text","value":"read_into_object_store()","marks":[{"type":"code"}],"data":{}}]},{"nodeType":"text","value":" function. Four optimization techniques are used here: 1) delay calling ray.get() until necessary or when batch data is needed 2) the function yields or returns a ","marks":[],"data":{}},{"nodeType":"text","value":"ray.ObjectRefGenerator","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" 3) the returned object reference generator, used as an iterator in the calling function to yield object ref, is sent to another remote Ray task","marks":[],"data":{}},{"nodeType":"text","value":" ","marks":[{"type":"italic"}],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/dmatrix/misc-code/blob/master/py/ray/tasks/mmo_utils.py"},"content":[{"nodeType":"text","value":"train_and_evaluate_optimized.remote(...)","marks":[{"type":"underline"},{"type":"code"}],"data":{}}]},{"nodeType":"text","value":", and 4) use the SPREAD scheduling strategy to load each file on a separate node as an OOM safeguard.","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YWaR7o7qel0GAzXSlt4EE","type":"Entry","createdAt":"2023-01-18T21:15:15.677Z","updatedAt":"2023-01-18T21:15:15.677Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"read_data_into_object_store","body":"@ray.remote(num_returns=\"dynamic\")\ndef read_into_object_store(file: str) -\u003e ray.ObjectRefGenerator:\n    \"\"\"\n    This function creates a Ray Task that is a generator, returning an\n    object reference generator. Read the table from the file. It stores the data\n    into the Ray object store. \n    Args: \n        str path to the file name\n    Returns:\n        Yields Ray Object reference as tuple of pickup_id and associated batch data\n    \"\"\"\n    # print(f\"Loading {file} into arrow table\")\n    # Read the entire single file into memory.\n    try:\n        locdf = pq.read_table(\n            file,\n            columns=[\n                \"pickup_at\",\n                \"dropoff_at\",\n                \"pickup_location_id\",\n                \"dropoff_location_id\",\n            ],\n        )\n        # print(f\"Size of pyarrow table: {locdf.shape}\")\n    except Exception:\n        return []\n\n    pickup_location_ids = locdf[\"pickup_location_id\"].unique()\n\n    for pickup_location_id in pickup_location_ids:\n        # Each id-data batch tuple will be put as a separate object into the Ray object store,\n        # part of the yield statement\n        # Cast PyArrow scalar to Python if needed.\n        try:\n            pickup_location_id = pickup_location_id.as_py()\n        except Exception:\n            pass\n\n        yield (\n            pickup_location_id,\n            locdf.filter(\n                pc.equal(locdf[\"pickup_location_id\"], pickup_location_id)\n            ).to_pandas(),\n        )","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Together, these optimization techniques allow for the batch data to ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-core/patterns/unnecessary-ray-get.html#unnecessary-ray-get"},"content":[{"nodeType":"text","value":"stay in the object store until it is actually needed","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Again, for blog brevity, we refer to the relevant functionsâ code defined in ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/dmatrix/misc-code/blob/master/py/ray/tasks/mmo_utils.py"},"content":[{"nodeType":"text","value":"mmo_utils.py","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", particularly the ","marks":[],"data":{}},{"nodeType":"text","value":"read_into_object_store()","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"text","value":"train_and_evaluate_optimized.remote(...)","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" functions. Like its counterpart approach 1 above, there are corresponding Python drivers command line and notebook ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/dmatrix/misc-code/blob/master/py/ray/tasks/mmo_tasks.py"},"content":[{"nodeType":"text","value":"mmo_tasks.py","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/dmatrix/misc-code/blob/master/py/ray/tasks/mmo_tasks.ipynb"},"content":[{"nodeType":"text","value":"mmo_tasks.ipynb","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" respectively.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"With an optimized approach of using Rayâs central object store, we see an average training time per batch of files approximately ","marks":[],"data":{}},{"nodeType":"text","value":"3-5X ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"faster than the previous approach, with an incremental number of files processed. See Figure 4.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Observations, Takeaways and Recap","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In the Ray Dashboard screenshots for approaches 1 and 2 below, you can observe the respective difference in utilization metrics for each resource, particularly node memory and object store.Â ","marks":[],"data":{}}]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"Approach 1: Distributed reads per taskÂ ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YexKUfg5tRo5M9UJeXqqE","type":"Asset","createdAt":"2023-01-18T21:08:54.406Z","updatedAt":"2023-01-18T21:08:54.406Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"dashboard_approach_1","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3YexKUfg5tRo5M9UJeXqqE/9c387d5248fa5e4009756428abc2d395/image2.png","details":{"size":156015,"image":{"width":1999,"height":542}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5R9gNO0hUKeNW3kXszJZG5","type":"Asset","createdAt":"2023-01-18T21:18:08.209Z","updatedAt":"2023-01-18T21:18:08.209Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"mmt blog images","description":"Figure 1. Ray dashboard showing all the relevant metrics for approach 1.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/5R9gNO0hUKeNW3kXszJZG5/1246cc0be7d17232640924c63cda3e5d/mmt_blog_images.png","details":{"size":117309,"image":{"width":960,"height":540}},"fileName":"mmt_blog_images.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\n\nWhile resources such as number of tasks, nodes, and CPU cores usage are similar across both approaches, we see more node memory consumption, hardly any Ray object store usage, and more CPU utilization, which explains why the training times take longer for approach 1.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"For example, the last batch of all 18 files takes about ","marks":[],"data":{}},{"nodeType":"text","value":"158","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" seconds to train ","marks":[],"data":{}},{"nodeType":"text","value":"14100","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" models for a total of ","marks":[],"data":{}},{"nodeType":"text","value":"4700","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" unique pickup locations.","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7GJjcaT5GWbVnVswYAk0yF","type":"Asset","createdAt":"2023-01-18T21:09:05.649Z","updatedAt":"2023-01-18T21:09:05.649Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"plotting_run_times","description":"Figure 2. Plots showing training times for approach 1.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/7GJjcaT5GWbVnVswYAk0yF/2b48d01168c39cc9f37fde0d506cb294/image14.png","details":{"size":74126,"image":{"width":960,"height":540}},"fileName":"image14.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"Approach 2: Centralized reads per task from Ray object store","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5a1mKVvC8JyVQkJLIHz9Br","type":"Asset","createdAt":"2023-01-18T21:09:16.785Z","updatedAt":"2023-01-18T21:09:16.785Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_dashboard_2","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5a1mKVvC8JyVQkJLIHz9Br/2d4fed311de56c626ce0fda975790e35/image8.png","details":{"size":149183,"image":{"width":1999,"height":531}},"fileName":"image8.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"13qrOaj5ZHp3wwizxnXGVb","type":"Asset","createdAt":"2023-01-18T21:09:29.686Z","updatedAt":"2023-01-18T21:09:29.686Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"dasboard_metrics_2","description":"Figure 3. . Ray dashboard showing all the relevant metrics for approach 2.","file":{"url":"//images.ctfassets.net/xjan103pcp94/13qrOaj5ZHp3wwizxnXGVb/c7f4067d08c4a58171456e03477f9014/mmt_blog_images__1_.png","details":{"size":110270,"image":{"width":960,"height":540}},"fileName":"mmt_blog_images (1).png","contentType":"image/png"}}}},"content":[]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"dRkAB0d77F7jpNoCwdFIS","type":"Asset","createdAt":"2023-01-18T21:09:39.487Z","updatedAt":"2023-01-18T21:09:39.487Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"training_times_approach_2","description":"Figure 4. Plots showing training times for approach 2.","file":{"url":"//images.ctfassets.net/xjan103pcp94/dRkAB0d77F7jpNoCwdFIS/dfd68315c92045b0f1149b23bb374b81/image4.png","details":{"size":66206,"image":{"width":960,"height":540}},"fileName":"image4.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"By contrast, we see more Ray object store memory usage, along with node memory spikes, at a benefit of significantly lower training times, in order of ","marks":[],"data":{}},{"nodeType":"text","value":"~3-5X","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" faster. Compared to approach 1, the training time for the last batch of 18 files is only ","marks":[],"data":{}},{"nodeType":"text","value":"35","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" seconds (hardly discernible in the above bar graph) compared to approach 1âs ","marks":[],"data":{}},{"nodeType":"text","value":"158","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" seconds.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Although this second approach was ~","marks":[],"data":{}},{"nodeType":"text","value":"5X","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" faster to train all models for all pickup locations, one caveat to keep in mind is that if the dataset partitions were larger and unable to fit into our cluster memory, we would have to resort to distributed reading of data, by further repartitioning parquet data files into smaller files that can fit into memory. That is, approach 1.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Both patterns, using Ray Core, are viable approaches to scale many models training on a specific feature attribute. Both patterns above showcase how to accomplish the use case and workloads to train many models to a single feature in the training set. Which one should you choose depends on your use case and size of the dataset. In either case, we recommend trying both and evaluating what works best for you, given your available cluster resources.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Lastly, if you have a similar Ray use case or story, our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.myeventi.events/raysummit23/cfp/"},"content":[{"nodeType":"text","value":"Ray Summit 2023 CfP","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" is open for call to proposals. Submit your Ray story and share with the growing global Ray community. Also, we have a regular cadence of monthly ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.meetup.com/bay-area-ray-meetup/"},"content":[{"nodeType":"text","value":"Ray Meetups","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Do join us to learn how the community is using Ray and Anyscale to ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/machine-learning"},"content":[{"nodeType":"text","value":"scale their machine learning workloads","marks":[],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"\nReferences and ResourcesÂ ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/training-one-million-machine-learning-models-in-record-time-with-ray"},"content":[{"nodeType":"text","value":"Training One Million Machine Learning Models in Record Time with Ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/model-batch-inference-in-ray-actors-actorpool-and-datasets"},"content":[{"nodeType":"text","value":"Model Batch Inference in Ray: Actors, ActorPool, and Datasets","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-core/examples/batch_training.html"},"content":[{"nodeType":"text","value":"Batch Training with Ray Core","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Driver ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/dmatrix/misc-code/blob/master/py/ray/tasks/mmd_tasks.ipynb"},"content":[{"nodeType":"text","value":"Notebook Approach 1","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Driver ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/dmatrix/misc-code/blob/master/py/ray/tasks/mmo_tasks.ipynb"},"content":[{"nodeType":"text","value":"Notebook Approach 2","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11LiFRlfWQUsx8mHP9Hnad","type":"Asset","createdAt":"2023-01-18T21:05:50.950Z","updatedAt":"2023-01-18T21:05:50.950Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"NYC taxi dirver data sete","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/11LiFRlfWQUsx8mHP9Hnad/8d8428f9096a5d3b4e1aaab33da2e820/image16.png","details":{"size":3914440,"image":{"width":2000,"height":1000}},"fileName":"image16.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1EVhG5BgmCBaMYiqRRQi5Y","type":"Entry","createdAt":"2023-01-12T22:29:04.945Z","updatedAt":"2023-03-20T18:41:24.405Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Ray Summit 2023 Call for Proposals is now open","seoTitle":"Ray Summit 2023 is coming in September, and we are accepting proposals from prospective speakers. This is a grand opportunity to share your Ray story with growing global community.","slug":"ray-summit-2023-call-for-proposals-is-now-open","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}}],"publishedDate":"2023-01-12","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"nodeType":"text","value":"Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", the annual Ray community conference, is back in 2023 for a two-day, in-person event later this year (","marks":[],"data":{}},{"nodeType":"text","value":"September 18-19","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":") in San Francisco, CA, along with an additional day for all-day training on ","marks":[],"data":{}},{"nodeType":"text","value":"September 20th","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":". ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Last yearâs ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/ray-summit-2022"},"content":[{"nodeType":"text","value":"Ray Summit 2022","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" was a stellar success, with luminary and visionaries keynotes from IBM, OpenAI, Uber, Meta AI, and UC Berkeley, along with two days of technical Ray deep dives, Ray use cases, lightning talks, and Ray training. This time around, weâll strive to double the sessions from the community, including training.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We are currently accepting proposals for conference talks, with a submission deadline of ","marks":[],"data":{}},{"nodeType":"text","value":"February 17th.","marks":[{"type":"bold"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this blog post, weâll cover the most frequently asked questions about the Call for Proposals (CfPs)Â  and will give you all the information you need to prepare a stellar Ray Summit proposal.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"What are the benefits of speaking at Ray Summit?","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"There are three main benefits to speaking at Ray Summit:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"First, itâs an opportunity to share your Ray expertise and knowledge with the growing global Ray community, giving your work wider technical recognition. This can lead to more machine learning practitionersÂ (including other speakers) connecting with you or seeking your advice, boosting your personal brand.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Second, youâll be able to build your public speaking and presentation skills. Particularly if you plan to apply to other conferences that require previous speaking experience; speaking at Ray Summit is a great opportunity to build that experience. We also record and share all sessions, which are publicly available for a global audience.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"And finally, as a speaker, youâll get a free pass to attend Ray Summit, including some social and networking events.","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"What are the key themes for this yearâs Ray Summit?","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The key themes and topics of interest for Ray Summit 2023 are no different from last year, with some additional topics since Ray is moving fast, and the community is using Ray for varying scalable workloads and use cases:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Simple and scalable ML use cases or workloads solved with Ray AIR or Ray libraries and ecosystem","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Scalable ML/AI: feature engineering, distributed training, or hyperparameter tuning for use cases such as graph neural networks, computer vision, time-series, and training and serving many multiple models at scale","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Training or tuning transformers for generative AI for NLP tasks","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"AI/ML platforms and infrastructure or end-to-end applications: data processing through model training, serving, workflow orchestration, and monitoring","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Deep dives into Ray core components and Ray AIR libraries","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Reinforcement learning in the real world (recommendation systems, trading agents, etc.) built with Ray AIR or other frameworks","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Building your own distributed framework using Ray Core API patterns or Ray AIR","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Cloud computing topics such as serverless computing and multi-cloud Ray deployments on Kubernetes, on-premises, or public cloud","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Integrations of common ML libraries and frameworks with the Ray AIR and ecosystem","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"What types of talks can I submit?","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We have two categories of talks you can submit on the above themes and topics:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Technical Lightning Talks","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" (15 minutes): Short technical talks covering the what, why, and how of a topic with digestible code examples or quick demos.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Technical Talks","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" (30 minutes): Talks that are rich in technical detail, including code examples or demos that help the audience grasp a problem you set out to solve, challenges you met along the way, your solution, what are the takeaways, and who is the audience: the what, why, how, and for whom.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"For all the talks, the audience should walk away having learned something new.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"This is my first time. Any tips for preparing a proposal?","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Submitting a talk to a CFP can be overwhelming, but if you focus on a few key areas, itâs easy to make sure the value of your idea shines through. Here are a few tips for submitting a successful proposal:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Start with a simple and straightforward title. You only have a few seconds to grab your audienceâs attention.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Avoid using your proposal as a sales or product pitch.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Keep it focused. You probably wonât have time to cover everything about your topic â choose a specific angle or technique to focus on.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Edit, edit, edit. Once youâve written your abstract, be sure to read it over several times to make sure that it tells a clear story. Eliminate unnecessary words and sentences: sentence is a form a thought takes. Share with a trusted peer and get feedback.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Finally, always keep your audience in mind. Explain why people will want to attend your talk and what theyâll learn from it with three numbered takeaways.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"For more guidance, check out the following resources:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://medium.com/@LachlanEvenson/lachies-7-step-guide-to-writing-a-winning-tech-conference-cfp-4fa36a0d2672"},"content":[{"nodeType":"text","value":"7 step guide to writing a winning tech conference CFP","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://snyk.io/blog/10-tips-for-getting-that-conference-cfp-accepted/"},"content":[{"nodeType":"text","value":"10 Tips for getting that conference CFP accepted","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://jonthebeach.com/blog/CFP-Tips-and-Tricks-for-Tech-Conferences"},"content":[{"nodeType":"text","value":"CFP Tips and Tricks for Tech Conferences","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"How many speakers per talk?","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We allow at most two speakers per proposal, so you can only include yourself and your co-speaker during the submission process.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"How do I submit my talk?","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Simple. Just head over to our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.myeventi.events/raysummit23/cfp/"},"content":[{"nodeType":"text","value":"Ray Summit 2023 CfP","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" page and click on Submit a talk. The submission deadline is ","marks":[],"data":{}},{"nodeType":"text","value":"March 6th, 2023","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":". We are looking forward to your submission. Good luck!","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1vp6UIJnIljX8lSRT53RsA","type":"Asset","createdAt":"2023-01-12T22:26:56.132Z","updatedAt":"2023-01-12T22:35:27.175Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"CfP Main page","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1vp6UIJnIljX8lSRT53RsA/608a6797a7184cc0e047d240abb2ae66/Screen_Shot_2023-01-12_at_2.32.08_PM.png","details":{"size":1238583,"image":{"width":1616,"height":1610}},"fileName":"Screen Shot 2023-01-12 at 2.32.08 PM.png","contentType":"image/png"}}},"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7AU8F91ESMdp1RI7sn1FEQ","type":"Entry","createdAt":"2022-06-01T13:35:50.341Z","updatedAt":"2022-06-15T17:56:08.033Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"7 must-attend Ray Summit sessions: RL-powered traffic control, infra-less ML, and more","slug":"7-must-attend-ray-summit-sessions-rl-powered-traffic-control-infra-less-ml","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2ZQcscC4cFydZNkAjQf0Cp","type":"Entry","createdAt":"2020-09-14T19:20:26.098Z","updatedAt":"2020-09-14T19:20:26.098Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Ben Lorica","slug":"ben-lorica","link":"https://twitter.com/bigdata"}}],"publishedDate":"2022-06-01","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},"intro":"This yearâs Ray Summit has it all: developer deep dives, Ray use cases across industries, Ray in production, scalable AI, and more. Here are just a few of the sessions weâre looking forward to attending this year.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/ray-summit-2022?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=blog1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is where the Ray community comes together to build, learn, and explore everything AI and more on Ray. This year, for the first in-person Ray Summit, we have a fantastic program of ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/ray-summit-2022/speakers?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=blog1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"keynotes","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/ray-summit-2022/agenda?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=blog1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"breakout sessions","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/ray-summit-2022/agenda?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=blog1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"lightning talks","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and meetup talks covering everything from developer deep dives and Ray use cases across industries to Ray in production for reinforcement learning and building scalable machine learning platforms â all to enable myriad use cases for ML practitioners.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Here are just a few of the sessions weâre looking forward to attending this year:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/ray-summit-2022/agenda/sessions/177?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=blog1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Gearing up for FIFA 2022 using RLlib-powered traffic control","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": The World Cup is a massive month-long football event that demands precise human and vehicle traffic control and coordination to and from stadiums. Ji Lucas and Mayuresh Kunjir, both from Qatar Computing Research Institute, will share how they built a multi-agent reinforcement learning framework based on RLlib to design a coordinated traffic light control policy system to control congestion and facilitate mobility.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/ray-summit-2022/agenda/sessions/180?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=blog1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"How Spotify sped up ML research and prototyping with Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": If youâre a fan of the Spotify mobile app, perhaps youâd be interested in the tech infrastructure that powers it. Spotify ML Platform engineers Keshi Dai and David Xia will reveal their journey with Ray: how they integrated their internal ML infrastructure on top of Ray, what specific use cases Ray integration enabled, and best practices gleaned along the way.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/ray-summit-2022/agenda/sessions/112?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=blog1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Predibase: A low-code deep learning platform built for scale","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": Travis Addair, chief technology officer at Predibase, will share a new kind of low-code machine learning platform, combining large-scale deep learning with state-of-the-art model architectures across a range of modalities, including NLP, computer vision, tabular, and hybrid data. Heâll explore how Ray provides a serverless abstraction to enable scaling.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/ray-summit-2022/agenda/sessions/172?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=blog1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Zero-copy model loading with Ray and PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": The cost of large ML model loading and serving can be prohibitive, but there are ways to reduce the costs. Fred Reiss, a principal staff member at IBM Research, will show how to reduce this cost to almost zero by leveraging features in PyTorch and Ray. In particular, heâll explain how to employ Rayâs distributed shared memory to store large deep learning model weights so that any process with access to the shared memory segment can load it instantaneously.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/ray-summit-2022/agenda/sessions/181?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=blog1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SkyML: Infra-less machine learning on any cloud","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": SkyML is a project being developed at UC Berkeley's Sky Computing Lab for \"infra-less\" machine learning on any cloud. The SkyML platform abstracts away the cloud and infrastructure on which ML applications run.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/ray-summit-2022/agenda/sessions/148?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=blog1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Large-scale deep learning to augment production RL workloads","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": Riot Games has been using deep reinforcement learning to build bots that can play games at various skill levels in order to provide additional feedback to game designers. This talk describes how they use Ray Data, Ray Train, and Ray Tune to build a supervised learning model to control and tune game servers utilized by bots that are learning to play the game.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/ray-summit-2022/agenda/sessions/166?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=blog1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"A serverless resilient graph analysis platform built on Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": This talk introduces ByteGAP, an enterprise-level Graph Analytics Platform for graph computing and graph mining. ByteGAP is used to process extremely large graphs such as those found in the popular social media applications Douyin and TikTok. ByteGAP is built on top of KubeRay to provide flexible resource management, automatic cloud deployment, scalability, and fault tolerance.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Thatâs just the beginning of what we have in store at this yearâs Ray Summit. Over the ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/ray-summit-2022/agenda?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=blog1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"two days of the conference","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", weâll learn how companies like ByteDance, Shopify, Uber, IBM, and more are building cutting-edge ML platforms and applications on Ray. Weâll hear ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/ray-summit-2022/speakers?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=blog1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"keynotes from AI and ML luminaries","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" like Greg Brockman, co-founder of OpenAI, and Soumith Chintala, creator of PyTorch. And, since itâs the first time many community members are getting together in person, weâll also have some fun during our community happy hour and Ray Summit meetup.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/ray-summit-2022?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=blog1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Register now","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" â early bird registration is open until June 17.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5plXNcDg9AqWzIRI0RiM0s","type":"Asset","createdAt":"2022-05-28T00:18:41.735Z","updatedAt":"2022-05-31T19:17:50.436Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"raysummit-blog-image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5plXNcDg9AqWzIRI0RiM0s/5314eaa7c3055473765bf93f6b79ad75/raysummit-blog-image.jpg","details":{"size":116726,"image":{"width":750,"height":500}},"fileName":"raysummit-blog-image.jpg","contentType":"image/jpeg"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Zk7jixFrLqHf3IJLzrO2B","type":"Entry","createdAt":"2022-05-03T18:47:31.422Z","updatedAt":"2022-05-03T18:47:31.422Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Ray Summit: Aug 23-24 in SF!","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5LTWfPAScfUtzXj2KOUALz","type":"Entry","createdAt":"2022-04-12T15:29:42.674Z","updatedAt":"2022-10-04T22:06:07.976Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":17,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"summit"}},"locale":"en-US"},"fields":{"title":"Ray Summit 2022","slug":"ray-summit-2022","description":"Centered on Ray, the open-source distributed computing framework, Ray Summit brings together ML practitioners, data scientists, and more to discuss the growing demand for distributed computing.","sections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xIJZXdAqhOEizUCUuQaiV","type":"Entry","createdAt":"2022-03-31T16:04:49.746Z","updatedAt":"2022-09-12T20:58:36.991Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":20,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-hero","header":"The Place for _Everything Ray_","body":"Missed us in San Francisco for Ray Summit 2022? Don't fret, you can watch your favorite sessions on demand! ","ctaText":"View Sessions","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"9F9S7NLaIdPL4ThYwVlFW","type":"Asset","createdAt":"2022-09-06T14:49:38.936Z","updatedAt":"2022-09-06T14:53:02.350Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Presented by Anyscale","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/9F9S7NLaIdPL4ThYwVlFW/8ecd09b334942298f0b8c2d5567ec032/presented-by-anyscale.svg","details":{"size":22813,"image":{"width":289,"height":32}},"fileName":"presented-by-anyscale.svg","contentType":"image/svg+xml"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4A0d4TUfp0TCqUA60CXopR","type":"Asset","createdAt":"2022-09-06T14:53:48.720Z","updatedAt":"2022-09-06T14:53:48.720Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4A0d4TUfp0TCqUA60CXopR/2e4240c268d074d840f7a00d069a768a/rs-collage_2x.jpg","details":{"size":568547,"image":{"width":2400,"height":1376}},"fileName":"rs-collage@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6FGUP0cXunTbgs2UemeAU","type":"Asset","createdAt":"2022-09-06T14:54:09.975Z","updatedAt":"2022-09-06T14:54:09.975Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event, stacked vertically","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6FGUP0cXunTbgs2UemeAU/16ed175024bc9cc0a2fae3cb13538dff/rs-collage-mobile_2x.jpg","details":{"size":243282,"image":{"width":750,"height":1420}},"fileName":"rs-collage-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4RKa7qVhWoIaoNSa1ZH2SK"}}],"type":"2022 Summit Hero","styling":["transparent-header"]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6GL7ZzuSLadWdVFKHj7yIH","type":"Entry","createdAt":"2022-04-20T18:00:12.964Z","updatedAt":"2022-08-08T22:50:29.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-keynote-speakers","header":"Keynote Speakers","ctaText":"See all speakers","ctaLink":"/ray-summit-2022/speakers","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6VhewtcICU9SCXjwd5qhHJ"}}],"type":"2022 Summit Speakers","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JTi5fsyo0Q3YrF3SAQglg","type":"Entry","createdAt":"2022-08-08T22:49:55.163Z","updatedAt":"2022-08-09T07:03:38.615Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-ray-summit-selected-speakers","header":"Highlighted Talks","ctaText":"Full Agenda","ctaLink":"/ray-summit-2022/agenda","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"50eS4cKXBNE4GHezdZ8Rws"}},{"sys":{"type":"Link","linkType":"Entry","id":"5iioKfOgEqVzgQ1DMmA1Wy"}},{"sys":{"type":"Link","linkType":"Entry","id":"YCBUZOXJGB0qcJF8TKjfI"}}],"type":"2022 Summit Agenda Preview"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5cgHuoJ713DewhVY6iUYrG","type":"Entry","createdAt":"2022-05-03T21:57:45.147Z","updatedAt":"2022-05-09T13:41:03.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-trainings","header":"Exclusive Ray Training Sessions","body":"Ray Summit is a great opportunity to level up your Ray skills through hands-on training sessions with the builders and maintainers of Ray. Weâre offering several pre-conference training sessions on August 22. Space is limited â [reserve your spot now](https://www.myeventi.events/raysummit22/attendee/)!","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3sYQTUmD5X5ncvGRFyIRR6","type":"Asset","createdAt":"2022-05-03T04:40:25.040Z","updatedAt":"2022-05-03T04:40:25.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image desktop","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3sYQTUmD5X5ncvGRFyIRR6/a77c675b07d63e73dd79ada9d182498a/training-image_2x.jpg","details":{"size":313509,"image":{"width":1136,"height":1480}},"fileName":"training-image@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7rsC56a4YCgENLyxsHyeIb","type":"Asset","createdAt":"2022-05-03T04:40:54.089Z","updatedAt":"2022-05-03T04:40:54.089Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image mobile","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7rsC56a4YCgENLyxsHyeIb/e2592c9b4424048b3c916fbbbc313065/training-image-mobile_2x.jpg","details":{"size":153801,"image":{"width":830,"height":874}},"fileName":"training-image-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4Hb3J9SrCFTDXqIhIdSIwa"}},{"sys":{"type":"Link","linkType":"Entry","id":"hZ4Tl1wvj7CwF00slwpLg"}},{"sys":{"type":"Link","linkType":"Entry","id":"YJ5KWyjg4NGf29NrfHcJX"}}],"type":"2022 Summit Trainings"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mRJQtuDbIhAuEbwLT2V1K","type":"Entry","createdAt":"2022-03-31T21:42:56.117Z","updatedAt":"2022-04-11T23:46:07.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-reasons-to-attend","header":"Why Attend?","body":"As the annual user conference for the [open source Ray community](https://www.ray.io/community), Ray Summit is a unique opportunity to enhance your Ray skills, build your professional network, and learn what's on the Ray roadmap.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"5f6gxk4a4ujE2y6FZ8ATQ5"}},{"sys":{"type":"Link","linkType":"Entry","id":"20QwyiOmkHCEOkMvp0Jhtk"}},{"sys":{"type":"Link","linkType":"Entry","id":"6QxivsuhhLoVSloRQs9ZlQ"}}],"type":"2022 Summit Content Tiles"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"15k0SGqB2ZKOUfAsM3JnyV","type":"Entry","createdAt":"2022-03-31T16:05:55.739Z","updatedAt":"2022-08-05T00:25:20.225Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":16,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-about","header":"Celebrate the Ray community","body":"Get ready for our epic community bash at the Art of Tinkering exhibit at the San Francisco Exploratorium on August 23 from 5:30 to 9:30 p.m. Come explore, play, and celebrate with the Ray Community. This is an exclusive opportunity to experience this amazing interactive exhibition before it closes in September.","ctaText":"Check out the agenda","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UcZT4h1P7uFDz6cn6JH8l","type":"Asset","createdAt":"2022-06-02T20:37:22.299Z","updatedAt":"2022-06-02T20:37:22.299Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Man taking a photo on his phone","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2UcZT4h1P7uFDz6cn6JH8l/5040a9909f18ab015c27b220077f916e/taking-picture.jpg","details":{"size":107846,"image":{"width":980,"height":772}},"fileName":"taking-picture.jpg","contentType":"image/jpeg"}}}],"type":"2022 Summit Split Image","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4P8Jn0q7nBU05iD6DmcQCC","type":"Entry","createdAt":"2022-04-01T19:15:31.735Z","updatedAt":"2022-04-11T16:36:59.573Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-social-tiles","header":"Join the Conversation","body":"Ready to get involved in the Ray community before the conference? Ask a question in the forums. Open a pull request. Or share why youâre excited with the hashtag __#RaySummit__ on Twitter.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6OMTlXkJ1OHXNkEjyPrXTH"}},{"sys":{"type":"Link","linkType":"Entry","id":"7EE28ee1ATQtodoLloT6mg"}},{"sys":{"type":"Link","linkType":"Entry","id":"1CRAzFxndL37wiNg4x8WKV"}},{"sys":{"type":"Link","linkType":"Entry","id":"2qrec5rR8kabZxCHaPS7NH"}}],"type":"2022 Summit Social Tiles"}}],"headerLogo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70DIdxwT8XY96aw7vMUHOu","type":"Asset","createdAt":"2022-06-01T15:21:46.804Z","updatedAt":"2022-06-01T15:21:46.804Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo Dark Blue","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/70DIdxwT8XY96aw7vMUHOu/5ca0a8b7533c8df70bc0e032b889862a/rs-logo-nav.svg","details":{"size":9954,"image":{"width":108,"height":48}},"fileName":"rs-logo-nav.svg","contentType":"image/svg+xml"}}},"headerLogoWhite":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3qyLXKGgGfWhv4oPqYl0SE","type":"Asset","createdAt":"2022-04-20T21:50:58.774Z","updatedAt":"2022-04-20T21:50:58.774Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3qyLXKGgGfWhv4oPqYl0SE/c4dee9e7a344c39db11fefafbf255b5b/rs-logo-horizontal.svg","details":{"size":9975,"image":{"width":108,"height":48}},"fileName":"rs-logo-horizontal.svg","contentType":"image/svg+xml"}}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KUtRQbdqJo1RrWMrVyfEm","type":"Entry","createdAt":"2022-03-31T16:21:13.402Z","updatedAt":"2022-04-08T17:25:33.228Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Home","link":"/ray-summit-2022"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4MvQcRSkHLMqIJoO9O2cQe","type":"Entry","createdAt":"2022-03-31T16:17:27.336Z","updatedAt":"2022-09-12T16:59:51.221Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"On-Demand","link":"/ray-summit-2022/agenda","subLinks":[{"sys":{"type":"Link","linkType":"Entry","id":"3N9kRDz36tieTI4YGVWKBJ"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46B5diwEDDr3kB2XDoGfDP","type":"Entry","createdAt":"2022-05-11T17:13:42.174Z","updatedAt":"2022-05-11T17:13:42.174Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Speakers","link":"/ray-summit-2022/speakers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Dt5fy6A7rBpXdG7HeKMGd","type":"Entry","createdAt":"2022-03-31T16:17:43.592Z","updatedAt":"2022-03-31T16:17:43.592Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Sponsors","link":"/ray-summit-2022/sponsors"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SvRE9pnyq3BXUPGKRWx6W","type":"Entry","createdAt":"2022-06-03T19:29:31.009Z","updatedAt":"2022-06-03T19:29:31.009Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Who Attends","link":"/ray-summit-2022/who-attends"}}],"mainSponsors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"xMQIOU9n7gVS4mkX2Nlag","type":"Entry","createdAt":"2021-01-31T16:33:06.495Z","updatedAt":"2021-05-11T13:28:31.312Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Ray","url":"https://ray.io","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jLroqm98RCMLGASthyuPv","type":"Asset","createdAt":"2021-05-11T13:28:26.930Z","updatedAt":"2021-05-11T13:28:26.930Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/jLroqm98RCMLGASthyuPv/f76cc04827316de5d3f8b529f6a3f02c/Ray_BM.svg","details":{"size":4742,"image":{"width":93,"height":36}},"fileName":"Ray_BM.svg","contentType":"image/svg+xml"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11syIHhtX8JajSlXHSHE1O","type":"Entry","createdAt":"2021-01-31T16:31:24.509Z","updatedAt":"2021-05-11T13:28:00.309Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Anyscale","url":"https://www.anyscale.com","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UkDmTzmZHIOG7yxFP4sVL","type":"Asset","createdAt":"2021-05-11T13:27:55.219Z","updatedAt":"2021-05-11T13:27:55.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Anyscale","file":{"url":"//images.ctfassets.net/xjan103pcp94/5UkDmTzmZHIOG7yxFP4sVL/db10aebda9a9aad10903470781a6f2e6/ANSC_BM.svg","details":{"size":7431,"image":{"width":158,"height":36}},"fileName":"ANSC_BM.svg","contentType":"image/svg+xml"}}}}}],"footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cfwECWrOqJ5Fegr3YpReS","type":"Entry","createdAt":"2022-04-01T21:22:12.001Z","updatedAt":"2022-04-13T17:35:29.511Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-footer","body":"Â©Anyscale, Inc 2022\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/privacy-policy\"\u003ePrivacy Policy\u003c/a\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/code-of-conduct\"\u003eCode of Conduct\u003c/a\u003e"}},"recommendations":[]}},"ctaText":"Register now","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5nTlh9josVs5zpZqU3vTbn","type":"Asset","createdAt":"2022-05-03T18:47:25.125Z","updatedAt":"2022-05-03T18:47:25.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-ray-summit-2022","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5nTlh9josVs5zpZqU3vTbn/e66043f62ed4a4b53d7e182e50289ec3/blog-recommended-content-ray-summit-2022.png","details":{"size":17234,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-ray-summit-2022.png","contentType":"image/png"}}},"recommendations":null}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true,"showMainImage":true,"thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5nTlh9josVs5zpZqU3vTbn","type":"Asset","createdAt":"2022-05-03T18:47:25.125Z","updatedAt":"2022-05-03T18:47:25.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-ray-summit-2022","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5nTlh9josVs5zpZqU3vTbn/e66043f62ed4a4b53d7e182e50289ec3/blog-recommended-content-ray-summit-2022.png","details":{"size":17234,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-ray-summit-2022.png","contentType":"image/png"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4oXAZIjytJhxRRCgyDVpLv","type":"Entry","createdAt":"2022-03-24T14:02:45.667Z","updatedAt":"2022-06-22T15:50:15.613Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Large-scale distributed training with TorchX and Ray","slug":"large-scale-distributed-training-with-torchx-and-ray","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7q31JzvmFGfytFMXDTuu5w","type":"Entry","createdAt":"2022-03-23T14:11:34.061Z","updatedAt":"2022-03-23T14:11:34.061Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Mark Saroufim","slug":"mark-saroufim"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}}],"publishedDate":"2022-03-24","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"We're excited to introduce a new Ray Scheduler for TorchX â a joint effort from the PyTorch and Anyscale teams that allows developers to run scalable and distributed PyTorch workloads without setting up infrastructure or changing training scripts.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", created at ","nodeType":"text"},{"data":{"uri":"https://rise.cs.berkeley.edu/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RISELab","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" by the founders of ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Anyscale","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":",Â provides a rich set of native libraries for ML workloads and a general-purpose core for building distributed applications.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"On top of the libraries provided by Ray, there is a rich ecosystem of libraries and integrations that enable PyTorch users to achieve greater scale. Two great examples are ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/raysgd/raysgd_pytorch.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch Distributed","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/introducing-ray-lightning-multi-node-gpu-training-for-pytorch-lightning-made"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch Lightning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"underline"}],"value":",","nodeType":"text"},{"data":{},"marks":[],"value":" enabling users to take advantage of the amazing PyTorch and Ray capabilities together.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This blog introduces how ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/torchx/latest/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"TorchX","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" extends functionality to submit PyTorch jobs via a newly developed ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/torchx/main/schedulers/ray.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Scheduler","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Using ","nodeType":"text"},{"data":{"uri":"https://youtu.be/vXbbaEZbrOI?t=4510"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"TorchX SDK","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the Ray ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/ray-job-submission/overview.html#ray-job-sdk"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Job Submission SDK","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", developers can build and deploy PyTorch machine learning applications from R\u0026D to production. TorchX provides the ability to string together built-in components like hyperparameter optimization, model serving, and distributed data-parallel into complex pipelines while leveraging the most popular job schedulers in open source.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A joint engineering effort between the ","nodeType":"text"},{"data":{"uri":"https://www.facebook.com/MetaAI/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Meta AI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" PyTorch and Anyscale ML teams, this new Ray Scheduler component allows developers to run scalable and distributed PyTorch workloads without setting up an infrastructure for their choice of the cloud or changing their training scripts â all can be done via the TorchX SDK or CLI.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can run all of the below live in a ","nodeType":"text"},{"data":{"uri":"https://colab.research.google.com/drive/1vVCpgQ9z_1SN8K9CJxUT2LtvUDN0AlND?usp=sharing"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Google Colab notebook","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"TorchX developer user journey","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To comprehend how PyTorch developers use TorchX SDK and convert or deploy their scripts into jobs deployed on a remote Ray cluster via a new Ray Scheduler, letâs examine some use cases and show some code examples.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In 5 steps, you can convert your PyTorch Python script into a TorchX job and submit it for execution on a Ray cluster in your cloud.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Step 1: ","nodeType":"text"},{"data":{},"marks":[],"value":"Install ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"torchX","nodeType":"text"},{"data":{},"marks":[],"value":" on your laptop.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"pip install ray âtorchx[dev]â","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Step 2: ","nodeType":"text"},{"data":{},"marks":[],"value":"Create your ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"simple_ray_job.py","nodeType":"text"},{"data":{},"marks":[],"value":" as you would for any PyTorch training script in your IDE or editor.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Step 3","nodeType":"text"},{"data":{},"marks":[],"value":": Launch a Ray cluster (as shown below) using a ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"\u003ccloud\u003e_cluster.yaml","nodeType":"text"},{"data":{},"marks":[],"value":", which specifies the kind of Ray cluster: number and kind of nodes, GPU vs. CPUs, etc. Look at the ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/msaroufim/2a6973385cc5687316afa6a1a234a467"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"example file","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in the repo; though comprehensive and expansive, the YAML file can be curtailed to the needs of your specific demand of a TorchX job.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Step 4: ","nodeType":"text"},{"data":{},"marks":[],"value":"Submit a TorchX job to the Ray cluster using the TorchX CLI as shown below.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Step 5","nodeType":"text"},{"data":{},"marks":[],"value":": Monitor the jobâs progress or final status by fetching the logs as shown below.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Submitting TorchX jobs to Ray Scheduler","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3mzANtDgqKixA23s6s94Ba","type":"Asset","createdAt":"2022-03-23T14:16:10.350Z","updatedAt":"2022-03-23T14:16:10.350Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-submitting-torchx-jobs-to-ray-scheduler","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3mzANtDgqKixA23s6s94Ba/49164f6982b75d9316e7e79efe7f1ea9/blog-submitting-torchx-jobs-to-ray-scheduler.png","details":{"size":105179,"image":{"width":1276,"height":531}},"fileName":"blog-submitting-torchx-jobs-to-ray-scheduler.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"As a user, you only need to write your training script. Weâll use a distributed data parallel training script in the below example. You can also pass in other parameters, like your working directory or ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"requirements.txt","nodeType":"text"},{"data":{},"marks":[],"value":". Then just call the TorchX CLI while specifying the Ray Scheduler, which will look for an available Ray cluster, start running your job, and stream back the logs to your local client. This helps decouple your training script from your infrastructure so that you can easily move to large multi-node workloads with multiple GPUs without changing your code.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Submit TorchX jobs to the cloud of your choice","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Submitting a job to a cloud of your choice is simple. For instance, if you wish to submit a TorchX job to a Ray cluster on AWS or GCP, assuming you have all your target cloudâs credentials set as required by the specific cloud provider, you can expand on a simple ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/config.html#minimal-configuration"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"aws_ray_cluster.yaml","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/config.html#minimal-configuration"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"gcp_ray_cluster.yaml","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" file to meet your compute node type and needs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Launching a cluster","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"ray up -y \u003ccloud\u003e_ray_cluster.yaml","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Through TorchXâs Ray Scheduler, you can just as easily pick the cloud of your choice to submit your job to.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Defining a TorchX component","nodeType":"text"}],"nodeType":"heading-4"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4hAfGcXW5KRInvhRXzbuQk","type":"Entry","createdAt":"2022-03-23T14:17:32.852Z","updatedAt":"2022-03-23T14:17:32.852Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"TorchX and Ray blog, code example 1","body":"# component.py\nfrom typing import Dict, List, Optional\n\nimport torchx\nimport torchx.specs as specs\nfrom torchx.specs import Resource, macros, named_resources\n\ndef trainer() -\u003e specs.AppDef:\n    # cmd = \"python -m torch.distributed.run -\"\n    return specs.AppDef(\n        name=\"compute-ws\",\n        roles=[\n            specs.Role(\n                name=\"worker\",\n                entrypoint=\"python\",\n                args=[\"-m\", \"compute_world_size\"],\n                env={},\n                image=\"\",\n                resource=specs.Resource(cpu=1, gpu=0, memMB=4000),\n                num_replicas=2,\n            )\n        ],\n    )","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Writing a distributed PyTorch job","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"The simplest possible distributed PyTorch job would compute the world size and make sure all nodes agree to that world size.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2s7tbYs9r158hjfssAosMB","type":"Entry","createdAt":"2022-03-23T14:18:15.525Z","updatedAt":"2022-03-23T14:18:15.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"TorchX and Ray blog, code example 2","body":"%%writefile scripts/compute_world_size.py\n#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.distributed import init_process_group, all_reduce, get_rank, get_world_size\n\ndef compute_world_size() -\u003e int:\n\n   rank = int(os.getenv(\"RANK\"))  # pyre-ignore[6]\n   world_size = int(os.getenv(\"WORLD_SIZE\"))  # pyre-ignore[6]\n   master_port = int(os.getenv(\"MASTER_PORT\"))  # pyre-ignore[6]\n   master_addr = os.getenv(\"MASTER_ADDR\")\n   backend = \"gloo\"\n\n   print(f\"initializing `{backend}` process group\")\n   init_process_group(  # pyre-ignore[16]\n       backend=backend,\n       init_method=f\"tcp://{master_addr}:{master_port}\",\n       rank=rank,\n       world_size=world_size,\n   )\n   print(\"successfully initialized process group\")\n\n   rank = get_rank()  # pyre-ignore[16]\n   world_size = get_world_size()  # pyre-ignore[16]\n\n   t = F.one_hot(torch.tensor(rank), num_classes=world_size)\n   all_reduce(t)  # pyre-ignore[16]\n   computed_world_size = int(torch.sum(t).item())\n   print(\n       f\"rank: {rank}, actual world_size: {world_size}, computed world_size: {computed_world_size}\"\n   )\n   return computed_world_size\n\ndef main() -\u003e None:\n   compute_world_size()\n\nif __name__ == \"__main__\":\n   main()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Submitting a job to a cluster","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"Once your cluster is up and running, you can now submit your TorchX job with the ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/msaroufim/8d0e4ea8c785db7c0dd08bc7672b8196#launching-a-job"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"TorchX CLI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" after the above command is successfully finished. Through TorchXâs Ray Scheduler, your job will be submitted to the Ray cluster launched above in the cloud of your choice. For example:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5iPibPo6joZtxBOJq0AU7F","type":"Entry","createdAt":"2022-03-23T14:19:13.982Z","updatedAt":"2022-03-23T14:19:13.982Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"TorchX and Ray blog, code example 3","body":"! LOGLEVEL=INFO torchx run -s ray -cfg dashboard_address=54.214.124.247:20002,working_dir=scripts ./component.py:trainer\n\nOUTPUT:\n38:2:238:76:44mtorchx 2022-02-17 23:33:28 INFO    Uploading package gcs://_ray_pkg_755775e16a6c3645.zip.\n38:2:238:76:44mtorchx 2022-02-17 23:33:28 INFO    Creating a file package for local directory '/tmp/tmppc9csbps'.\nray://torchx/54.214.124.247:20002-raysubmit_ntquG1dDV6CtFUC5\n38:2:238:76:44mtorchx 2022-02-17 23:33:29 INFO    Launched app: ray://torchx/54.214.124.247:20002-raysubmit_ntquG1dDV6CtFUC5\n38:2:238:76:44mtorchx 2022-02-17 23:33:30 INFO    AppStatus:\n  msg: \u003cNONE\u003e\n  num_restarts: -1\n  roles:\n  - replicas:\n    - hostname: ''\n      id: 0\n      role: ray\n      state: !!python/object/apply:torchx.specs.api.AppState\n      - 2\n      structured_error_msg: \u003cNONE\u003e\n    role: ray\n  state: PENDING (2)\n  structured_error_msg: \u003cNONE\u003e\n  ui_url: null\n\n38:2:238:76:44mtorchx 2022-02-17 23:33:30 INFO    Job URL: None"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Use TorchX SDK and Ray Jobs API","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"A common way to check the status of your submission is by using TorchX SDK and Ray Jobs API. The sections below show the commands and results from common tasks such as examining the job status and monitoring a jobâs progress, which are common queries for developers.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Examine job status","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"After submitting a job, itâs queued and in a pending state, and after that the job can either be successful, fail because of some application error, or be interrupted by you.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"455qjvEsmNCYKDR53Z2Rrd","type":"Entry","createdAt":"2022-03-23T14:19:54.262Z","updatedAt":"2022-03-23T14:19:54.262Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"TorchX and Ray blog, code example 4","body":"# Get a job status\n# PENDING, FAILED, INTERRUPTED ETC..\n! torchx describe ray://torchx/54.214.124.247:20002-raysubmit_ntquG1dDV6CtFUC5\n\nOutput\n{ 'metadata': {},\n  'name': '54.214.124.247:20002-raysubmit_ntquG1dDV6CtFUC5',\n  'roles': [ { 'args': [],\n               'base_image': None,\n               'entrypoint': '\u003cMISSING\u003e',\n               'env': {},\n               'image': '',\n               'max_retries': 0,\n               'metadata': {},\n               'name': 'ray',\n               'num_replicas': 1,\n               'port_map': {},\n               'resource': { 'capabilities': {},\n                             'cpu': -1,\n                             'gpu': -1,\n                             'memMB': -1},\n               'retry_policy': \u003cRetryPolicy.APPLICATION: 'APPLICATION'\u003e}]}"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Monitor a jobâs progress","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Collecting logs works in much the same way as getting the job description works. The key here is that logs are actually distributed over multiple machines, yet you get them all streamed back to your console without having to worry about which machine has which logs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4H8HopMSvdDlyRB2u992YQ","type":"Entry","createdAt":"2022-03-23T14:20:28.088Z","updatedAt":"2022-03-23T14:20:28.088Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"TorchX and Ray blog, code example 5","body":"# Get job logs\n# Aggregate logs from all machines in the same place\n! torchx log ray://torchx/54.214.124.247:20002-raysubmit_ntquG1dDV6CtFUC5\n\nOutput:\nray/0 2022-02-17 15:33:32,983\tINFO worker.py:853 -- Connecting to existing Ray cluster at address: 10.10.12.9:6379\n(CommandActor pid=3575) successfully initialized process group\n(CommandActor pid=3575) rank: 1, actual world_size: 2, computed world_size: 2\n(CommandActor pid=3574) successfully initialized process group\n(CommandActor pid=3574) rank: 0, actual world_size: 2, computed world_size: 2"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Whatâs next","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Any existing TorchX component can now run on top of Ray. Over time, weâre looking to add more reference projects to TorchX to let anyone bootstrap an end-to-end ML infrastructure and seamlessly run it and scale it on top of Ray.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Check out the ","nodeType":"text"},{"data":{"uri":"https://colab.research.google.com/drive/1vVCpgQ9z_1SN8K9CJxUT2LtvUDN0AlND?usp=sharing"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"e-2-e example of TorchX and Ray Scheduler","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Visit ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/torchx/main/schedulers/ray.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"TorchX Ray Scheduler documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Check out the ","nodeType":"text"},{"data":{"uri":"https://www.youtube.com/watch?t=2520\u0026v=e-A93QftCfc\u0026feature=youtu.be"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"TorchX Ray Scheduler talk at the Ray Meetup","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Acknowlegements","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We want to acknowledge the joint engineering efforts by the ","nodeType":"text"},{"data":{"uri":"https://www.facebook.com/MetaAI/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Meta AI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Anyscale","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" teams for this endeavor.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Meta AI Team","nodeType":"text"},{"data":{},"marks":[],"value":": Mark Saroufim, Aliaksandr Ivanou, Can Balioglu, Tristan Rice, Kiuk Chung, Geeta Chauhan","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Anyscale Team","nodeType":"text"},{"data":{},"marks":[],"value":": Amog Kamsetty, Jiao Dong, Jules S. Damji, Richard Liaw","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"CXoaEC3OyVlcmQ8rt31tL","type":"Asset","createdAt":"2022-03-23T19:10:51.690Z","updatedAt":"2022-03-23T19:10:51.690Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-torchx-ray-thumb","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/CXoaEC3OyVlcmQ8rt31tL/021fef2894f1f79ccb67abdcebe7a997/blog-torchx-ray-thumb.png","details":{"size":839689,"image":{"width":1500,"height":1000}},"fileName":"blog-torchx-ray-thumb.png","contentType":"image/png"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Zk7jixFrLqHf3IJLzrO2B","type":"Entry","createdAt":"2022-05-03T18:47:31.422Z","updatedAt":"2022-05-03T18:47:31.422Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Ray Summit: Aug 23-24 in SF!","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5LTWfPAScfUtzXj2KOUALz","type":"Entry","createdAt":"2022-04-12T15:29:42.674Z","updatedAt":"2022-10-04T22:06:07.976Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":17,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"summit"}},"locale":"en-US"},"fields":{"title":"Ray Summit 2022","slug":"ray-summit-2022","description":"Centered on Ray, the open-source distributed computing framework, Ray Summit brings together ML practitioners, data scientists, and more to discuss the growing demand for distributed computing.","sections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xIJZXdAqhOEizUCUuQaiV","type":"Entry","createdAt":"2022-03-31T16:04:49.746Z","updatedAt":"2022-09-12T20:58:36.991Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":20,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-hero","header":"The Place for _Everything Ray_","body":"Missed us in San Francisco for Ray Summit 2022? Don't fret, you can watch your favorite sessions on demand! ","ctaText":"View Sessions","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"9F9S7NLaIdPL4ThYwVlFW","type":"Asset","createdAt":"2022-09-06T14:49:38.936Z","updatedAt":"2022-09-06T14:53:02.350Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Presented by Anyscale","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/9F9S7NLaIdPL4ThYwVlFW/8ecd09b334942298f0b8c2d5567ec032/presented-by-anyscale.svg","details":{"size":22813,"image":{"width":289,"height":32}},"fileName":"presented-by-anyscale.svg","contentType":"image/svg+xml"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4A0d4TUfp0TCqUA60CXopR","type":"Asset","createdAt":"2022-09-06T14:53:48.720Z","updatedAt":"2022-09-06T14:53:48.720Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4A0d4TUfp0TCqUA60CXopR/2e4240c268d074d840f7a00d069a768a/rs-collage_2x.jpg","details":{"size":568547,"image":{"width":2400,"height":1376}},"fileName":"rs-collage@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6FGUP0cXunTbgs2UemeAU","type":"Asset","createdAt":"2022-09-06T14:54:09.975Z","updatedAt":"2022-09-06T14:54:09.975Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event, stacked vertically","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6FGUP0cXunTbgs2UemeAU/16ed175024bc9cc0a2fae3cb13538dff/rs-collage-mobile_2x.jpg","details":{"size":243282,"image":{"width":750,"height":1420}},"fileName":"rs-collage-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4RKa7qVhWoIaoNSa1ZH2SK"}}],"type":"2022 Summit Hero","styling":["transparent-header"]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6GL7ZzuSLadWdVFKHj7yIH","type":"Entry","createdAt":"2022-04-20T18:00:12.964Z","updatedAt":"2022-08-08T22:50:29.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-keynote-speakers","header":"Keynote Speakers","ctaText":"See all speakers","ctaLink":"/ray-summit-2022/speakers","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6VhewtcICU9SCXjwd5qhHJ"}}],"type":"2022 Summit Speakers","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JTi5fsyo0Q3YrF3SAQglg","type":"Entry","createdAt":"2022-08-08T22:49:55.163Z","updatedAt":"2022-08-09T07:03:38.615Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-ray-summit-selected-speakers","header":"Highlighted Talks","ctaText":"Full Agenda","ctaLink":"/ray-summit-2022/agenda","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"50eS4cKXBNE4GHezdZ8Rws"}},{"sys":{"type":"Link","linkType":"Entry","id":"5iioKfOgEqVzgQ1DMmA1Wy"}},{"sys":{"type":"Link","linkType":"Entry","id":"YCBUZOXJGB0qcJF8TKjfI"}}],"type":"2022 Summit Agenda Preview"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5cgHuoJ713DewhVY6iUYrG","type":"Entry","createdAt":"2022-05-03T21:57:45.147Z","updatedAt":"2022-05-09T13:41:03.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-trainings","header":"Exclusive Ray Training Sessions","body":"Ray Summit is a great opportunity to level up your Ray skills through hands-on training sessions with the builders and maintainers of Ray. Weâre offering several pre-conference training sessions on August 22. Space is limited â [reserve your spot now](https://www.myeventi.events/raysummit22/attendee/)!","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3sYQTUmD5X5ncvGRFyIRR6","type":"Asset","createdAt":"2022-05-03T04:40:25.040Z","updatedAt":"2022-05-03T04:40:25.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image desktop","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3sYQTUmD5X5ncvGRFyIRR6/a77c675b07d63e73dd79ada9d182498a/training-image_2x.jpg","details":{"size":313509,"image":{"width":1136,"height":1480}},"fileName":"training-image@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7rsC56a4YCgENLyxsHyeIb","type":"Asset","createdAt":"2022-05-03T04:40:54.089Z","updatedAt":"2022-05-03T04:40:54.089Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image mobile","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7rsC56a4YCgENLyxsHyeIb/e2592c9b4424048b3c916fbbbc313065/training-image-mobile_2x.jpg","details":{"size":153801,"image":{"width":830,"height":874}},"fileName":"training-image-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4Hb3J9SrCFTDXqIhIdSIwa"}},{"sys":{"type":"Link","linkType":"Entry","id":"hZ4Tl1wvj7CwF00slwpLg"}},{"sys":{"type":"Link","linkType":"Entry","id":"YJ5KWyjg4NGf29NrfHcJX"}}],"type":"2022 Summit Trainings"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mRJQtuDbIhAuEbwLT2V1K","type":"Entry","createdAt":"2022-03-31T21:42:56.117Z","updatedAt":"2022-04-11T23:46:07.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-reasons-to-attend","header":"Why Attend?","body":"As the annual user conference for the [open source Ray community](https://www.ray.io/community), Ray Summit is a unique opportunity to enhance your Ray skills, build your professional network, and learn what's on the Ray roadmap.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"5f6gxk4a4ujE2y6FZ8ATQ5"}},{"sys":{"type":"Link","linkType":"Entry","id":"20QwyiOmkHCEOkMvp0Jhtk"}},{"sys":{"type":"Link","linkType":"Entry","id":"6QxivsuhhLoVSloRQs9ZlQ"}}],"type":"2022 Summit Content Tiles"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"15k0SGqB2ZKOUfAsM3JnyV","type":"Entry","createdAt":"2022-03-31T16:05:55.739Z","updatedAt":"2022-08-05T00:25:20.225Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":16,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-about","header":"Celebrate the Ray community","body":"Get ready for our epic community bash at the Art of Tinkering exhibit at the San Francisco Exploratorium on August 23 from 5:30 to 9:30 p.m. Come explore, play, and celebrate with the Ray Community. This is an exclusive opportunity to experience this amazing interactive exhibition before it closes in September.","ctaText":"Check out the agenda","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UcZT4h1P7uFDz6cn6JH8l","type":"Asset","createdAt":"2022-06-02T20:37:22.299Z","updatedAt":"2022-06-02T20:37:22.299Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Man taking a photo on his phone","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2UcZT4h1P7uFDz6cn6JH8l/5040a9909f18ab015c27b220077f916e/taking-picture.jpg","details":{"size":107846,"image":{"width":980,"height":772}},"fileName":"taking-picture.jpg","contentType":"image/jpeg"}}}],"type":"2022 Summit Split Image","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4P8Jn0q7nBU05iD6DmcQCC","type":"Entry","createdAt":"2022-04-01T19:15:31.735Z","updatedAt":"2022-04-11T16:36:59.573Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-social-tiles","header":"Join the Conversation","body":"Ready to get involved in the Ray community before the conference? Ask a question in the forums. Open a pull request. Or share why youâre excited with the hashtag __#RaySummit__ on Twitter.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6OMTlXkJ1OHXNkEjyPrXTH"}},{"sys":{"type":"Link","linkType":"Entry","id":"7EE28ee1ATQtodoLloT6mg"}},{"sys":{"type":"Link","linkType":"Entry","id":"1CRAzFxndL37wiNg4x8WKV"}},{"sys":{"type":"Link","linkType":"Entry","id":"2qrec5rR8kabZxCHaPS7NH"}}],"type":"2022 Summit Social Tiles"}}],"headerLogo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70DIdxwT8XY96aw7vMUHOu","type":"Asset","createdAt":"2022-06-01T15:21:46.804Z","updatedAt":"2022-06-01T15:21:46.804Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo Dark Blue","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/70DIdxwT8XY96aw7vMUHOu/5ca0a8b7533c8df70bc0e032b889862a/rs-logo-nav.svg","details":{"size":9954,"image":{"width":108,"height":48}},"fileName":"rs-logo-nav.svg","contentType":"image/svg+xml"}}},"headerLogoWhite":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3qyLXKGgGfWhv4oPqYl0SE","type":"Asset","createdAt":"2022-04-20T21:50:58.774Z","updatedAt":"2022-04-20T21:50:58.774Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3qyLXKGgGfWhv4oPqYl0SE/c4dee9e7a344c39db11fefafbf255b5b/rs-logo-horizontal.svg","details":{"size":9975,"image":{"width":108,"height":48}},"fileName":"rs-logo-horizontal.svg","contentType":"image/svg+xml"}}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KUtRQbdqJo1RrWMrVyfEm","type":"Entry","createdAt":"2022-03-31T16:21:13.402Z","updatedAt":"2022-04-08T17:25:33.228Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Home","link":"/ray-summit-2022"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4MvQcRSkHLMqIJoO9O2cQe","type":"Entry","createdAt":"2022-03-31T16:17:27.336Z","updatedAt":"2022-09-12T16:59:51.221Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"On-Demand","link":"/ray-summit-2022/agenda","subLinks":[{"sys":{"type":"Link","linkType":"Entry","id":"3N9kRDz36tieTI4YGVWKBJ"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46B5diwEDDr3kB2XDoGfDP","type":"Entry","createdAt":"2022-05-11T17:13:42.174Z","updatedAt":"2022-05-11T17:13:42.174Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Speakers","link":"/ray-summit-2022/speakers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Dt5fy6A7rBpXdG7HeKMGd","type":"Entry","createdAt":"2022-03-31T16:17:43.592Z","updatedAt":"2022-03-31T16:17:43.592Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Sponsors","link":"/ray-summit-2022/sponsors"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SvRE9pnyq3BXUPGKRWx6W","type":"Entry","createdAt":"2022-06-03T19:29:31.009Z","updatedAt":"2022-06-03T19:29:31.009Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Who Attends","link":"/ray-summit-2022/who-attends"}}],"mainSponsors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"xMQIOU9n7gVS4mkX2Nlag","type":"Entry","createdAt":"2021-01-31T16:33:06.495Z","updatedAt":"2021-05-11T13:28:31.312Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Ray","url":"https://ray.io","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jLroqm98RCMLGASthyuPv","type":"Asset","createdAt":"2021-05-11T13:28:26.930Z","updatedAt":"2021-05-11T13:28:26.930Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/jLroqm98RCMLGASthyuPv/f76cc04827316de5d3f8b529f6a3f02c/Ray_BM.svg","details":{"size":4742,"image":{"width":93,"height":36}},"fileName":"Ray_BM.svg","contentType":"image/svg+xml"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11syIHhtX8JajSlXHSHE1O","type":"Entry","createdAt":"2021-01-31T16:31:24.509Z","updatedAt":"2021-05-11T13:28:00.309Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Anyscale","url":"https://www.anyscale.com","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UkDmTzmZHIOG7yxFP4sVL","type":"Asset","createdAt":"2021-05-11T13:27:55.219Z","updatedAt":"2021-05-11T13:27:55.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Anyscale","file":{"url":"//images.ctfassets.net/xjan103pcp94/5UkDmTzmZHIOG7yxFP4sVL/db10aebda9a9aad10903470781a6f2e6/ANSC_BM.svg","details":{"size":7431,"image":{"width":158,"height":36}},"fileName":"ANSC_BM.svg","contentType":"image/svg+xml"}}}}}],"footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cfwECWrOqJ5Fegr3YpReS","type":"Entry","createdAt":"2022-04-01T21:22:12.001Z","updatedAt":"2022-04-13T17:35:29.511Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-footer","body":"Â©Anyscale, Inc 2022\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/privacy-policy\"\u003ePrivacy Policy\u003c/a\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/code-of-conduct\"\u003eCode of Conduct\u003c/a\u003e"}},"recommendations":[]}},"ctaText":"Register now","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5nTlh9josVs5zpZqU3vTbn","type":"Asset","createdAt":"2022-05-03T18:47:25.125Z","updatedAt":"2022-05-03T18:47:25.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-ray-summit-2022","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5nTlh9josVs5zpZqU3vTbn/e66043f62ed4a4b53d7e182e50289ec3/blog-recommended-content-ray-summit-2022.png","details":{"size":17234,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-ray-summit-2022.png","contentType":"image/png"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6c6PvXgWrpvWhaGipA2VqT","type":"Entry","createdAt":"2022-03-23T14:54:17.006Z","updatedAt":"2022-03-23T14:55:37.562Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Ray Meetup: Distributed training with TorchX and Ray","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"m3P3aUjUupwT8e7qmvnPT","type":"Entry","createdAt":"2022-02-03T01:19:53.417Z","updatedAt":"2022-07-07T22:28:17.904Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":14,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"event"}},"locale":"en-US"},"fields":{"title":"Ray Train, PyTorch, TorchX, and distributed deep learning","slug":"ray-train-pytorch-torchx-and-distributed-deep-learning","startDate":"2022-03-02T18:00-08:00","endDate":"2022-03-02T20:00-08:00","category":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QvFWse5FrsEUpxQNLnSJA","type":"Entry","createdAt":"2021-12-08T23:22:14.384Z","updatedAt":"2021-12-08T23:54:35.016Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"eventCategory"}},"locale":"en-US"},"fields":{"title":"Ray Meetup","slug":"ray_meetup"}},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6iEHV0lavYNyE9rLTiQmR7","type":"Asset","createdAt":"2022-07-07T22:28:14.897Z","updatedAt":"2022-07-07T22:28:14.897Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Meetup-Ray Train","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6iEHV0lavYNyE9rLTiQmR7/3e66dc2dbaa03c9efa6081a032380645/Ray_Meetup-Ray_Train.png","details":{"size":768739,"image":{"width":1500,"height":1000}},"fileName":"Ray Meetup-Ray Train.png","contentType":"image/png"}}},"videoLink":"https://youtu.be/e-A93QftCfc","summary":"Welcome to our second Ray meetup, where we focus on Rayâs native libraries for scaling machine learning workloads. \n\nWe'll discuss Ray Train, a production-ready distributed training library for deep learning workloads. And will present TorchX and Ray Integration. Through this integration, PyTorch developers can submit PyTorch-based scripts and workloads to a Ray Cluster using TorchXâs SDK and CLI via its new Ray Scheduler.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://speakerdeck.com/anyscale/ray-meetup-ray-train-pytorch-torchx-and-distributed-deep-learning"},"content":[{"data":{},"marks":[],"value":"\"Ray Train: For production-ready distributed deep learning\" slides \u003e\u003e\u003e","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://speakerdeck.com/anyscale/large-scale-distributed-training-with-torchx-and-ray"},"content":[{"data":{},"marks":[],"value":"\"Large scale distributed training with TorchX and Ray\" slides \u003e\u003e\u003e","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[{"type":"bold"},{"type":"underline"}],"value":"Agenda:","nodeType":"text"},{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"6:00 PM","nodeType":"text"},{"data":{},"marks":[],"value":" Welcome remarks, Announcements \u0026 Agenda by Jules Damji, Anyscale. Inc\n","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"6:05 PM","nodeType":"text"},{"data":{},"marks":[],"value":" âRay Train: For production-ready distributed deep learningâ Will Drevo, Amog Kamsetty \u0026 Matthew, Anyscale, Inc.\n","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"6:40 PM","nodeType":"text"},{"data":{},"marks":[],"value":" Q\u0026A\n","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"6:50 PM","nodeType":"text"},{"data":{},"marks":[],"value":" âLarge scale distributed training with TorchX and Rayâ by Mark Saroufim, Meta AI\n","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"7:30 PM","nodeType":"text"},{"data":{},"marks":[],"value":" Q\u0026A\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Talk 1: Ray Train: For Production-ready Distributed Deep Learning Workloads","nodeType":"text"}],"nodeType":"heading-5"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nToday, most frameworks for deep learning prototyping, training, and distributing to a cluster are either powerful and inflexible or nimble and toy-like. Data scientists are forced to choose between a great developer experience and a production-ready framework.\n\nTo fix this gap, the Ray ML team has developed Ray Train.\n\nRay Train is a library built on top of the Ray ecosystem that simplifies distributed deep learning. Currently, in stable beta in Ray 1.9, Ray Train offers the following features:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scales to multi-GPU and multi-node training with zero code changes","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Runs seamlessly on any cloud (AWS, GCP, Azure, Kubernetes, or on-prem)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Supports PyTorch, TensorFlow, and Horovod","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed data shuffling and loading with Ray Datasets","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed hyperparameter tuning with Ray Tune","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Built-in loggers for TensorBoard and MLflow","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nIn this session, we'll talk through some of the challenges in large-scale computer vision ML training, and show a demo of Ray Train in action.\n\n","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"Bios: Matthew Deng \u0026 Amog Kamsetty are senior software engineers at Anyscale in the Ray ML team. Will Drevo is ML product manager at Anyscale.","nodeType":"text"},{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Talk 2: Large Scale Distributed Training with TorchX and Ray","nodeType":"text"}],"nodeType":"heading-5"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nLarge-scale model training has generally been out of reach for people in open source because it requires an engineer to learn how to set up an infrastructure, how to build composable software systems, and how to set up robust machine learning scripts.\n\nTo that end, weâve built the TorchX Ray scheduler which leverages the newly created Ray Job API to allow scientists to focus on writing their scripts and making infrastructure and systems setup relatively easy.\n\n1. Setting up a multi GPU setup on any cloud provider is as easy as calling ray up the cluster.yaml\n2. TorchX embraces a component-based approach to designing systems that makes your ops workflows composable\n3. Running a distributed PyTorch script is then as simple as calling torchx run\n\nIn this session, weâll go through a practical live demo of how to train multi GPU models, set up the infrastructure live, and provide some tips and best practices to productionize such workflows\n\n","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"Bio: Mark Saroufim is a senior software engineer with the Meta AI and PyTorch Engineering Group and works on the PyTorch ecosystem.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"speakers":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3XZBgZdQF0b0hXsYTk9ymf","type":"Entry","createdAt":"2021-10-06T16:35:26.676Z","updatedAt":"2021-10-19T16:04:46.969Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Will Drevo","position":"Product Manager, Anyscale","affiliation":"Anyscale","bio":"Will is a Product Manager for ML at Anyscale. Previously, he was the first ML Engineer at Coinbase, and ran a couple of ML-related startups, one in the data labeling space and the other in the pharmaceutical space. He has a BS in CS and Music Composition from MIT, and did his master's thesis at MIT in machine learning systems. In his spare time, he produces electronic music, travels, and tries to find the best Ethiopian food in the Bay Area.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7nlMFH6QxymR43jMVfhLpp","type":"Asset","createdAt":"2021-10-06T16:35:20.496Z","updatedAt":"2021-10-06T16:35:20.496Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Will Drevo","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7nlMFH6QxymR43jMVfhLpp/e0d43d13afa5117a8ed6e814638b671a/Will_Drevo.jpeg","details":{"size":20940,"image":{"width":361,"height":361}},"fileName":"Will_Drevo.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3SvWPYblmudEPyDCFJPf6g","type":"Entry","createdAt":"2021-04-23T22:29:26.083Z","updatedAt":"2021-05-14T16:07:26.109Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","position":"Software Engineer, Anyscale","affiliation":"Anyscale","bio":"Amog Kamsetty is a software engineer at Anyscale where he works on building distributed training libraries and integrations on top of Ray. He previously completed his MS degree at UC Berkeley working with Ion Stoica on machine learning for database systems.\n","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ai9DjjcGVGsx5wCIQgNrE","type":"Asset","createdAt":"2021-04-23T22:27:16.833Z","updatedAt":"2021-04-23T22:27:16.833Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/6ai9DjjcGVGsx5wCIQgNrE/e0339f546995da6cb3f8a408d3fd76a7/Amog_Kamsetty.jpg","details":{"size":847883,"image":{"width":2125,"height":2125}},"fileName":"Amog_Kamsetty.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6xUgY2DL1lQTVYZi9BwOts","type":"Entry","createdAt":"2022-01-28T23:25:48.801Z","updatedAt":"2022-01-31T22:27:32.120Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Matthew Deng","position":"Software Engineer ","affiliation":"Software Engineer, Anyscale","bio":"Matthew Deng is a software engineer at Anyscale where he works on distributed machine learning libraries built on top of Ray. Before that, he was a software engineer at LinkedIn. He holds a BS in Electrical Engineering and Computer Science from UC Berkeley.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1qWWz3bTjcePpvnRxihDzU","type":"Asset","createdAt":"2022-01-31T22:27:26.678Z","updatedAt":"2022-01-31T22:27:26.678Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Matthew Deng","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1qWWz3bTjcePpvnRxihDzU/5872139b24bb33b347b74c7a585d0e26/Matt_Deng.jpg","details":{"size":39301,"image":{"width":435,"height":435}},"fileName":"Matt_Deng.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7l43ONRfIPSkxm1AqDVIM9","type":"Entry","createdAt":"2022-02-03T01:41:18.547Z","updatedAt":"2022-02-03T01:41:18.547Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Mark Saroufim","position":"OSS at Pytorch","affiliation":"Pytorch","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"s8NUJftUBLqJlrH8vzgsE","type":"Asset","createdAt":"2022-02-03T01:41:12.770Z","updatedAt":"2022-08-09T07:03:38.582Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Mark Saroufim","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/s8NUJftUBLqJlrH8vzgsE/12b99d44fa0a7507205dc1068d5c0476/Mark_Saroufim.jpeg","details":{"size":21630,"image":{"width":230,"height":222}},"fileName":"Mark_Saroufim.jpeg","contentType":"image/jpeg"}}}}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}}],"recommendations":[]}},"ctaText":"Watch now","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5pTTmJEBOmVLRGXx91RBWP","type":"Asset","createdAt":"2022-03-23T14:54:54.477Z","updatedAt":"2022-03-23T14:54:54.477Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-clock-gears","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5pTTmJEBOmVLRGXx91RBWP/73eda7b84a4e592aa0e52394fcd4d17a/blog-recommended-content-clock-gears.jpg","details":{"size":42120,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-clock-gears.jpg","contentType":"image/jpeg"}}},"recommendations":null}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}}],"hideIntro":false,"showMainImage":true,"thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4pvt4qBgpargy3hSa4cICE","type":"Asset","createdAt":"2022-03-24T19:32:14.959Z","updatedAt":"2022-03-24T19:32:14.959Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-neural-net-light","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4pvt4qBgpargy3hSa4cICE/f573f93ebc1283f5e78948e9a6bf0ff3/blog-recommended-content-neural-net-light.jpg","details":{"size":42583,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-neural-net-light.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2BqE4NF2ZDGFBeoka1tcQ7","type":"Entry","createdAt":"2022-03-23T13:44:04.268Z","updatedAt":"2022-06-22T15:50:52.566Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Call for Papers is now open","slug":"ray-summit-2022-call-for-papers-is-now-open","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4W4CsBpJcrALb90x0HXkRQ","type":"Entry","createdAt":"2022-02-07T15:00:35.988Z","updatedAt":"2022-02-07T15:00:35.988Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Chandler Gibbons","slug":"chandler-gibbons"}}],"publishedDate":"2022-03-23","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},"intro":"Ray Summit, the annual Ray user conference, is back this August, and the Call for Papers is open until April 18! In this blog post, weâll give you all the information you need to prepare a stellar talk proposal.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/ray-summit-2022?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=cfp_blog"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", the annual Ray community conference, is back in 2022 for a two-day, in-person event later this year (August 23-24). We are currently accepting proposals for conference talks, with a submission deadline of ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"April 18","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, weâll cover the most frequently asked questions about the Call for Papers and will give you all the information you need to prepare a stellar Ray Summit proposal.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What are the benefits of speaking at Ray Summit?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are three main benefits to speaking at Ray Summit.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"First, itâs an ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"opportunity to share your Ray expertise and knowledge with the wider Ray community","nodeType":"text"},{"data":{},"marks":[],"value":", giving your work wider technical recognition. This can lead to more people (including other speakers) connecting with you or seeking your advice, boosting your personal brand.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Second, ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"youâll be able to build your public speaking and presentation skills","nodeType":"text"},{"data":{},"marks":[],"value":". Particularly if you plan to apply to other conferences that require previous speaking experience, speaking at Ray Summit is a great opportunity to build that experience.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"And finally, as a speaker, youâll get a ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"free pass to attend Ray Summit","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What are the key themes for this yearâs Ray Summit?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The key themes and topics of interest for Ray Summit 2022 are:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Scalable ML","nodeType":"text"},{"data":{},"marks":[],"value":": feature engineering, distributed training, or hyperparameter tuning for use cases such as graph neural networks, computer vision, time-series, and NLP/transformers","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Reinforcement learning in the real world","nodeType":"text"},{"data":{},"marks":[],"value":" (recommendation systems, trading agents, etc.) built with Ray-native libraries or other frameworks","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"ML platforms or end-to-end applications","nodeType":"text"},{"data":{},"marks":[],"value":": data processing through model training, serving, workflow orchestration, and monitoring","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Deep dives","nodeType":"text"},{"data":{},"marks":[],"value":" into Ray core components and libraries","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Building your own ","nodeType":"text"},{"data":{},"marks":[],"value":"distributed framework using Ray API patterns","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Cloud computing topics","nodeType":"text"},{"data":{},"marks":[],"value":" such as serverless computing and multi-cloud","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray deployments","nodeType":"text"},{"data":{},"marks":[],"value":" on Kubernetes, on-premises, or public cloud","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Integrations","nodeType":"text"},{"data":{},"marks":[],"value":" of ML libraries and frameworks with the Ray ecosystem","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What types of talks can I submit?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We have three types of talks you can submit on the above themes and topics:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Technical Lightning Talks","nodeType":"text"},{"data":{},"marks":[],"value":" (10-15 minutes): Short technical talks covering the what, why, and how of a topic with digestible code examples or quick demos.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Technical Talks","nodeType":"text"},{"data":{},"marks":[],"value":" (30 minutes): Talks that are rich in technical detail, including code examples or demos, and help the audience grasp a problem you set out to solve, challenges you met along the way, and your solution.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Technical Deep Dives","nodeType":"text"},{"data":{},"marks":[],"value":" (45-60 minutes): More in-depth, highly technical talks aimed at an advanced audience, such as an in-depth tutorial or a highly technical presentation on a deep technical topic.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"For all talks, the audience should walk away having learned something new.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"I havenât done this before. Any tips for preparing a proposal?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Submitting a talk to a CFP can be overwhelming, but if you focus on a few key areas, itâs easy to make sure the value of your idea shines through.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Here are a few tips for submitting a successful proposal:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Start with a simple and straightforward title","nodeType":"text"},{"data":{},"marks":[],"value":". You only have a few seconds to grab your audienceâs attention.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Avoid using your proposal as a sales pitch","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Keep it focused","nodeType":"text"},{"data":{},"marks":[],"value":". You probably wonât have time to cover everything about your topic â choose a specific angle or technique to focus on.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Edit, edit, edit","nodeType":"text"},{"data":{},"marks":[],"value":". Once youâve written your abstract, be sure to read it over several times to make sure that it tells a clear story. Eliminate unnecessary words and sentences. Share with a trusted peer and get feedback.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Finally, always keep your audience in mind","nodeType":"text"},{"data":{},"marks":[],"value":". Explain why people will want to attend your talk and what theyâll learn from it with three to five numbered takeaways.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"For more guidance, check out the following resources:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://medium.com/@LachlanEvenson/lachies-7-step-guide-to-writing-a-winning-tech-conference-cfp-4fa36a0d2672"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"7 step guide to writing a winning tech conference CFP","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://snyk.io/blog/10-tips-for-getting-that-conference-cfp-accepted/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"10 Tips for getting that conference CFP accepted","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://jonthebeach.com/blog/CFP-Tips-and-Tricks-for-Tech-Conferences"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"CFP Tips and Tricks for Tech Conferences","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"OK, Iâm ready! How do I submit my talk?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Just head over to our ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/ray-summit-2022?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=cfp_blog"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit page","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and click on ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Submit a talk","nodeType":"text"},{"data":{},"marks":[],"value":". The submission deadline is April 18.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1Y8lcUmjrSdCvyOwqvZxCy","type":"Entry","createdAt":"2022-03-22T15:24:42.598Z","updatedAt":"2022-03-22T15:24:42.598Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"Ray Summit CFP CTA","body":"\u003ca href=\"/ray-summit-2022?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2022\u0026utm_content=cfp_blog\"\u003e\u003cimg src=\"//images.ctfassets.net/xjan103pcp94/3FcjpofhUfmp6vXTLJOTMo/bb1888306f3a53764012eb51fa542399/CFP-email-sig_2x.png\" style=\"width: 100%\"\u003e\u003c/a\u003e"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6aIPJECkOeCLEfisrAiBA7","type":"Asset","createdAt":"2022-03-22T15:25:01.374Z","updatedAt":"2022-03-22T15:25:01.374Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-ray-summit-thumb","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6aIPJECkOeCLEfisrAiBA7/a4cc895cbba7ca3e42bdd28ce94ac873/blog-ray-summit-thumb.jpg","details":{"size":70326,"image":{"width":750,"height":500}},"fileName":"blog-ray-summit-thumb.jpg","contentType":"image/jpeg"}}},"hideIntro":true,"showMainImage":false,"thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2QGlRgYmZvDrG6EBilSxLm","type":"Asset","createdAt":"2022-03-21T15:16:18.478Z","updatedAt":"2022-03-21T17:24:57.314Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"blog-recommended-content-ray-summit","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2QGlRgYmZvDrG6EBilSxLm/5c96f8b78dbc63066906ee582164c5b5/Recommended_content_thumbnails.png","details":{"size":9327,"image":{"width":140,"height":140}},"fileName":"Recommended content thumbnails.png","contentType":"image/png"}}},"recommendations":[]}}],"activeTag":null,"activeType":null,"author":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/","recommendations":[]}},"page":1,"totalPages":1,"allTypes":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KeH0JzrIEpuheazhMAhKf","type":"Entry","createdAt":"2022-06-15T01:31:23.950Z","updatedAt":"2022-06-15T01:31:23.950Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"User Story","slug":"user-story","tintColor":"#6D59C6"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42F9iHs5a05rRrOuqEQwvB","type":"Entry","createdAt":"2022-06-15T01:30:47.322Z","updatedAt":"2022-06-15T01:30:47.322Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Culture","slug":"culture","tintColor":"#39C8E5"}}],"allTags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KwkbI6zRqcE9KD5iKuP8W","type":"Entry","createdAt":"2021-12-05T04:51:33.974Z","updatedAt":"2021-12-05T04:51:33.974Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"RayDP","identifier":"ray_dp"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1OiVmfyPaDqRNavVG5Yp1l","type":"Entry","createdAt":"2021-12-05T04:50:12.541Z","updatedAt":"2021-12-05T04:50:12.541Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Retail","identifier":"retail"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"161LHBXwPkxUmhjt1YbfJH","type":"Entry","createdAt":"2021-12-05T04:49:42.528Z","updatedAt":"2021-12-05T04:49:42.528Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Gaming","identifier":"gaming"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"BDTwwkSW9VtmSkJojGbx8","type":"Entry","createdAt":"2021-12-05T04:49:30.169Z","updatedAt":"2021-12-05T04:49:30.169Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Government","identifier":"government"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4fzKvVOn2R8U6TTi2bcb2R","type":"Entry","createdAt":"2021-12-05T04:49:10.881Z","updatedAt":"2021-12-05T04:49:10.881Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"HLS","identifier":"hls"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"GGg4W2UlqfVP2iPMsc8J1","type":"Entry","createdAt":"2021-12-05T04:44:22.472Z","updatedAt":"2021-12-05T04:44:22.472Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Financial","identifier":"financial"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i6fp6Gfdy5wFeDDOaKcfh","type":"Entry","createdAt":"2021-12-03T22:33:42.248Z","updatedAt":"2021-12-03T22:33:42.248Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Anyscale","identifier":"anyscale"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6chYLcIc6yEB2EtLv2vngw","type":"Entry","createdAt":"2021-11-23T01:06:51.725Z","updatedAt":"2021-11-30T22:20:19.315Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Healthcare","identifier":"healthcare"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BzSmyQ2ojATOq6qOpD4PX","type":"Entry","createdAt":"2021-10-02T19:52:20.790Z","updatedAt":"2021-10-02T19:52:20.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Technology","identifier":"technology"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}}]},"__N_SSP":true},"page":"/blog","query":{"author":"jules-s-damji"},"buildId":"-oT8_WeW9m_lAxr26cJvY","isFallback":false,"gssp":true,"customServer":true,"scriptLoader":[]}</script><script type="text/javascript" id="hs-script-loader" async="" defer="" src="https://js.hs-scripts.com/20523749.js"></script><script type="text/javascript">
  _linkedin_partner_id = "4210313";
  window._linkedin_data_partner_ids = window._linkedin_data_partner_ids || [];
  window._linkedin_data_partner_ids.push(_linkedin_partner_id);
</script><script type="text/javascript">
  (function(l) { if (!l){window.lintrk = function(a,b){window.lintrk.q.push([a,b])};
  window.lintrk.q=[]} var s = document.getElementsByTagName("script")[0];
  var b = document.createElement("script");
  b.type = "text/javascript";b.async = true;
  b.src = "https://snap.licdn.com/li.lms-analytics/insight.min.js";
  s.parentNode.insertBefore(b, s);})(window.lintrk);
</script><noscript><img height="1" width="1" style="display:none" alt="" src="https://px.ads.linkedin.com/collect/?pid=4210313&amp;fmt=gif"/></noscript></body></html>