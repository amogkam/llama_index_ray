<!DOCTYPE html><html lang="en-US"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Blog | Anyscale</title><meta name="description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta property="og:title" content="Blog | Anyscale"/><meta property="og:description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta property="og:site_name" content="Anyscale"/><meta property="og:image" content=""/><meta property="fb:app_id" content=""/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta name="twitter:site" content=""/><meta name="twitter:title" content="Blog | Anyscale"/><meta name="twitter:image" content=""/><meta name="next-head-count" content="14"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><link rel="icon" href="static/favicon.ico"/><script>
(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-P8H6KQG');
</script><script src="https://tag.clearbitscripts.com/v1/pk_7a96be585f731437b35c7bad25c432b7/tags.js"></script><script src="https://www.googleoptimize.com/optimize.js?id=OPT-NPGB9W3"></script><script>
  !function(f,b,e,v,n,t,s)
  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
  n.queue=[];t=b.createElement(e);t.async=!0;
  t.src=v;s=b.getElementsByTagName(e)[0];
  s.parentNode.insertBefore(t,s)}(window,document,'script',
  'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '168099398554987');
  fbq('track', 'PageView');
</script><link rel="stylesheet" href="static/font-awesome-4.7.0/css/font-awesome.min.css"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="true"/><script src="https://fast.wistia.com/assets/external/E-v1.js" async=""></script><meta name="ahrefs-site-verification" content="4be3c5d78cdad8823d038256ac06d4336ae7a84632dac0ed57975703fc14fe6b"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="_next/static/css/dbba207740458c19.css" as="style"/><link rel="stylesheet" href="_next/static/css/dbba207740458c19.css" data-n-g=""/><link rel="preload" href="_next/static/css/13fbfc51931a4b43.css" as="style"/><link rel="stylesheet" href="_next/static/css/13fbfc51931a4b43.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="_next/static/chunks/webpack-94adde92c5e1c75b.js" defer=""></script><script src="_next/static/chunks/framework-95c3ca6de27174dd.js" defer=""></script><script src="_next/static/chunks/main-774d7304f1973b2a.js" defer=""></script><script src="_next/static/chunks/pages/_app-fd8cb5a9775605f5.js" defer=""></script><script src="_next/static/chunks/0f1ac474-1cd8c801d37caef0.js" defer=""></script><script src="_next/static/chunks/1812-a871a62a8e8aa88c.js" defer=""></script><script src="_next/static/chunks/9065-e24d5adee6cbba35.js" defer=""></script><script src="_next/static/chunks/6139-f3c4647afbd26b94.js" defer=""></script><script src="_next/static/chunks/8423-007cfdc371b27898.js" defer=""></script><script src="_next/static/chunks/3167-65b612e959dd1945.js" defer=""></script><script src="_next/static/chunks/9027-e83e1bb65c284840.js" defer=""></script><script src="_next/static/chunks/pages/blog-1eafbb689a124ac5.js" defer=""></script><script src="_next/static/-oT8_WeW9m_lAxr26cJvY/_buildManifest.js" defer=""></script><script src="_next/static/-oT8_WeW9m_lAxr26cJvY/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Rubik&display=swap">@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4i1Uw.woff) format('woff')}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFWkUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFU0UzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFVUUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFWUUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFV0UzdYPFkZVO.woff) format('woff');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href="https://fonts.googleapis.com/css2?family=Outfit:wght@400;500;600;700&display=swap">@font-face{font-family:'Outfit';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4TC1C4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4QK1C4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4e6yC4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4deyC4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,400;0,700;1,400;1,700&display=swap">@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTeEw.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINscg.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKtdU.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7MIU.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGqZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuE6ZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuFKZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGKZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGaZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuF6ZJW9XjDg.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINce_fuJGl18QRY.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINccvfuJGl18QRY.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcdffuJGl18QRY.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINceffuJGl18QRY.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcePfuJGl18QRY.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcdvfuJGl18Q.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdzeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdXeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdLeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhd7eFaxOedfTDw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhd_eFaxOedfTDw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdHeFaxOedc.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIxsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIVsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIJsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AI5sdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AI9sdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIFsdP3pBms.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body class=""><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-P8H6KQG" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next" data-reactroot=""><svg xmlns="http://www.w3.org/2000/svg" class="sr-only"><symbol id="svg-arrow-right-gradient"><path d="M1.5 7.5h18" stroke="url(#paint0_linear_3927_132230)" stroke-width="1.5" stroke-linecap="round"></path><path d="m16.6562 3.65625 3.8442 3.84412-3.8442 3.84413" stroke="url(#paint1_linear_3927_132230)" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><defs><linearGradient id="paint0_linear_3927_132230" x1="1.06972" y1="8.58929" x2="12.2319" y2="-1.53819" gradientUnits="userSpaceOnUse"><stop stop-color="#8B20BE"></stop><stop offset="1" stop-color="#3579FF" stop-opacity=".97"></stop></linearGradient><linearGradient id="paint1_linear_3927_132230" x1="12.377" y1="7.75171" x2="16.9307" y2="11.8684" gradientUnits="userSpaceOnUse"><stop stop-color="#8B20BE"></stop><stop offset="1" stop-color="#3579FF" stop-opacity=".97"></stop></linearGradient></defs></symbol></svg><div class="Banner_container__zEmFV"><a href="https://raysummit.anyscale.com/?utm_source=anyscale&amp;utm_medium=website&amp;utm_campaign=ray_summit_2023&amp;utm_content=homepage_ticker" rel="noreferrer nofollow" target="_blank"><span class="Banner_banner-text__wr6Xi"><span class="Banner_main-text__cpJ6H">Are you passionate about Ray, scalable AI and distributed computing? Join us at Ray Summit!</span><span class="Banner_cta-button__0JCtt"> Find out more! <!-- --> <svg width="23" height="14" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1.5 6.75a.75.75 0 0 0 0 1.5v-1.5Zm18 1.5a.75.75 0 0 0 0-1.5v1.5Zm-18 0h18v-1.5h-18v1.5Z" fill="#fff"></path><path d="M17.187 3.126a.75.75 0 0 0-1.061 1.06l1.06-1.06ZM20.5 7.5l.53.53a.75.75 0 0 0 0-1.06l-.53.53Zm-4.374 3.314a.75.75 0 1 0 1.06 1.06l-1.06-1.06Zm0-6.627L19.97 8.03l1.06-1.061-3.843-3.844-1.061 1.06ZM19.97 6.97l-3.844 3.844 1.06 1.06 3.845-3.843L19.97 6.97Z" fill="#fff"></path></svg></span></span></a><button class="Banner_close-button__9ksfY" type="button" aria-label="Close Banner"></button></div><header class="inner-shadow sticky top-0 z-[1000] h-24 transform py-5 transition-transform duration-500 bg-primary translate-y-0"><div class="container mx-auto flex items-center justify-between"><a href="index.html" class="text-white"><svg width="141" height="32" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><g clip-path="url(#anyscale-logo_svg__a)" fill="currentColor"><path d="M10.653 17.46H3.865C1.697 17.46 0 19.16 0 21.332v6.797c0 2.17 1.697 3.87 3.865 3.87h6.788c2.169 0 3.866-1.7 3.866-3.87V21.33c0-2.171-1.792-3.87-3.866-3.87Zm1.509 10.573c0 .85-.66 1.51-1.509 1.51H3.865a1.49 1.49 0 0 1-1.508-1.51v-6.796c0-.85.66-1.51 1.508-1.51h6.788c.849 0 1.509.66 1.509 1.51v6.796Z"></path><path d="M28.095 11.988h-8.108V3.87C19.892 1.7 18.196 0 16.027 0H3.865C1.697 0 0 1.7 0 3.87v6.608c0 2.171 1.697 3.87 3.865 3.87h13.67v13.687c0 2.171 1.698 3.87 3.866 3.87h6.6c2.168 0 3.865-1.699 3.865-3.87V15.858c0-2.076-1.697-3.775-3.771-3.87Zm-24.23 0a1.49 1.49 0 0 1-1.508-1.51V3.87c0-.85.66-1.51 1.508-1.51h12.162c.849 0 1.509.66 1.509 1.51v8.118H3.866Zm25.738 16.047c0 .85-.66 1.51-1.508 1.51h-6.6a1.49 1.49 0 0 1-1.508-1.51V14.348h8.108c.848 0 1.508.66 1.508 1.51v12.177ZM79.194 17.462l-3.017-8.118h-3.3l4.714 11.327-2.546 5.853h3.112l7.165-17.18H82.21l-3.016 8.118ZM93.335 14.819c-.66-.284-1.414-.472-2.075-.661-.66-.189-1.225-.378-1.79-.66-.378-.19-.566-.473-.566-.85 0-.378.188-.661.472-.85.847-.472 1.79-.378 2.639.094.377.284.566.661.566 1.133h2.828c0-1.133-.566-2.171-1.414-2.832-.943-.755-2.169-1.133-3.3-1.038-.849 0-1.697.094-2.45.472-.66.283-1.227.66-1.604 1.227-.377.472-.565 1.133-.565 1.794 0 .66.188 1.321.565 1.887.377.472.849.85 1.414 1.039.66.283 1.414.472 2.168.66.66.19 1.226.378 1.886.661.377.19.566.472.566.85 0 .377-.189.66-.471.85-.377.283-.85.377-1.32.377-.472 0-1.038-.094-1.415-.472-.377-.283-.565-.66-.66-1.133h-2.922c0 .755.282 1.416.754 1.982.472.567 1.131 1.133 1.791 1.416.849.378 1.698.567 2.546.567.849 0 1.697-.095 2.45-.472.661-.283 1.227-.66 1.604-1.227.377-.567.565-1.133.565-1.794 0-.66-.188-1.321-.66-1.888-.565-.66-1.037-.944-1.602-1.132ZM102.575 11.702c.566 0 1.036.095 1.508.472.377.283.66.755.849 1.227h3.205a5.366 5.366 0 0 0-1.886-3.02 5.348 5.348 0 0 0-3.582-1.039c-1.037 0-2.074.284-3.017.755a5.486 5.486 0 0 0-2.074 2.077 7.427 7.427 0 0 0 0 6.325c.472.85 1.131 1.604 2.074 2.076.943.472 1.98.756 3.017.756 1.32 0 2.546-.378 3.582-1.133a5.366 5.366 0 0 0 1.886-3.02h-3.111c-.283 1.037-1.32 1.698-2.357 1.604-.754 0-1.509-.283-1.98-.944a5.119 5.119 0 0 1 0-5.192 1.914 1.914 0 0 1 1.886-.944ZM118.412 10.1a.405.405 0 0 1-.377.378c-.094 0-.188 0-.188-.095a3.382 3.382 0 0 0-.944-.66c-.659-.378-1.507-.567-2.262-.567-.943 0-1.885.283-2.734.755a4.459 4.459 0 0 0-1.885 2.077c-.472.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.376.85 1.037 1.604 1.885 2.17.849.473 1.697.756 2.64.756.754 0 1.603-.189 2.263-.567.659-.283 1.131-.755 1.602-1.321v1.699h2.923V9.345h-2.923v.755Zm-.377 6.891c-.283.472-.66.944-1.132 1.227-.47.283-1.037.378-1.507.378-.567 0-1.037-.189-1.509-.378-.472-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.094-1.228.472-1.794.848-1.51 2.64-1.982 4.147-1.227h.095c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM55.907 10.1a.406.406 0 0 1-.377.378c-.095 0-.19 0-.19-.095a3.384 3.384 0 0 0-.942-.66c-.66-.378-1.508-.567-2.263-.567-.942 0-1.885.283-2.733.755a4.463 4.463 0 0 0-1.886 2.077c-.471.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.377.85 1.037 1.604 1.886 2.17.848.473 1.697.756 2.64.756.753 0 1.602-.189 2.262-.567.66-.283 1.131-.755 1.603-1.321v1.699h2.922V9.345h-2.922v.755Zm-.472 6.891c-.283.472-.66.944-1.131 1.227-.471.283-1.037.378-1.509.378-.565 0-1.037-.189-1.508-.378-.471-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.095-1.228.472-1.794.848-1.51 2.64-1.982 4.148-1.227h.094c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM126.708 5.477h-2.923v15.575h2.923V5.477ZM140.191 14.914c0-1.038-.189-2.077-.755-3.02-.471-.85-1.131-1.605-2.074-1.983a6.78 6.78 0 0 0-6.033 0 5.485 5.485 0 0 0-2.075 2.077c-.565 1.038-.754 2.077-.754 3.21 0 1.132.189 2.17.754 3.114a5.485 5.485 0 0 0 2.075 2.077c.942.472 1.979.755 3.016.755a6.56 6.56 0 0 0 3.489-1.038c.943-.661 1.602-1.7 1.885-2.738h-3.016c-.377.944-1.321 1.51-2.358 1.416-.659 0-1.414-.283-1.885-.755-.566-.472-.848-1.227-.848-1.982h8.484c.095-.378.095-.755.095-1.133Zm-8.579-.85c.093-.755.377-1.415.942-1.887.566-.472 1.226-.661 1.886-.661.754 0 1.414.189 1.979.66.566.473.849 1.133.849 1.794l-5.656.095ZM67.692 9.159a4.7 4.7 0 0 0-1.98.472c-.377.188-.754.377-1.037.66a.286.286 0 0 1-.377 0c-.094-.094-.094-.094-.094-.188v-.756H61.28v11.705h2.923v-6.418c-.094-.755.188-1.51.754-2.171.471-.472 1.226-.756 1.886-.756.66 0 1.414.284 1.885.755.472.567.754 1.322.66 2.172v6.418h2.923v-6.89c.094-1.322-.283-2.644-1.226-3.682-.848-.85-2.168-1.416-3.394-1.321Z"></path></g><defs><clipPath id="anyscale-logo_svg__a"><path fill="currentColor" d="M0 0h140.19v32H0z"></path></clipPath></defs></svg></a><div class="lg:relative lg:flex lg:flex-row hidden"><div class="max-h-[calc(100vh-192px)] items-center overflow-y-auto py-[1px] shadow-[0_-1px_0_#DAE3F2] lg:flex lg:overflow-visible lg:p-0 lg:shadow-none"><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Products</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="platform.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Anyscale Platform</div></a><a href="ray-open-source.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Open Source</div></a></div></div></div></div></div></div><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Solutions</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="container mx-auto mb-4 lg:mb-0 mt-2 lg:mt-0 lg:mr-12"><div class="header-card h-full rounded-[5px] p-6 lg:w-[296px]"><h5 id="scalable-ai-and-python">Scalable AI and Python</h5>
<p>Leverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more.</p></div></div><div class="grid lg:flex grid-cols-1 md:grid-cols-2 gap-x-10 lg:gap-x-12 container mx-auto"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><p class="text-base leading-normal font-normal text-darkGrey-300 px-6 lg:px-4 mb-4">Scalable AI and Python</p><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2"><a href="data-ingestion.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Data Ingestion</div></a><a href="reinforcement-learning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Reinforcement Learning</div></a><a href="ray-air.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray AIR</div></a><a href="model-serving.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Model Serving</div></a><a href="hyperparameter-tuning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Hyperparameter Tuning</div></a></div></div></div><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><p class="text-base leading-normal font-normal text-darkGrey-300 px-6 lg:px-4 mb-4">Use Cases</p><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2"><a href="large-language-models.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Large Language Models</div></a><a href="demand-forecasting.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Demand Forecasting/Pricing</div></a><a href="industrial-automation.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Industrial Automation</div></a><a href="machine-learning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Scalable ML Platforms</div></a><a href="natural-language-processing.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">NLP</div></a><a href="recommendation-system.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Recommendation System</div></a></div></div></div></div></div></div></div><a class="block font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4 shadow-[0_1px_0_#DAE3F2] lg:shadow-none" href="ray-ecosystem.html"><div class="w-full container mx-auto">Ecosystem</div></a><a class="block font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4 shadow-[0_1px_0_#DAE3F2] lg:shadow-none" href="user-stories.html"><div class="w-full container mx-auto">Success Stories</div></a><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Learn</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="blog/how-ray-solves-common-production-challenges-for-generative-ai.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Blog</div></a><a href="https://raysummit.anyscale.com/?utm_source=anyscale&amp;utm_medium=website&amp;utm_campaign=ray_summit_2023&amp;utm_content=home_nav-pulldown"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Summit 2023</div></a><a href="event-category/rl-summit.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Videos &amp; Webinars</div></a><a href="https://github.com/ray-project/ray-educational-materials"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Training</div></a><a href="https://docs.anyscale.com"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Anyscale Docs</div></a></div></div></div></div></div></div><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Company</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="about.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">About us</div></a><a href="press.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">News</div></a><a href="careers.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Careers</div></a><a href="community.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Community</div></a></div></div></div></div></div></div></div><div class="absolute bottom-0 left-0 right-0 bg-white py-5 shadow-[0_-1px_0_#DAE3F2] lg:relative lg:ml-5 lg:bg-transparent lg:p-0 lg:shadow-none"><div class="container mx-auto flex justify-end"><a class="btn bg-gradient min-w-[320px] justify-center bg-blue-900 text-center lg:min-w-0" href="beta.html">Try It Now</a></div></div></div><div class="cursor-pointer py-4 pl-6 lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M20.05 11H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95ZM20.05 16H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95ZM20.05 6H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95Z" fill="#fff"></path></svg></div></div></header><div class="Layout_page-sections__2__gb overflow-x-hidden"><div class="ArticlesList_container__mBEpW"><div class="ArticlesList_inner__QWc69"><div class="ArticlesList_header__45BKa"><h1>Posts by Michael Galarnyk</h1><div class="ArticlesList_spacer__8l_nL"></div></div><div class="BlogFilters_root__mrUMs"><div class="BlogFilters_inner__87PZK"><div class="BlogFilters_item__1GpID"><div class="wrapper"><select aria-label="Types" class="SelectDropdown_select__hNpf2"><option selected="">All Types</option><option value="news">News</option><option value="culture">Culture</option><option value="engineering">Engineering</option><option value="user-story">User Story</option></select><div class="SelectDropdown_select__hNpf2 SelectDropdown_select-dropdown__Jabke"><button type="button" aria-haspopup="true" aria-expanded="false" style="width:245px">All Types</button><ul role="menu" class="SelectDropdown_menu__3wacL"><li class="SelectDropdown_item__LO5La SelectDropdown_active__YhRi7" tabindex="0" role="menuitem">All Types</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">News</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Culture</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Engineering</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">User Story</li></ul></div></div></div><div class="BlogFilters_item__1GpID"><div class="wrapper"><select aria-label="Tags" class="SelectDropdown_select__hNpf2"><option selected="">All Products / Libraries</option><option value="anyscale">Anyscale</option><option value="ray_core">Ray Core</option><option value="ray-datasets">Ray Datasets</option><option value="ray_train">Ray Train</option><option value="ray-tune">Ray Tune</option><option value="ray_serve">Ray Serve</option><option value="rllib">Ray RLlib</option></select><div class="SelectDropdown_select__hNpf2 SelectDropdown_select-dropdown__Jabke"><button type="button" aria-haspopup="true" aria-expanded="false" style="width:245px">All Products / Libraries</button><ul role="menu" class="SelectDropdown_menu__3wacL"><li class="SelectDropdown_item__LO5La SelectDropdown_active__YhRi7" tabindex="0" role="menuitem">All Products / Libraries</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Anyscale</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Core</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Datasets</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Train</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Tune</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Serve</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray RLlib</li></ul></div></div></div></div></div><div class="empty"><h2>No posts found.</h2><p><a href="blog/how-ray-solves-common-production-challenges-for-generative-ai.html">View all</a></p></div><div class="ArticlesList_list__uP0RC"></div><div class="Pagination_container__FdBHw"></div></div></div></div><footer class="relative bg-primary pt-30 pb-30 md:pt-36 md:pb-[180px] lg:pt-20"><div class="container mx-auto flex flex-col space-y-12 md:space-y-16 lg:flex-row lg:justify-between lg:space-y-0"><div class="flex flex-col gap-16"><div class="flex flex-col gap-6"><div class="h-10 text-white"><svg width="141" height="32" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="origin-top-left scale-125"><g clip-path="url(#anyscale-logo_svg__a)" fill="currentColor"><path d="M10.653 17.46H3.865C1.697 17.46 0 19.16 0 21.332v6.797c0 2.17 1.697 3.87 3.865 3.87h6.788c2.169 0 3.866-1.7 3.866-3.87V21.33c0-2.171-1.792-3.87-3.866-3.87Zm1.509 10.573c0 .85-.66 1.51-1.509 1.51H3.865a1.49 1.49 0 0 1-1.508-1.51v-6.796c0-.85.66-1.51 1.508-1.51h6.788c.849 0 1.509.66 1.509 1.51v6.796Z"></path><path d="M28.095 11.988h-8.108V3.87C19.892 1.7 18.196 0 16.027 0H3.865C1.697 0 0 1.7 0 3.87v6.608c0 2.171 1.697 3.87 3.865 3.87h13.67v13.687c0 2.171 1.698 3.87 3.866 3.87h6.6c2.168 0 3.865-1.699 3.865-3.87V15.858c0-2.076-1.697-3.775-3.771-3.87Zm-24.23 0a1.49 1.49 0 0 1-1.508-1.51V3.87c0-.85.66-1.51 1.508-1.51h12.162c.849 0 1.509.66 1.509 1.51v8.118H3.866Zm25.738 16.047c0 .85-.66 1.51-1.508 1.51h-6.6a1.49 1.49 0 0 1-1.508-1.51V14.348h8.108c.848 0 1.508.66 1.508 1.51v12.177ZM79.194 17.462l-3.017-8.118h-3.3l4.714 11.327-2.546 5.853h3.112l7.165-17.18H82.21l-3.016 8.118ZM93.335 14.819c-.66-.284-1.414-.472-2.075-.661-.66-.189-1.225-.378-1.79-.66-.378-.19-.566-.473-.566-.85 0-.378.188-.661.472-.85.847-.472 1.79-.378 2.639.094.377.284.566.661.566 1.133h2.828c0-1.133-.566-2.171-1.414-2.832-.943-.755-2.169-1.133-3.3-1.038-.849 0-1.697.094-2.45.472-.66.283-1.227.66-1.604 1.227-.377.472-.565 1.133-.565 1.794 0 .66.188 1.321.565 1.887.377.472.849.85 1.414 1.039.66.283 1.414.472 2.168.66.66.19 1.226.378 1.886.661.377.19.566.472.566.85 0 .377-.189.66-.471.85-.377.283-.85.377-1.32.377-.472 0-1.038-.094-1.415-.472-.377-.283-.565-.66-.66-1.133h-2.922c0 .755.282 1.416.754 1.982.472.567 1.131 1.133 1.791 1.416.849.378 1.698.567 2.546.567.849 0 1.697-.095 2.45-.472.661-.283 1.227-.66 1.604-1.227.377-.567.565-1.133.565-1.794 0-.66-.188-1.321-.66-1.888-.565-.66-1.037-.944-1.602-1.132ZM102.575 11.702c.566 0 1.036.095 1.508.472.377.283.66.755.849 1.227h3.205a5.366 5.366 0 0 0-1.886-3.02 5.348 5.348 0 0 0-3.582-1.039c-1.037 0-2.074.284-3.017.755a5.486 5.486 0 0 0-2.074 2.077 7.427 7.427 0 0 0 0 6.325c.472.85 1.131 1.604 2.074 2.076.943.472 1.98.756 3.017.756 1.32 0 2.546-.378 3.582-1.133a5.366 5.366 0 0 0 1.886-3.02h-3.111c-.283 1.037-1.32 1.698-2.357 1.604-.754 0-1.509-.283-1.98-.944a5.119 5.119 0 0 1 0-5.192 1.914 1.914 0 0 1 1.886-.944ZM118.412 10.1a.405.405 0 0 1-.377.378c-.094 0-.188 0-.188-.095a3.382 3.382 0 0 0-.944-.66c-.659-.378-1.507-.567-2.262-.567-.943 0-1.885.283-2.734.755a4.459 4.459 0 0 0-1.885 2.077c-.472.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.376.85 1.037 1.604 1.885 2.17.849.473 1.697.756 2.64.756.754 0 1.603-.189 2.263-.567.659-.283 1.131-.755 1.602-1.321v1.699h2.923V9.345h-2.923v.755Zm-.377 6.891c-.283.472-.66.944-1.132 1.227-.47.283-1.037.378-1.507.378-.567 0-1.037-.189-1.509-.378-.472-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.094-1.228.472-1.794.848-1.51 2.64-1.982 4.147-1.227h.095c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM55.907 10.1a.406.406 0 0 1-.377.378c-.095 0-.19 0-.19-.095a3.384 3.384 0 0 0-.942-.66c-.66-.378-1.508-.567-2.263-.567-.942 0-1.885.283-2.733.755a4.463 4.463 0 0 0-1.886 2.077c-.471.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.377.85 1.037 1.604 1.886 2.17.848.473 1.697.756 2.64.756.753 0 1.602-.189 2.262-.567.66-.283 1.131-.755 1.603-1.321v1.699h2.922V9.345h-2.922v.755Zm-.472 6.891c-.283.472-.66.944-1.131 1.227-.471.283-1.037.378-1.509.378-.565 0-1.037-.189-1.508-.378-.471-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.095-1.228.472-1.794.848-1.51 2.64-1.982 4.148-1.227h.094c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM126.708 5.477h-2.923v15.575h2.923V5.477ZM140.191 14.914c0-1.038-.189-2.077-.755-3.02-.471-.85-1.131-1.605-2.074-1.983a6.78 6.78 0 0 0-6.033 0 5.485 5.485 0 0 0-2.075 2.077c-.565 1.038-.754 2.077-.754 3.21 0 1.132.189 2.17.754 3.114a5.485 5.485 0 0 0 2.075 2.077c.942.472 1.979.755 3.016.755a6.56 6.56 0 0 0 3.489-1.038c.943-.661 1.602-1.7 1.885-2.738h-3.016c-.377.944-1.321 1.51-2.358 1.416-.659 0-1.414-.283-1.885-.755-.566-.472-.848-1.227-.848-1.982h8.484c.095-.378.095-.755.095-1.133Zm-8.579-.85c.093-.755.377-1.415.942-1.887.566-.472 1.226-.661 1.886-.661.754 0 1.414.189 1.979.66.566.473.849 1.133.849 1.794l-5.656.095ZM67.692 9.159a4.7 4.7 0 0 0-1.98.472c-.377.188-.754.377-1.037.66a.286.286 0 0 1-.377 0c-.094-.094-.094-.094-.094-.188v-.756H61.28v11.705h2.923v-6.418c-.094-.755.188-1.51.754-2.171.471-.472 1.226-.756 1.886-.756.66 0 1.414.284 1.885.755.472.567.754 1.322.66 2.172v6.418h2.923v-6.89c.094-1.322-.283-2.644-1.226-3.682-.848-.85-2.168-1.416-3.394-1.321Z"></path></g><defs><clipPath id="anyscale-logo_svg__a"><path fill="currentColor" d="M0 0h140.19v32H0z"></path></clipPath></defs></svg></div><div class="font-light leading-normal text-lightBlue-300 opacity-70 hidden lg:flex"><span>Â© Anyscale, Inc 2022</span></div></div><div class="z-10 hidden flex-col gap-10 lg:flex"><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Anyscale</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://www.linkedin.com/company/joinanyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M15.784 14.168a1.667 1.667 0 1 1-3.334-.002 1.667 1.667 0 0 1 3.334.002Zm.05 2.9H12.5V27.5h3.334V17.067Zm5.266 0h-3.317V27.5h3.284v-5.475c0-3.05 3.975-3.334 3.975 0V27.5h3.291v-6.609c0-5.141-5.883-4.95-7.266-2.424l.033-1.4Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://www.facebook.com/AnyscaleCompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M21.667 21.247h2.083l.834-3.333h-2.917v-1.667c0-.858 0-1.666 1.666-1.666h1.25v-2.8a23.49 23.49 0 0 0-2.38-.117c-2.263 0-3.87 1.38-3.87 3.917v2.333h-2.5v3.333h2.5v7.084h3.334v-7.084Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://twitter.com/anyscalecompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/anyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Ray</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://twitter.com/raydistributed"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/ray-project/ray"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div></div></div><div class="grid grid-cols-2 gap-x-8 gap-y-12 md:grid-cols-3 md:gap-y-10 md:gap-x-16"><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Products</div><div><ul>
<li><a href="platform.html">Anyscale Compute Platform</a></li>
<li><a href="ray-open-source.html">Ray Open Source</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Events</div><div><ul>
<li><a href="event-category/rl-summit.html">Webinars</a></li>
<li><a href="event-category/rl-summit.html">Meetups</a></li>
<li><a href="event-category/rl-summit.html">Summits</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Company</div><div><ul>
<li><a href="about.html">About Us</a></li>
<li><a href="press.html">News</a></li>
<li><a href="careers.html">Careers</a></li>
<li><a href="community.html">Community</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Learn</div><div><ul>
<li><a href="blog/how-ray-solves-common-production-challenges-for-generative-ai.html">Blog</a></li>
<li><a href="event-category/rl-summit.html">Demos &amp; Webinars</a></li>
<li><a href="event-category/rl-summit.html">Anyscale Academy</a></li>
<li><a href="https://docs.anyscale.com">Anyscale Docs</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Scalable AI and Python</div><div><ul>
<li><a href="data-ingestion.html">Data Ingestion</a></li>
<li><a href="reinforcement-learning.html">Reinforcement Learning</a></li>
<li><a href="ray-air.html">Ray AIR</a></li>
<li><a href="model-serving.html">Model Serving</a></li>
<li><a href="hyperparameter-tuning.html">Hyperparameter Tuning</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Use Cases</div><div><ul>
<li><a href="demand-forecasting.html">Demand Forecasting/Pricing</a></li>
<li><a href="industrial-automation.html">Industrial Automation</a></li>
<li><a href="machine-learning.html">Scalable ML Platforms</a></li>
<li><a href="natural-language-processing.html">NLP</a></li>
<li><a href="recommendation-system.html">Recommendation System</a></li>
</ul>
</div></div></div><div class="grid-col-1 grid gap-12 md:grid-cols-3 md:gap-16 lg:hidden"><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Anyscale</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://www.linkedin.com/company/joinanyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M15.784 14.168a1.667 1.667 0 1 1-3.334-.002 1.667 1.667 0 0 1 3.334.002Zm.05 2.9H12.5V27.5h3.334V17.067Zm5.266 0h-3.317V27.5h3.284v-5.475c0-3.05 3.975-3.334 3.975 0V27.5h3.291v-6.609c0-5.141-5.883-4.95-7.266-2.424l.033-1.4Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://www.facebook.com/AnyscaleCompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M21.667 21.247h2.083l.834-3.333h-2.917v-1.667c0-.858 0-1.666 1.666-1.666h1.25v-2.8a23.49 23.49 0 0 0-2.38-.117c-2.263 0-3.87 1.38-3.87 3.917v2.333h-2.5v3.333h2.5v7.084h3.334v-7.084Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://twitter.com/anyscalecompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/anyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Ray</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://twitter.com/raydistributed"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/ray-project/ray"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="md:self-end md:text-right"><div class="font-light leading-normal text-lightBlue-300 opacity-70"><span>Â© Anyscale, Inc 2022</span></div></div></div></div><div class="pointer-events-none absolute bottom-0 left-0"><svg width="612" height="297" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="hidden lg:block"><g opacity="0.5" clip-path="url(#dots_svg__a)"><mask id="dots_svg__c" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="0" y="0" width="612" height="297"><path transform="rotate(90 611.5 .5)" fill="url(#dots_svg__b)" stroke="#F2F3F7" d="M611.5.5h296v611h-296z"></path></mask><g mask="url(#dots_svg__c)" stroke="#A2B7D4"><g opacity="0.2" stroke-linecap="round"><path d="M598 350v-688M582 350v-688M566 350v-688M550 350v-688M534 350v-688M518 350v-688M502 350v-688M486 350v-688M470 350v-688M454 350v-688M438 350v-688M422 350v-688M406 350v-688M390 350v-688M374 350v-688M358 350v-688M342 350v-688M326 350v-688M310 350v-688M294 350v-688M278 350v-688M262 350v-688M246 350v-688M230 350v-688M214 350v-688M198 350v-688M182 350v-688M166 350v-688M150 350v-688M134 350v-688M118 350v-688M102 350v-688M86 350v-688M70 350v-688M54 350v-688M38 350v-688M22 350v-688M6 350v-688"></path></g><g opacity="0.2" stroke-linecap="round"><path d="M1014 14H0M1014 30H0M1014 46H0M1014 62H0M1014 78H0M1014 94H0M1014 110H0M1014 126H0M1014 142H0M1014 158H0M1014 174H0M1014 190H4M1014 206H4M1014 222H4M1014 238H4M1014 254H4M1014 270H4M1014 286H4"></path></g><path fill="#4C6996" d="M615.5 12.5v3h-3v-3zM615.5 28.5v3h-3v-3zM615.5 44.5v3h-3v-3zM615.5 60.5v3h-3v-3zM615.5 76.5v3h-3v-3zM615.5 92.5v3h-3v-3zM615.5 108.5v3h-3v-3zM615.5 124.5v3h-3v-3zM615.5 140.5v3h-3v-3zM615.5 156.5v3h-3v-3zM615.5 172.5v3h-3v-3zM615.5 188.5v3h-3v-3zM615.5 204.5v3h-3v-3zM615.5 220.5v3h-3v-3zM615.5 236.5v3h-3v-3zM615.5 252.5v3h-3v-3zM615.5 268.5v3h-3v-3zM615.5 284.5v3h-3v-3zM599.5 12.5v3h-3v-3zM599.5 28.5v3h-3v-3zM599.5 44.5v3h-3v-3zM599.5 60.5v3h-3v-3zM599.5 76.5v3h-3v-3zM599.5 92.5v3h-3v-3zM599.5 108.5v3h-3v-3zM599.5 124.5v3h-3v-3zM599.5 140.5v3h-3v-3zM599.5 156.5v3h-3v-3zM599.5 172.5v3h-3v-3zM599.5 188.5v3h-3v-3zM599.5 204.5v3h-3v-3zM599.5 220.5v3h-3v-3zM599.5 236.5v3h-3v-3zM599.5 252.5v3h-3v-3zM599.5 268.5v3h-3v-3zM599.5 284.5v3h-3v-3zM583.5 12.5v3h-3v-3zM583.5 28.5v3h-3v-3zM583.5 44.5v3h-3v-3zM583.5 60.5v3h-3v-3zM583.5 76.5v3h-3v-3zM583.5 92.5v3h-3v-3zM583.5 108.5v3h-3v-3zM583.5 124.5v3h-3v-3zM583.5 140.5v3h-3v-3zM583.5 156.5v3h-3v-3zM583.5 172.5v3h-3v-3zM583.5 188.5v3h-3v-3zM583.5 204.5v3h-3v-3zM583.5 220.5v3h-3v-3zM583.5 236.5v3h-3v-3zM583.5 252.5v3h-3v-3zM583.5 268.5v3h-3v-3zM583.5 284.5v3h-3v-3zM567.5 12.5v3h-3v-3zM567.5 28.5v3h-3v-3zM567.5 44.5v3h-3v-3zM567.5 60.5v3h-3v-3zM567.5 76.5v3h-3v-3zM567.5 92.5v3h-3v-3zM567.5 108.5v3h-3v-3zM567.5 124.5v3h-3v-3zM567.5 140.5v3h-3v-3zM567.5 156.5v3h-3v-3zM567.5 172.5v3h-3v-3zM567.5 188.5v3h-3v-3zM567.5 204.5v3h-3v-3zM567.5 220.5v3h-3v-3zM567.5 236.5v3h-3v-3zM567.5 252.5v3h-3v-3zM567.5 268.5v3h-3v-3zM567.5 284.5v3h-3v-3zM551.5 12.5v3h-3v-3zM551.5 28.5v3h-3v-3zM551.5 44.5v3h-3v-3zM551.5 60.5v3h-3v-3zM551.5 76.5v3h-3v-3zM551.5 92.5v3h-3v-3zM551.5 108.5v3h-3v-3zM551.5 124.5v3h-3v-3zM551.5 140.5v3h-3v-3zM551.5 156.5v3h-3v-3zM551.5 172.5v3h-3v-3zM551.5 188.5v3h-3v-3zM551.5 204.5v3h-3v-3zM551.5 220.5v3h-3v-3zM551.5 236.5v3h-3v-3zM551.5 252.5v3h-3v-3zM551.5 268.5v3h-3v-3zM551.5 284.5v3h-3v-3zM535.5 12.5v3h-3v-3zM535.5 28.5v3h-3v-3zM535.5 44.5v3h-3v-3zM535.5 60.5v3h-3v-3zM535.5 76.5v3h-3v-3zM535.5 92.5v3h-3v-3zM535.5 108.5v3h-3v-3zM535.5 124.5v3h-3v-3zM535.5 140.5v3h-3v-3zM535.5 156.5v3h-3v-3zM535.5 172.5v3h-3v-3zM535.5 188.5v3h-3v-3zM535.5 204.5v3h-3v-3zM535.5 220.5v3h-3v-3zM535.5 236.5v3h-3v-3zM535.5 252.5v3h-3v-3zM535.5 268.5v3h-3v-3zM535.5 284.5v3h-3v-3zM519.5 12.5v3h-3v-3zM519.5 28.5v3h-3v-3zM519.5 44.5v3h-3v-3zM519.5 60.5v3h-3v-3zM519.5 76.5v3h-3v-3zM519.5 92.5v3h-3v-3zM519.5 108.5v3h-3v-3zM519.5 124.5v3h-3v-3zM519.5 140.5v3h-3v-3zM519.5 156.5v3h-3v-3zM519.5 172.5v3h-3v-3zM519.5 188.5v3h-3v-3zM519.5 204.5v3h-3v-3zM519.5 220.5v3h-3v-3zM519.5 236.5v3h-3v-3zM519.5 252.5v3h-3v-3zM519.5 268.5v3h-3v-3zM519.5 284.5v3h-3v-3zM503.5 12.5v3h-3v-3zM503.5 28.5v3h-3v-3zM503.5 44.5v3h-3v-3zM503.5 60.5v3h-3v-3zM503.5 76.5v3h-3v-3zM503.5 92.5v3h-3v-3zM503.5 108.5v3h-3v-3zM503.5 124.5v3h-3v-3zM503.5 140.5v3h-3v-3zM503.5 156.5v3h-3v-3zM503.5 172.5v3h-3v-3zM503.5 188.5v3h-3v-3zM503.5 204.5v3h-3v-3zM503.5 220.5v3h-3v-3zM503.5 236.5v3h-3v-3zM503.5 252.5v3h-3v-3zM503.5 268.5v3h-3v-3zM503.5 284.5v3h-3v-3zM487.5 12.5v3h-3v-3zM487.5 28.5v3h-3v-3zM487.5 44.5v3h-3v-3zM487.5 60.5v3h-3v-3zM487.5 76.5v3h-3v-3zM487.5 92.5v3h-3v-3zM487.5 108.5v3h-3v-3zM487.5 124.5v3h-3v-3zM487.5 140.5v3h-3v-3zM487.5 156.5v3h-3v-3zM487.5 172.5v3h-3v-3zM487.5 188.5v3h-3v-3zM487.5 204.5v3h-3v-3zM487.5 220.5v3h-3v-3zM487.5 236.5v3h-3v-3zM487.5 252.5v3h-3v-3zM487.5 268.5v3h-3v-3zM487.5 284.5v3h-3v-3zM471.5 12.5v3h-3v-3zM471.5 28.5v3h-3v-3zM471.5 44.5v3h-3v-3zM471.5 60.5v3h-3v-3zM471.5 76.5v3h-3v-3zM471.5 92.5v3h-3v-3zM471.5 108.5v3h-3v-3zM471.5 124.5v3h-3v-3zM471.5 140.5v3h-3v-3zM471.5 156.5v3h-3v-3zM471.5 172.5v3h-3v-3zM471.5 188.5v3h-3v-3zM471.5 204.5v3h-3v-3zM471.5 220.5v3h-3v-3zM471.5 236.5v3h-3v-3zM471.5 252.5v3h-3v-3zM471.5 268.5v3h-3v-3zM471.5 284.5v3h-3v-3zM455.5 12.5v3h-3v-3zM455.5 28.5v3h-3v-3zM455.5 44.5v3h-3v-3zM455.5 60.5v3h-3v-3zM455.5 76.5v3h-3v-3zM455.5 92.5v3h-3v-3zM455.5 108.5v3h-3v-3zM455.5 124.5v3h-3v-3zM455.5 140.5v3h-3v-3zM455.5 156.5v3h-3v-3zM455.5 172.5v3h-3v-3zM455.5 188.5v3h-3v-3zM455.5 204.5v3h-3v-3zM455.5 220.5v3h-3v-3zM455.5 236.5v3h-3v-3zM455.5 252.5v3h-3v-3zM455.5 268.5v3h-3v-3zM455.5 284.5v3h-3v-3zM439.5 12.5v3h-3v-3zM439.5 28.5v3h-3v-3zM439.5 44.5v3h-3v-3zM439.5 60.5v3h-3v-3zM439.5 76.5v3h-3v-3zM439.5 92.5v3h-3v-3zM439.5 108.5v3h-3v-3zM439.5 124.5v3h-3v-3zM439.5 140.5v3h-3v-3zM439.5 156.5v3h-3v-3zM439.5 172.5v3h-3v-3zM439.5 188.5v3h-3v-3zM439.5 204.5v3h-3v-3zM439.5 220.5v3h-3v-3zM439.5 236.5v3h-3v-3zM439.5 252.5v3h-3v-3zM439.5 268.5v3h-3v-3zM439.5 284.5v3h-3v-3zM423.5 12.5v3h-3v-3zM423.5 28.5v3h-3v-3zM423.5 44.5v3h-3v-3zM423.5 60.5v3h-3v-3zM423.5 76.5v3h-3v-3zM423.5 92.5v3h-3v-3zM423.5 108.5v3h-3v-3zM423.5 124.5v3h-3v-3zM423.5 140.5v3h-3v-3zM423.5 156.5v3h-3v-3zM423.5 172.5v3h-3v-3zM423.5 188.5v3h-3v-3zM423.5 204.5v3h-3v-3zM423.5 220.5v3h-3v-3zM423.5 236.5v3h-3v-3zM423.5 252.5v3h-3v-3zM423.5 268.5v3h-3v-3zM423.5 284.5v3h-3v-3zM407.5 12.5v3h-3v-3zM407.5 28.5v3h-3v-3zM407.5 44.5v3h-3v-3zM407.5 60.5v3h-3v-3zM407.5 76.5v3h-3v-3zM407.5 92.5v3h-3v-3zM407.5 108.5v3h-3v-3zM407.5 124.5v3h-3v-3zM407.5 140.5v3h-3v-3zM407.5 156.5v3h-3v-3zM407.5 172.5v3h-3v-3zM407.5 188.5v3h-3v-3zM407.5 204.5v3h-3v-3zM407.5 220.5v3h-3v-3zM407.5 236.5v3h-3v-3zM407.5 252.5v3h-3v-3zM407.5 268.5v3h-3v-3zM407.5 284.5v3h-3v-3zM391.5 12.5v3h-3v-3zM391.5 28.5v3h-3v-3zM391.5 44.5v3h-3v-3zM391.5 60.5v3h-3v-3zM391.5 76.5v3h-3v-3zM391.5 92.5v3h-3v-3zM391.5 108.5v3h-3v-3zM391.5 124.5v3h-3v-3zM391.5 140.5v3h-3v-3zM391.5 156.5v3h-3v-3zM391.5 172.5v3h-3v-3zM391.5 188.5v3h-3v-3zM391.5 204.5v3h-3v-3zM391.5 220.5v3h-3v-3zM391.5 236.5v3h-3v-3zM391.5 252.5v3h-3v-3zM391.5 268.5v3h-3v-3zM391.5 284.5v3h-3v-3zM375.5 12.5v3h-3v-3zM375.5 28.5v3h-3v-3zM375.5 44.5v3h-3v-3zM375.5 60.5v3h-3v-3zM375.5 76.5v3h-3v-3zM375.5 92.5v3h-3v-3zM375.5 108.5v3h-3v-3zM375.5 124.5v3h-3v-3zM375.5 140.5v3h-3v-3zM375.5 156.5v3h-3v-3zM375.5 172.5v3h-3v-3zM375.5 188.5v3h-3v-3zM375.5 204.5v3h-3v-3zM375.5 220.5v3h-3v-3z"></path><rect x="375.5" y="236.5" width="3" height="3" rx="1.5" transform="rotate(90 375.5 236.5)" fill="#4C6996"></rect><path fill="#4C6996" d="M375.5 252.5v3h-3v-3zM375.5 268.5v3h-3v-3zM375.5 284.5v3h-3v-3zM359.5 12.5v3h-3v-3zM359.5 28.5v3h-3v-3zM359.5 44.5v3h-3v-3zM359.5 60.5v3h-3v-3zM359.5 76.5v3h-3v-3zM359.5 92.5v3h-3v-3zM359.5 108.5v3h-3v-3zM359.5 124.5v3h-3v-3zM359.5 140.5v3h-3v-3zM359.5 156.5v3h-3v-3zM359.5 172.5v3h-3v-3zM359.5 188.5v3h-3v-3zM359.5 204.5v3h-3v-3zM359.5 220.5v3h-3v-3zM359.5 236.5v3h-3v-3zM359.5 252.5v3h-3v-3zM359.5 268.5v3h-3v-3zM359.5 284.5v3h-3v-3zM343.5 12.5v3h-3v-3zM343.5 28.5v3h-3v-3zM343.5 44.5v3h-3v-3zM343.5 60.5v3h-3v-3zM343.5 76.5v3h-3v-3zM343.5 92.5v3h-3v-3zM343.5 108.5v3h-3v-3zM343.5 124.5v3h-3v-3zM343.5 140.5v3h-3v-3zM343.5 156.5v3h-3v-3zM343.5 172.5v3h-3v-3zM343.5 188.5v3h-3v-3zM343.5 204.5v3h-3v-3zM343.5 220.5v3h-3v-3zM343.5 236.5v3h-3v-3zM343.5 252.5v3h-3v-3zM343.5 268.5v3h-3v-3zM343.5 284.5v3h-3v-3zM327.5 12.5v3h-3v-3zM327.5 28.5v3h-3v-3zM327.5 44.5v3h-3v-3zM327.5 60.5v3h-3v-3zM327.5 76.5v3h-3v-3zM327.5 92.5v3h-3v-3zM327.5 108.5v3h-3v-3zM327.5 124.5v3h-3v-3zM327.5 140.5v3h-3v-3zM327.5 156.5v3h-3v-3zM327.5 172.5v3h-3v-3zM327.5 188.5v3h-3v-3zM327.5 204.5v3h-3v-3zM327.5 220.5v3h-3v-3zM327.5 236.5v3h-3v-3zM327.5 252.5v3h-3v-3zM327.5 268.5v3h-3v-3zM327.5 284.5v3h-3v-3zM311.5 12.5v3h-3v-3zM311.5 28.5v3h-3v-3zM311.5 44.5v3h-3v-3zM311.5 60.5v3h-3v-3zM311.5 76.5v3h-3v-3zM311.5 92.5v3h-3v-3zM311.5 108.5v3h-3v-3zM311.5 124.5v3h-3v-3zM311.5 140.5v3h-3v-3zM311.5 156.5v3h-3v-3zM311.5 172.5v3h-3v-3zM311.5 188.5v3h-3v-3zM311.5 204.5v3h-3v-3zM311.5 220.5v3h-3v-3zM311.5 236.5v3h-3v-3zM311.5 252.5v3h-3v-3zM311.5 268.5v3h-3v-3zM311.5 284.5v3h-3v-3zM295.5 12.5v3h-3v-3zM295.5 28.5v3h-3v-3zM295.5 44.5v3h-3v-3zM295.5 60.5v3h-3v-3zM295.5 76.5v3h-3v-3zM295.5 92.5v3h-3v-3zM295.5 108.5v3h-3v-3zM295.5 124.5v3h-3v-3zM295.5 140.5v3h-3v-3zM295.5 156.5v3h-3v-3zM295.5 172.5v3h-3v-3zM295.5 188.5v3h-3v-3zM295.5 204.5v3h-3v-3zM295.5 220.5v3h-3v-3zM295.5 236.5v3h-3v-3zM295.5 252.5v3h-3v-3zM295.5 268.5v3h-3v-3zM295.5 284.5v3h-3v-3zM279.5 12.5v3h-3v-3zM279.5 28.5v3h-3v-3zM279.5 44.5v3h-3v-3zM279.5 60.5v3h-3v-3zM279.5 76.5v3h-3v-3zM279.5 92.5v3h-3v-3zM279.5 108.5v3h-3v-3zM279.5 124.5v3h-3v-3zM279.5 140.5v3h-3v-3zM279.5 156.5v3h-3v-3zM279.5 172.5v3h-3v-3zM279.5 188.5v3h-3v-3zM279.5 204.5v3h-3v-3zM279.5 220.5v3h-3v-3zM279.5 236.5v3h-3v-3zM279.5 252.5v3h-3v-3zM279.5 268.5v3h-3v-3zM279.5 284.5v3h-3v-3zM263.5 12.5v3h-3v-3zM263.5 28.5v3h-3v-3zM263.5 44.5v3h-3v-3zM263.5 60.5v3h-3v-3zM263.5 76.5v3h-3v-3zM263.5 92.5v3h-3v-3zM263.5 108.5v3h-3v-3zM263.5 124.5v3h-3v-3zM263.5 140.5v3h-3v-3zM263.5 156.5v3h-3v-3zM263.5 172.5v3h-3v-3zM263.5 188.5v3h-3v-3zM263.5 204.5v3h-3v-3zM263.5 220.5v3h-3v-3zM263.5 236.5v3h-3v-3zM263.5 252.5v3h-3v-3zM263.5 268.5v3h-3v-3zM263.5 284.5v3h-3v-3zM247.5 12.5v3h-3v-3zM247.5 28.5v3h-3v-3zM247.5 44.5v3h-3v-3zM247.5 60.5v3h-3v-3zM247.5 76.5v3h-3v-3zM247.5 92.5v3h-3v-3zM247.5 108.5v3h-3v-3zM247.5 124.5v3h-3v-3zM247.5 140.5v3h-3v-3zM247.5 156.5v3h-3v-3zM247.5 172.5v3h-3v-3zM247.5 188.5v3h-3v-3zM247.5 204.5v3h-3v-3zM247.5 220.5v3h-3v-3zM247.5 236.5v3h-3v-3zM247.5 252.5v3h-3v-3zM247.5 268.5v3h-3v-3zM247.5 284.5v3h-3v-3zM231.5 12.5v3h-3v-3zM231.5 28.5v3h-3v-3zM231.5 44.5v3h-3v-3zM231.5 60.5v3h-3v-3zM231.5 76.5v3h-3v-3zM231.5 92.5v3h-3v-3zM231.5 108.5v3h-3v-3zM231.5 124.5v3h-3v-3zM231.5 140.5v3h-3v-3zM231.5 156.5v3h-3v-3zM231.5 172.5v3h-3v-3zM231.5 188.5v3h-3v-3zM231.5 204.5v3h-3v-3zM231.5 220.5v3h-3v-3zM231.5 236.5v3h-3v-3zM231.5 252.5v3h-3v-3zM231.5 268.5v3h-3v-3zM231.5 284.5v3h-3v-3zM215.5 12.5v3h-3v-3zM215.5 28.5v3h-3v-3zM215.5 44.5v3h-3v-3zM215.5 60.5v3h-3v-3zM215.5 76.5v3h-3v-3zM215.5 92.5v3h-3v-3zM215.5 108.5v3h-3v-3zM215.5 124.5v3h-3v-3zM215.5 140.5v3h-3v-3zM215.5 156.5v3h-3v-3zM215.5 172.5v3h-3v-3zM215.5 188.5v3h-3v-3zM215.5 204.5v3h-3v-3zM215.5 220.5v3h-3v-3zM215.5 236.5v3h-3v-3zM215.5 252.5v3h-3v-3zM215.5 268.5v3h-3v-3zM215.5 284.5v3h-3v-3zM199.5 12.5v3h-3v-3zM199.5 28.5v3h-3v-3zM199.5 44.5v3h-3v-3zM199.5 60.5v3h-3v-3zM199.5 76.5v3h-3v-3zM199.5 92.5v3h-3v-3zM199.5 108.5v3h-3v-3zM199.5 124.5v3h-3v-3zM199.5 140.5v3h-3v-3zM199.5 156.5v3h-3v-3zM199.5 172.5v3h-3v-3zM199.5 188.5v3h-3v-3zM199.5 204.5v3h-3v-3zM199.5 220.5v3h-3v-3zM199.5 236.5v3h-3v-3zM199.5 252.5v3h-3v-3zM199.5 268.5v3h-3v-3zM199.5 284.5v3h-3v-3zM183.5 12.5v3h-3v-3zM183.5 28.5v3h-3v-3zM183.5 44.5v3h-3v-3zM183.5 60.5v3h-3v-3zM183.5 76.5v3h-3v-3zM183.5 92.5v3h-3v-3zM183.5 108.5v3h-3v-3zM183.5 124.5v3h-3v-3zM183.5 140.5v3h-3v-3zM183.5 156.5v3h-3v-3zM183.5 172.5v3h-3v-3zM183.5 188.5v3h-3v-3zM183.5 204.5v3h-3v-3zM183.5 220.5v3h-3v-3zM183.5 236.5v3h-3v-3zM183.5 252.5v3h-3v-3zM183.5 268.5v3h-3v-3zM183.5 284.5v3h-3v-3zM167.5 12.5v3h-3v-3zM167.5 28.5v3h-3v-3zM167.5 44.5v3h-3v-3zM167.5 60.5v3h-3v-3zM167.5 76.5v3h-3v-3zM167.5 92.5v3h-3v-3zM167.5 108.5v3h-3v-3zM167.5 124.5v3h-3v-3zM167.5 140.5v3h-3v-3zM167.5 156.5v3h-3v-3zM167.5 172.5v3h-3v-3zM167.5 188.5v3h-3v-3zM167.5 204.5v3h-3v-3zM167.5 220.5v3h-3v-3zM167.5 236.5v3h-3v-3zM167.5 252.5v3h-3v-3zM167.5 268.5v3h-3v-3zM167.5 284.5v3h-3v-3zM151.5 12.5v3h-3v-3zM151.5 28.5v3h-3v-3zM151.5 44.5v3h-3v-3zM151.5 60.5v3h-3v-3zM151.5 76.5v3h-3v-3zM151.5 92.5v3h-3v-3zM151.5 108.5v3h-3v-3zM151.5 124.5v3h-3v-3zM151.5 140.5v3h-3v-3zM151.5 156.5v3h-3v-3zM151.5 172.5v3h-3v-3zM151.5 188.5v3h-3v-3zM151.5 204.5v3h-3v-3zM151.5 220.5v3h-3v-3zM151.5 236.5v3h-3v-3zM151.5 252.5v3h-3v-3zM151.5 268.5v3h-3v-3zM151.5 284.5v3h-3v-3zM135.5 12.5v3h-3v-3zM135.5 28.5v3h-3v-3zM135.5 44.5v3h-3v-3zM135.5 60.5v3h-3v-3zM135.5 76.5v3h-3v-3zM135.5 92.5v3h-3v-3zM135.5 108.5v3h-3v-3zM135.5 124.5v3h-3v-3zM135.5 140.5v3h-3v-3zM135.5 156.5v3h-3v-3zM135.5 172.5v3h-3v-3zM135.5 188.5v3h-3v-3zM135.5 204.5v3h-3v-3zM135.5 220.5v3h-3v-3zM135.5 236.5v3h-3v-3zM135.5 252.5v3h-3v-3zM135.5 268.5v3h-3v-3zM135.5 284.5v3h-3v-3zM119.5 12.5v3h-3v-3zM119.5 28.5v3h-3v-3zM119.5 44.5v3h-3v-3zM119.5 60.5v3h-3v-3zM119.5 76.5v3h-3v-3zM119.5 92.5v3h-3v-3zM119.5 108.5v3h-3v-3zM119.5 124.5v3h-3v-3zM119.5 140.5v3h-3v-3zM119.5 156.5v3h-3v-3zM119.5 172.5v3h-3v-3zM119.5 188.5v3h-3v-3zM119.5 204.5v3h-3v-3zM119.5 220.5v3h-3v-3zM119.5 236.5v3h-3v-3zM119.5 252.5v3h-3v-3zM119.5 268.5v3h-3v-3zM119.5 284.5v3h-3v-3zM103.5 12.5v3h-3v-3zM103.5 28.5v3h-3v-3zM103.5 44.5v3h-3v-3zM103.5 60.5v3h-3v-3zM103.5 76.5v3h-3v-3zM103.5 92.5v3h-3v-3zM103.5 108.5v3h-3v-3zM103.5 124.5v3h-3v-3zM103.5 140.5v3h-3v-3zM103.5 156.5v3h-3v-3zM103.5 172.5v3h-3v-3zM103.5 188.5v3h-3v-3zM103.5 204.5v3h-3v-3zM103.5 220.5v3h-3v-3zM103.5 236.5v3h-3v-3zM103.5 252.5v3h-3v-3zM103.5 268.5v3h-3v-3zM103.5 284.5v3h-3v-3zM87.5 12.5v3h-3v-3zM87.5 28.5v3h-3v-3zM87.5 44.5v3h-3v-3zM87.5 60.5v3h-3v-3zM87.5 76.5v3h-3v-3zM87.5 92.5v3h-3v-3zM87.5 108.5v3h-3v-3zM87.5 124.5v3h-3v-3zM87.5 140.5v3h-3v-3zM87.5 156.5v3h-3v-3zM87.5 172.5v3h-3v-3zM87.5 188.5v3h-3v-3zM87.5 204.5v3h-3v-3zM87.5 220.5v3h-3v-3zM87.5 236.5v3h-3v-3zM87.5 252.5v3h-3v-3zM87.5 268.5v3h-3v-3zM87.5 284.5v3h-3v-3zM71.5 12.5v3h-3v-3zM71.5 28.5v3h-3v-3zM71.5 44.5v3h-3v-3zM71.5 60.5v3h-3v-3zM71.5 76.5v3h-3v-3zM71.5 92.5v3h-3v-3zM71.5 108.5v3h-3v-3zM71.5 124.5v3h-3v-3zM71.5 140.5v3h-3v-3zM71.5 156.5v3h-3v-3zM71.5 172.5v3h-3v-3zM71.5 188.5v3h-3v-3zM71.5 204.5v3h-3v-3zM71.5 220.5v3h-3v-3zM71.5 236.5v3h-3v-3zM71.5 252.5v3h-3v-3zM71.5 268.5v3h-3v-3zM71.5 284.5v3h-3v-3zM55.5 12.5v3h-3v-3zM55.5 28.5v3h-3v-3zM55.5 44.5v3h-3v-3zM55.5 60.5v3h-3v-3zM55.5 76.5v3h-3v-3zM55.5 92.5v3h-3v-3zM55.5 108.5v3h-3v-3zM55.5 124.5v3h-3v-3zM55.5 140.5v3h-3v-3zM55.5 156.5v3h-3v-3zM55.5 172.5v3h-3v-3zM55.5 188.5v3h-3v-3zM55.5 204.5v3h-3v-3zM55.5 220.5v3h-3v-3zM55.5 236.5v3h-3v-3zM55.5 252.5v3h-3v-3zM55.5 268.5v3h-3v-3zM55.5 284.5v3h-3v-3zM39.5 12.5v3h-3v-3zM39.5 28.5v3h-3v-3zM39.5 44.5v3h-3v-3zM39.5 60.5v3h-3v-3zM39.5 76.5v3h-3v-3zM39.5 92.5v3h-3v-3zM39.5 108.5v3h-3v-3zM39.5 124.5v3h-3v-3zM39.5 140.5v3h-3v-3zM39.5 156.5v3h-3v-3zM39.5 172.5v3h-3v-3zM39.5 188.5v3h-3v-3zM39.5 204.5v3h-3v-3zM39.5 220.5v3h-3v-3zM39.5 236.5v3h-3v-3zM39.5 252.5v3h-3v-3zM39.5 268.5v3h-3v-3zM39.5 284.5v3h-3v-3zM23.5 12.5v3h-3v-3zM23.5 28.5v3h-3v-3zM23.5 44.5v3h-3v-3zM23.5 60.5v3h-3v-3zM23.5 76.5v3h-3v-3zM23.5 92.5v3h-3v-3zM23.5 108.5v3h-3v-3zM23.5 124.5v3h-3v-3zM23.5 140.5v3h-3v-3zM23.5 156.5v3h-3v-3zM23.5 172.5v3h-3v-3zM23.5 188.5v3h-3v-3zM23.5 204.5v3h-3v-3zM23.5 220.5v3h-3v-3zM23.5 236.5v3h-3v-3zM23.5 252.5v3h-3v-3zM23.5 268.5v3h-3v-3zM23.5 284.5v3h-3v-3zM7.5 12.5v3h-3v-3zM7.5 28.5v3h-3v-3zM7.5 44.5v3h-3v-3zM7.5 60.5v3h-3v-3zM7.5 76.5v3h-3v-3zM7.5 92.5v3h-3v-3zM7.5 108.5v3h-3v-3zM7.5 124.5v3h-3v-3zM7.5 140.5v3h-3v-3zM7.5 156.5v3h-3v-3zM7.5 172.5v3h-3v-3zM7.5 188.5v3h-3v-3zM7.5 204.5v3h-3v-3zM7.5 220.5v3h-3v-3zM7.5 236.5v3h-3v-3zM7.5 252.5v3h-3v-3zM7.5 268.5v3h-3v-3zM7.5 284.5v3h-3v-3z"></path></g></g><defs><radialGradient id="dots_svg__b" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="rotate(-136.519 627.256 92.637) scale(929.292 677.674)"><stop stop-color="#D9D9D9"></stop><stop offset="0.574" stop-color="#D9D9D9" stop-opacity="0"></stop></radialGradient><clipPath id="dots_svg__a"><path fill="#fff" transform="rotate(90 306 306)" d="M0 0h297v612H0z"></path></clipPath></defs></svg></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"site":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ke4xTi5OD9gnn2YtRXyCV","type":"Entry","createdAt":"2022-10-14T08:31:23.161Z","updatedAt":"2023-06-06T20:01:24.167Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"site"}},"locale":"en-US"},"fields":{"title":"Anyscale","description":"From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab.","identifier":"site-master-new","footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"16eb9I0yhaIALNOPR8WKTY","type":"Entry","createdAt":"2020-09-27T16:03:59.581Z","updatedAt":"2022-10-27T01:20:31.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer","body":"Â© Anyscale, Inc 2022","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"C7AXPFHtleWPOZ9BuT73K","type":"Asset","createdAt":"2020-09-28T14:40:50.925Z","updatedAt":"2020-09-28T14:40:50.925Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"anyscale_logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/C7AXPFHtleWPOZ9BuT73K/927f5130df1357db041d7a7908ab572e/Graphic_only__transparent_background_.png","details":{"size":1415,"image":{"width":331,"height":331}},"fileName":"Graphic only (transparent background).png","contentType":"image/png"}}}],"subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1cDdKq2eUfklc6w52dU6dg","type":"Entry","createdAt":"2020-09-27T16:05:42.307Z","updatedAt":"2022-10-27T01:20:26.714Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-1","header":"Products","body":"- [Anyscale Compute Platform](/platform)\n- [Ray Open Source](/ray-open-source)\n"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5tpXhcAJwSdhX8Wqw9HYTt","type":"Entry","createdAt":"2020-09-27T16:06:37.409Z","updatedAt":"2022-10-27T01:20:21.887Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":10,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-2","header":"Events","body":"- [Webinars](/events)\n- [Meetups](/events)\n- [Summits](/events)\n"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3c3LEomScM93hIg3i2zItU","type":"Entry","createdAt":"2020-09-27T16:07:42.856Z","updatedAt":"2022-10-27T01:20:18.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-3","header":"Company","body":"- [About Us](/about)\n- [News](/press)\n- [Careers](/careers)\n- [Community](/community)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6du4SSXPTxDZKoIVNHVVnT","type":"Entry","createdAt":"2022-10-27T01:20:13.320Z","updatedAt":"2022-10-27T01:20:13.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-4","header":"Learn","body":"- [Blog](/blog)\n- [Demos \u0026 Webinars](/events)\n- [Anyscale Academy](/events)\n- [Anyscale Docs](https://docs.anyscale.com)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1hovCXhGcWlgJmrqNzJ79k","type":"Entry","createdAt":"2022-10-27T01:20:09.895Z","updatedAt":"2022-10-27T01:20:09.895Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-5","header":"Scalable AI and Python","body":"- [Data Ingestion](/data-ingestion)\n- [Reinforcement Learning](/reinforcement-learning)\n- [Ray AIR](/ray-air)\n- [Model Serving](/model-serving)\n- [Hyperparameter Tuning](/hyperparameter-tuning)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"YFy5fUqwOD5ShdMaYEvzB","type":"Entry","createdAt":"2022-10-27T01:20:06.631Z","updatedAt":"2022-10-27T01:20:06.631Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-6","header":"Use Cases","body":"- [Demand Forecasting/Pricing](/demand-forecasting)\n- [Industrial Automation](/industrial-automation)\n- [Scalable ML Platforms](/machine-learning)\n- [NLP](/natural-language-processing)\n- [Recommendation System](/recommendation-system)"}}]}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"KBAZ5OavkdZfNTVjdJkp3","type":"Entry","createdAt":"2022-10-14T10:47:29.682Z","updatedAt":"2022-10-14T10:47:29.682Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Products","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KNN1Kmzo7jxSlxX1k74XQ","type":"Entry","createdAt":"2022-10-07T13:14:07.892Z","updatedAt":"2022-10-07T13:14:07.892Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"52Mmru1brS6ZkmItPeIW86","type":"Entry","createdAt":"2022-10-07T13:13:22.198Z","updatedAt":"2022-11-03T01:33:41.860Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Anyscale Platform","link":"/platform"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6cN6ZwdiFNiPcqIhd7EiQY","type":"Entry","createdAt":"2022-10-07T13:13:49.673Z","updatedAt":"2022-10-07T13:13:49.673Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Open Source","link":"/ray-open-source"}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5R1OTMfuLnPqont0Fjn9Xb","type":"Entry","createdAt":"2022-10-07T13:20:19.578Z","updatedAt":"2022-10-14T10:43:21.550Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Solutions","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1mp3IF3ZJE5JAViu3L9E95","type":"Entry","createdAt":"2022-10-07T13:16:00.346Z","updatedAt":"2022-10-07T13:21:01.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Scalable AI and Python","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4VUyKorAkgQP2V295h8eW3","type":"Entry","createdAt":"2022-10-07T13:15:35.949Z","updatedAt":"2022-10-07T13:15:35.949Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Data Ingestion","link":"/data-ingestion"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"Ouf5WgAzYmLovCZFNWqYm","type":"Entry","createdAt":"2022-10-07T13:17:09.728Z","updatedAt":"2022-10-07T13:17:09.728Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Reinforcement Learning","link":"/reinforcement-learning"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5kVPKqUbXBbKmrkvsImcIi","type":"Entry","createdAt":"2022-10-07T13:17:50.307Z","updatedAt":"2022-10-07T13:17:50.307Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray AIR","link":"/ray-air"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5uUidEvWw1oqjWrRcskEW","type":"Entry","createdAt":"2022-10-07T13:18:21.246Z","updatedAt":"2022-10-07T13:18:21.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Model Serving","link":"/model-serving"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Rbw0CrBN1KecPafCPvF4H","type":"Entry","createdAt":"2022-10-07T13:18:45.601Z","updatedAt":"2022-10-07T13:18:45.601Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Hyperparameter Tuning","link":"/hyperparameter-tuning"}}],"card":"##### Scalable AI and Python\n\nLeverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more."}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2RgWCuIiZe2TI6Nx5Ns7sB","type":"Entry","createdAt":"2022-10-07T13:25:03.101Z","updatedAt":"2023-05-31T12:47:25.274Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Use Cases","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3XMbpU1mPdsU7gktltlAzX","type":"Entry","createdAt":"2023-05-31T12:43:54.011Z","updatedAt":"2023-05-31T12:43:54.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Large Language Models","link":"/large-language-models"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6MAYw6HTSmH5JHcOoYjCzC","type":"Entry","createdAt":"2022-10-07T13:22:29.165Z","updatedAt":"2022-10-07T13:22:29.165Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Demand Forecasting/Pricing","link":"/demand-forecasting"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"30BlKhv6h4neaZwjA2jlxO","type":"Entry","createdAt":"2022-10-07T13:23:09.471Z","updatedAt":"2022-10-07T13:23:09.471Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Industrial Automation","link":"/industrial-automation"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2gGwUVxDuBeUso1uQH2gEW","type":"Entry","createdAt":"2022-10-07T13:23:32.145Z","updatedAt":"2022-10-07T13:23:32.145Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Scalable ML Platforms","link":"/machine-learning"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1cbwbmIvP5ZkV9iNEraQGP","type":"Entry","createdAt":"2022-10-07T13:23:59.732Z","updatedAt":"2022-10-07T13:23:59.732Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"NLP","link":"/natural-language-processing"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2A7FMbckpN5bgW1wp5ZuSh","type":"Entry","createdAt":"2022-10-07T13:24:41.861Z","updatedAt":"2022-10-07T13:24:41.861Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Recommendation System","link":"/recommendation-system"}}],"card":"##### Use Cases\n\nLeverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more."}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QNdrsmqz82oiBJvFTQ26n","type":"Entry","createdAt":"2023-04-13T11:37:45.354Z","updatedAt":"2023-04-13T11:37:45.354Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ecosystem","link":"/ray-ecosystem"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i0nHjF3pmjzGAdEE4V5MG","type":"Entry","createdAt":"2021-12-05T02:08:18.719Z","updatedAt":"2022-03-03T02:01:08.894Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Success Stories","link":"/user-stories"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5mFZvzY56aQCY4xvlBDi9W","type":"Entry","createdAt":"2022-10-14T10:46:35.218Z","updatedAt":"2022-10-14T10:46:35.218Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Learn","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"32tQWjMRAI5qci97W3cuie","type":"Entry","createdAt":"2022-10-07T13:28:46.057Z","updatedAt":"2023-03-22T20:31:55.243Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JQbOCOkxHfH2ZPGNNQVbt","type":"Entry","createdAt":"2021-12-05T14:57:28.664Z","updatedAt":"2021-12-05T14:57:28.664Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Blog","link":"/blog"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4qEkJgYkXZmiDq3lsxg0xW","type":"Entry","createdAt":"2023-03-22T20:31:21.666Z","updatedAt":"2023-03-22T20:31:21.666Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Summit 2023","link":"https://raysummit.anyscale.com/?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=home_nav-pulldown"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48sdeWXptwt3UgEZzeTt3k","type":"Entry","createdAt":"2022-10-07T13:26:28.331Z","updatedAt":"2022-10-07T13:26:28.331Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Videos \u0026 Webinars","link":"/events"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6hMfRHFDZUXcJH1tIq0xhY","type":"Entry","createdAt":"2021-10-21T20:59:36.918Z","updatedAt":"2023-03-06T20:24:26.811Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Training","link":"https://github.com/ray-project/ray-educational-materials"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cVWy7hmqbAzJCW6d7cLUx","type":"Entry","createdAt":"2022-10-07T13:28:33.537Z","updatedAt":"2022-10-07T13:28:33.537Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Anyscale Docs","link":"https://docs.anyscale.com"}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Qjs7zNfhrinHTNnzMpVIO","type":"Entry","createdAt":"2022-10-14T10:44:42.403Z","updatedAt":"2022-10-14T10:44:42.403Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Company","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ZFDrG8YGgU0oyNgBOId7X","type":"Entry","createdAt":"2022-10-07T13:32:38.746Z","updatedAt":"2022-10-07T13:32:38.746Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2RD65wmpn4gmL5Bvxu5VOD","type":"Entry","createdAt":"2020-09-08T17:53:32.413Z","updatedAt":"2021-06-20T15:04:11.482Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"About us","link":"/about"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71EHE1t3QrjnQb8hD9O8Ww","type":"Entry","createdAt":"2022-10-07T13:32:07.593Z","updatedAt":"2022-10-07T13:32:07.593Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"News","link":"/press"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3TNUPaz7ViQ6hbiZciBOQV","type":"Entry","createdAt":"2020-09-28T23:19:39.838Z","updatedAt":"2022-05-12T19:18:18.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Careers","link":"/careers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ubc5oYY5moS7HexIp7AjO","type":"Entry","createdAt":"2022-07-22T20:40:53.600Z","updatedAt":"2022-07-22T20:40:53.600Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Community","link":"/community"}}]}}]}}],"bannerText":"Are you passionate about Ray, scalable AI and distributed computing? Join us at Ray Summit!","bannerLink":"https://raysummit.anyscale.com/?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=homepage_ticker","bannerCta":" Find out more! "}},"title":"Blog","slug":"blog","sections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6nWVrKik3UzKQc0m6QjMQ7","type":"Entry","createdAt":"2020-09-01T18:35:40.585Z","updatedAt":"2023-06-20T17:29:52.368Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":59,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"featured-posts","header":"Blog","subheader":"Featured Posts and News","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7xkHJWpQ3mRxrgAQw3oYFk","type":"Entry","createdAt":"2023-06-15T13:47:45.558Z","updatedAt":"2023-06-16T13:24:48.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Introducing the Anyscale Databricks Connector","seoTitle":"Integrating Anyscale Ray with Databricks","slug":"introducing-the-anyscale-databricks-connector","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3e6JquwKuIz6QvCWO0BAz","type":"Entry","createdAt":"2023-06-15T13:16:27.453Z","updatedAt":"2023-06-15T13:16:27.453Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Greene","slug":"eric-greene","link":"https://www.linkedin.com/in/greene/"}}],"publishedDate":"2023-06-15","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Introducing the Anyscale Databricks connector. Simple Data Transfer and Efficient AI Workflows with Anyscale and Databricks.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The Anyscale Databricks Connector is a new Ray Datasets capability that facilitates easy data transfer between Databricks clusters and Anyscale hosted Ray clusters. The connector makes it easy for Data Scientists using Anyscale to leverage their Databricks data lake during machine learning and discovery.Â  It also enables a simpler way for machine learning engineers to create end-to-end workloads by allowing the entire ML pipeline to be executed within a single Python script. By taking advantage of the highly-scalable nature of Ray and Ray Datasets, machine learning workloads such as training, tuning and batch serving jobs can be executed more quickly and at lower cost, all while taking advantage of the latest advancements in AI through other Ray integrations with Hugging Face, XGBoost, LightGBM and many AI frameworks and libraries.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Key Benefits","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simple access to data","nodeType":"text"},{"data":{},"marks":[],"value":" - Access Databricks data through SQL queries within Anyscale Workspaces Visual Code or JupyterHub environments, empowering data science discovery and development.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Improved data security and governance","nodeType":"text"},{"data":{},"marks":[],"value":" - Ensure data security and governance by directly copying data from Databricks data lakes to Ray clusters, eliminating the need for intermediate steps and maintaining separate controls over sensitive data. All data is encrypted in transit and at rest.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Highly-scalable data exchange","nodeType":"text"},{"data":{},"marks":[],"value":" - Take advantage of the parallel read and write capabilities of Ray Datasets, enabling the exchange of terabytes of data in minutes. This capability significantly speeds up data-intensive operations and enhances scalability while reducing job run times and overall costs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simplified workload development","nodeType":"text"},{"data":{},"marks":[],"value":" - Simplify machine learning workflows by consolidating all the necessary logic into a single script. This script can query features, train and tune models, and score and materialize results back into the data lake, streamlining the entire process.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Unlock the latest AI Innovations","nodeType":"text"},{"data":{},"marks":[],"value":" - Leverage the power of Databricks for querying and joining data, while benefiting from the scalability and simplicity of Ray AIR for machine learning and AI development. Ray AIR provides integration to the latest AI innovations such as pretrained Hugging Face language models.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Simple, Secure and Scalable Data Access","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing the Anyscale Databricks connector, large datasets can be queried from a Databricks SQL warehouse, and quickly transferred into a Ray Dataset distributed across the Ray cluster. The Databricks connector reads query results in chunks of data, in parallel across the Ray cluster. The size of the dataset and the speed at which it can be transferred scales based on the size of your Ray cluster and the number of simultaneous requests the underlying Databricks SQL warehouse supports.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6taTlZK9ffeivO5RCUJiQ4","type":"Asset","createdAt":"2023-06-15T13:27:43.116Z","updatedAt":"2023-06-15T13:27:43.116Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"1 - Anyscale Databricks Architecture","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6taTlZK9ffeivO5RCUJiQ4/c5fc0bfef768f0fb6e8785bf71ded1e2/1_-_Anyscale_Databricks_Arch.jpg","details":{"size":95989,"image":{"width":761,"height":478}},"fileName":"1 - Anyscale Databricks Arch.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Anyscale Databricks connectors, parallel reads and writes and overall speed will scale with the size of the Ray cluster. The benchmarks for reading and writing demonstrate how the cluster scales out to support larger data sets.  ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2KimYEe845rk6fb2oYaCmz","type":"Asset","createdAt":"2023-06-15T13:35:36.597Z","updatedAt":"2023-06-15T13:35:36.597Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2 - Anyscale Databricks Table Example","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2KimYEe845rk6fb2oYaCmz/8302017d6535b5d676d3dfb50dc12cbd/2_-_Anyscale_Databricks_Table_Example.jpg","details":{"size":218942,"image":{"width":2178,"height":1002}},"fileName":"2 - Anyscale Databricks Table Example.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Simplified ML Development","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing the Databricks connector from within an Anyscale Workspaces, Data Scientists and Machine Learning Engineers have a unified experience while developing workloads. The data query, feature engineering, training, tuning and inference can all be executed within a single Python script that scales across a Ray cluster. Ray integrates with the latest AI and Machine learning libraries, enabling the most advanced ML workloads that work with 3rd party libs  like Hugging Face, XGBoost, LightGBM, and most PyTorch and TensorFlow model architectures.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7c7IX8TpJKbaXdnLIwoD88","type":"Asset","createdAt":"2023-06-15T13:39:22.298Z","updatedAt":"2023-06-15T13:39:22.298Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"3 - Anyscale Databricks UI","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7c7IX8TpJKbaXdnLIwoD88/f17fb8c8884ce197c9ad35a8ce3f6b4e/3_-_Anyscale_Databricks_Product_UI.jpg","details":{"size":242920,"image":{"width":2226,"height":1230}},"fileName":"3 - Anyscale Databricks Product UI.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2muX02EO70X2MXN8wBRyUJ","type":"Asset","createdAt":"2023-06-15T13:41:08.580Z","updatedAt":"2023-06-15T13:41:08.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"4 - Anyscale Databricks Product UI 2","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2muX02EO70X2MXN8wBRyUJ/cc5301c192c66e632cb3141a9e5bde55/4_-_Anyscale_Databricks_Product_UI_2.jpg","details":{"size":383181,"image":{"width":1860,"height":1282}},"fileName":"4 - Anyscale Databricks Product UI 2.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlock new use cases with AI and ML ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nAnyscale and Ray integrate with most open source AI and ML libraries, enabling the latest innovation in AI to be applied to Databricks data. Ray makes working with Hugging face, XGBoost, LightGBM, TensorFlow and PyTorch and SciKit Learn a unified experience, with the added benefits of scaling with distributed training, tuning and serving.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical ML workload to train a LightGBM can be implemented in 20 lines of code.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6rUeSQ88fvbTd68jkhrii0","type":"Asset","createdAt":"2023-06-15T13:42:58.984Z","updatedAt":"2023-06-15T13:42:58.984Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"5 - Anyscale Databricks deployment","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6rUeSQ88fvbTd68jkhrii0/43de8dc64eaf8792ad73ac6faf8b4551/5_-_Anyscale_Databricks_deployment.jpg","details":{"size":272363,"image":{"width":1470,"height":1402}},"fileName":"5 - Anyscale Databricks deployment.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Want to see for yourself? ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/signup"},"content":[{"data":{},"marks":[],"value":"Request a Trial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" today!\n","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71IpYQvsxplh7okIL7RJVx","type":"Asset","createdAt":"2023-06-15T13:50:54.646Z","updatedAt":"2023-06-16T13:19:09.178Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"locale":"en-US"},"fields":{"title":"0 -Anyscale Databricks headline image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/71IpYQvsxplh7okIL7RJVx/b193e65fbc2d6500e3ccb2162d323080/0_-_Updated_Databricks_Anyscale_Image.png","details":{"size":90723,"image":{"width":1500,"height":1000}},"fileName":"0 - Updated Databricks Anyscale Image.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2GdxAQ2xXKoNlzKc7DXvLS","type":"Entry","createdAt":"2023-05-31T19:04:09.560Z","updatedAt":"2023-06-01T03:29:01.792Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Announcing Aviary: Open Source Multi-LLM Serving","slug":"announcing-aviary-open-source-multi-llm-serving-solution","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-05-31","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Exec Summary","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": Open source LLMs are getting better every day, so today weâre open sourcing Aviary to make testing, evaluating and deploying Open Source LLMs easier â we found it harder than we thought it should be so we used Ray Serve to fix it.Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Try it out with Aviary Explorer ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://aviary.anyscale.com/"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Github Repo ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/aviary"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Video demo and explanation ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.youtube.com/watch?v=WmqPfQOXJ-4"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Slack support ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Forum support ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/c/llms-generative-ai/27"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Want a managed version of Aviary? Sign up ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://bit.ly/run-aviary"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". \n\nWeâre excited to announce the release of Aviary: a new open source project that simplifies and enables easy self-hosted serving of multiple LLM models efficiently.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Weâre big fans of open source LLMs here at Anyscale. The rate of improvement of open source LLMs has been ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.semianalysis.com/p/google-we-have-no-moat-and-neither"},"content":[{"nodeType":"text","value":"nothing short of phenomenal","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". This has given the AI community many options beyond the big âclosedâ players like OpenAI, Anthropic, Cohere and more.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Why are companies exploring self hosted open source LLMs? There are a few reasons: ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Cost: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"LLMs are very expensive to operate. A single query can cost tens of cents. Using open source can be considerably cheaper.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Latency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"By colocating LLMs and business logic, sometimes even on the same machine, latency â one of the biggest issues with deploying LLMs â can be kept low.Â Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Transparency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Organizations want to understand exactly what is happening in their models. Open source allows them to understand whatâs happening inside them.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Deployment flexibility: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Open Source models can be deployed on premise, in the cloud using the userâs cloud resources, or as part of a SaaS offering.Â Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Data Control: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Data governance issues are much easier to guarantee if the data does not leave your control.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Customization:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" Customizing/fine-tuning on proprietary data to enable domain specific answers at high quality.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We were writing some tools for ourselves to really understand the answers to the questions above, as well as the advantages of closed models, mainly focused around quality. A funny thing happened on the way, however: we discovered that while the open source models have improved, the infrastructure for serving LLMs has not kept up. We started building our own library for loading the models, autoscaling them efficiently, etc on top of Ray Serve, dealing with variations between the models on things like stop tokens etc.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It was then that we realized, actually the rest of the LLM community might benefit from this library as well. So, today we are releasing Aviary as open source.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary helps leverage the advantages of open source models by building on the solid foundation of ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io"},"content":[{"nodeType":"text","value":"Ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", the popular framework for scalable AI. In particular, it takes advantage of ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"nodeType":"text","value":"Ray Serve","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", the highly flexible serving framework that is part of Ray.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary does this by:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Providing an extensive suite of pre-configured open source LLMs, with reasonable defaults that work out of the box.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Integrating acceleration approaches like DeepSpeed with the packaged LLMs.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Simplifying the deployment of multiple LLMs within a single unified framework.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Simplifying the addition of new LLMs to within minutes in most cases","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Offering unique autoscaling support, including scale-to-zero (a first in open source).Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Weâve also included a demo Gradio frontend that shows off whatâs possible with these capabilities, as well as some command line tools to address the evaluation questions that originally motivated this project.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"You can see these features in the video below:Â ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jx26kCBQB1GRKEulIDxoi","type":"Entry","createdAt":"2023-05-31T02:10:20.974Z","updatedAt":"2023-05-31T02:10:20.974Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Aviary Announcement Video","videoUrl":"https://www.youtube.com/watch?v=WmqPfQOXJ-4"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"While existing solutions have some of these features individually and we are grateful to build on their foundations (such as Hugging Faceâs text-generation-inference), none of the existing solutions brings these capabilities together in a way that is convenient for users. We are also planning to continue to expand the feature set of Aviary, adding support for features like streaming, continuous batching and others.Â Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary is also open to community contributions â especially for adding new LLMs. Weâll then redeploy these in production so the rest of the world can use those LLMs too. We will also be supporting Aviary through Slack, Discourse, GitHub issues and an LLM-based RayBot.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"At the same time we understand that not everyone wants to take on the additional challenges of maintaining their own aviary of models. For that reason, we will also be offering a hosted version of Aviary that builds on top of our Anyscale managed Ray platform and offers additional features around deployment including:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Using spot instances with on-demand fallback for large potential savings","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Zero downtime upgrades with no dropped requests","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Faster deployment and autoscaling","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Improved observability tools.Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We are also announcing that Aviary will be available at no additional charge for existing Anyscale customers via our workspaces solution, and weâre actively onboarding new Aviary customers now. If youâd like to deploy Aviary, please reach out to us ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://bit.ly/run-aviary"},"content":[{"nodeType":"text","value":"here","marks":[],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1c2aG5pfLWU6YenPBZIHvI","type":"Asset","createdAt":"2023-05-31T19:00:13.845Z","updatedAt":"2023-05-31T19:00:13.845Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Aviary Light Mode","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1c2aG5pfLWU6YenPBZIHvI/742f647b979b4eb4b682e37fc35c71a7/aviary-light-mode.png","details":{"size":712180,"image":{"width":3030,"height":1750}},"fileName":"aviary-light-mode.png","contentType":"image/png"}}},"mainImageFit":"contain","hideIntro":true,"showMainImage":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UsGSYssf1ebf8N5mRbNxT","type":"Entry","createdAt":"2023-05-17T17:26:50.771Z","updatedAt":"2023-05-24T20:17:43.464Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Numbers every LLM Developer should know","slug":"num-every-llm-developer-should-know","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-05-17","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"When I was at Google, there was a document put together by Jeff Dean, the legendary engineer, called ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://brenocon.com/dean_perf.html"},"content":[{"nodeType":"text","value":"Numbers every Engineer should know","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Itâs really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Prompts","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"40-90%: Amount saved by appending âBe Conciseâ to your prompt","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Itâs important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money [1]. This can be broadened beyond simply appending âbe conciseâ to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"1.3: Average tokens per word","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"LLMs operate on tokens. Tokens are words or sub-parts of words, so âeatingâ might be broken into two tokens âeatâ and âingâ.Â A 750 word document will be about 1000 tokens.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Knowing this ratio is important because most billing is done in tokens, and the LLMâs context window size is also defined in tokens.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Prices","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Prices [2] are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~50: Cost Ratio of GPT-4 to GPT-3.5 Turbo","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"What this means is that for many practical applications, itâs much better to use GPT-4 for things like generation and then use that data to fine tune a smaller model. It is roughly 50 times cheaper to use GPT-3.5-Turbo [3] than GPT-4 (the âroughlyâ is because GPT-4 charges differently for the prompt and the generated output)Â  â so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"5: Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embeddingÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. âWhat is the capital of Delaware?â when looked up in an neural information retrieval system costs about 5x less [4] than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x!Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"10: Cost Ratio of OpenAI embedding to Self-Hosted embeddingÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using HuggingFaceâs SentenceTransformers (which are pretty much as good as OpenAIâs embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees).Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"6: Cost Ratio of OpenAI base vs fine tuned model queries","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"1: Cost Ratio of Self-Hosted base vs fine-tuned model queriesÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If youâre self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Training and Fine Tuning\n","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://arxiv.org/abs/2302.13971"},"content":[{"nodeType":"text","value":"LLaMa paper","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. Thatâs not something most companies can do (shameless plug time: of course, we at Anyscale can â thatâs our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray"},"content":[{"nodeType":"text","value":"bread and butter","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"! Contact us if youâd like to learn more). The point is that training your own LLM is possible, but itâs not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"\u003c 0.001: Cost ratio of fine tuning vs training from scratch","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using"},"content":[{"nodeType":"text","value":"6B parameter model for about $7","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Even at OpenAIâs rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), youâre looking at $40. However, fine tuning is one thing and training from scratch is another [5].\n","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"GPU Memory","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If youâre self-hosting a model, itâs really important to understand GPU memory because LLMs push your GPUâs memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It may seem strange, but itâs important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"2x number of parameters: Typical GPU memory requirements of an LLM for serving","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. Thereâs usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy there is too much loss of resolution. Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but thatâs atypical.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~1GB: Typical GPU memory requirements of an embedding model","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/"},"content":[{"nodeType":"text","value":"sentence transformers","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". OpenAI also has its own embeddings that they provide commercially.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"You typically donât have to worry about how much memory embeddings take on the GPU, theyâre fairly small. Weâve even had the embedding and the LLM on the same GPU.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"\u003e10x: Throughput improvement from batching LLM requestsÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second.Â  The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~1 MB: GPU Memory required for 1 token of output with a 13B parameter model","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say â I have 24GB to spare, whatâs 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but itâs still a real issue.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"embedded-entry-inline","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1ACoDuIrrm71E0BWViBO4Y","type":"Entry","createdAt":"2023-05-17T17:26:34.593Z","updatedAt":"2023-05-17T17:26:34.593Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"inlineImage"}},"locale":"en-US"},"fields":{"name":"numbers-cheatsheet","image":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1AkaPcJlWoSpqixtqeKcD1","type":"Asset","createdAt":"2023-05-17T16:48:22.989Z","updatedAt":"2023-05-17T20:48:45.978Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"numbers-cheatsheet","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1AkaPcJlWoSpqixtqeKcD1/9505980d855c36120b4818980745fd00/Screenshot_2023-05-17_at_1.46.09_PM.png","details":{"size":550573,"image":{"width":2194,"height":1734}},"fileName":"Screenshot 2023-05-17 at 1.46.09 PM.png","contentType":"image/png"}}},"isRetina2x":false}}},"content":[]},{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Next Steps","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Review the up-to-date metrics referenced in this blog in the following ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/llm-numbers"},"content":[{"nodeType":"text","value":"Github repo","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". \n\nSee our earlier ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"nodeType":"text","value":"blog series on solving Generative AI infrastructure","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" ","marks":[{"type":"underline"}],"data":{}},{"nodeType":"text","value":"and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"nodeType":"text","value":"using LangChain with Ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in learning more about Ray, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io/"},"content":[{"nodeType":"text","value":"Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://docs.ray.io/"},"content":[{"nodeType":"text","value":"Docs.Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nTo connect with the Ray community join #LLM on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"Ray Slack","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" or our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"Discuss forum","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://www.anyscale.com/platform"},"content":[{"nodeType":"text","value":"Anyscale.com/Platform ","marks":[],"data":{}}]},{"nodeType":"text","value":"and click the 'Try it now' button","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Summit 2023:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"nodeType":"text","value":"Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"nodeType":"text","value":"practical training focused on LLMs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"hr","data":{},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[1] Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2022-05-14.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[2] Retrieved from ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://openai.com/pricing"},"content":[{"nodeType":"text","value":"http://openai.com/pricing","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on 2022-05-14.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[3] ","marks":[],"data":{}},{"nodeType":"text","value":"GPT-4","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). ","marks":[],"data":{}},{"nodeType":"text","value":"GPT-3.5 Turbo","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": 0.2c/1k tokens.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[4] This assumes the vector lookup is âfree.â Itâs not, but it uses CPUs (much cheaper) and is fairly fast.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[5] 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3ZnD84ZZAfH60U5ncBebaO","type":"Asset","createdAt":"2023-05-17T15:56:36.431Z","updatedAt":"2023-05-17T18:04:30.622Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"numbers-cover-image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3ZnD84ZZAfH60U5ncBebaO/227f68207ec7731e789f342e7ec320e8/Screenshot_2023-05-17_at_10.12.01_AM.png","details":{"size":445922,"image":{"width":2348,"height":1616}},"fileName":"Screenshot 2023-05-17 at 10.12.01 AM.png","contentType":"image/png"}}},"mainImageFit":"contain","hideIntro":true,"showMainImage":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3vUo7K4aHmhARGamZX7Ujj","type":"Entry","createdAt":"2023-05-11T15:17:24.618Z","updatedAt":"2023-06-16T21:13:49.520Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Streaming distributed execution across CPUs and GPUs","seoTitle":"Streaming distributed execution across CPUs and GPUs","slug":"streaming-distributed-execution-across-cpus-and-gpus","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2aJRcdEj9iAktAnqn0wucE","type":"Entry","createdAt":"2021-02-16T07:30:38.421Z","updatedAt":"2021-02-16T07:30:38.421Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Stephanie Wang","slug":"stephanie-wang","link":"https://github.com/stephanie-wang"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2edothjNBpyQEu2aX55KRl","type":"Entry","createdAt":"2023-05-10T15:30:16.699Z","updatedAt":"2023-05-10T15:30:16.699Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Cheng Su","slug":"cheng-su"}}],"publishedDate":"2023-05-11","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"In a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/offline-batch-inference-comparing-ray-apache-spark-and-sagemaker"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"previous blog,","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" we showed how Ray Data can provide speedups over SageMaker and frameworks like Apache Spark for large-scale batch inference workloads. This blog post delves further into how Ray Data streaming works and how to use it for your own ML pipelines.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Some of the most demanding machine learning (ML) use cases we have encountered involve pipelines that span both CPU and GPU devices in distributed environments. These situations arise in various workloads, including:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Batch inference, which involves a CPU-intensive preprocessing stage (e.g., video decoding or image resizing) before utilizing a GPU-intensive model to make predictions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed training, where similar CPU-heavy transformations are required to prepare or augment the dataset prior to GPU training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Interestingly, in many of these workloads, the preprocessing steps often prove to be the bottleneck. This can happen when preprocessing requires ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"parallelism across multiple nodes","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"significant memory to buffer results","nodeType":"text"},{"data":{},"marks":[],"value":" between the CPU and GPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For instance, consider the decoding of compressed images in memory. A typical JPEG decompression ratio is 10:1, which implies that the output memory size may be ten times that of the input, resulting in substantial memory pressure in addition to CPU load. The challenges become more complex with other data modalities, such as video; for example, H264 can decompress at a 2000:1 ratio, producing 200GB of frame outputs for a 100MB file input. This means that practitioners trying to offload CPU-heavy preprocessing onto multiple CPUs or machines have to deal with intermediate results that are far larger than their already sizable source datasets:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7ntuAY4zsUXtrtIxSofnyg","type":"Asset","createdAt":"2023-05-10T15:33:44.406Z","updatedAt":"2023-05-10T18:55:03.562Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"streaming_figure_1","description":"Figure 1. Decoding of compressed images with large images spilled to disk","file":{"url":"//images.ctfassets.net/xjan103pcp94/7ntuAY4zsUXtrtIxSofnyg/1fdb9631162d6b66777d205d1d3a7e1c/Screenshot_from_2023-05-10_11-00-43.png","details":{"size":64895,"image":{"width":1493,"height":436}},"fileName":"Screenshot from 2023-05-10 11-00-43.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Executed naively, these compute and memory-intensive workloads fail to fully utilize expensive GPU hardware. For example, we could buffer 200GB of frame outputs on disk, but that could add minutes of overheads. Even if this is overlapped with GPU computation, we can end up with long pauses on the GPU. This motivates fully streamed execution in the cluster setting, which avoids such delays by streaming intermediate data through cluster memory:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"40VpN8OKxEM7wHohV802gE","type":"Asset","createdAt":"2023-05-10T15:36:35.326Z","updatedAt":"2023-05-10T18:56:06.767Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_2","description":"Figure 2. Decoding intermediate data streamed through memory","file":{"url":"//images.ctfassets.net/xjan103pcp94/40VpN8OKxEM7wHohV802gE/c07086bfe677dc5e3109496f5a4ec083/Screenshot_from_2023-05-10_11-01-00.png","details":{"size":35974,"image":{"width":1199,"height":268}},"fileName":"Screenshot from 2023-05-10 11-01-00.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we guide you through our development of a versatile streaming backend for Ray Data, and show examples of how to implement the demanding use cases mentioned earlier. We discuss the performance benefits of implementing pipelined execution across CPUs and GPUs compared to bulk processing, and discuss its broader applicability to various workloads, including distributed training.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pipelined execution in a cluster","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To understand the necessity and advantages of pipelined (or streaming) execution across CPU and GPU devices, let's first examine how a ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Bulk_synchronous_parallel"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"bulk synchronous parallel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (BSP) framework might handle batch inference. Because of its simplicity and generality, BSP is a common way frameworks (e.g., MapReduce, Apache Spark) parallelize distributed computations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Note that pipelining within a single machine is already commonly used in data loading libraries such as ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Torch DataLoader","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://petastorm.readthedocs.io/en/latest/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Petastorm.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" Here, we discuss pipelining computations across an entire cluster of machines.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical batch inference job consists of the following operations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6mkS4HlXNovPW6Dl387jYe","type":"Asset","createdAt":"2023-05-10T15:40:10.724Z","updatedAt":"2023-05-10T19:28:57.091Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"locale":"en-US"},"fields":{"title":"streaming_figure_3","description":"Figure 3. A typical batch inference job sequential operations","file":{"url":"//images.ctfassets.net/xjan103pcp94/6mkS4HlXNovPW6Dl387jYe/1a707558e51ac00b776ae6f5d808b74e/Screenshot_from_2023-05-10_12-23-58.png","details":{"size":33844,"image":{"width":1526,"height":244}},"fileName":"Screenshot from 2023-05-10 12-23-58.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To implement such an inference job, you'd write the code using Ray Data (or another similar framework):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1kYVOacE6epR4czS5YoOHX","type":"Entry","createdAt":"2023-05-10T15:41:24.239Z","updatedAt":"2023-05-10T15:41:24.239Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_1","body":"import ray\n\n# Define model and preprocessor.\nmodel = ...\npreprocess_fn = ...\n\n# Define the inference function.\ndef model_fn(batch: Dict[str, np.ndarray]):\n    return {\"results\": model(batch)}\n\n# The following snippet implements the pipeline.\nray.data.read_parquet(...) \\           # 1. Load\n    .map_batches(preprocess_fn) \\      # 2. Preprocess\n    .map_batches(model_fn) \\           # 3. Inference\n    .write_parquet(...)                # 4. Save\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a BSP framework, each of these operations can be modeled as a 1:1 transformation, which can be fused together into a single stage. Each task within the stage will execute the mentioned operations locally in sequence:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4oQ40RlSK1AF7Y4ZnVGV8J","type":"Asset","createdAt":"2023-05-10T15:44:10.631Z","updatedAt":"2023-05-10T18:59:16.654Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_4","description":"Figure 4. BSP framework operations modeled as a 1:1 transformation","file":{"url":"//images.ctfassets.net/xjan103pcp94/4oQ40RlSK1AF7Y4ZnVGV8J/a0413e90a55ec69d7d0ba49554a7f2a0/Screenshot_from_2023-05-10_11-07-39.png","details":{"size":103326,"image":{"width":1511,"height":583}},"fileName":"Screenshot from 2023-05-10 11-07-39.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This execution strategy is ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"memory optimal","nodeType":"text"},{"data":{},"marks":[],"value":", as there is no intermediate data and execution is inherently pipelined (since there is only a single stage). However, challenges emerge in a heterogeneous setting, where certain pipeline steps may prefer to run on CPUs or GPUs independently. In Ray Data, such inference pipelines are expressed with transformations that launch actors scheduled onto GPUs ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"(num_gpus=1 and compute=ActorPoolStrategy)","nodeType":"text"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3nLf8c4wMq9KInsI4ca7HB","type":"Entry","createdAt":"2023-05-10T15:46:05.506Z","updatedAt":"2023-05-10T16:00:09.475Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_2","body":"import ray\nfrom ray.data import ActorPoolStrategy\n\nModel = ...\npreprocess_fn = ...\n\n# Define the model as a stateful class with cached setup.\nclass MyModelCallableCls:\n    def __init__(self):\n        self.model = Model(...)\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e\n        return {\"results\": self.model(batch)}\n\n# Modify the pipeline to use GPU actors.\nray.data.read_parquet(...) \\\n    .map_batches(preprocess_fn) \\\n    .map_batches(\n          MyModelCallableCls,\n          num_gpus=1,\n          compute=ActorPoolStrategy(size=N)) \\\n    .write_parquet(...)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the above code snippet, the load and preprocessing steps run on CPU (Stage 1), inference runs on GPUs (Stage 2), and then the result saving runs on CPU again (Stage 3). This configuration leads to spilling of data to remote storage when intermediate results (e.g., decoded video frames) exceed cluster memory sizes. Hence, we see that ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"BSP is not memory optimal for heterogeneous workloads","nodeType":"text"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"66FIu8nzl2kZRB1Ht5xZFL","type":"Asset","createdAt":"2023-05-10T15:49:26.155Z","updatedAt":"2023-05-10T19:00:24.815Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_5","description":"Figure 5. BSP is not memory optimal for heterogeneous workloads","file":{"url":"//images.ctfassets.net/xjan103pcp94/66FIu8nzl2kZRB1Ht5xZFL/295ab9176fab5d04ee47ac53c5e1b418/Screenshot_from_2023-05-10_11-02-18.png","details":{"size":162611,"image":{"width":1750,"height":730}},"fileName":"Screenshot from 2023-05-10 11-02-18.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"These unnecessary overheads can be avoided with end-to-end pipelining (i.e., streaming) across the cluster, which we present in the next section.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data streaming","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In Ray 2.4, we've changed the default execution strategy of Ray Data to streaming from bulk synchronous. These streaming Datasets are fully backwards compatible with the existing API, i.e., they can be transformed lazily with map operations, support shuffle operations, and also caching / materialization in memory:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5CSJWRWgN6pQwxcFMBhMSr","type":"Entry","createdAt":"2023-05-10T15:50:53.654Z","updatedAt":"2023-05-10T15:59:44.870Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_3","body":"# Create a dataset over parquet files\nds: ray.data.Dataset = ray.data.read_parquet(...)\n\n# Transform the dataset\nds = ds.map_batches(my_preprocess_fn)\nds = ds.map_batches(my_model_fn)\n\n# Iterate over dataset batches in streaming fashion\nfor batch in ds.iter_batches():\n   print(batch)\n\n# Materialize all contents of the dataset in memory\nds = ds.materialize()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In other words, the Ray Data API now leverages streaming execution for improved performance on large datasets, with the same simple transformation API as in previous Ray versions.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Video processing example","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To more fully understand the capabilities of this API, let's walk through what this looks like in an example video processing pipeline. In the pipeline, we'll first decode video frames, annotate each frame with an ML model, apply a classification model to the annotated frames, and then save the results to JSON:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"s1kmF3kQraHomGuDsvOQt","type":"Asset","createdAt":"2023-05-10T15:55:25.682Z","updatedAt":"2023-05-10T19:30:26.377Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"streaming_figure_6","description":"Figure 6. A pipeline decoding video frames, annotating frames, applying classification, and saving results to JSON.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/s1kmF3kQraHomGuDsvOQt/9385b71522afe76eebc6342f54387ffa/Screenshot_from_2023-05-10_12-24-36.png","details":{"size":38452,"image":{"width":1478,"height":230}},"fileName":"Screenshot from 2023-05-10 12-24-36.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's start by declaring the components of our pipeline. For brevity, we'll just provide stub classes here, but you can find the ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/ericl/ac086f0a9bdf41b6101dca665045dfec"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"full skeleton here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (and the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference docs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") that can be adapted to your own workloads. Data between these steps is kept in dicts of numpy arrays:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3DoR7DQ01JPe4k0MjPdubR","type":"Entry","createdAt":"2023-05-10T15:57:40.210Z","updatedAt":"2023-05-10T15:59:24.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_5","body":"# Declare a function that decodes frames.\ndef decode_frames(batch: Dict[str, np.ndarray]) -\u003e Dict:\n    video_data = batch[\"bytes\"]\n    ...\n    return {\"frames\": decoded_frames}\n\n# Declare a model that annotates decoded frame data.\nclass FrameAnnotator:\n    def __init__(self):\n        self.model = ...\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e Dict:\n        frames = batch[\"frames\"]\n        return {\"annotated_frames\": self.model(frames)}\n\n# Declare a model that classifies annotated frame data.\nclass FrameClassifier:\n    def __init__(self):\n        self.model = ...\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e Dict:\n        frames = batch[\"annotated_frames\"]\n        return {\"results\": self.model(frames)}","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next, we'll use Ray Data to connect these steps together. This takes just a few lines of code, and we can customize the resource requirements per step to configure CPUs and GPUs:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1U2wSU4PEADDsQpVQqJJAa","type":"Entry","createdAt":"2023-05-10T15:59:06.340Z","updatedAt":"2023-05-10T15:59:06.340Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_6","body":"# Create a dataset from video binary data.\nds = ray.data.read_binary(...)\n\n# Apply the decode step. We can customize the resources per\n# task. Here each decode task requests 4 CPUs from Ray.\nds = ds.map_batches(decode_frames, num_cpus=4)\n\n# Apply the annotation step, using an actor pool of size 5.\n# Each actor runs on a CPU.\nds = ds.map_batches(\n    FrameAnnotator,\n    compute=ActorPoolStrategy(size=5))\n\n# Apply the classification step, using a pool of 2 GPU actors,\n# and a fixed data batch size of 64 for the actor calls.\nds = ds.map_batches(\n    FrameClassifier,\n    num_gpus=1,\n    batch_size=64,\n    compute=ActorPoolStrategy(size=2))\n\n# Trigger execution and write outputs to json.\nds.write_json(\"/tmp/output\")\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The configured streaming topology from above looks like this logically. Since each step has a unique resource requirement / task vs actor strategy, here every step becomes its own stage (Ray Data fuses steps with equivalent resource requirements):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2XaeSiV04QKLILBLAlD0t5","type":"Asset","createdAt":"2023-05-11T15:44:03.608Z","updatedAt":"2023-05-11T15:44:03.608Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"streaming_figure_7","description":"Figure 7: Streaming execution across CPUs and GPUs","file":{"url":"//images.ctfassets.net/xjan103pcp94/2XaeSiV04QKLILBLAlD0t5/d75fceb54323c7587312028b40a4b37c/Screenshot_from_2023-05-10_11-02-51.png","details":{"size":68124,"image":{"width":1445,"height":299}},"fileName":"Screenshot from 2023-05-10 11-02-51.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We run the above script (","nodeType":"text"},{"data":{"uri":"https://gist.github.com/ericl/ac086f0a9bdf41b6101dca665045dfec"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GitHub gist","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") in an Anyscale workspace with 2 GPU nodes and 5 CPU nodes (you can use the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/getting-started.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray cluster launcher","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to also launch a cluster for this):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3hsRFIiW2RGkFM5Igr2jVT","type":"Entry","createdAt":"2023-05-10T16:04:25.585Z","updatedAt":"2023-05-10T16:06:19.918Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_7","body":"$ python workload.py\n\n2023-05-02 15:10:01,105 INFO streaming_executor.py:91 -- Executing DAG\nInputDataBuffer[Input] -\u003e TaskPoolMapOperator[MapBatches(decode_frames)] -\u003e \nActorPoolMapOperator[MapBatches(FrameAnnotator)] -\u003e\nActorPoolMapOperator[MapBatches(FrameClassifier)] -\u003e \nTaskPoolMapOperator[Write]\n2023-05-02 15:10:01,128 INFO actor_pool_map_operator.py:114 -- MapBatches(FrameAnnotator): Waiting for 5 pool actors to start...\n2023-05-02 15:10:02,691 INFO actor_pool_map_operator.py:114 -- MapBatches(FrameClassifier): Waiting for 2 pool actors to start...\n\nRunning: 25.0/112.0 CPU, 2.0/2.0 GPU, 33.19 GiB/32.27 GiB object_store_memory:  28%|ââââ        | 285/1000 [01:40\u003c03:12,  3.71it/s]","language":"Shell"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"While the workload is running, we can view the observability graphs in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-core/ray-dashboard.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Dashboard","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". We can see from the actor metrics that our 7 worker actors were active for the entire run (RUNNING_TASK), and that Ray Data kept them busy with MapBatches(FrameClassifier) as well as MapBatches(FrameAnnotator) tasks. Because of the streaming execution strategy, tasks for all stages run concurrently:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5MDgPW8470uVoy6MFBqowq","type":"Asset","createdAt":"2023-05-10T16:09:18.740Z","updatedAt":"2023-05-10T16:09:27.512Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_8","description":"Figure 8. View active tasks and actors for this job in the Ray Dashboard","file":{"url":"//images.ctfassets.net/xjan103pcp94/5MDgPW8470uVoy6MFBqowq/39f6300387d895145313c0caee8149cf/image3.png","details":{"size":77415,"image":{"width":1216,"height":516}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can also inspect hardware utilization in the Ray dashboard. We'll focus on the network activity. We can see two nodes receiving ~500MiB/s (these are the two GPU nodes hosting the FrameClassifier actors), and a number of other nodes driving significant traffic from 100-200MiB/s, likely sending data to the GPU nodes or between the decode and FrameAnnotator steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6QEOgHncDDrSl3SKl9YzWF","type":"Asset","createdAt":"2023-05-10T16:12:30.030Z","updatedAt":"2023-05-10T19:54:24.559Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"streaming_figure_9","description":"Figure 9. Ray Dashboard showing network activity","file":{"url":"//images.ctfassets.net/xjan103pcp94/6QEOgHncDDrSl3SKl9YzWF/bae6e33fce2749f2ade8afd73681e16b/Screenshot_from_2023-05-10_12-42-27.png","details":{"size":107840,"image":{"width":1635,"height":707}},"fileName":"Screenshot from 2023-05-10 12-42-27.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The overall pipeline finishes in ~5 minutes, successfully processing ~350GB of video frame data total on a heterogeneous cluster of CPUs and GPUs. We see that Ray Data was able to pipeline the entire execution across the cluster to make the best use of memory.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Discussion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Optimizations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"In order to seamlessly execute such streaming topologies, Ray Data provides a number of optimizations under the hood. This includes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Memory stability","nodeType":"text"},{"data":{},"marks":[],"value":": Ray Data relies on the underlying Ray scheduler to schedule tasks and actors, but still needs to manage back-pressure across the streaming topology to bound memory usage and avoid object store spilling. It does this by only scheduling new tasks if it would keep the streaming execution under ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/data-internals.html#streaming-execution"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"configured resources limits","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Intuitively, enforcing a cap on intermediate result memory usage is needed to avoid degrading to bulk execution.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data locality","nodeType":"text"},{"data":{},"marks":[],"value":": While Ray will already place tasks on nodes where their input arguments are local, Ray Data's streaming backend extends this to optimize the scheduling of actor tasks. For example, in the above example, a lot of network traffic is avoided between the `decode_frames` and `FrameAnnotator` steps by routing decoded frames to actors that are on the same node.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Fault tolerance","nodeType":"text"},{"data":{},"marks":[],"value":": Ray Data leverages Ray's built-in ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-core/fault-tolerance.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"fault tolerance","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in order to handle object loss in large jobs. When objects are lost, they are recomputed based on their task lineage tracked by Ray. If actors were needed to produce these objects, Ray restarts these actors prior to re-submitting the task.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scalability","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"To give an idea of the upper scalability envelope of Ray Data streaming, we ran a number of synthetic benchmarks that stressed Ray's object manager and Ray Data's ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/execution/streaming_executor.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"streaming executor","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" processing a 20TiB array dataset. Under the hood, Ray Data is using ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.remote() / ray.wait() ","nodeType":"text"},{"data":{},"marks":[],"value":"to orchestrate the streaming topology, which means it is similarly scalable as Ray itself:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2vKUP0VZYl0yPNspQmfU05","type":"Asset","createdAt":"2023-05-10T16:17:37.701Z","updatedAt":"2023-05-10T16:17:37.701Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"streaming_figure_10","description":"Figure 10. Synthetic benchmarks stressing Ray's object manager and Ray Data's streaming executor processing a 20TiB array dataset","file":{"url":"//images.ctfassets.net/xjan103pcp94/2vKUP0VZYl0yPNspQmfU05/0566c92beb5ea743d9ecc3404a551705/image9.png","details":{"size":44857,"image":{"width":1636,"height":640}},"fileName":"image9.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"These benchmarks were run on a cluster of 500 machines, and test pipelines with between one to four stages similar to the video streaming example. We see that for the simplest pipeline on this cluster, Ray Data can process input data at a rate exceeding 1TiB/s. For more complex multi-stage pipelines, Ray Data can still sustain about 100-200GiB/s of end-to-end throughput on a large cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed Training","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"A significant portion of ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" users today are also using Ray Data as a distributed data backend for ML training. Ray Data streaming allows these users to efficiently work with datasets that are larger than cluster memory for training, leveraging both CPU and GPU nodes to speed up their jobs. We are working on enhancing Ray Train's API to natively work with Ray Data streaming.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What's next?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Today, early users are taking advantage of the Ray Data streaming backend to create efficient large-scale inference pipelines over unstructured data, including video and audio data. To learn more about how to build your own pipeline, check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference tutorial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in our docs. We'll also be following up with a blog on more unstructured application examples. To learn more generally about streaming in Ray Data, check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/data.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"library docs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and let us know what ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/issues"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"improvements","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" you'd like to see to Ray Data.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Also, we will be presenting at the ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Register now","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â â early bird registration is open until","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Â June 30, 2023.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5qTVayH4YnKM7pGL8CiSwc","type":"Asset","createdAt":"2023-05-11T15:46:46.691Z","updatedAt":"2023-05-11T15:46:46.691Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_streaming_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5qTVayH4YnKM7pGL8CiSwc/99806986301b595308c6c89a8bad6249/images_and_diagrams__13_.png","details":{"size":83586,"image":{"width":960,"height":540}},"fileName":"images and diagrams (13).png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1n9cmtmuJ3wQPGW1TtXZ4t","type":"Entry","createdAt":"2023-05-08T15:57:32.674Z","updatedAt":"2023-05-09T00:38:09.625Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Building a Self Hosted Question Answering Service using LangChain + Ray in 20 minutes","slug":"building-a-self-hosted-question-answering-service-using-langchain-ray","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-05-08","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 3 of a blog series. In this blog, weâll show you how to build an LLM question and answering service. In future parts, we will optimize the code and measure performance: cost, latency and throughput.","body":{"nodeType":"document","data":{},"content":[{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4laoMTyctlD4QnuM9KzTI6","type":"Entry","createdAt":"2023-05-08T20:47:41.452Z","updatedAt":"2023-05-08T20:47:41.452Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Building a Question Answering Chatbot","videoUrl":"https://youtu.be/Sy-Xp-sdlh0"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\nThis blog post builds on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"nodeType":"text","value":"Part 1","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" of our LangChain series, where we built a semantic search service in about 100 lines. Still, search is so â¦ 2022. What if we wanted to build a question answering service?Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"One option is we could just ask the LLM directly without any background at all. Unfortunately one of the biggest problems with LLMs is not just ignorance (âI donât knowâ) but hallucination (âI think I know but I actually donât ","marks":[],"data":{}},{"nodeType":"text","value":"at all","marks":[{"type":"italic"}],"data":{}},{"nodeType":"text","value":".â) This is perhaps the biggest issue facing LLMs at the current time. The way we overcome that is by combining factual information from our search engine and the capabilities of an LLM together.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Again, as we demonstrated before, there is a powerful combination in Ray + LangChain. LangChain provides a chain that is well suited to this (Retrieval QA). To give a fuller picture of how the pieces come together, weâre going to implement some parts that could usually just as easily be wrapped.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The Question Answering Service we will build will query the results from our Search Engine, and then use an LLM to summarize the results of the search.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Previously we had shown this diagram for how to serve semantic search results: ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1yspRkXhEt1xIRdiuztbyh","type":"Asset","createdAt":"2023-05-08T15:43:10.951Z","updatedAt":"2023-05-08T15:43:10.951Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"qna-save-search-queries","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1yspRkXhEt1xIRdiuztbyh/f5a50d1046b085b95cd18742e51d5393/qna-save-search-queries.png","details":{"size":243832,"image":{"width":1600,"height":794}},"fileName":"qna-save-search-queries.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We are going to augment that now by creating a chain that consists of the above stage, then generating a prompt, and feeding that to an LLM to generate the answer.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Hence, the resulting system we are trying to build looks like this:Â ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mD1pRX9Do1SDRCUor1Yex","type":"Asset","createdAt":"2023-05-08T15:44:48.379Z","updatedAt":"2023-05-08T15:44:48.379Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"qna-answering-questions-with","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2mD1pRX9Do1SDRCUor1Yex/d4041f6d0d296c73edd77de26e076a17/qna-answering-questions-with.png","details":{"size":170666,"image":{"width":1556,"height":1024}},"fileName":"qna-answering-questions-with.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In other words, we take the vector search results, we take a prompt template, generate the prompt and then pass that to the LLM. Today we will use ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/Stability-AI/StableLM"},"content":[{"nodeType":"text","value":"StableLM","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" but you can easily swap in whatever model you want to.Â \n\nBefore we get started, itâs worth noting that you can find the source code to this project in our LangChain Ray examples repo at: ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/langchain-ray"},"content":[{"nodeType":"text","value":"https://github.com/ray-project/langchain-ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 1: The Prompt Template","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The prompt template is derived from the suggested one from StableLM, but modified for our use case.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In serve.py, we declare the following template:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2MvraLhg4SD47Hq41r5Jd7","type":"Entry","createdAt":"2023-05-08T15:46:38.818Z","updatedAt":"2023-05-08T15:46:38.818Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-1","body":"TEMPLATE = \"\"\"\n\u003c|SYSTEM|\u003e# StableLM Tuned (Alpha version)\n- You are a helpful, polite, fact-based agent for answering questions about Ray. \n- Your answers include enough detail for someone to follow through on your suggestions. \n\u003c|USER|\u003e\n\nPlease answer the following question using the context provided. If you don't know the answer, just say that you don't know. Base your answer on the context below. Say \"I don't know\" if the answer does not appear to be in the context below. \n\nQUESTION: {question} \nCONTEXT: \n{context}\n\nANSWER: \u003c|ASSISTANT|\u003e\n\"\"\"","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Letâs go through the template. The first part is the â\u003c|SYSTEM|\u003eâ tag. You can think of this as setting the âpersonalityâ of the LLM. LLMs are trained to treat the system tag differently. Not all LLMs support this, but OpenAI and StableLM do.Â  The second part is the â\u003c|USER|\u003eâ tag which is the question we want to ask. Note that the question and context are âtemplatizedâ â we will provide them from another source. Finally, since LLMs generate outputs by continuing on from the input, we say to the LLM âOK, hereâs where you take over.âÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The question will come from the userâs query. The context will use what we built last time: the search results from our semantic search.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 2: Setting up the embeddings and the LLM","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now letâs have a look at the __init__ method for our deployment. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3A4WRFfgvE3dNjwHACGYRB","type":"Entry","createdAt":"2023-05-08T15:47:56.437Z","updatedAt":"2023-05-08T15:47:56.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-2","body":"def __init__(self):\n       #... the same code from Part 1 .. \n       self.llm = StableLMPipeline.from_model_id(model_id=\"stabilityai/stablelm-tuned-alpha-7b\",\n                                                    task=\"text-generation\", model_kwargs=\n                                                    {\"device_map\":\"auto\", \"torch_dtype\": torch.float16})\n       WandbTracer.init({\"project\": \"wandb_prompts_2\"})\n       self.chain = load_qa_chain(\n           llm=self.llm,\n           chain_type=\"stuff\",\n           prompt=PROMPT)","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"As you can see, weâve added just 3 lines.Â \n\nThe first line creates a new StableLM LLM. In this case we had to write a little bit of glue code because we wanted to specify using ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://en.wikipedia.org/wiki/Half-precision_floating-point_format"},"content":[{"nodeType":"text","value":"float16","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" (halving the memory consumption of the model). We are working with the authors of Langchain to make this unnecessary. The key line from that file is this one: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"lthNpTGxWTEV31PX65mYb","type":"Entry","createdAt":"2023-05-08T15:50:17.006Z","updatedAt":"2023-05-08T15:50:17.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-3","body":" response = self.pipeline(prompt, temperature=0.1, max_new_tokens=256, do_sample=True)\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Here we specify the maximum number of tokens, and that we want it to pretty much answer the question the same way every time, and that we want to do one word at a time.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The second line sets up our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.wandb.ai/ref/python/integrations/wandbtracer"},"content":[{"nodeType":"text","value":"tracing with Weights and Biases","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". This is completely optional, but will allow us to visualize the input. You can find out more about Weights and Biases ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://wandb.ai"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The third thing we do is create a new chain that is specifically designed for answering questions. We specify the LLM to use, the prompt to use and finally the âchain typeâ â for now we set this to âstuffâ but there are other options like âmap_reduceâ, and also pass in the prompt.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 3: Respond to questions","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Weâve now got our Langchain ready, now all we have to do is write the code that uses the chain to answer questions!Â ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4Z72OU7xq4tPFzV8zLks6","type":"Entry","createdAt":"2023-05-08T15:51:41.475Z","updatedAt":"2023-05-08T15:51:41.475Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-4","body":"   def answer_question(self, query):\n       search_results = self.db.similarity_search(query)\n       print(f'Results from db are: {search_results}')\n       result = self.chain({\"input_documents\": search_results, \"question\":query})\n\n       print(f'Result is: {result}')\n       return result[\"output_text\"]","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Youâll notice the first line is identical to our previous version. Now we execute the chain with both our search results and the question being fed into the template.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 4: Go!","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now letâs get this started. If youâre using Weights and Biases, donât forget to log in using wandb login. To start, letâs do serve run serve:deployment.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now itâs started, letâs use a simple query script to test it.","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4XXeXIabNrMpB1kEND5KBK","type":"Entry","createdAt":"2023-05-08T15:53:45.904Z","updatedAt":"2023-05-08T15:53:45.904Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-5","body":"$ python query.py 'What are placement groups?'\nPlacement groups are a way for users to group resources together and schedule tasks or actors on those resources. They allow users to reserve resources across multiple nodes and can be used for gang-scheduling actors or for spreading resources apart. Placement groups are represented by a list of bundles, which are used to group resources together and schedule tasks or actors. After the placement group is created, tasks or actors can be scheduled according to the placement group and even on individual bundles.\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Yay! It works (well, mostly, that part at the end is a bit weird)! Letâs also check that it doesnât make stuff up: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12iqa2l4xNWuK18GLTu9Ly","type":"Entry","createdAt":"2023-05-08T15:54:22.876Z","updatedAt":"2023-05-08T15:54:22.876Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-6","body":"$ python query.py 'How do I make fried rice?'\nI don't know.\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Letâs just check the prompt that was sent to StableLM. This is where Weights and Biases comes in. Pulling up our interface we can find the prompt that was sent:Â ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"64oHWc7Rnz0fgCIBOR8gEF","type":"Entry","createdAt":"2023-05-08T15:55:06.135Z","updatedAt":"2023-05-08T15:55:06.135Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-7","body":"\u003c|SYSTEM|\u003e# StableLM Tuned (Alpha version)\n- You are a helpful, polite, fact-based agent for answering questions about Ray. \n- Your answers include enough detail for someone to follow through on your suggestions. \n\u003c|USER|\u003e\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\n\nPlease answer the following question using the context provided. \nQUESTION: What are placement groups? \nCONTEXT: \nPlacement Groups#\nPlacement groups allow users to atomically reserve groups of resources across multiple nodes (i.e., gang scheduling).\nThey can be then used to schedule Ray tasks and actors packed as close as possible for locality (PACK), or spread apart\n(SPREAD). Placement groups are generally used for gang-scheduling actors, but also support tasks.\nHere are some real-world use cases:\n\nray list placement-groups provides the metadata and the scheduling state of the placement group.\nray list placement-groups --detail provides statistics and scheduling state in a greater detail.\nNote\nState API is only available when you install Ray is with pip install \"ray[default]\"\nInspect Placement Group Scheduling State#\nWith the above tools, you can see the state of the placement group. The definition of states are specified in the following files:\nHigh level state\nDetails\n\nPlacement groups are represented by a list of bundles. For example, {\"CPU\": 1} * 4 means youâd like to reserve 4 bundles of 1 CPU (i.e., it reserves 4 CPUs).\nBundles are then placed according to the placement strategies across nodes on the cluster.\nAfter the placement group is created, tasks or actors can be then scheduled according to the placement group and even on individual bundles.\nCreate a Placement Group (Reserve Resources)#\n\nSee the User Guide for Objects.\nPlacement Groups#\nPlacement groups allow users to atomically reserve groups of resources across multiple nodes (i.e., gang scheduling). They can be then used to schedule Ray tasks and actors packed as close as possible for locality (PACK), or spread apart (SPREAD). Placement groups are generally used for gang-scheduling actors, but also support tasks.\nSee the User Guide for Placement Groups.\nEnvironment Dependencies#\n\nANSWER: \u003c|ASSISTANT|\u003e\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We can see the search results that we made in are being included. StableLM is then using this to synthesize its answer.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Conclusion","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this blog post we showed how we could build on the simple search engine we built in the previous blog in this series and make a retrieval-based question answering service. It didnât need us to do much: we needed to bring up a new LLM (StableLM),Â  we needed to generate a prompt with the search results in it, and then feed that result to the LLM asking it to derive an answer from it. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Next Steps","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Review the code and data used in this blog in the following ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/langchain-ray/tree/main/open_source_LLM_retrieval_qa"},"content":[{"nodeType":"text","value":"Github repo","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". \n\nIf you are interested in learning more about Ray, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io/"},"content":[{"nodeType":"text","value":"Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://docs.ray.io/"},"content":[{"nodeType":"text","value":"Docs.Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\n","marks":[],"data":{}},{"nodeType":"text","value":"See our earlierÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"nodeType":"text","value":"blog series on solving Generative AI infrastructure","marks":[],"data":{}}]},{"nodeType":"text","value":"Â with Ray.","marks":[],"data":{}},{"nodeType":"text","value":"\n\nTo connect with the Ray community join #LLM on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"Ray Slack","marks":[],"data":{}}]},{"nodeType":"text","value":" or our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"Discuss forum","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://www.anyscale.com/platform"},"content":[{"nodeType":"text","value":"Anyscale.com/Platform ","marks":[],"data":{}}]},{"nodeType":"text","value":"and click the 'Try it now' button","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Summit 2023:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"nodeType":"text","value":"Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"nodeType":"text","value":"practical training focused on LLMs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mD1pRX9Do1SDRCUor1Yex","type":"Asset","createdAt":"2023-05-08T15:44:48.379Z","updatedAt":"2023-05-08T15:44:48.379Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"qna-answering-questions-with","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2mD1pRX9Do1SDRCUor1Yex/d4041f6d0d296c73edd77de26e076a17/qna-answering-questions-with.png","details":{"size":170666,"image":{"width":1556,"height":1024}},"fileName":"qna-answering-questions-with.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2C7QtMnASHfVdag9xYamXh","type":"Entry","createdAt":"2023-05-04T15:03:36.052Z","updatedAt":"2023-06-06T14:20:09.992Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker","seoTitle":"Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker","slug":"offline-batch-inference-comparing-ray-apache-spark-and-sagemaker","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DwU0wh6AmJoGv8JOy6n0Q","type":"Entry","createdAt":"2020-11-11T23:34:15.165Z","updatedAt":"2021-02-05T22:22:08.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","slug":"amog-kamsetty","link":"https://www.linkedin.com/in/amogkamsetty/","bio":"Amog is a software engineer at Anyscale. He helps maintain and develop two machine learning libraries: Ray Tune for distributed hyperparameter tuning and RaySGD for distributed deep learning training.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J134jX2ybZRLPfcAIGLTI","type":"Asset","createdAt":"2020-11-13T00:05:05.533Z","updatedAt":"2020-11-13T00:05:05.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J134jX2ybZRLPfcAIGLTI/12589bb0c6792051050ebc6e1e83af25/Amog_Kamsetty_.jpg","details":{"size":57660,"image":{"width":480,"height":640}},"fileName":"Amog Kamsetty .jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}}],"publishedDate":"2023-05-04","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Update 5/10: ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"Thanks to the Databricks Apache Spark developers for pointing out the prefetching flag ","nodeType":"text"},{"data":{},"marks":[{"type":"code"},{"type":"italic"}],"value":"spark.databricks.execution.pandasUDF.prefetch.maxBatches ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"available in Databricks Runtime that can be used with the Iterator API. It is also possible to implement this manually in Spark open source with background threads. With prefetching set to 4 batches, Spark reaches ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"},{"type":"italic"}],"value":"159.86 images/sec","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":". The code can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/blob/main/spark/code/torch-batch-inference-s3-10G-standard-iterator-databricks-prefetch.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"italic"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"TL;DR","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As more companies use large scale machine learning (ML) models for training and evaluation, offline batch inference becomes an essential workload. A number of challenges come with it: managing compute infrastructure; optimizing use of all heterogeneous resources; and transferring data from storage to hardware accelerators. Addressing these challenges, Ray performs significantly better as it can coordinate clusters of diverse resources, allowing for better utilization of the specific resource requirements of the workload.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we conduct a comparison of three different solutions for offline batch inference: AWS ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker Batch Transform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Our experiments demonstrate that Ray Data achieves speeds up to ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"17x ","nodeType":"text"},{"data":{},"marks":[],"value":"faster than SageMaker Batch Transform and","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" 2x","nodeType":"text"},{"data":{},"marks":[],"value":" faster than Spark for offline image classification. Ray Data also scales effectively to terabyte-sized datasets. The code for these benchmarks is publicly available and can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introduction","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46bNG3QTa1atN02rsemfe8","type":"Asset","createdAt":"2023-05-03T22:20:44.214Z","updatedAt":"2023-05-03T22:20:44.214Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_1_batch_offline_inference","description":"Figure 1. General parallel batch processing architecture\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/46bNG3QTa1atN02rsemfe8/120bf3aac26ae94257b41ac674f42e94/image3.png","details":{"size":48540,"image":{"width":960,"height":540}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Offline batch inference is a critical workload for many AI products, especially with the growing usage of pre-trained foundation models. At its core, it seems like a simple problem: given a trained model and a dataset, get model predictions for each data point. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, there are many challenges to doing this at scale:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 1: ","nodeType":"text"},{"data":{},"marks":[],"value":"Managing compute infrastructure and cloud clusters, especially when needing to use ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"heterogeneous clusters","nodeType":"text"},{"data":{},"marks":[],"value":", consisting of different instance types to maximize throughput.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 2: ","nodeType":"text"},{"data":{},"marks":[],"value":"Parallelizing data processing and utilizing all resources (CPUs \u0026 GPUs) in the cluster at any given point in time.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 3: ","nodeType":"text"},{"data":{},"marks":[],"value":"Efficiently transferring data between cloud storage, CPU RAM for preprocessing, and GPU RAM for model inference, especially for unstructured data like images and text. This has ramifications for both performance and developer velocity.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 4:","nodeType":"text"},{"data":{},"marks":[],"value":" A user experience that makes it easy to iterate and develop while working at scale.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What does the industry recommend for offline batch inference?","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The industry hasnât converged on a standard solution. ","nodeType":"text"},{"data":{"uri":"https://twitter.com/benhamner/status/1605730034450169857"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"It seems like every ML practitioner has their own favorite approach.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5hRMwuEGUS4nJ7eV5riIK9","type":"Asset","createdAt":"2023-05-03T22:23:14.590Z","updatedAt":"2023-05-03T22:23:14.590Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"tweet_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5hRMwuEGUS4nJ7eV5riIK9/05b76abcbe4ea58ae3839fd319858594/image4.png","details":{"size":177304,"image":{"width":1798,"height":588}},"fileName":"image4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We see that there are three categories of solutions that attempt to address the above challenges. However, only category three addresses all the challenges.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Batch Services: ","nodeType":"text"},{"data":{},"marks":[],"value":"Cloud providers such as AWS, GCP, and Azure provide batch services to manage compute infrastructure for you. Some newer products like ","nodeType":"text"},{"data":{"uri":"https://modal.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Modal Labs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provide a user experience layer around cloud providers to abstract away even more complexity. Regardless of which service you choose, the process is the same: you provide your code, and the service runs your code on each node in a cluster. However, while infrastructure management is necessary (Challenge #1), it is not enough. These services have limitations, such as a lack of software libraries to address Challenges #2, #3, and #4 which makes them suitable only for experienced users who can write their own optimized batch inference code.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Online Inference solutions: ","nodeType":"text"},{"data":{},"marks":[],"value":"Â Solutions like ","nodeType":"text"},{"data":{"uri":"https://www.bentoml.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Bento ML","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provide APIs to make it easy to write performant inference code and can abstract away infrastructure complexities. But they are designed for online inference rather than offline batch inferenceâtwo different problems with different sets of requirements. These solutions often donât perform well in the offline case, leading inference service providers like ","nodeType":"text"},{"data":{"uri":"https://modelserving.com/blog/unifying-real-time-and-batch-inference-with-bentoml-and-spark"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Bento ML to integrating with Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for offline inference.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed data systems:","nodeType":"text"},{"data":{},"marks":[],"value":" These solutions are designed to handle large amounts of data and are better suited to handle all three challenges listed above. They break large datasets into reasonable batches for processing and can map a given function (i.e., the model) over the dataset efficiently, effectively handling the challenges of offline batch inference at scale. Examples include ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ultimately, we have observed that Ray Data is the best practical solution for offline batch inference for modern deep learning applications, both in terms of performance and user experience.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We ran the following comparisons and experiments to corroborate the above observations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing AWS SageMaker Batch Transform, Apache Spark, and Ray Data performance and UX on an image classification use case","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling up Ray Data batch inference to 10 TB, showing 90%+ GPU utilization","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Image Classification: SageMaker Batch Transform vs. Apache Spark vs. Ray Data","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We ran an image classification task from the ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1911.02549"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLPerf Inference Benchmark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" suite in the offline setting. This benchmark uses images from the ","nodeType":"text"},{"data":{"uri":"https://image-net.org/challenges/LSVRC/2012/2012-downloads.php#Images"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"ImageNet 2012 Dataset","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and a pre-trained ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch ResNet50 model.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We timed a full end-to-end batch inference workload. With images stored in the parquet format in a public S3 bucket, the workload involved the following three steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Reading images from S3","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Simple CPU preprocessing (resizing, cropping, normalization)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Model inference on GPU","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"While all Ray experiments were run on Anyscale, Spark experiments were run on Databricks runtime v12.0, with Apache Spark 3.3.1. More details on the full configurations can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"in the repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In particular for Spark, we used two different setups:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Single-cluster","nodeType":"text"},{"data":{},"marks":[],"value":": Both the CPU preprocessing and GPU inference were done on the same cluster. We tried both versions of the PySpark Pandas UDF API: ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.pandas_udf.html#pyspark.sql.functions.pandas_udf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Series-based and Iterator-based","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Multi-cluster","nodeType":"text"},{"data":{},"marks":[],"value":": CPU preprocessing is done on one cluster, the preprocessed data is saved to a shared file system, and GPU inference is done on a different cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"All the source code for the experiments conducted above is available ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", including additional details on various other configurations that we attempted.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Â Results of throughput from experiments","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"When running with a 10 GB dataset on a single g4dn.12xlarge instance (48 CPUs, 4 GPUs), we observe the following throughput (images/sec) results for ResNet-50 batch inference:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Urc9cqOTvtI8d4w9DjiEm","type":"Asset","createdAt":"2023-05-04T00:09:19.405Z","updatedAt":"2023-05-04T00:09:19.405Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_5_table","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Urc9cqOTvtI8d4w9DjiEm/42718e584dcc4d0d16592749b0782b4f/Screen_Shot_2023-05-03_at_5.01.12_PM.png","details":{"size":91393,"image":{"width":1211,"height":541}},"fileName":"Screen Shot 2023-05-03 at 5.01.12 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The above throughput differences translate directly into cost savings as the batch inference job with Ray can run much fasterâso the faster it finishes the fewer resources are consumed and the cheaper it gets. Additionally, Ray is fully open source.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2dNPqjBTFiDZn2gcXkdir9","type":"Asset","createdAt":"2023-05-03T22:50:03.518Z","updatedAt":"2023-05-03T22:50:03.518Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 1","description":"Figure 1. Throughput difference among three different solutions for 10GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/2dNPqjBTFiDZn2gcXkdir9/f07a675dcb4f124e29aeb2f6618dbf15/image1.png","details":{"size":163062,"image":{"width":1334,"height":948}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"SageMaker Batch Transform for batch inference","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As we can see, SageMaker Batch Transform is not well suited for batch inference, both for throughput and cost.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While SageMaker handles","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" Challenge #1 ","nodeType":"text"},{"data":{},"marks":[],"value":"to some degree and abstracts away compute infrastructure management, it doesnât fully address the remaining challenges","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The low-throughput results are primarily because while SageMaker Batch Transform claims to be a ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"solution for batch inference on large datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", under the hood, it uses the same architecture as for online serving systems. It starts an HTTP server, and deploys your model as an endpoint. And then for each image, a separate request is sent to the server.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"SageMaker also does not provide support for batching across ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"multiple files","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and even if you batched your data beforehand, the max payload per request is 100 MB, nowhere near enough to fully saturate your GPUs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In terms of developer velocity, when running these benchmarks, we ran into a few additional challenges with SageMaker. In particular, the need to spin up a new cluster every time we ran a script and the difficult to parse stack traces slowed down developer iterations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While online inference solutions have their place, the additional overhead they have, as well as the inability to fully utilize cluster resources and saturate GPUs by maximizing batch sizes and effectively pipelining data make them highly inefficient for offline batch inference.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing Spark and Ray for batch inference","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Clearly, as we observed above, shoehorning online inference solutions for offline use cases is not the right approach. Letâs compare the two distributed data system approaches: Spark and Ray Data.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We see that Ray Data has","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" 2x ","nodeType":"text"},{"data":{},"marks":[],"value":"faster throughput than Spark. Letâs break down how Ray Data and Spark fare for the challenges mentioned earlier:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #2: Hybrid CPU and GPU workload","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deep learning workloads involve ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"both CPU and GPU computation","nodeType":"text"},{"data":{},"marks":[],"value":". For optimal parallelization, all the CPUs and GPUs in the cluster should be fully utilized at any given time. However, this is a limitation of Spark scheduling as all the stages are fused together, regardless of their resource requirements. This means that in the single cluster setup, the total parallelism is limited by the number of GPUs, leading to underutilization of CPUs. This also means that GPUs are completely idle during the reading and preprocessing steps.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"When using a separate CPU and GPU cluster, data needs to be persisted and orchestrated between stages. Overall, for these CPU+GPU workloads, Spark does not fully address ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"challenge #2","nodeType":"text"},{"data":{},"marks":[],"value":" and cannot utilize all resources in the cluster concurrently. We attempted to use Sparkâs stage level scheduling feature to address this issue, but were ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/tree/main/spark#stage-level-scheduling"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"unsuccessful","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6m5c6zQiMVCithTLEIazb8","type":"Asset","createdAt":"2023-05-03T23:02:01.263Z","updatedAt":"2023-05-03T23:02:01.263Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 2","description":"Figure 2. How Ray data efficiently pipelines and optimizes hardware: CPUs/GPUs","file":{"url":"//images.ctfassets.net/xjan103pcp94/6m5c6zQiMVCithTLEIazb8/1be9788a7aa54707a42f38023597ae2f/5mb.gif","details":{"size":4991804,"image":{"width":3260,"height":1182}},"fileName":"5mb.gif","contentType":"image/gif"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data, on the other hand, can independently scale the CPU preprocessing and GPU inference steps. It also streams data ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"through these stages","nodeType":"text"},{"data":{},"marks":[],"value":", increasing CPUs and GPUs utilization and reducing out-of-memory errors (OOMs), and fine-grained resource allocation for ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html#scaling-and-resource-allocation"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"autoscaling and management","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #3: Large, multi-dimensional tensors","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deep learning workloads often involve dealing with complex data types like images, audio, or video, which are represented by multi-dimensional ","nodeType":"text"},{"data":{"uri":"https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"arrays","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Having first-class support for these data types is necessary for better performance and developer experience. Ray Data has Numpy batch format as a first-class citizen, and does not require Pandas, which is suited for deep learning and leads to higher performance and more memory efficiency, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-2-2-improved-developer-experience-performance-and-stability"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"as shown in previous benchmarks","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #4: Ray is Python native","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another fundamental difference is that Ray is Python native whereas Spark is not, which also impacts performance. We ran a microbenchmark that applies a ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"time.sleep(1)","nodeType":"text"},{"data":{},"marks":[],"value":" UDF. Surprisingly, running the UDF takes ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/tree/main/spark#microbenchmark"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"80 seconds","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"; we believe due to JVM\u003c\u003ePyarrow overhead! On Ray Data, the same microbenchmark takes a little ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/blob/main/ray/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"under 5 seconds","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Not to mention, this makes Spark development more difficult. In particular when debugging, the recommended way is to include debug information in the ","nodeType":"text"},{"data":{"uri":"https://stackoverflow.com/a/57175768"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"output pandas dataframe rather than using print statements","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Even as we scale to a larger 300 GB dataset using 4 nodes and 16 GPUs, we see that these performance differences still hold:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5wDKGXZNHB2Hlt6JEMMKOt","type":"Asset","createdAt":"2023-05-03T23:04:40.956Z","updatedAt":"2023-05-03T23:04:40.956Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_3_batch_inference","description":"Figure 3. Throughput difference between Spark and Ray solution for 300 GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/5wDKGXZNHB2Hlt6JEMMKOt/da27ee6ea7e5773abef42eb291dd1a97/image5.png","details":{"size":92846,"image":{"width":1102,"height":830}},"fileName":"image5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"While big data processing systems are the best solutions for offline inference, for deep learning workloads, Ray Data outperforms Spark.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling up Ray Data and maximizing performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we test the scalability of Ray Data and attempt to maximize performance by using a ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"heterogeneous cluster","nodeType":"text"},{"data":{},"marks":[],"value":" consisting of a mix of GPU nodes and CPU-only nodes. Deep learning workloads involving complex data types are often memory-bound, so by using additional CPU-only nodes, we add more total memory to our cluster to better maximize GPU utilization.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlike SageMaker or Spark, Ray works natively with heterogeneous clusters consisting of different instance types, fully addressing ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"challenge #1.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We run the same benchmark, except with 10 TB worth of images, using a single cluster of 10 g4dn.12xlarge instances and 10 m5.16xlarge instances.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Throughput: 11,580.958 img/sec","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"As we can see below, Ray Data achieves over 90%+ GPU utilization throughout the workload run:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"16XJHcynegHxIdae1hupAF","type":"Asset","createdAt":"2023-05-03T23:07:22.875Z","updatedAt":"2023-05-03T23:07:22.875Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"batch_inference_figure_4","description":"Figure 4. GPU utilization of all 40 GPUs in the cluster. We are roughly at 90%+ GPU utilization during the entire duration of the workload.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/16XJHcynegHxIdae1hupAF/8eab9567ac78a8d558bcd04218f7e1ca/image2.png","details":{"size":466601,"image":{"width":1999,"height":621}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nThe ability to scale to many GPUs and large data sizes without needing to rewrite code and not sacrificing on throughput is what sets Ray Data apart compared to other solutions for deep learning workloads.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog, we identified four technical challenges for doing batch inference at scale and investigated three commonly cited industry solutions for offline batch inferences. Using the cited approaches, we compared Ray, Spark, and Amazon SageMaker Batch Transform.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We concluded that Ray Data best meets all four challenges:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data abstracts away compute infrastructure management, and can maximize performance for memory-bound workloads as it natively supports heterogeneous clusters.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"streams","nodeType":"text"},{"data":{},"marks":[],"value":" data from cloud storage -\u003e CPU -\u003e GPU, ensuring that all resources in the cluster are utilized at any given point in time. This improves throughput and reduces overall cost.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data has native support for multi-dimensional tensors, which is vital for deep learning workloads.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data is Python native and programmatic, making it easy to develop and debug.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Overall, Ray Data is the best option for deep learning offline batch inference. In our image classification benchmarks, as shown in the figures above, Ray Data significantly outperforms SageMaker Batch Transform (by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"17x)","nodeType":"text"},{"data":{},"marks":[],"value":" and Spark (by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"2x ","nodeType":"text"},{"data":{},"marks":[],"value":"and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"3x","nodeType":"text"},{"data":{},"marks":[],"value":") while linearly scaling to TB level data sizes. All code for these benchmarks can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Lastly, as a result of higher throughput and Ray as an open source solution, Ray lowers the overall costs on top of the base EC2 cost.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Whatâs Next","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stay tuned for a follow up post where we dive into Ray Data streaming's design and how to use it for your own ML pipelines. Meanwhile, you can get started with Ray Data for batch inference ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Alternatively, ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfC6rmBeb9pLPSKRf1e7VCy7BtkjSGu20gh0xjQhdo6rcUflQ/viewform"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"fill out this form","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â to connect with us if you are interested in working on a proof of concept (POC) for your offline batch inference workload with Ray.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have our ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit 2023 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"early-bird registration open. Secure your spot, and save some money.\n\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n\n\n\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2dNPqjBTFiDZn2gcXkdir9","type":"Asset","createdAt":"2023-05-03T22:50:03.518Z","updatedAt":"2023-05-03T22:50:03.518Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 1","description":"Figure 1. Throughput difference among three different solutions for 10GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/2dNPqjBTFiDZn2gcXkdir9/f07a675dcb4f124e29aeb2f6618dbf15/image1.png","details":{"size":163062,"image":{"width":1334,"height":948}},"fileName":"image1.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"51oIQOHymWExFWIRYQRXge","type":"Entry","createdAt":"2023-05-02T17:47:40.421Z","updatedAt":"2023-05-12T07:23:26.789Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Turbocharge LangChain: guide to 20x faster embedding","slug":"turbocharge-langchain-now-guide-to-20x-faster-embedding","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DwU0wh6AmJoGv8JOy6n0Q","type":"Entry","createdAt":"2020-11-11T23:34:15.165Z","updatedAt":"2021-02-05T22:22:08.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","slug":"amog-kamsetty","link":"https://www.linkedin.com/in/amogkamsetty/","bio":"Amog is a software engineer at Anyscale. He helps maintain and develop two machine learning libraries: Ray Tune for distributed hyperparameter tuning and RaySGD for distributed deep learning training.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J134jX2ybZRLPfcAIGLTI","type":"Asset","createdAt":"2020-11-13T00:05:05.533Z","updatedAt":"2020-11-13T00:05:05.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J134jX2ybZRLPfcAIGLTI/12589bb0c6792051050ebc6e1e83af25/Amog_Kamsetty_.jpg","details":{"size":57660,"image":{"width":480,"height":640}},"fileName":"Amog Kamsetty .jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6G9xTnF76ZUKKvgcVOqhtm","type":"Entry","createdAt":"2022-08-23T02:58:28.957Z","updatedAt":"2023-05-02T04:18:43.884Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Philipp Moritz","slug":"philipp-moritz","link":"https://www.linkedin.com/in/philipp-moritz-61419682/","bio":"Co-founder and CTO at Anyscale"}}],"publishedDate":"2023-05-03","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 2([part 1 here](https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray)) of a blog series. In this blog, weâll show you how to turbocharge embeddings. In future parts, we will show you how to combine a vector database and an LLM to create a fact-based question answering service. Additionally, we will optimize the code and measure performance: cost, latency and throughput.","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7nxbgQNlTcYZECfJ0vAabS","type":"Entry","createdAt":"2023-05-02T17:16:29.953Z","updatedAt":"2023-05-02T17:16:29.953Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"LangChain + Ray Tutorial: How to Generate Embeddings For 33,000 Pages for $1","videoUrl":"https://youtu.be/hGnZajytlac"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Generating embeddings from documents is a critical step for LLM workflows. Many LLM apps are being built today through retrieval based similarity search:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Documents are embedded and stored in a vector database.Â Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Incoming queries are used to pull semantically relevant passages from the vector database, and these passages are used as context for LLMs to answer the query.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"previous blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we showed how to use ","nodeType":"text"},{"data":{"uri":"https://github.com/hwchase17/langchain"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to do step 1, and we also showed how to parallelize this step by using Ray tasks for faster embedding creation.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we take it one step further, scaling out to many more documents. Continue reading to see how to use ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a distributed data processing system thatâs a part of the Ray framework, to generate and store embeddings for 2,000 PDF documents from cloud storage, parallelizing across 20 GPUs, all in under 4 minutes and in less than 100 lines of code.\n\nWhile in this walkthrough we use ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to read PDF files from S3 cloud storage, it also supports a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/api/input_output.html#input-output"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"wide number of other data formats","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" like text data, parquet, images, and can read data from a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/api/input_output.html#input-output"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"variety of sources","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" like MongoDB and SQL Databases.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Why do I need to parallelize this?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2CHfkngLbr34XsDZ7IKBAW","type":"Asset","createdAt":"2023-05-02T02:38:14.040Z","updatedAt":"2023-05-02T02:38:14.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Why do I need to parallelize this?","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2CHfkngLbr34XsDZ7IKBAW/af3491b43c8afcdb307890881b2adf5c/embedding-why-do-I-need-to-parallelize-this.png","details":{"size":153788,"image":{"width":1600,"height":1004}},"fileName":"embedding-why-do-I-need-to-parallelize-this.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"LangChain provides all the tools and the integrations for building LLM applications, including loading, embedding, and storing documents. While LangChain works great for quickly getting started with a handful of documents, when you want to scale your corpus up to thousands or more documents, this can quickly become unwieldy.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Naively using a for loop to do this for each document within a corpus of a 2,000 documents takes 75 minutes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4eHSySPomCQud1Y6m1yiyB","type":"Entry","createdAt":"2023-05-02T02:45:13.859Z","updatedAt":"2023-05-02T04:15:57.187Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Embedded-Code-Snippet-1","body":"import os\nfrom tqdm import tqdm\n\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=100,\n    length_function=len,\n)\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nhf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n\n# Put your directory containing PDFs here\ndirectory = '/tmp/data/'\npdf_documents = [os.path.join(directory, filename) for filename in os.listdir(directory)]\n\nlangchain_documents = []\nfor document in tqdm(pdf_documents):\n    try:\n        loader = PyPDFLoader(document)\n        data = loader.load()\n        langchain_documents.extend(data)\n    except Exception:\n        continue\n\nprint(\"Num pages: \", len(langchain_documents))\nprint(\"Splitting all documents\")\nsplit_docs = text_splitter.split_documents(langchain_documents)\n\nprint(\"Embed and create vector index\")\ndb = FAISS.from_documents(split_docs, embedding=hf)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Clearly, if you want to iterate quickly and try out different multiple document corpuses, splitting techniques, chunk sizes, or embedding models, just doing this in a for loop wonât cut it.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Instead, for faster development, you need to horizontally scale, and for this you need a framework to make this parallelization very easy.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"By using Ray Data, we can define our embedding generation pipeline and execute it in a few lines of code, and it will automatically scale out, leveraging the compute capabilities of all the CPUs and GPUs in our cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stages of our Data Pipeline","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this example, we want to generate embeddings for our document corpus consisting of the top 2,000 arxiv papers on âlarge language modelsâ. There are over 30,000 pages in all these documents. The code for generating this dataset can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/blob/main/embedding_pdf_documents/arxiv_dataset_generation.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J2BxxlXHAsmjVdrjCqLdn","type":"Asset","createdAt":"2023-05-02T02:52:04.314Z","updatedAt":"2023-05-02T02:52:04.314Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Stages of our Data Pipeline","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J2BxxlXHAsmjVdrjCqLdn/2ee02bfb6d2d96c7be6ab848c757d45e/embedded-stages-of-our-pipeline.png","details":{"size":127047,"image":{"width":1512,"height":556}},"fileName":"embedded-stages-of-our-pipeline.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs take a look at the stages of our data pipeline:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Load the PDF documents from our S3 bucket as raw bytes","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use PyPDF to convert those bytes into string text","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use LangChainâs text splitter to split the text into chunks","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use a pre-trained sentence-transformers model to embed each chunk","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Store the embeddings and the original text into a FAISS vector store","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The full data pipeline was run on 5 g4dn.12xlarge instances on AWS EC2, consisting of 20 GPUs in total. The code for the full data pipeline can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/blob/main/embedding_pdf_documents/embedding_ray.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Starting the Ray Cluster","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Follow the steps ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/blob/main/embedding_pdf_documents/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to set up a multi-node Ray cluster on AWS.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray clusters can also be started on GCP, Azure, or other cloud providers. See the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/getting-started.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Cluster documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for full info.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Alternatively, you can use ","nodeType":"text"},{"data":{"uri":"https://docs.anyscale.com/develop/workspaces/get-started"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Workspaces on Anyscale","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to manage your Ray clusters.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now that we have the cluster setup, letâs go through the steps in our script.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Installing Dependencies","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"First, we need to install the necessary dependencies on all the nodes in our Ray cluster. We can do this via Rayâs ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#id1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"runtime environment","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" feature.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4EbdJbi2f8hfsYWczaTcqZ","type":"Entry","createdAt":"2023-05-02T03:46:04.461Z","updatedAt":"2023-05-02T03:46:04.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Embedding-code-snippet-2","body":"import ray\n\nray.init(runtime_env={\"pip\": [\"langchain\", \"pypdf\", \"sentence_transformers\", \"transformers\"]})","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 1","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Load the 2,143 documents from our S3 bucket as raw bytes.The S3 bucket contains unmodified PDF files that have been downloaded from arxiv.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can easily do this via Ray Dataâs read APIs, which creates a Ray Dataset object. Ray Datasets are lazy. Further operations can be chained and the stages are run only when execution is triggered.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Cuzf28zxzpUhECAe0PxyR","type":"Entry","createdAt":"2023-05-02T03:47:03.011Z","updatedAt":"2023-05-02T17:11:29.904Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-3","body":"from ray.data.datasource import FileExtensionFilter\n\n# Filter out non-PDF files.\nds = ray.data.read_binary_files(\"s3://ray-llm-batch-inference/\", partition_filter=FileExtensionFilter(\"pdf\"))","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 2","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Use PyPDF to load in the raw bytes and parse them as string text. We also skip over any documents or pages that are unparseable. Even after skipping these, we still have over 33,642 pages in our dataset.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We use the pypdf library directly to read PDFs directly from bytes rather than file paths. Once ","nodeType":"text"},{"data":{"uri":"https://github.com/hwchase17/langchain/pull/3915"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://github.com/hwchase17/langchain/pull/3915","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is merged, LangChainâs PyPdfLoader can be used directly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"60msecWkdwI269c9fYmRih","type":"Entry","createdAt":"2023-05-02T03:49:06.119Z","updatedAt":"2023-05-02T04:15:18.317Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-4","body":"def convert_to_text(pdf_bytes: bytes):\n    pdf_bytes_io = io.BytesIO(pdf_bytes)\n\n    try:\n        pdf_doc = PdfReader(pdf_bytes_io)\n    except pypdf.errors.PdfStreamError:\n        # Skip pdfs that are not readable.\n        # We still have over 30,000 pages after skipping these.\n        return []\n\n    text = []\n    for page in pdf_doc.pages:\n        try:\n            text.append(page.extract_text())\n        except binascii.Error:\n            # Skip all pages that are not parseable due to malformed characters.\n            print(\"parsing failed\")\n    return text\n\n# We use `flat_map` as `convert_to_text` has a 1-\u003eN relationship.\n# It produces N strings for each PDF (one string per page).\n# Use `map` for 1-\u003e1 relationship.\nds = ds.flat_map(convert_to_text)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 3","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Split the text into chunks using LangChainâs TextSplitter abstraction. After applying this transformation, the 33,642 pages are split into 144,411 chunks. Each chunk will then be encoded into an embedding in Step 4.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3S1Tcod9rihoVKoyg9RMMz","type":"Entry","createdAt":"2023-05-02T03:50:32.629Z","updatedAt":"2023-05-02T04:14:39.402Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-5","body":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ndef split_text(page_text: str):\n    # Use chunk_size of 1000.\n    # We felt that the answer we would be looking for would be \n    # around 200 words, or around 1000 characters.\n    # This parameter can be modified based on your documents and use case.\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000, chunk_overlap=100, length_function=len\n    )\n    split_text: List[str] = text_splitter.split_text(page_text)\n\n    split_text = [text.replace(\"\\n\", \" \") for text in split_text]\n    return split_text\n\n# We use `flat_map` as `split_text` has a 1-\u003eN relationship.\n# It produces N output chunks for each input string.\n# Use `map` for 1-\u003e1 relationship.\nds = ds.flat_map(split_text)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 4","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Then, we can embed each of our chunks using a pre-trained sentence transformer model on GPUs. Here, we leverage Ray Actors for stateful computation, allowing us to initialize a model only once per GPU, rather than for every single batch.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"At the end of this stage, we have 144,411 encodings by running 20 model replicas across 20 GPUs, each processing a batch of 100 chunks at a time to maximize GPU utilization.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Dp9uXgJznvx0VgoNwZt0e","type":"Entry","createdAt":"2023-05-02T03:53:27.990Z","updatedAt":"2023-05-02T04:13:54.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-6","body":"class Embed:\n    def __init__(self):\n        # Specify \"cuda\" to move the model to GPU.\n        self.transformer = SentenceTransformer(model_name, device=\"cuda\")\n\n    def __call__(self, text_batch: List[str]):\n        # We manually encode using sentence_transformer since LangChain\n        # HuggingfaceEmbeddings does not support specifying a batch size yet.\n        embeddings = self.transformer.encode(\n            text_batch,\n            batch_size=100,  # Large batch size to maximize GPU utilization.\n            device=\"cuda\",\n        ).tolist()\n\n        return list(zip(text_batch, embeddings))\n\n# Use `map_batches` since we want to specify a batch size to maximize GPU utilization.\nds = ds.map_batches(\n    Embed,\n    # Large batch size to maximize GPU utilization.\n    # Too large a batch size may result in GPU running out of memory.\n    # If the chunk size is increased, then decrease batch size.\n    # If the chunk size is decreased, then increase batch size.\n    batch_size=100,  # Large batch size to maximize GPU utilization.\n    compute=ray.data.ActorPoolStrategy(min_size=20, max_size=20),  # I have 20 GPUs in my cluster\n    num_gpus=1,  # 1 GPU for each actor.\n)\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We use the `sentence_transformers` library directly so that we can provide a specific batch size. Once ","nodeType":"text"},{"data":{"uri":"https://github.com/hwchase17/langchain/pull/3914"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://github.com/hwchase17/langchain/pull/3914","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is merged, LangChainâs `HuggingfaceEmbeddings` can be used instead.\n\n","nodeType":"text"},{"data":{},"marks":[{"type":"underline"}],"value":"Stage 5","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we can execute this Data Pipeline by iterating through it, and we store the results in a persisted FAISS vector database for future querying.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3WstnFWdDedO9wxSKYDGR8","type":"Entry","createdAt":"2023-05-02T03:54:47.846Z","updatedAt":"2023-05-02T04:17:51.737Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-7","body":"from langchain import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\ntext_and_embeddings = []\nfor output in ds.iter_rows():\n    text_and_embeddings.append(output)\n\nvectore_store = FAISS.from_embeddings(\n    text_and_embeddings,\n    # Provide the embedding model to embed the query.\n    # The documents are already embedded.\n    embedding=HuggingFaceEmbeddings(model_name=model_name)\n)\n\n# Persist the vector store.\nvectore_store.save_local(\"faiss_index\")","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Execution","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Executing this code, we see that all 20 GPUs are utilized at near 100% utilization. And what would normally take over an hour to run, can now be done in under 4 minutes! If you use AWS spot instances, this would only cost $0.95 total.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5rAh6B2lrImLTTXvpBMeHG","type":"Asset","createdAt":"2023-05-02T04:00:42.271Z","updatedAt":"2023-05-02T04:00:42.271Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Execution-1","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5rAh6B2lrImLTTXvpBMeHG/70baf2935bfad28c70d5fb310fe15ef1/embedding-execution-1.png","details":{"size":9146,"image":{"width":352,"height":130}},"fileName":"embedding-execution-1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1yk9oF8K2IIuK88xWBTfGr","type":"Asset","createdAt":"2023-05-02T04:01:19.132Z","updatedAt":"2023-05-02T04:43:16.984Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"locale":"en-US"},"fields":{"title":"Embedding - Execution 2","file":{"url":"//images.ctfassets.net/xjan103pcp94/1yk9oF8K2IIuK88xWBTfGr/ee185a15ff08445d14f3e47ed7f0f2b9/embedding-execution-2.jpg","details":{"size":127225,"image":{"width":1097,"height":780}},"fileName":"embedding-execution-2.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Querying the Vector Database","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can now load in our persisted FAISS database, and query it for similarity search. Letâs see the top document thatâs most relevant to the âprompt engineeringâ query:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3DulKBBh4pLynITCBlacTf","type":"Entry","createdAt":"2023-05-02T04:04:27.586Z","updatedAt":"2023-05-02T04:04:27.586Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-8","body":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain import FAISS\n\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nquery_embedding = HuggingFaceEmbeddings(model_name=model_name)\ndb = FAISS.load_local(\"faiss_index\", query_embedding)\ndocuments = db.similarity_search(query=\"prompt engineering\", k=1)\n[doc.page_content for doc in documents]","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"SMSzMi5hzq9x7y2pUs2Ja","type":"Entry","createdAt":"2023-05-02T04:05:20.246Z","updatedAt":"2023-05-02T04:05:35.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-9","body":"['Prompt Engineering for Job Classiï¬cation 7 5 LLMs \u0026 Prompt Engineering Table '\n '3. Overview of the various prompt modiï¬cations explored in thi s study. '\n 'Short name Description Baseline Provide a a job posting and asking if it is '\n 'ï¬t for a graduate. CoT Give a few examples of accurate classiï¬cation before '\n 'queryi ng. Zero-CoT Ask the model to reason step-by-step before providing '\n 'its an swer. rawinst Give instructions about its role and the task by adding '\n 'to the user msg. sysinst Give instructions about its role and the task as a '\n 'system msg. bothinst Split instructions with role as a system msg and task '\n 'as a user msg. mock Give task instructions by mocking a discussion where it '\n 'ackn owledges them. reit Reinforce key elements in the instructions by '\n 'repeating the m. strict Ask the model to answer by strictly following a '\n 'given templat e. loose Ask for just the ï¬nal answer to be given following a '\n 'given temp late. right Asking the model to reach the right conclusion.']\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next Steps","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"See part 3 ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/building-a-self-hosted-question-answering-service-using-langchain-ray"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Review the code and data used in this blog in the following ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/tree/main/embedding_pdf_documents"},"content":[{"data":{},"marks":[],"value":"Github repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\n","nodeType":"text"},{"data":{},"marks":[],"value":"See our earlierÂ ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[],"value":"blog series on solving Generative AI infrastructure","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â with Ray.","nodeType":"text"},{"data":{},"marks":[],"value":"\n\nIf you are interested in learning more about Ray, see ","nodeType":"text"},{"data":{"uri":"http://ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"http://docs.ray.io/"},"content":[{"data":{},"marks":[],"value":"Docs.Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nTo connect with the Ray community join #LLM on the ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"data":{},"marks":[],"value":"Ray Slack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or our ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[],"value":"Discuss forum","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","nodeType":"text"},{"data":{"uri":"http://www.anyscale.com/platform"},"content":[{"data":{},"marks":[],"value":"Anyscale.com/Platform ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"and click the 'Try it now' button","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray Summit 2023:","nodeType":"text"},{"data":{},"marks":[],"value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"practical training focused on LLMs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J2BxxlXHAsmjVdrjCqLdn","type":"Asset","createdAt":"2023-05-02T02:52:04.314Z","updatedAt":"2023-05-02T02:52:04.314Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Stages of our Data Pipeline","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J2BxxlXHAsmjVdrjCqLdn/2ee02bfb6d2d96c7be6ab848c757d45e/embedded-stages-of-our-pipeline.png","details":{"size":127047,"image":{"width":1512,"height":556}},"fileName":"embedded-stages-of-our-pipeline.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2rfQivy2LoE7OTc9bGVGvm","type":"Entry","createdAt":"2023-04-27T13:59:53.933Z","updatedAt":"2023-06-06T14:28:19.779Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Announcing Ray 2.4.0: Infrastructure for LLM training, tuning, inference, and serving","seoTitle":"Announcing Ray 2.4.0: Infrastructure for LLM training, tuning, inference, and serving","slug":"announcing-ray-2-4-0-infrastructure-for-llm-training-tuning-inference-and","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6TqWiyUibvC9ETfyYljP8b","type":"Entry","createdAt":"2022-02-11T15:42:52.165Z","updatedAt":"2022-02-11T15:42:52.165Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jiajun Yao","slug":"jiajun-yao"}}],"publishedDate":"2023-04-27","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.4.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2.4 release ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"features exciting improvements across the Ray ecosystem. In this blog post, we highlight new features and examples for Generative AI workloads and share four overall additions:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements to Ray data for ease of use, stability, and observability","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Improved Serve observability","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Introduce RLlibâs module for custom reinforcement learning","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Improved Ray scalability for large clusters","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Generative AI model examples and new features using Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Over the past few months, we have seen a flurry of innovative activities around ","nodeType":"text"},{"data":{"uri":"https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-generative-ai"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"generative AI models","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Large_language_model"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"large language models (LLMs)","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". To continue our steadfast commitment to make Ray a pivotal compute substrate for ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"generative AI workloads","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and address the challenges (as explained in our ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"blog series","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"), we have added significant enhancements in this release to ensure that these open source LLM models and workloads are accessible to the open source community and performant with Ray.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QDe7dun9hKhLCoGFSXKh9","type":"Asset","createdAt":"2023-04-26T19:23:25.596Z","updatedAt":"2023-04-26T19:23:25.596Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_image","file":{"url":"//images.ctfassets.net/xjan103pcp94/1QDe7dun9hKhLCoGFSXKh9/d9f93233f6ee72608e34735eac545a74/image1.png","details":{"size":154908,"image":{"width":960,"height":540}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Generative AI Workloads:","nodeType":"text"},{"data":{},"marks":[],"value":" First, with 2.4 we are releasing a set of new working examples showing how to use Ray with ","nodeType":"text"},{"data":{"uri":"https://stablediffusionweb.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stable Diffusion","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and LLMs like ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/transformers/model_doc/gptj"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GPT-J","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Below we tabulate the workload of each of these models and their respective links. More are underway with respect to Rayâs integration with ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"49AVYyPR2gbRD9P3NOH7My","type":"Entry","createdAt":"2023-04-26T19:44:22.407Z","updatedAt":"2023-04-26T22:30:40.568Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"generative_ai_features_and_examples","header":"Table 1. New LLM features, examples, and tutorials","body":"| Generative AI Workload| Description| Links|\n| ---------- | ---------- | ---------- |\n|Fine-tuning GPT-J|In this example, we showcase how to use [Ray Train](https://docs.ray.io/en/releases-2.4.0/train/train.html) for fine-tuning [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) using [DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed) and Hugging Face. GPT-J is a GPT-2-like causal language mode, with 6B parameters, trained on the PILE dataset.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html)|\n|Fine-tuning Dreambooth|This example shows how to fine-tune a [DreamBooth](https://dreambooth.github.io/) model using [Ray Train](https://docs.ray.io/en/releases-2.4.0/train/train.html). Dreambooth is a technique to fine-tune diffusion models (like Stable Diffusion) by injecting a custom subject to the model.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/dreambooth_finetuning.html)|\n|Scalable Batch Inference  with GPT-J|This example showcases how to use[ Ray Data](https://docs.ray.io/en/releases-2.4.0/data/dataset.html) for scalable batch inference with  GPT-J.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_batch_prediction.html)| Scalable Batch Inference  with Stable Diffusion|In this example, showcase how to use [Ray Data](https://docs.ray.io/en/releases-2.4.0/data/dataset.html) for scalable batch inference with [Stable Diffusion](https://stablediffusionweb.com/). Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from CompVis, Stability AI and LAION.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/stablediffusion_batch_prediction.html)|\n|Serving GPT-J|In this example, we showcase how to use [Ray Serve](https://docs.ray.io/en/releases-2.4.0/serve/index.html) for serving GPT-J.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_serving.html)|\n|Serving Stable Diffusion|In this example, we showcase how to use [Ray Serve](https://docs.ray.io/en/releases-2.4.0/serve/index.html) for serving [Stable Diffusion.](https://stablediffusionweb.com/)|[Source and documentation](https://docs.ray.io/en/master/serve/tutorials/stable-diffusion.html)|\n|Ray integration with LangChain|In this tutorial, we showcase how to build an open source search engine with Ray and [LangChain](https://python.langchain.com/en/latest/index.html)|[Source and blog](https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray)|"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray AIR new trainers:","nodeType":"text"},{"data":{},"marks":[],"value":" Second, to further enable additional large model workloads on Ray, weâre also releasing an ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"AccelerateTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", allowing you to run ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/accelerate"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"HuggingFace Accelerate","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/accelerate/usage_guides/deepspeed"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"DeepSpeed","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on Ray with minimal code changes. This Trainer integrates with the rest of the Ray ecosystemâincluding the ability to run distributed ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"hyperparameter tuning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with each trial dispatched as a distributed training job.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"See how ","nodeType":"text"},{"data":{"uri":"https://github.com/CarperAI/trlx/blob/9bc08369ca9ec83342c4d7755205dab1a7723006/trlx/sweep.py#L325-L340"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"CarperAI/trlx","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" uses this for RLHF hyperparameter sweeps, and take a peek at the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html#ray-train-huggingface-accelerate-acceleratetrainer"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" too.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4CHwg0j8NDYgH399cu2hGC","type":"Entry","createdAt":"2023-04-26T21:23:30.012Z","updatedAt":"2023-04-26T21:23:30.012Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"accelerate_trainer_code_snippet","body":"from ray.train.huggingface.accelerate import AccelerateTrainer\n\ndef train_func():\n    ...\n\ntrainer = AccelerateTrainer(\n    train_func,\n    accelerate_config={}  # you can pass in a config filepath here as well\n    scaling_config=ScalingConfig(\n        trainer_resources={\"CPU\": 0},\n        num_workers=4,\n        use_gpu=True,\n        resources_per_worker={\"CPU\": 2},\n    ),\n)\n\nresult = trainer.fit()\n\n# Integration with Ray Tune\ntuner = tune.Tuner(\n    trainer,\n    param_space={\"lr\": tune.grid_search([0.1, 0.2, 0.3]),\n)\n\nresults = tuner.fit()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can see the full example for AccelerateTrainer ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html#ray-train-huggingface-accelerate-acceleratetrainer"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Updated LightningTrainer:","nodeType":"text"},{"data":{},"marks":[],"value":" Third, in the broader deep learning space, weâre introducing the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.lightning.LightningTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LightningTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", allowing you to scale your ","nodeType":"text"},{"data":{"uri":"https://lightning.ai/docs/pytorch/stable//index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch Lightning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on Ray. As part of our continued effort for seamless integration and ease of use, we have enhanced and replaced our existing ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray_lightning","nodeType":"text"},{"data":{},"marks":[],"value":" integration, which was widely adopted, with the latest changes to PyTorch Lighting.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The new ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"LightningTrainer","nodeType":"text"},{"data":{},"marks":[],"value":" provides better compatibility with the other Ray libraries: Ray Tune or Ray Data or Ray Serve directly with Ray connecting APIs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Check out ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.lightning.LightningTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"the documentation for this component","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or take a look ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/examples/lightning/lightning_mnist_example.html#lightning-mnist-example"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"at an example","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1p8EMLSFTFDI0RuBceMrPe","type":"Entry","createdAt":"2023-04-26T21:27:50.764Z","updatedAt":"2023-04-26T21:27:50.764Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"updated_lightning_trainder_code_snippet","body":"from pytorch_lightning.loggers.csv_logs import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\nfrom ray.train.lightning import (\n    LightningTrainer,\n    LightningConfigBuilder,\n    LightningCheckpoint,\n)\n\nclass MNISTClassifier:\n\n    â¦\n\nclass MNISTDataModule:\n    â¦\n\nlightning_config = (\n    LightningConfigBuilder()\n    .module(MNISTClassifier, lr=1e-3, feature_dim=128)\n    .trainer(\n        max_epochs=10,\n        accelerator=\"cpu\",\n        log_every_n_steps=100,\n        logger=CSVLogger(\"logs\"),\n    )\n    .fit_params(datamodule=datamodule)\n    .checkpointing(monitor=\"val_accuracy\", mode=\"max\", save_top_k=3)\n    .build()\n)\nscaling_config = ScalingConfig(\n    num_workers=4, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n)\n\nrun_config = RunConfig(\n    name=\"ptl-mnist-example\",\n    local_dir=\"/tmp/ray_results\",\n    checkpoint_config=CheckpointConfig(\n        num_to_keep=3,\n        checkpoint_score_attribute=\"val_accuracy\",\n        checkpoint_score_order=\"max\",\n    ),\n)\ntrainer = LightningTrainer(\n    lightning_config=lightning_config,\n    scaling_config=scaling_config,\n    run_config=run_config,\n)\n\nresult = trainer.fit()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"See the full source code ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/examples/lightning/lightning_mnist_example.html#lightning-mnist-example"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements in Ray DataÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The enhancements and features in the release cut across a few dimensions: stability, ease of use, and observability.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data and ease of use: ","nodeType":"text"},{"data":{},"marks":[],"value":"Data ingestion and preprocessing for machine learning (ML) at scale are central to any end-to-end ML.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To enable ease of use, the default backend execution model for Ray Data is streaming-based under the hood. As such developers need not worry about instructing Ray Data to use the streaming execution backend; itâs the default execution mode. Second, the default streaming backend lends itself well to large scale workloads such as setting up distributed training data streaming and batch inference or preprocessing. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4fRyCVJVYH8NAjkcFhTgDX","type":"Entry","createdAt":"2023-04-26T21:30:40.030Z","updatedAt":"2023-04-26T21:30:40.030Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_data_code_snippet","body":"import ray\nfrom ray.data import ActorPoolStrategy\n\nimport time\n\ndef preprocess_func(x):\n   # Do some work with the data x\n   x = ...\n   time.sleep(0.1)\n   return x\n\nfor _ in (\n   ray.data.range_tensor(5000, shape=(80, 80, 3), parallelism=200)\n   .map_batches(preprocess_func, compute=ActorPoolStrategy(size=4))\n   .map_batches(preprocess_func, compute=ActorPoolStrategy(size=3))\n   .map_batches(preprocess_func, num_cpus=1)\n   .iter_batches()\n\n):\n\n   pass","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This launches a simple 4-stage pipeline. We use different compute args for each stage, which forces them to be run as separate operators instead of getting fused together. You should see a log message indicating streaming execution is being used.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While the above example is using dummy tasks, these sort of operator pipelines are a common pattern Ray users desire to scale:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6T6VTlZroKCZ6eyyII0HC1","type":"Asset","createdAt":"2023-04-26T21:34:22.865Z","updatedAt":"2023-04-26T21:34:22.865Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_data_distributed_image","description":"Figure 1. Ray data streaming data with ActorPool ","file":{"url":"//images.ctfassets.net/xjan103pcp94/6T6VTlZroKCZ6eyyII0HC1/d7202c977c1a0c25eeb4563ab5c50319/image3.png","details":{"size":106561,"image":{"width":1249,"height":515}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray data and SQL","nodeType":"text"},{"data":{},"marks":[],"value":": Additionally, we have extended Ray Dataâs ability to fetch data from a common ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/creating-datastreams.html#reading-from-sql-databases"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SQL data source","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", with ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"read_sql(...)","nodeType":"text"},{"data":{},"marks":[],"value":", enabling you to connect to a common SQL data store.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3imkshawnZw7DglAUBNTN3","type":"Entry","createdAt":"2023-04-26T21:36:16.889Z","updatedAt":"2023-04-26T21:36:16.889Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_data_sql_connector_code_snippet","body":"import mysql.connector\n\nimport ray\n\ndef create_connection():\n    return mysql.connector.connect(\n        user=\"admin\",\n        password=...,\nhost=\"example-mysql-database.c2c2k1yfll7o.us-west-2.rds.amazonaws.com\",\n      connection_timeout=30,\n      database=\"example\",\n    )\n\n# Get all movies\ndatastream = ray.data.read_sql(\"SELECT * FROM movie\", create_connection)\n# Get movies after the year 1980\ndatastream = ray.data.read_sql(\n    \"SELECT title, score FROM movie WHERE year \u003e= 1980\", create_connection\n)\n# Get the number of movies per year\ndatastream = ray.data.read_sql(\n    \"SELECT year, COUNT(*) FROM movie GROUP BY year\", create_connection\n)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements to Serve observability","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In 2.4, we are adding new functionality for observability and monitoring for Ray Serve applications. You can use the Ray dashboard to get a high-level overview of your Ray cluster and Ray Serve applicationâs states. This includes details such as:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"the number of deployment replicas currently running","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"logs for your Serve controller, deployment replicas, and HTTP proxies","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"the Ray nodes (i.e., machines) running in your Ray cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can access the Ray dashboard at port 8265 at your clusterâs URI. For example, if youâre running Ray Serve locally, use ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"http://localhost:8265","nodeType":"text"},{"data":{},"marks":[],"value":" in your browser.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3CtBAZRebvQVRJsbp63rD2","type":"Asset","createdAt":"2023-04-26T21:39:28.459Z","updatedAt":"2023-04-26T21:39:28.459Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_serve_observebility","description":"Figure 1. Serve observesability metrics in the Ray Dashboard.","file":{"url":"//images.ctfassets.net/xjan103pcp94/3CtBAZRebvQVRJsbp63rD2/0951a7ad1f4d4ec1ae2a3ba60a3af218/image2.png","details":{"size":146211,"image":{"width":1999,"height":1040}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"For more information on the dashboard, read the documentation for the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/ray-core/ray-dashboard.html#dash-serve-view"},"content":[{"data":{},"marks":[],"value":"Serve page","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"RLlibâs new module for custom reinforcement","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In RLlib, we are introducing a new ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/rllib-rlmodule.html#rl-modules-alpha"},"content":[{"data":{},"marks":[],"value":"RLModule","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" abstraction (alpha). RLModule API provides a unified way to define custom reinforcement learning models in RLlib. This API enables you to design and implement your own models to suit specific needs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To maintain consistency and usability, RLlib offers a standardized approach for defining module objects for both single-agent and multi-agent reinforcement learning environments through the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/package_ref/rl_modules.html"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"SingleAgentRLModuleSpec","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/package_ref/doc/ray.rllib.core.rl_module.marl_module.MultiAgentRLModuleSpec.html#ray.rllib.core.rl_module.marl_module.MultiAgentRLModuleSpec"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"MultiAgentRLModuleSpec","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" classes. The built-in RLModules in RLlib follow this consistent design pattern, making it easier for you to understand and utilize these modules.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1w6lK0sBUAVl4BmOGBQcQ9","type":"Entry","createdAt":"2023-04-26T21:48:25.122Z","updatedAt":"2023-04-26T21:48:25.122Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"rllib_multiagent_code_snippet","body":"import gymnasium as gym\nfrom ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec\nfrom ray.rllib.core.testing.torch.bc_module import DiscreteBCTorchModule\n\nenv = gym.make(\"CartPole-v1\")\n\nspec = SingleAgentRLModuleSpec(\n    module_class=DiscreteBCTorchModule,\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    model_config_dict={\"fcnet_hiddens\": [64]},\n)\n\nmodule = spec.build()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is an experimental module that serves as a general replacement for ModelV2, and is subject to change. It will eventually match the functionality of the previous stack. If you only use high-level RLlib APIs such as ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html#ray.rllib.algorithms.algorithm.Algorithm"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Algorithm","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", you should not experience significant changes, except for a few new parameters to the configuration object. Read more about it in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/rllib/rllib-rlmodule.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Core scaling to 2000 nodes","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Weâve also invested time in making sure Ray can support larger scale workloads. With this release, we are announcing official support for Ray clusters to up to 2000 nodes. See","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/release/benchmarks/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":" scalability envelope","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for more details.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"First and foremost, we want to thank all contributors for their valuable contributions to this new release of ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.4.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2.4","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Your enduring support continues to foster the wider use of Ray adoption.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This release is a milestone in our efforts to offer Ray as a performant compute infrastructure for LLM training, inference, and serving. We shared a number of examples that speak to these efforts.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Additionally, we enhanced Ray Data, improved Serve observability, introduced a new RLlib customization module, and scaled Ray core to support large workloads, up to 2000 nodes. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Have a go at the latest release with pip install âray[default]â and let us know of your feedback. Weâre always interested to hear from you â feel free to reach out to us on ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Github","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discuss","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Whatâs Next?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stay tuned for more Generative AI how-to blog series as we continue our efforts to make Ray the platform choice and compute substrate for all your Generative AI workloads. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you missed our four-part blog series on Generative AI, in which lay out our vision, position, and solution on how Ray addresses the challenges associated with OSS Generative AI workloads, peruse the links below. Much of the above Generative AI examples and features stem from this series.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-ray-solves-common-production-challenges-for-generative-ai"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"How Ray solves common production challenges for generative AI infrastructure","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Training 175B Parameter Language Models at 1000 GPU scale with Alpa and Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/faster-stable-diffusion-fine-tuning-with-ray-air"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Faster stable diffusion fine-tuning with Ray AIR","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"How to fine tune and serve LLMs simply, quickly and cost effectively using Ray + DeepSpeed + HuggingFace","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Building an LLM open source search engine in 100 lines using LangChain and Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Join our ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/community"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Community","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the ","nodeType":"text"},{"data":{"uri":"https://forms.gle/9TSdDYUgxYs8SA9e8"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray #LLM slack channel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have our ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit 2023 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"early-bird registration open. Secure your spot, and save some money.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QDe7dun9hKhLCoGFSXKh9","type":"Asset","createdAt":"2023-04-26T19:23:25.596Z","updatedAt":"2023-04-26T19:23:25.596Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_image","file":{"url":"//images.ctfassets.net/xjan103pcp94/1QDe7dun9hKhLCoGFSXKh9/d9f93233f6ee72608e34735eac545a74/image1.png","details":{"size":154908,"image":{"width":960,"height":540}},"fileName":"image1.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"574o0Bh7HzHlCEc6AXphfF","type":"Entry","createdAt":"2023-04-19T16:00:08.661Z","updatedAt":"2023-05-31T18:34:15.880Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":16,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Building an LLM open source search engine in 100 lines using LangChain and Ray","seoTitle":"Building an LLM Open-Source Search Engine in 100 Lines","slug":"llm-open-source-search-engine-langchain-ray","description":"In part 1 of a new blog series, we show how to build a search engine in 100 lines using LLM embeddings and a vector database.","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-04-18","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 1 of a blog series. In this blog, weâll introduce you to LangChain and Ray Serve and how to use them to build a search engine using LLM embeddings and a vector database. In future parts, we will show you how to turbocharge embeddings and how to combine a vector database and an LLM to create a fact-based question answering service. Additionally, we will optimize the code and measure performance: cost, latency and throughput.\n\n\u003cscript type=\"application/ld+json\"\u003e{\n\"@context\": \"http://schema.org\",\n\"@type\": \"VideoObject\",\n\"name\": \"Open Source LLM Search Engine with LangChain on Ray\",\n\"description\": \"Waleed, Head of Engineering at Anyscale, explains how to use LangChain and Ray Serve to build a search engine using LLM embeddings and a vector database. Blog: https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray\",\n\"thumbnailUrl\": \"https://i.ytimg.com/vi/v7a8SR-sZpI/default.jpg\",\n\"uploadDate\": \"2023-04-19T16:00:41Z\",\n\"duration\": \"PT7M36S\",\n\"embedUrl\": \"https://www.youtube.com/embed/v7a8SR-sZpI\",\n\"interactionCount\": \"4540\"\n}\u003c/script\u003e","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"b9V0HkkRoYEqCnIrMO9kC","type":"Entry","createdAt":"2023-04-19T17:16:15.326Z","updatedAt":"2023-04-19T17:16:15.326Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"LLM Search Engine with Langchain","videoUrl":"https://www.youtube.com/watch?v=v7a8SR-sZpI"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog, we'll cover:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"An introduction to ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and show why itâs awesome.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"An explanation of how Ray complements ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" by:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Showing how with a few minor changes, we can speed parts of the process up by a factor of 4x or more","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Making ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"âs capabilities available in the cloud using Ray Serve","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Using self-hosted models by running ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the model all in the same Ray cluster without having to worry about maintaining individual machines.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introduction","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a very powerful framework for ML orchestration, but with great power comes voluminous documentation. 120 megabytes in fact. How can we make that documentation more accessible? ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Answer: make it searchable! It used to be that creating your own high quality search results was hard. But by using ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we can build it in about 100 lines of code.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is where ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" comes in. ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provides an amazing suite of tools for everything around LLMs. Itâs kind of like ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/"},"content":[{"data":{},"marks":[],"value":"HuggingFace","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" but specialized for LLMs. There are tools (chains) for prompting, indexing, generating and summarizing text. While an amazing tool, using Ray with it can make ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" even more powerful. In particular, it can:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Simply and quickly help you deploy a ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" service.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Rather than relying on remote API calls, allow Chains to run co-located and auto-scalable with the LLMs itself. This brings all the advantages we ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"discussed in a previous blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": lower cost, lower latency, and control over your data.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Building the index","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4OzISThpksdKgjZ0gVJUiB","type":"Asset","createdAt":"2023-04-19T16:59:49.253Z","updatedAt":"2023-05-01T09:40:14.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"doc-index-starter-langchain","description":"Build a document index easily with Ray and Langchain","file":{"url":"//images.ctfassets.net/xjan103pcp94/4OzISThpksdKgjZ0gVJUiB/85bb7fccdfef1df3d061c57e9af1062a/index-langchain.jpg","details":{"size":28918,"image":{"width":900,"height":381}},"fileName":"index-langchain.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"First we will build the index via the following steps.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Download the content we want to index locally.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Read the content and cut it into tiny little pieces (about a sentence each). This is because it is easier to match queries against pieces of a page rather than the whole page.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use the ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/sentence-transformers"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Sentence Transformers","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" library from HuggingFace to generate a vector representation of each sentence.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Embed those vectors in a Vector database (we use ","nodeType":"text"},{"data":{"uri":"https://github.com/facebookresearch/faiss"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FAISS","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", but you could use whatever you like).Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The amazing thing about this code is how simple it is - ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/d06097768abbea54d59e5d3ed4f045f3"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"See Here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". As you will see, thanks to LangChain, all the heavy lifting is done for us. Letâs pick a few excerpts.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Assuming weâve downloaded the Ray docs, this is all we have to do to read all the docs in: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5XcvTPG6LASLRu3arklXJ0","type":"Entry","createdAt":"2023-04-17T08:22:33.719Z","updatedAt":"2023-04-17T08:22:33.719Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-1","body":"loader = ReadTheDocsLoader(\"docs.ray.io/en/master/\")\ndocs = loader.load() ","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The next step is to break each document into little chunks. LangChain uses splitters to do this. So all we have to do is this: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"72vZr3kpoioEza3WTS1vfT","type":"Entry","createdAt":"2023-04-17T08:23:19.379Z","updatedAt":"2023-04-17T08:23:19.379Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-2","body":"chunks = text_splitter.create_documents(\n    [doc.page_content for doc in docs], \n    metadatas=[doc.metadata for doc in docs])","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We want to preserve the metadata of what the original URL was, so we make sure to retain the metadata along with these documents.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now we have the chunks we can embed them as vectors. LLM providers do offer APIs for doing this remotely (and this is how most people use LangChain). But, with just a little bit of glue we can download Sentence Transformers from HuggingFace and run them locally (inspired by LangChainâs support for llama.cpp). ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/aea1d312d68c9431949442cc562d5f2c"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Hereâs the glue code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"By doing so, we reduce latency, stay on open source technologies, and donât need a HuggingFace key or to pay for API usage.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have the embeddings, now we can use a vector database â in this case FAISS â to store the embeddings. Vector databases are optimized for doing quick searches in high dimensional spaces. Again, LangChain makes this effortless. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4YljOvEjiKrfhFRmpA1PDG","type":"Entry","createdAt":"2023-04-17T08:23:35.223Z","updatedAt":"2023-04-17T08:23:35.223Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-3","body":"from langchain.vectorstores import FAISS\n\ndb = FAISS.from_documents(chunks, embeddings)\ndb.save_local(FAISS_INDEX_PATH)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"And thatâs it. The code for this is ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/d0915e52cbe56dff328f5c00ded21107"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Now we can build the store.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"39FzcaaXnaSmciROgXSD0x","type":"Entry","createdAt":"2023-04-17T08:29:45.440Z","updatedAt":"2023-04-17T08:29:45.440Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-python-build-snippet","body":"% python build_vector_store.py"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This takes about 8 minutes to execute. Most of that time is spent doing the embeddings. Of course, itâs not a big deal in this case, but imagine if you were indexing hundreds of gigabytes instead of hundreds of megabytes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Accelerating indexing using Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"[Note: This is a slightly more advanced topic and can be skipped on first reading. It just shows how we can do it more quickly â 4x to 8x times more quickly]","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How can we speed up indexing? The great thing is that embedding is easy to parallelize. What if we:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Sliced the list of chunks into 8 shards.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Embedded each of the 8 shards separately.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Merge the shards.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7tDpD5Q7nxtRyX9lgDvbkI","type":"Asset","createdAt":"2023-04-17T08:04:57.120Z","updatedAt":"2023-04-17T08:04:57.120Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"langchain-ray-accelerated-indexing","description":"Build a document index 4-8x faster with Ray","file":{"url":"//images.ctfassets.net/xjan103pcp94/7tDpD5Q7nxtRyX9lgDvbkI/6209fbd875c5cd379c2289ef6f6554f0/Screen_Shot_2023-04-16_at_6.20.10_PM.png","details":{"size":277144,"image":{"width":2284,"height":936}},"fileName":"Screen Shot 2023-04-16 at 6.20.10 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"One key thing to realize is that embedding is GPU accelerated, so if we want to do this, we need 8 GPUs. Thanks to Ray, those 8 GPUS donât have to be on the same machine. But even on a single machine, there are significant advantages to using Ray. And one does not have to go to the complexity of setting up a Ray cluster, all you need to do is ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"pip install ray[default]","nodeType":"text"},{"data":{},"marks":[],"value":" and then ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"import ray","nodeType":"text"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This requires some minor surgery to the code. Hereâs what we have to do.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"First, create a task that creates the embedding and then uses it to index a shard. Note the Ray annotation and us telling us each task will need a whole GPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3NmJuC8SstZJpoHLrzrLgg","type":"Entry","createdAt":"2023-04-17T08:23:48.410Z","updatedAt":"2023-04-17T08:23:48.410Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-4","body":"@ray.remote(num_gpus=1)\ndef process_shard(shard): \n    embeddings = LocalHuggingFaceEmbeddings('multi-qa-mpnet-base-dot-v1')\n    result = FAISS.from_documents(shard, embeddings)\n    return result\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next, split the workload in the shards. NumPy to the rescue! This is a single line:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7lotIjqpm0Sm03BPHE4t2n","type":"Entry","createdAt":"2023-04-17T08:24:01.572Z","updatedAt":"2023-04-17T08:24:01.572Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-5","body":"shards = np.array_split(chunks, db_shards)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Then, create one task for each shard and wait forÂ the results.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7m8iFk9DvJOBZc3r4maPTY","type":"Entry","createdAt":"2023-04-17T08:24:13.248Z","updatedAt":"2023-04-17T08:24:13.248Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-6","body":"futures = [process_shard.remote(shards[i]) for i in range(db_shards)]\nresults = ray.get(futures)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, letâs merge the shards together. We do this using simple linear merging.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"21KuagZ35WVzVVJ7Xq4qHy","type":"Entry","createdAt":"2023-04-17T08:24:27.839Z","updatedAt":"2023-04-17T08:29:23.908Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-7","body":"db = results[0]\nfor i in range(1,db_shards):\n    db.merge_from(results[i])","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/4c41f3ee66040f57d34c6a40e42b5969"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Hereâs what the sped up code looks like.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"You might be wondering, does this actually work? We ran some tests on a g4dn.metal instance with 8 GPUs. The original code took 313 seconds to create the embeddings, the new code took 70 seconds, thatâs about a 4.5x improvement. Thereâs still some one-time overheads to creating tasks, setting up the GPUs etc. This reduces as the data increases. For example, we did a simple test with 4 times the data, and it was around 80% of the theoretical maximum performance (ie. 6.5x faster vs theoretical maximum of 8x faster from the 8 GPUs).Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can use the Ray Dashboard to see how hard those GPUs are working. Sure enough theyâre all close to 100% running the process_shard method we just wrote. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"14IoasmHxwEeQdAEGJqlyO","type":"Asset","createdAt":"2023-04-17T08:10:19.509Z","updatedAt":"2023-04-17T21:38:58.348Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"accelerated-index-langchain-dashboard","description":"Dashboard shows that GPU utilization is maxed out across all instances","file":{"url":"//images.ctfassets.net/xjan103pcp94/14IoasmHxwEeQdAEGJqlyO/5ae6e6739e258252a78c889ca7959683/raydash.png","details":{"size":278628,"image":{"width":2442,"height":842}},"fileName":"raydash.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"It turns out merging vector databasesÂ  is pretty fast, taking only 0.3 seconds for all 8 shards to be merged.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Serving","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1g6zBePU72Rmz5MBH2reaB","type":"Asset","createdAt":"2023-04-17T08:08:50.696Z","updatedAt":"2023-04-17T08:08:50.696Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Serving-Queries-Ray-Langchain","description":"Serve search queries with Ray and Langchain","file":{"url":"//images.ctfassets.net/xjan103pcp94/1g6zBePU72Rmz5MBH2reaB/db400e9bbbc445d7214d45658f81992f/Screen_Shot_2023-04-16_at_9.42.46_PM.png","details":{"size":380753,"image":{"width":2718,"height":1348}},"fileName":"Screen Shot 2023-04-16 at 9.42.46 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Serving is another area where the combination of LangChain and Ray Serve shows its power. This is just scratching the surface: weâll explore amazing capabilities like independent auto scaling and request batching in our next blog post in the series. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The steps required to do this are: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Load the FAISS database we created and the instantiate the embedding","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Start using FAISS to do similarity searches. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve makes this magically easy. Ray uses a âdeploymentâ to wrap a simple python class. The __init__ method does the loading and then __call__ is what actually does the work. Ray takes care of connecting it to the internet, bringing up a service, http and so on. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hereâs a simplified version of the code: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7GwywaSkEi9NnwUhk3wMMD","type":"Entry","createdAt":"2023-04-17T08:24:42.996Z","updatedAt":"2023-04-17T08:24:42.996Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-8","body":"@serve.deployment\nclass VectorSearchDeployment:\n    def __init__(self):\n        self.embeddings = â¦ \n        self.db = FAISS.load_local(FAISS_INDEX_PATH, self.embeddings)\n\n    def search(self,query): \n        results = self.db.max_marginal_relevance_search(query)\n        retval = \u003csome string processing of the results\u003e\n        return retval\n\n    async def __call__(self, request: Request) -\u003e List[str]:\n        return self.search(request.query_params[\"query\"])\n\ndeployment = VectorSearchDeployment.bind()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Thatâs it! ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs now start this service with the command line (of course Serve has more deployment options than this, but this is an easy way):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"o0cCaJMv4yeC2ikcQuJhf","type":"Entry","createdAt":"2023-04-17T08:29:00.307Z","updatedAt":"2023-04-17T08:29:00.307Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-serve-snippet","body":"% serve run serve_vector_store:deployment"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now we can write a simple python script to query the service to get relevant vectors(itâs just a web server running on port 8000). ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3E0la6cbL8n7UPpvve36nJ","type":"Entry","createdAt":"2023-04-17T08:24:56.873Z","updatedAt":"2023-04-17T08:24:56.873Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-9","body":"import requests\nimport sys\nquery = sys.argv[1]\nresponse = requests.post(f'http://localhost:8000/?query={query}')\nprint(response.content.decode())","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"And now letâs try it out: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SL7auZuJzHEyaq8fVIifv","type":"Entry","createdAt":"2023-04-17T08:25:16.327Z","updatedAt":"2023-04-17T21:14:41.421Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-10","body":"$ python query.py 'Does Ray Serve support batching?'\nFrom http://docs.ray.io/en/master/serve/performance.html\n\nYou can check out our microbenchmark instructions\nto benchmark Ray Serve on your hardware.\nRequest Batching#\n====\n\nFrom http://docs.ray.io/en/master/serve/performance.html\n\nYou can enable batching by using the ray.serve.batch decorator. Letâs take a look at a simple example by modifying the MyModel class to accept a batch.\nfrom ray import serve\nimport ray\n@serve.deployment\nclass Model:\n    def __call__(self, single_sample: int) -\u003e int:\n        return single_sample * 2\n====\n\nFrom http://docs.ray.io/en/master/ray-air/api/doc/ray.train.lightgbm.LightGBMPredictor.preferred_batch_format.html\n\nnative batch format.\nDeveloperAPI: This API may change across minor Ray releases.\n====\n\nFrom http://docs.ray.io/en/master/serve/performance.html\n\nMachine Learning (ML) frameworks such as Tensorflow, PyTorch, and Scikit-Learn support evaluating multiple samples at the same time.\nRay Serve allows you to take advantage of this feature via dynamic request batching.\n===="}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We showed in the above code just how easy it is to build key components of an LLM-based search engine and serve its responses to the entire world by combining the power of LangChain and Ray Serve. And we didnât have to deal with a single pesky API key!Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Tune in for Part 2, where we will show how to turn this into a chatgpt-like answering system. Weâll use open source LLMs like Dolly 2.0 to do that.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"And finally weâll share Part 3 where weâll talk about scalability and cost. The above is fine for a few hundred queries per second, but what if you need to scale to a lot more? And are the claims about latency correct? ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next Steps","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"See part 2 ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/turbocharge-langchain-now-guide-to-20x-faster-embedding"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nReview the code and data used in this blog in the following ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/tree/main/open_source_LLM_search_engine"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Github repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". \n\nSee our earlier ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[],"value":"blog series on solving Generative AI infrastructure","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with Ray.\n\nIf you are interested in learning more about Ray, see ","nodeType":"text"},{"data":{"uri":"http://ray.io"},"content":[{"data":{},"marks":[],"value":"Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"http://docs.ray.io"},"content":[{"data":{},"marks":[],"value":"Docs.Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To connect with the Ray community join #LLM on the ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"data":{},"marks":[],"value":"Ray Slack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or our ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[],"value":"Discuss forum","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","nodeType":"text"},{"data":{"uri":"http://www.anyscale.com/platform"},"content":[{"data":{},"marks":[],"value":"Anyscale.com/Platform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and click the 'Try it now' button","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray Summit 2023:","nodeType":"text"},{"data":{},"marks":[],"value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"practical training focused on LLMs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4OzISThpksdKgjZ0gVJUiB","type":"Asset","createdAt":"2023-04-19T16:59:49.253Z","updatedAt":"2023-05-01T09:40:14.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"doc-index-starter-langchain","description":"Build a document index easily with Ray and Langchain","file":{"url":"//images.ctfassets.net/xjan103pcp94/4OzISThpksdKgjZ0gVJUiB/85bb7fccdfef1df3d061c57e9af1062a/index-langchain.jpg","details":{"size":28918,"image":{"width":900,"height":381}},"fileName":"index-langchain.jpg","contentType":"image/jpeg"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7bptc6mEv7bhFZvq5AOXqc","type":"Entry","createdAt":"2023-04-18T20:36:43.249Z","updatedAt":"2023-04-19T16:00:08.646Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Why I Joined Anyscale: Solving Cutting-Edge Problems in a Time of Enormous Change","seoTitle":"why i joined anyscale by sidney rabsatt","slug":"why-i-joined-anyscale-solving-cutting-edge-problems-in-a-time-of-enormous","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"461P1q4TML68SzYxOG9sxm","type":"Entry","createdAt":"2023-04-18T20:05:20.525Z","updatedAt":"2023-04-18T20:05:20.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Sidney Rabsatt","slug":"sidney-rabsatt","link":"https://www.linkedin.com/in/sidney-rabsatt/"}}],"publishedDate":"2023-04-19","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42F9iHs5a05rRrOuqEQwvB","type":"Entry","createdAt":"2022-06-15T01:30:47.322Z","updatedAt":"2022-06-15T01:30:47.322Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Culture","slug":"culture","tintColor":"#39C8E5"}},"intro":"Why Sidney Rabsatt joined Anyscale as Head of Product.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Throughout my career, I've had the opportunity to work on cutting-edge problems across some of the most impactful technological transitions of our time. From the early days of the World Wide Web to the rise of Mobile and Cloud computing, Iâve worked on numerous commercial products across Networking, Observability, and App/Cloud Infrastructure and on some of the most widely-adopted open-source projects including Wireshark, Nginx, and now Ray. I've been deeply involved in resolving the complexities and, at times, unexpected infrastructure obstacles that technologists encounter, while empowering companies to fully benefit from these advancements.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7uPFFDyMQg5GZ5FU5kM8rw","type":"Asset","createdAt":"2023-04-18T20:34:23.318Z","updatedAt":"2023-04-18T20:34:23.318Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Sidney Rabsatt Headshot","description":"Picture of Sidney Rabsatt, Anyscale Head of Product","file":{"url":"//images.ctfassets.net/xjan103pcp94/7uPFFDyMQg5GZ5FU5kM8rw/225783b86665705f9c198768754f8722/Sidney_pic.jpeg","details":{"size":501555,"image":{"width":3540,"height":2832}},"fileName":"Sidney pic.jpeg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"AI","nodeType":"text"},{"data":{},"marks":[],"value":" promises to be the most complex transition across the most dimensions that affect our lives. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"I joined Anyscale to embrace and be part of solving those challenges. I joined to make AI available to all organizations, to give people better tools, and do it responsibly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray and Anyscale work at the core of the best known and most advanced AI applications such as Generative AI with OpenAIâs ChatGPT and Prediction for Uber rides / ETAs, not to mention what Spotify, Instacart and others are doing.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The team is one of the strongest in the industry, comprised of AI/ML PhDâs and experienced AI experts from top schools like UC Berkeley and companies like Uber, Google, Amazon, Meta, and more!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Our technology is proven and continues to evolve rapidly with strong community involvement. And the opportunity is vast to define how best to develop and run AI.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"AI will touch and shape our lives in many ways and joining Anyscale to lead Product is how Iâm getting involved.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7uPFFDyMQg5GZ5FU5kM8rw","type":"Asset","createdAt":"2023-04-18T20:34:23.318Z","updatedAt":"2023-04-18T20:34:23.318Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Sidney Rabsatt Headshot","description":"Picture of Sidney Rabsatt, Anyscale Head of Product","file":{"url":"//images.ctfassets.net/xjan103pcp94/7uPFFDyMQg5GZ5FU5kM8rw/225783b86665705f9c198768754f8722/Sidney_pic.jpeg","details":{"size":501555,"image":{"width":3540,"height":2832}},"fileName":"Sidney pic.jpeg","contentType":"image/jpeg"}}},"mainImageFit":"contain","hideIntro":true,"showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7e5A9XUzg0NfneWobPOKxf","type":"Entry","createdAt":"2023-04-10T23:01:28.472Z","updatedAt":"2023-05-31T18:31:32.938Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":12,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"How to fine tune and serve LLMs simply, quickly and cost effectively using Ray + DeepSpeed + HuggingFace","seoTitle":"How to Fine-Tune a 6 Billion Parameter LLM for Less Than $7","slug":"how-to-fine-tune-and-serve-llms","description":"In part 4 of our Generative AI series, we share how to build a system for fine-tuning \u0026 serving LLMs in 40 minutes or less.","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2yKvtARlwNkkIYOYLFgkhB","type":"Entry","createdAt":"2022-06-22T19:54:47.912Z","updatedAt":"2022-06-22T19:54:47.912Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jun Gong","slug":"jun-gong"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BGRfsYFKUgIpGjaXTUYml","type":"Entry","createdAt":"2021-08-09T15:38:43.390Z","updatedAt":"2021-08-09T15:38:43.390Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Antoni Baum","slug":"antoni-baum","link":"https://www.linkedin.com/in/yard1/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}}],"publishedDate":"2023-04-10","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 4 of our blog series on Generative AI. In the previous blog posts we explained:\n1.[Why Ray is a sound platform for Generative AI](https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure)\n2.[we showed how it can push the performance limits](https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray)\n3.[how you can use Ray for stable diffusion](https://www.anyscale.com/blog/faster-stable-diffusion-fine-tuning-with-ray-air). \n","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this blog, we share aÂ  practical approach on how you can use the combination of HuggingFace, DeepSpeed, and Ray to build a system for fine-tuning and serving LLMs, in 40 minutes for less than $7 for a 6 billion parameter model. In particular, we illustrate the following:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Using these three components, you can simply and quickly put together an open-source LLM fine-tuning and serving system.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"By taking advantage of Rayâs distributed capabilities, we show how this can be both more cost-effective and faster than using a single large (and often ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://aiascendant.substack.com/p/taiwan-is-pandora-gpus-are-unobtainium"},"content":[{"nodeType":"text","value":"unobtainable","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":") machine.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Hereâs what weâll be doing:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Discussing why you might want to run your own LLM instead of using one of the new API providers.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing you the evolving tech stack we are seeing for cost-effective LLM fine-tuning and serving, combining HuggingFace, DeepSpeed, Pytorch, and Ray.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing you 40 lines of Python code that can enable you to serve a 6 billion parameter GPT-J model.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing you, for less than $7, how you can fine-tune the model to sound more medieval using the works of Shakespeare by doing it in a distributed fashion on low-cost machines, which is considerably more cost-effective than using a single large powerful machine.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing how you can serve the fine-tuned 6B LLM compiled model binary.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing how the fine-tuned model compares to a prompt engineering approach with large systems. ","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Why would I want to run my own LLM?Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"There are ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://openai.com"},"content":[{"nodeType":"text","value":"many","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://anthropic.com/"},"content":[{"nodeType":"text","value":"many","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://cohere.ai/"},"content":[{"nodeType":"text","value":"providers","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" of LLM APIs online. Why would you want to run your own? There are a few reasons:Â ","marks":[],"data":{}}]},{"nodeType":"ordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Cost, especially for fine-tuned inference","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": For example, OpenAI charges 12c per 1000 tokens (about 700 words) for a fine-tuned model on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://openai.com/pricing"},"content":[{"nodeType":"text","value":"Davinci","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Itâs important to remember that many user interactions require multiple backend calls (e.g. one to help with the prompt generation, post-generation moderation, etc), so itâs very possible that a single interaction with an end user could cost a few dollars. For many applications, this is cost prohibitive.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Latency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"using these LLMs is especially slow. A GPT-3.5 query for example can take up to 30 seconds. Combine a few round trips from your data center to theirs and it is possible for a query to take minutes. Again, this makes many applications impossible. Bringing the processing in-house allows you to optimize the stack for your application, e.g. by using low-resolution models, tightly packing queries to GPUs, and so on. We have heard from users that optimizing their workflow has often resulted in a 5x or more latency improvement.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Data Security \u0026 Privacy: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"In order to get the response from these APIs, you have to send them a lot of data for many applications (e.g. send a few snippets of internal documents and ask the system to summarize them). Many of the API providers reserve the right to use those instances for retraining. Given the sensitivity of organizational data and also frequent legal constraints like data residency, this is especially limiting. One, particularly concerning recent development, is the ability to ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://arxiv.org/pdf/2301.13188.pdf"},"content":[{"nodeType":"text","value":"regenerate training data from learned models","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", and people ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.techradar.com/news/samsung-workers-leaked-company-secrets-by-using-chatgpt"},"content":[{"nodeType":"text","value":"unintentionally disclosing secret information","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"How to create and run your own LLMÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The LLM space is an incredibly fast-moving space, and it is currently evolving very rapidly. What we are seeing is a particular technology stack that combines multiple technologies: ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70GhsrIVrh0Qz0fNDUp4A8","type":"Asset","createdAt":"2023-04-07T14:17:45.223Z","updatedAt":"2023-04-07T14:17:45.223Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"llm-stack","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/70GhsrIVrh0Qz0fNDUp4A8/458e076a02c4745369c683851c378536/Screenshot_2023-04-07_at_10.17.23_AM.png","details":{"size":144021,"image":{"width":1073,"height":663}},"fileName":"Screenshot 2023-04-07 at 10.17.23 AM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"What weâve also seen is a reluctance to go beyond a single machine for training. In part, because there is a perception that moving to multiple machines is seen as complicated. The good news is this is where ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io"},"content":[{"nodeType":"text","value":"Ray.io","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" shines (ba-dum-tish). It simplifies cross-machine coordination and orchestration aspects using not much more than Python and Ray decorators, but also is a great framework for composing this entire stack together.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\nRecent results on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html"},"content":[{"nodeType":"text","value":"Dolly","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/lm-sys/FastChat"},"content":[{"nodeType":"text","value":"Vicuna","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" (both trained on Ray or trained on models built with Ray like ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://huggingface.co/EleutherAI/gpt-j-6b"},"content":[{"nodeType":"text","value":"GPT-J","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":") are small LLMs (relatively speaking â say the open source model GPT-J-6B with 6 billion parameters) that can be incredibly powerful when fine-tuned on the right data. The key is fine-tuning and the right data parts. So you do not always need to use the latest and greatest model with 150 billion-plus parameters to get useful results. Letâs get started! \n","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Serving a pre-existing model with Ray for text generation","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The detailed steps on how to serve the GPT-J model with Ray can be found ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/gptj_serving.html"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", so letâs highlight some of the aspects of how we do that. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"56imF50uP87DrWXWfjxNqV","type":"Entry","createdAt":"2023-04-07T18:37:36.999Z","updatedAt":"2023-04-07T18:47:21.234Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"serve-llm","body":"Â Â Â @serve.deployment(ray_actor_options={\"num_gpus\":1})\nÂ Â Â classPredictDeployment:\nÂ Â Â Â Â def__init__(self, model_id:str, revision:str=None):\nÂ Â Â Â Â Â Â Â from transformers import AutoModelForCausalLM, AutoTokenizer\nÂ Â Â Â Â Â Â Â import torch\nÂ Â Â Â Â Â Â Â self.model = AutoModelForCausalLM.from_pretrained(\nÂ Â Â Â Â Â Â Â Â Â Â Â \"EleutherAI/gpt-j-6B\",\nÂ Â Â Â Â Â Â Â Â Â Â Â revision=revision,\nÂ Â Â Â Â Â Â Â Â Â Â Â torch_dtype=torch.float16,\nÂ Â Â Â Â Â Â Â Â Â Â Â low_cpu_mem_usage=True,\nÂ Â Â Â Â Â Â Â Â Â Â Â device_map=\"auto\",Â  # automatically makes use of all GPUs available to the Actor\nÂ Â Â Â Â Â Â Â )\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Serving in Ray happens in actors, in this case, one called PredictDeployment. This code shows the __init__ method of the action that downloads the model from Hugging Face. To launch the model on the current node, we simply do: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7jOn5zhnrYvHQZg3fqmwjE","type":"Entry","createdAt":"2023-04-07T18:39:26.501Z","updatedAt":"2023-04-07T18:47:32.494Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"serve-cmd","body":"deployment = PredictDeployment.bind(model_id=model_id, revision=revision)\nserve.run(deployment)\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"That starts a service on port 8000 of the local machine.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We can now query that service using a few lines of Python","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6HQg2dtvaXrNsKuUvSIX3S","type":"Entry","createdAt":"2023-04-07T18:41:24.773Z","updatedAt":"2023-04-07T18:47:42.594Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"serve-query","body":"import requests\nprompt = (\n    âOnce upon a time, there was a horse. â\n)\nsample_input = {\"text\": prompt}\noutput = requests.post(\"http://localhost:8000/\", json=[sample_input]).json()\nprint(output)","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"And sure enough, this prints out a continuation of the above opening. Each time it runs, there is something slightly different.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\"Once upon a time, there was a horse.","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"But this particular horse was too big to be put into a normal stall. Instead, the animal was moved into an indoor pasture, where it could take a few hours at a time out of the stall. The problem was that this pasture was so roomy that the horse would often get a little bored being stuck inside. The pasture also didnât have a roof, and so it was a good place for snow to accumulate.\"","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is certainly an interesting direction and story â¦ but now we want to set it in the medieval era. What can we do? ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Fine Tuning Your LLM","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now that weâve shown how to serve a model, how do we fine-tune it to be more medieval? What about if we train it on 2500 lines from Shakespeare?Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is where ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.deepspeed.ai/"},"content":[{"nodeType":"text","value":"DeepSpeed","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" comes in. DeepSpeed is a set of optimized algorithms for training and fine-tuning networks. The problem is that DeepSpeed doesnât have an orchestration layer. This is not so much of a problem on a single machine, but if you want to use multiple machines, this typically involves a bunch of bespoke ssh commands, complex managed keys, and so on.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is where Ray can help.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html"},"content":[{"nodeType":"text","value":"page","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" in the Ray documentation discusses how to fine-tune it to sound more like something from the 15th century with a bit of flair.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Letâs go through the key parts. First, we load the data from hugging face","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12gknw4CuwTUwzyVwR5MEH","type":"Entry","createdAt":"2023-04-07T18:42:15.242Z","updatedAt":"2023-04-07T18:47:51.606Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hf-load-data","body":"from datasets import load_dataset\nprint(\"Loading tiny_shakespeare dataset\")\ncurrent_dataset = load_dataset(\"tiny_shakespeare\")\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Skipping the tokenization code, hereâs the heart of the code that we will run for each worker.Â ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5633wVi5PVmd43ciiJRLrv","type":"Entry","createdAt":"2023-04-07T18:43:13.431Z","updatedAt":"2023-04-07T18:48:00.199Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hf-trainer","body":"def trainer_init_per_worker(train_dataset, eval_dataset=None,**config):\nÂ Â Â Â # Use the actual number of CPUs assigned by Ray\nÂ Â Â Â model = GPTJForCausalLM.from_pretrained(model_name, use_cache=False)\nÂ Â Â Â model.resize_token_embeddings(len(tokenizer))\nÂ Â Â Â enable_progress_bar()\nÂ Â Â Â metric = evaluate.load(\"accuracy\")\nÂ Â Â Â trainer = Trainer(\nÂ Â Â Â Â Â Â Â model=model,\nÂ Â Â Â Â Â Â Â args=training_args,\nÂ Â Â Â Â Â Â Â train_dataset=train_dataset,\nÂ Â Â Â Â Â Â Â eval_dataset=eval_dataset,\nÂ Â Â Â Â Â Â Â compute_metrics=compute_metrics,\nÂ Â Â Â Â Â Â Â tokenizer=tokenizer,\nÂ Â Â Â Â Â Â Â data_collator=default_data_collator,\nÂ Â Â Â )\nÂ Â Â Â return trainer\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\nAnd now we create a Ray AIR HuggingFaceTrainer that orchestrates the distributed run and wraps around multiple copies of the training loop above: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"Qo7c20UkHRTIAgy2zvQ4G","type":"Entry","createdAt":"2023-04-07T18:43:58.073Z","updatedAt":"2023-04-07T18:48:09.302Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hf-trainer2","body":"trainer = HuggingFaceTrainer(\nÂ Â Â Â trainer_init_per_worker=trainer_init_per_worker,\nÂ Â Â Â trainer_init_config={\nÂ Â Â Â Â Â Â Â \"batch_size\":16,Â  # per device\nÂ Â Â Â Â Â Â Â \"epochs\":1,\nÂ Â Â Â },\nÂ Â Â Â scaling_config=ScalingConfig(\nÂ Â Â Â Â Â Â Â num_workers=num_workers,\nÂ Â Â Â Â Â Â Â use_gpu=use_gpu,\nÂ Â Â Â Â Â Â Â resources_per_worker={\"GPU\":1,\"CPU\": cpus_per_worker},\nÂ Â Â Â ),\nÂ Â Â Â datasets={\"train\": ray_datasets[\"train\"],\"evaluation\": ray_datasets[\"validation\"]},\nÂ Â Â Â preprocessor=Chain(splitter, tokenizer),\n)\nresults = trainer.fit()\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"While there is some complexity here, it is not much more complex than the code to get it to run on a simple machine. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Fine-tuning and Performance","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"One of the most important topics related to LLMs is the question of cost. In this particular case, the costs are small (in part because we ran only one epoch of fine-tuning, depending on the problem 1-10 epochs of fine-tuning are used, and also in part because this dataset is not so large). But running the tests on different configurations shows us that understanding performance is not always easy. The below shows some benchmarking results with different configurations of machines on AWS. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3k8l58AKMayxIzIhrO7Btr","type":"Entry","createdAt":"2023-04-07T14:39:44.461Z","updatedAt":"2023-04-07T14:39:44.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"fine-tune-performance","body":"\n| Configuration| #instances| Time (mins)| Total Cost (on-demand)|Total Cost (spot)| Cost Ratio|\n| ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n| 16 x g4dn.4xlarge (1 x T4 16GB GPU)|16|48|$15.41|__$6.17__|100%|\n| 32 x g4dn.4xlarge (1 x T4 16GB GPU)|32|__30__|$19.26|$7.71|125%|\n| 1 x p3.16xlarge (8 x V100 16GB GPU)|1|44|$17.95|$9.27|150%|\n| 1 x g5.48xlarge (8 x A10G 24GB GPU)|1|84|$22.81|$10.98|178%|\n"}}},"content":[]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3C7MbWRZ8oYJoE00IW4mIi","type":"Asset","createdAt":"2023-04-11T16:45:53.578Z","updatedAt":"2023-04-11T16:45:53.578Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"llm-graph","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3C7MbWRZ8oYJoE00IW4mIi/93f686b238a84d2ef64abe3aa7670791/Screenshot_2023-04-11_at_12.44.46_PM.png","details":{"size":76511,"image":{"width":1047,"height":644}},"fileName":"Screenshot 2023-04-11 at 12.44.46 PM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Note:","marks":[{"type":"underline"}],"data":{}},{"nodeType":"text","value":" we tried to run the same test with A100s, but we were unable to obtain the p4d machines to do so.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Looking at these numbers, we see some surprises:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Perhaps the most obvious machine to use â the g5.48xlarge â the machine with the highest on-paper performance â is both the most expensive and the slowest at almost twice the price when using spot instances.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The p3.16xlarge with its use of NVLink between the GPUs is a considerably better option.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Most surprising of all, using multiple machines is both the ","marks":[],"data":{}},{"nodeType":"text","value":"cheapest ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"and the ","marks":[],"data":{}},{"nodeType":"text","value":"fastest ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"option.Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The exact same code is running on all the machines, and aside from tweaking the number of GPU workers, nothing else was changed. Using multiple machines gave us the cheapest (16 machines) and the fastest (32 machines) option of the ones we benchmarked.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is the beauty and power of Ray. The code itself was simple enough, and in fact, was able to use a standard library âÂ  DeepSpeed â with no modifications. So it was no more complex in this case than a single machine. Simultaneously, it gave more options and flexibility to optimize to be both cheaper and faster than a single machine.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Closing the loop: Serving the fine-tuned model","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now that we have a fine-tuned model, letâs try to serve it. The only change we need to make is to (a) copy the model to s3 from the fine-tuning process and (b) load it from there. In other words, the only change from the previous code we started with originally is: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"202WfToEkr4LafSdvnynbx","type":"Entry","createdAt":"2023-04-07T18:45:09.318Z","updatedAt":"2023-04-07T18:48:20.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"load-model","body":"        checkpoint = Checkpoint.from_uri(\n             \"s3://demo-pretrained-model/gpt-j-6b-shakespeare\"\n        )\n        with checkpoint.as_directory() as dir:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                dir,\n                torch_dtype=torch.float16,\n                low_cpu_mem_usage=True,\n                device_map=\"auto\")\n            self.tokenizer = AutoTokenizer.from_pretrained(dir)\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"And now letâs try querying it again:Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Once upon a time there was a horse. This horse was in my youth, a little unruly, but yet the best of all. I have, sir; I know every horse in the field, and the best that I have known is the dead. And now I thank the gods, and take my leave.","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"As you can see, it definitely has more of a Shakespearean flavor.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Conclusion","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We have shown a new tech stack that combines Ray, HuggingFace, DeepSpeed, and PyTorch to make a system that:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Makes it simple and quick to deploy as a service.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Can be used to cost-effectively fine-tune and is actually most cost-effective when using multiple machines without the complexity.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"How fine-tuning â even a single epoch â can change the output of a trained model.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Deploying a fine-tuned model is only marginally harder than deploying a standard one.Â ","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Next Steps","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If you want to use LangChain + Ray to serve LLM's, see our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"nodeType":"text","value":"LangChain blog series","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in learning more about Ray, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io"},"content":[{"nodeType":"text","value":"Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://docs.ray.io"},"content":[{"nodeType":"text","value":"Docs.Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"To connect with the Ray community join #LLM on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"Ray Slack","marks":[],"data":{}}]},{"nodeType":"text","value":" or our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"Discuss forum","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in our Ray hosted service for ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/model-serving"},"content":[{"nodeType":"text","value":"ML Training and Serving","marks":[],"data":{}}]},{"nodeType":"text","value":", see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://www.anyscale.com/platform"},"content":[{"nodeType":"text","value":"Anyscale.com/Platform","marks":[],"data":{}}]},{"nodeType":"text","value":" and click the 'Try it now' button","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Summit 2023:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"nodeType":"text","value":"Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"nodeType":"text","value":"practical training focused on LLMs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"19zr72hDLSKFt8vQMz3hb6","type":"Asset","createdAt":"2023-04-11T00:38:37.097Z","updatedAt":"2023-04-11T00:38:37.097Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"fine-tune-stack","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/19zr72hDLSKFt8vQMz3hb6/fd9f6b83a9fe5b66456ae54ecf9bb04d/fine-tune-stack.png","details":{"size":344489,"image":{"width":1716,"height":1180}},"fileName":"fine-tune-stack.png","contentType":"image/png"}}}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70ZthWUkgA42DqmZ1GVmuM","type":"Entry","createdAt":"2022-06-15T16:41:59.066Z","updatedAt":"2022-06-15T16:43:47.038Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"blog-types-tags","body":"This section is used to order the \"Types\" and \"Tags\" that show up for filters on the Blog Index","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"56lORqEsxxZXgpuGzAhJBC","type":"Entry","createdAt":"2022-06-15T16:42:23.797Z","updatedAt":"2022-06-15T16:44:24.730Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"list"}},"locale":"en-US"},"fields":{"title":"Types","identifier":"blog-type-filters","items":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42F9iHs5a05rRrOuqEQwvB","type":"Entry","createdAt":"2022-06-15T01:30:47.322Z","updatedAt":"2022-06-15T01:30:47.322Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Culture","slug":"culture","tintColor":"#39C8E5"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KeH0JzrIEpuheazhMAhKf","type":"Entry","createdAt":"2022-06-15T01:31:23.950Z","updatedAt":"2022-06-15T01:31:23.950Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"User Story","slug":"user-story","tintColor":"#6D59C6"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1fDHWgr5HgjPURy6aaDlnB","type":"Entry","createdAt":"2022-06-15T16:42:41.243Z","updatedAt":"2022-06-22T15:37:31.744Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"list"}},"locale":"en-US"},"fields":{"title":"Products / Libraries","identifier":"blog-tag-filters","items":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i6fp6Gfdy5wFeDDOaKcfh","type":"Entry","createdAt":"2021-12-03T22:33:42.248Z","updatedAt":"2021-12-03T22:33:42.248Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Anyscale","identifier":"anyscale"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}}]}}]}}],"recommendations":[],"articles":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Ts2gaWudS6ZXPHFLIz8bx","type":"Entry","createdAt":"2022-04-26T14:25:38.319Z","updatedAt":"2023-01-23T06:20:36.659Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"What is distributed training?","slug":"what-is-distributed-training","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"xHBDAcVEhd1SMmcRMKZrK","type":"Entry","createdAt":"2022-04-21T14:15:11.162Z","updatedAt":"2022-04-21T14:15:11.162Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Keith Pijanowski","slug":"keith-pijanowski"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2022-04-26","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"In this article, weâll introduce distributed training and how it works by parallelizing the workload across multiple processors (data parallelism or model parallelism). Then, weâll discuss how to choose between distributed machine learning tools.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"If youâre new to distributed training, or youâre looking for a refresher on what distributed training is and how it works, youâve come to the right place.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this article, weâll start with a quick definition of distributed training and the problem it solves. Then, weâll explore different things you should take into consideration when youâre selecting a distributed training tool, and introduce you to Ray Train, a one-stop distributed training toolkit.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs jump in.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introducing distributed training","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Training machine learning models is a slow process. To compound this problem, successful models â those that make accurate predictions â are created by running many experiments, where engineers play with different options for model creation.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"These options include feature engineering, choosing an algorithm, selecting a network topology, using different loss functions, trying various optimization functions, and experimenting with varying batch sizes and epochs. A handful of best practices help narrow these options based on your data and the prediction you hope to make. Ultimately, though, you still have to run many experiments. The faster your experiments execute, the more experiments you can run, and the better your models will be.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed machine learning addresses this problem by taking advantage of recent advances in distributed computing. The goal is to use low-cost infrastructure in a clustered environment to parallelize training models.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Kubernetes is the most popular cluster example. Deploying machine learning models on Kubernetes in a cloud machine learning environment (AWS, Azure, or GCP) enables us to access hundreds of instances of a model-training service. When we divide and distribute the training workload within this cluster, model-training time improves from hours to minutes versus training on a local workstation.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Data parallelism and model parallelism","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"When we parallelize our training model, weâre dividing our workload across multiple processors, or workers, in order to speed up the training process. There are two main types of parallelization. ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Data parallelism","nodeType":"text"},{"data":{},"marks":[],"value":" is when we divide our training data across our available workers and run a copy of the model on each worker. Each worker then runs a different fragment of the data on the same model. In contrast, ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"model (or network) parallelism","nodeType":"text"},{"data":{},"marks":[],"value":" is when we divide our model across different workers, with each worker running the same data on a different segment of the model.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3dXMEU8MDlwyreIB7bFMwI","type":"Asset","createdAt":"2022-04-21T14:09:23.789Z","updatedAt":"2022-04-21T14:09:23.789Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-what-is-distributed-training-data-vs-model-parallelism","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3dXMEU8MDlwyreIB7bFMwI/9c755e4a7c5aa9f314c49cbeac21ab4c/blog-what-is-distributed-training-data-vs-model-parallelism.png","details":{"size":105653,"image":{"width":1500,"height":1000}},"fileName":"blog-what-is-distributed-training-data-vs-model-parallelism.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The intricacies of data and model parallelism are beyond the scope of this article, but know that the approach should be based on the size of the model â model parallelism comes in handy in situations where the model is too big for any single worker, such as natural language processing and large-scale deep learning.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The gist is that distributed training tools spread the training workload within a cluster and on a local workstation with multiple CPUs. Letâs review the distributed training landscape to get a feel for all the options available in the marketplace.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Choosing a distributed machine learning training tool","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are many distributed machine learning toolkits. Some are extensions of existing products, a few are APIs that come with existing frameworks, and some are frameworks themselves with their own object models. Each has its pros and cons to consider.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you want to get started quickly with an effective distributed machine learning toolkit, consider these requirements when shopping:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Easy installation procedures that donât require infrastructure expertise.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Workstation friendly. Engineers like to play around with their toys locally before deciding if a tool is worth incurring additional costs on a cloud machine learning cluster. A distributed training tool should efficiently run on an engineerâs workstation and take advantage of multiple CPUs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Supports tools like Jupyter Notebooks for efficient experimentation.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Easy to use. Engineers shouldnât be impacted by the complexities of distributed computing, allowing them to focus their attention on training and fine-tuning models.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Fault-tolerant. If an instance of your cluster fails, the framework should be smart enough to reassign the work. You shouldnât have to start your training over.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is designed with these concerns in mind. This framework promises to be easy to install â youâre a pip install away from your machine learning deployment and tinkering with your model. You donât have to set up a cluster to play with Ray Train. It uses multiple processes on your workstation to seamlessly emulate a clustered environment. Also, Ray Train can run on low-cost infrastructure since itâs fault-tolerant and can recover from the failure of individual instances.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Learn more","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"If youâre ready to learn more about Ray Train, check out the following resources:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Meetup: ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/events/2022/03/03/ray-train-pytorch-torchx-and-distributed-deep-learning"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train, PyTorch, TorchX, and distributed deep learning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Webinar: ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/events/2022/02/09/ray-train-production-ready-distributed-deep-learning"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train: Production-ready distributed deep learning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Blog: ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/distributed-deep-learning-with-ray-train-is-now-in-beta"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Distributed deep learning with Ray Train is now in Beta","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4pncjIZFAz5pKUlzwi2Agb","type":"Asset","createdAt":"2022-04-21T14:10:09.832Z","updatedAt":"2022-04-21T14:10:09.832Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-what-is-distributed-training-thumb","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4pncjIZFAz5pKUlzwi2Agb/c6eb6957fafd8042133799b33490d4a2/blog-what-is-distributed-training-thumb.png","details":{"size":148964,"image":{"width":1500,"height":1000}},"fileName":"blog-what-is-distributed-training-thumb.png","contentType":"image/png"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Zk7jixFrLqHf3IJLzrO2B","type":"Entry","createdAt":"2022-05-03T18:47:31.422Z","updatedAt":"2022-05-03T18:47:31.422Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Ray Summit: Aug 23-24 in SF!","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5LTWfPAScfUtzXj2KOUALz","type":"Entry","createdAt":"2022-04-12T15:29:42.674Z","updatedAt":"2022-10-04T22:06:07.976Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":17,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"summit"}},"locale":"en-US"},"fields":{"title":"Ray Summit 2022","slug":"ray-summit-2022","description":"Centered on Ray, the open-source distributed computing framework, Ray Summit brings together ML practitioners, data scientists, and more to discuss the growing demand for distributed computing.","sections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xIJZXdAqhOEizUCUuQaiV","type":"Entry","createdAt":"2022-03-31T16:04:49.746Z","updatedAt":"2022-09-12T20:58:36.991Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":20,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-hero","header":"The Place for _Everything Ray_","body":"Missed us in San Francisco for Ray Summit 2022? Don't fret, you can watch your favorite sessions on demand! ","ctaText":"View Sessions","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"9F9S7NLaIdPL4ThYwVlFW","type":"Asset","createdAt":"2022-09-06T14:49:38.936Z","updatedAt":"2022-09-06T14:53:02.350Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Presented by Anyscale","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/9F9S7NLaIdPL4ThYwVlFW/8ecd09b334942298f0b8c2d5567ec032/presented-by-anyscale.svg","details":{"size":22813,"image":{"width":289,"height":32}},"fileName":"presented-by-anyscale.svg","contentType":"image/svg+xml"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4A0d4TUfp0TCqUA60CXopR","type":"Asset","createdAt":"2022-09-06T14:53:48.720Z","updatedAt":"2022-09-06T14:53:48.720Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4A0d4TUfp0TCqUA60CXopR/2e4240c268d074d840f7a00d069a768a/rs-collage_2x.jpg","details":{"size":568547,"image":{"width":2400,"height":1376}},"fileName":"rs-collage@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6FGUP0cXunTbgs2UemeAU","type":"Asset","createdAt":"2022-09-06T14:54:09.975Z","updatedAt":"2022-09-06T14:54:09.975Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event, stacked vertically","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6FGUP0cXunTbgs2UemeAU/16ed175024bc9cc0a2fae3cb13538dff/rs-collage-mobile_2x.jpg","details":{"size":243282,"image":{"width":750,"height":1420}},"fileName":"rs-collage-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4RKa7qVhWoIaoNSa1ZH2SK"}}],"type":"2022 Summit Hero","styling":["transparent-header"]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6GL7ZzuSLadWdVFKHj7yIH","type":"Entry","createdAt":"2022-04-20T18:00:12.964Z","updatedAt":"2022-08-08T22:50:29.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-keynote-speakers","header":"Keynote Speakers","ctaText":"See all speakers","ctaLink":"/ray-summit-2022/speakers","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6VhewtcICU9SCXjwd5qhHJ"}}],"type":"2022 Summit Speakers","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JTi5fsyo0Q3YrF3SAQglg","type":"Entry","createdAt":"2022-08-08T22:49:55.163Z","updatedAt":"2022-08-09T07:03:38.615Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-ray-summit-selected-speakers","header":"Highlighted Talks","ctaText":"Full Agenda","ctaLink":"/ray-summit-2022/agenda","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"50eS4cKXBNE4GHezdZ8Rws"}},{"sys":{"type":"Link","linkType":"Entry","id":"5iioKfOgEqVzgQ1DMmA1Wy"}},{"sys":{"type":"Link","linkType":"Entry","id":"YCBUZOXJGB0qcJF8TKjfI"}}],"type":"2022 Summit Agenda Preview"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5cgHuoJ713DewhVY6iUYrG","type":"Entry","createdAt":"2022-05-03T21:57:45.147Z","updatedAt":"2022-05-09T13:41:03.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-trainings","header":"Exclusive Ray Training Sessions","body":"Ray Summit is a great opportunity to level up your Ray skills through hands-on training sessions with the builders and maintainers of Ray. Weâre offering several pre-conference training sessions on August 22. Space is limited â [reserve your spot now](https://www.myeventi.events/raysummit22/attendee/)!","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3sYQTUmD5X5ncvGRFyIRR6","type":"Asset","createdAt":"2022-05-03T04:40:25.040Z","updatedAt":"2022-05-03T04:40:25.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image desktop","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3sYQTUmD5X5ncvGRFyIRR6/a77c675b07d63e73dd79ada9d182498a/training-image_2x.jpg","details":{"size":313509,"image":{"width":1136,"height":1480}},"fileName":"training-image@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7rsC56a4YCgENLyxsHyeIb","type":"Asset","createdAt":"2022-05-03T04:40:54.089Z","updatedAt":"2022-05-03T04:40:54.089Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image mobile","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7rsC56a4YCgENLyxsHyeIb/e2592c9b4424048b3c916fbbbc313065/training-image-mobile_2x.jpg","details":{"size":153801,"image":{"width":830,"height":874}},"fileName":"training-image-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4Hb3J9SrCFTDXqIhIdSIwa"}},{"sys":{"type":"Link","linkType":"Entry","id":"hZ4Tl1wvj7CwF00slwpLg"}},{"sys":{"type":"Link","linkType":"Entry","id":"YJ5KWyjg4NGf29NrfHcJX"}}],"type":"2022 Summit Trainings"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mRJQtuDbIhAuEbwLT2V1K","type":"Entry","createdAt":"2022-03-31T21:42:56.117Z","updatedAt":"2022-04-11T23:46:07.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-reasons-to-attend","header":"Why Attend?","body":"As the annual user conference for the [open source Ray community](https://www.ray.io/community), Ray Summit is a unique opportunity to enhance your Ray skills, build your professional network, and learn what's on the Ray roadmap.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"5f6gxk4a4ujE2y6FZ8ATQ5"}},{"sys":{"type":"Link","linkType":"Entry","id":"20QwyiOmkHCEOkMvp0Jhtk"}},{"sys":{"type":"Link","linkType":"Entry","id":"6QxivsuhhLoVSloRQs9ZlQ"}}],"type":"2022 Summit Content Tiles"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"15k0SGqB2ZKOUfAsM3JnyV","type":"Entry","createdAt":"2022-03-31T16:05:55.739Z","updatedAt":"2022-08-05T00:25:20.225Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":16,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-about","header":"Celebrate the Ray community","body":"Get ready for our epic community bash at the Art of Tinkering exhibit at the San Francisco Exploratorium on August 23 from 5:30 to 9:30 p.m. Come explore, play, and celebrate with the Ray Community. This is an exclusive opportunity to experience this amazing interactive exhibition before it closes in September.","ctaText":"Check out the agenda","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UcZT4h1P7uFDz6cn6JH8l","type":"Asset","createdAt":"2022-06-02T20:37:22.299Z","updatedAt":"2022-06-02T20:37:22.299Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Man taking a photo on his phone","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2UcZT4h1P7uFDz6cn6JH8l/5040a9909f18ab015c27b220077f916e/taking-picture.jpg","details":{"size":107846,"image":{"width":980,"height":772}},"fileName":"taking-picture.jpg","contentType":"image/jpeg"}}}],"type":"2022 Summit Split Image","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4P8Jn0q7nBU05iD6DmcQCC","type":"Entry","createdAt":"2022-04-01T19:15:31.735Z","updatedAt":"2022-04-11T16:36:59.573Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-social-tiles","header":"Join the Conversation","body":"Ready to get involved in the Ray community before the conference? Ask a question in the forums. Open a pull request. Or share why youâre excited with the hashtag __#RaySummit__ on Twitter.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6OMTlXkJ1OHXNkEjyPrXTH"}},{"sys":{"type":"Link","linkType":"Entry","id":"7EE28ee1ATQtodoLloT6mg"}},{"sys":{"type":"Link","linkType":"Entry","id":"1CRAzFxndL37wiNg4x8WKV"}},{"sys":{"type":"Link","linkType":"Entry","id":"2qrec5rR8kabZxCHaPS7NH"}}],"type":"2022 Summit Social Tiles"}}],"headerLogo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70DIdxwT8XY96aw7vMUHOu","type":"Asset","createdAt":"2022-06-01T15:21:46.804Z","updatedAt":"2022-06-01T15:21:46.804Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo Dark Blue","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/70DIdxwT8XY96aw7vMUHOu/5ca0a8b7533c8df70bc0e032b889862a/rs-logo-nav.svg","details":{"size":9954,"image":{"width":108,"height":48}},"fileName":"rs-logo-nav.svg","contentType":"image/svg+xml"}}},"headerLogoWhite":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3qyLXKGgGfWhv4oPqYl0SE","type":"Asset","createdAt":"2022-04-20T21:50:58.774Z","updatedAt":"2022-04-20T21:50:58.774Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3qyLXKGgGfWhv4oPqYl0SE/c4dee9e7a344c39db11fefafbf255b5b/rs-logo-horizontal.svg","details":{"size":9975,"image":{"width":108,"height":48}},"fileName":"rs-logo-horizontal.svg","contentType":"image/svg+xml"}}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KUtRQbdqJo1RrWMrVyfEm","type":"Entry","createdAt":"2022-03-31T16:21:13.402Z","updatedAt":"2022-04-08T17:25:33.228Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Home","link":"/ray-summit-2022"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4MvQcRSkHLMqIJoO9O2cQe","type":"Entry","createdAt":"2022-03-31T16:17:27.336Z","updatedAt":"2022-09-12T16:59:51.221Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"On-Demand","link":"/ray-summit-2022/agenda","subLinks":[{"sys":{"type":"Link","linkType":"Entry","id":"3N9kRDz36tieTI4YGVWKBJ"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46B5diwEDDr3kB2XDoGfDP","type":"Entry","createdAt":"2022-05-11T17:13:42.174Z","updatedAt":"2022-05-11T17:13:42.174Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Speakers","link":"/ray-summit-2022/speakers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Dt5fy6A7rBpXdG7HeKMGd","type":"Entry","createdAt":"2022-03-31T16:17:43.592Z","updatedAt":"2022-03-31T16:17:43.592Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Sponsors","link":"/ray-summit-2022/sponsors"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SvRE9pnyq3BXUPGKRWx6W","type":"Entry","createdAt":"2022-06-03T19:29:31.009Z","updatedAt":"2022-06-03T19:29:31.009Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Who Attends","link":"/ray-summit-2022/who-attends"}}],"mainSponsors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"xMQIOU9n7gVS4mkX2Nlag","type":"Entry","createdAt":"2021-01-31T16:33:06.495Z","updatedAt":"2021-05-11T13:28:31.312Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Ray","url":"https://ray.io","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jLroqm98RCMLGASthyuPv","type":"Asset","createdAt":"2021-05-11T13:28:26.930Z","updatedAt":"2021-05-11T13:28:26.930Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/jLroqm98RCMLGASthyuPv/f76cc04827316de5d3f8b529f6a3f02c/Ray_BM.svg","details":{"size":4742,"image":{"width":93,"height":36}},"fileName":"Ray_BM.svg","contentType":"image/svg+xml"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11syIHhtX8JajSlXHSHE1O","type":"Entry","createdAt":"2021-01-31T16:31:24.509Z","updatedAt":"2021-05-11T13:28:00.309Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Anyscale","url":"https://www.anyscale.com","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UkDmTzmZHIOG7yxFP4sVL","type":"Asset","createdAt":"2021-05-11T13:27:55.219Z","updatedAt":"2021-05-11T13:27:55.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Anyscale","file":{"url":"//images.ctfassets.net/xjan103pcp94/5UkDmTzmZHIOG7yxFP4sVL/db10aebda9a9aad10903470781a6f2e6/ANSC_BM.svg","details":{"size":7431,"image":{"width":158,"height":36}},"fileName":"ANSC_BM.svg","contentType":"image/svg+xml"}}}}}],"footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cfwECWrOqJ5Fegr3YpReS","type":"Entry","createdAt":"2022-04-01T21:22:12.001Z","updatedAt":"2022-04-13T17:35:29.511Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-footer","body":"Â©Anyscale, Inc 2022\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/privacy-policy\"\u003ePrivacy Policy\u003c/a\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/code-of-conduct\"\u003eCode of Conduct\u003c/a\u003e"}},"recommendations":[]}},"ctaText":"Register now","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5nTlh9josVs5zpZqU3vTbn","type":"Asset","createdAt":"2022-05-03T18:47:25.125Z","updatedAt":"2022-05-03T18:47:25.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-ray-summit-2022","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5nTlh9josVs5zpZqU3vTbn/e66043f62ed4a4b53d7e182e50289ec3/blog-recommended-content-ray-summit-2022.png","details":{"size":17234,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-ray-summit-2022.png","contentType":"image/png"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"mL2pcw31fmyQcsj7INsPN","type":"Entry","createdAt":"2022-04-25T21:41:59.997Z","updatedAt":"2022-04-25T21:41:59.997Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Ray Train: Production-ready distributed deep learning","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"daYk6TnKli7zugQmS8OY1","type":"Entry","createdAt":"2022-01-31T21:06:54.819Z","updatedAt":"2022-06-28T16:49:15.161Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":17,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"event"}},"locale":"en-US"},"fields":{"title":"Ray Train: Production-ready distributed deep learning","slug":"ray-train-production-ready-distributed-deep-learning","startDate":"2022-02-09T09:00-08:00","endDate":"2022-02-09T10:00-08:00","category":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3IY1nTIqpsQSkyGN72wgym","type":"Entry","createdAt":"2022-06-28T16:20:55.714Z","updatedAt":"2022-06-28T17:42:17.187Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"eventCategory"}},"locale":"en-US"},"fields":{"title":"Webinar","slug":"webinars"}},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7kS5Mc1LeZRPqVlItSGMNI","type":"Asset","createdAt":"2022-01-25T05:17:10.580Z","updatedAt":"2022-06-06T23:16:27.147Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"locale":"en-US"},"fields":{"title":"Ray Train","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7kS5Mc1LeZRPqVlItSGMNI/28c234fac2d658dbca32e136b2030396/1382642_BlogImageIllustration-11_2_060322.jpg","details":{"size":545837,"image":{"width":1500,"height":1000}},"fileName":"1382642_BlogImageIllustration-11_2_060322.jpg","contentType":"image/jpeg"}}},"videoLink":"https://youtu.be/oQ0N-G7MsTw","summary":"Today, most frameworks for deep learning prototyping, training, and distributing to a cluster are either powerful and inflexible, or nimble and toy-like. Data scientists are forced to choose between a great developer experience and a production-ready framework. \n\nTo fix this gap, the Ray ML team has developed Ray Train. ","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train is a library built on top of the Ray ecosystem that simplifies distributed deep learning. Currently in stable beta in Ray 1.9, Ray Train offers the following features: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scales to multi-GPU and multi-node training with zero code changes ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Runs seamlessly on any cloud (AWS, GCP, Azure, Kubernetes, or on-prem) ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Supports PyTorch, TensorFlow, and Horovod ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed data shuffling and loading with Ray Datasets ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed hyperparameter tuning with Ray Tune","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Built-in loggers for TensorBoard and MLflow ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this webinar, we'll talk through some of the challenges in large-scale computer vision ML training, and show a demo of Ray Train in action.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Resources","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://speakerdeck.com/anyscale/ray-train-production-ready-distributed-deep-learning"},"content":[{"data":{},"marks":[],"value":"Slides \u003e\u003e","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-5"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.google.com/document/d/1zRgwyabrffjy9sffw_XNfu8GkrNrv3bLOodgbRRTnsk/edit"},"content":[{"data":{},"marks":[],"value":"Webinar Q\u0026A \u003e\u003e","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-5"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/matthewdeng/ray-train-demos"},"content":[{"data":{},"marks":[],"value":"GitHub: Ray Train demos \u003e\u003e","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-5"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/train.html"},"content":[{"data":{},"marks":[],"value":"Ray Train Docs \u003e\u003e","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-5"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/architecture.html"},"content":[{"data":{},"marks":[],"value":"How Ray Train parallelizes data and distributes computation \u003e\u003e","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"heading-5"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"speakers":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3XZBgZdQF0b0hXsYTk9ymf","type":"Entry","createdAt":"2021-10-06T16:35:26.676Z","updatedAt":"2021-10-19T16:04:46.969Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Will Drevo","position":"Product Manager, Anyscale","affiliation":"Anyscale","bio":"Will is a Product Manager for ML at Anyscale. Previously, he was the first ML Engineer at Coinbase, and ran a couple of ML-related startups, one in the data labeling space and the other in the pharmaceutical space. He has a BS in CS and Music Composition from MIT, and did his master's thesis at MIT in machine learning systems. In his spare time, he produces electronic music, travels, and tries to find the best Ethiopian food in the Bay Area.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7nlMFH6QxymR43jMVfhLpp","type":"Asset","createdAt":"2021-10-06T16:35:20.496Z","updatedAt":"2021-10-06T16:35:20.496Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Will Drevo","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7nlMFH6QxymR43jMVfhLpp/e0d43d13afa5117a8ed6e814638b671a/Will_Drevo.jpeg","details":{"size":20940,"image":{"width":361,"height":361}},"fileName":"Will_Drevo.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6xUgY2DL1lQTVYZi9BwOts","type":"Entry","createdAt":"2022-01-28T23:25:48.801Z","updatedAt":"2022-01-31T22:27:32.120Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Matthew Deng","position":"Software Engineer ","affiliation":"Software Engineer, Anyscale","bio":"Matthew Deng is a software engineer at Anyscale where he works on distributed machine learning libraries built on top of Ray. Before that, he was a software engineer at LinkedIn. He holds a BS in Electrical Engineering and Computer Science from UC Berkeley.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1qWWz3bTjcePpvnRxihDzU","type":"Asset","createdAt":"2022-01-31T22:27:26.678Z","updatedAt":"2022-01-31T22:27:26.678Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Matthew Deng","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1qWWz3bTjcePpvnRxihDzU/5872139b24bb33b347b74c7a585d0e26/Matt_Deng.jpg","details":{"size":39301,"image":{"width":435,"height":435}},"fileName":"Matt_Deng.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3SvWPYblmudEPyDCFJPf6g","type":"Entry","createdAt":"2021-04-23T22:29:26.083Z","updatedAt":"2021-05-14T16:07:26.109Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","position":"Software Engineer, Anyscale","affiliation":"Anyscale","bio":"Amog Kamsetty is a software engineer at Anyscale where he works on building distributed training libraries and integrations on top of Ray. He previously completed his MS degree at UC Berkeley working with Ion Stoica on machine learning for database systems.\n","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ai9DjjcGVGsx5wCIQgNrE","type":"Asset","createdAt":"2021-04-23T22:27:16.833Z","updatedAt":"2021-04-23T22:27:16.833Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/6ai9DjjcGVGsx5wCIQgNrE/e0339f546995da6cb3f8a408d3fd76a7/Amog_Kamsetty.jpg","details":{"size":847883,"image":{"width":2125,"height":2125}},"fileName":"Amog_Kamsetty.jpg","contentType":"image/jpeg"}}}}}],"form":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7C3GkiBGQ7PWOCkfQD6XKF","type":"Entry","createdAt":"2022-01-28T23:11:30.765Z","updatedAt":"2022-02-10T19:52:02.867Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"form"}},"locale":"en-US"},"fields":{"identifier":"Ray-train-(part 1)-Jan-2022","title":"View Presentation","formId":"377fc345-40b8-42e7-bc20-eaa745cf6233","submitText":"Watch on-demand"}},"gatedType":"On-demand","gatedDescription":"Register below to access video recording","tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}}],"recommendations":[]}},"ctaText":"Watch on demand","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4pvt4qBgpargy3hSa4cICE","type":"Asset","createdAt":"2022-03-24T19:32:14.959Z","updatedAt":"2022-03-24T19:32:14.959Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-neural-net-light","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4pvt4qBgpargy3hSa4cICE/f573f93ebc1283f5e78948e9a6bf0ff3/blog-recommended-content-neural-net-light.jpg","details":{"size":42583,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-neural-net-light.jpg","contentType":"image/jpeg"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3g8UxypZTmhxIeC11LjyDZ","type":"Entry","createdAt":"2022-04-25T21:43:21.449Z","updatedAt":"2022-04-25T21:43:21.449Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Distributed deep learning with Ray Train is now in beta","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"24GtFGUCPeM44gdlGTnhJ2","type":"Entry","createdAt":"2022-01-25T21:01:15.754Z","updatedAt":"2022-06-22T16:03:03.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":10,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Distributed deep learning with Ray Train is now in Beta","slug":"distributed-deep-learning-with-ray-train-is-now-in-beta","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"9prabYTxr4y4oV4rUkzaP","type":"Entry","createdAt":"2022-01-25T05:14:31.358Z","updatedAt":"2022-01-25T05:14:31.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Matthew Deng","slug":"matthew-deng"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DwU0wh6AmJoGv8JOy6n0Q","type":"Entry","createdAt":"2020-11-11T23:34:15.165Z","updatedAt":"2021-02-05T22:22:08.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","slug":"amog-kamsetty","link":"https://www.linkedin.com/in/amogkamsetty/","bio":"Amog is a software engineer at Anyscale. He helps maintain and develop two machine learning libraries: Ray Tune for distributed hyperparameter tuning and RaySGD for distributed deep learning training.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J134jX2ybZRLPfcAIGLTI","type":"Asset","createdAt":"2020-11-13T00:05:05.533Z","updatedAt":"2020-11-13T00:05:05.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J134jX2ybZRLPfcAIGLTI/12589bb0c6792051050ebc6e1e83af25/Amog_Kamsetty_.jpg","details":{"size":57660,"image":{"width":480,"height":640}},"fileName":"Amog Kamsetty .jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7fm8mbdJEOBiNMuOJKqLJj","type":"Entry","createdAt":"2021-08-09T15:40:10.306Z","updatedAt":"2021-08-09T15:40:10.306Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Will Drevo","slug":"will-drevo","link":"https://www.linkedin.com/in/willdrevo/"}}],"publishedDate":"2022-01-25","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},"intro":"Introducing Ray Train, an easy-to-use library for distributed deep learning. In this post, we show how Ray Train improves developer velocity, is production-ready, and comes with batteries included.","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7kS5Mc1LeZRPqVlItSGMNI","type":"Asset","createdAt":"2022-01-25T05:17:10.580Z","updatedAt":"2022-06-06T23:16:27.147Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"locale":"en-US"},"fields":{"title":"Ray Train","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7kS5Mc1LeZRPqVlItSGMNI/28c234fac2d658dbca32e136b2030396/1382642_BlogImageIllustration-11_2_060322.jpg","details":{"size":545837,"image":{"width":1500,"height":1000}},"fileName":"1382642_BlogImageIllustration-11_2_060322.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Ray is simplifying the APIs of its ML ecosystem as it heads towards Ray 2.0. This blog announces a core feature, distributed deep learning, as part of a broader series of changes to the Ray ML ecosystem.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Todayâs distributed deep learning tools suffer from a major problem: there exists a wide gap between prototyping and production model training.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Existing solutions such as Kubeflow and Sagemaker force practitioners to make a tradeoff between developer velocity and scalability.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To address this gap, we built ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a library that simplifies distributed training. Currently in its Beta release, it offers the following features:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scale to multi-GPU and multi-node training with 0 code changes","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Runs seamlessly on any cloud (AWS, GCP, Azure, Kubernetes, or on-prem)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Supports PyTorch, TensorFlow, and HorovodÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed data loading and hyperparameter tuning","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Built-in loggers for TensorBoard and MLflow","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The upcoming roadmap for Ray Train can be found ","nodeType":"text"},{"data":{"uri":"#next-steps"},"content":[{"data":{},"marks":[],"value":"below","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nIn this post, we will introduce some of the benefits and values that Ray Train provides for distributed deep learning training today. We will showcase examples of using Ray Train with ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", but they can be adapted to work with ","nodeType":"text"},{"data":{"uri":"https://www.tensorflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"TensorFlow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://github.com/horovod/horovod"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Horovod","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" as well.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Background: Why Ray Train?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Existing solutions for distributed training often fall on either side of the wide gap between prototyping and production model training. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The prototyping side is populated with frameworks and libraries that are lightweight, focusing on development velocity and fast iteration, such as Huggingface Transformers and PyTorch Lightning. These frameworks are targeted towards data scientists -- and leave the burden of cluster management and operations to the MLOps practitioner, who has to manage these frameworks at scale.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"On the other side are heavyweight production frameworks like ","nodeType":"text"},{"data":{"uri":"https://www.kubeflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Kubeflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/sagemaker/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". The Kubeflow Training Operator provides a solution for distributed training on Kubernetes, but it lacks ease of use for development. Debugging a distributed training environment often requires relaunching the entire job and waiting for multiple minutes to see results.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We wanted to build a framework that could bring the best of both worlds together -- extremely fast iteration while making it really easy to scale on different cluster environments.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We built Ray Train with the following requirements in mind:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Developer velocity:","nodeType":"text"},{"data":{},"marks":[],"value":" Reduce the friction to go from training on your laptop to training on any distributed cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Production-ready:","nodeType":"text"},{"data":{},"marks":[],"value":" Run end-to-end distributed model training with first class support for cloud compute and experiment monitoring.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Batteries included: ","nodeType":"text"},{"data":{},"marks":[],"value":"Capable of integrating with third party libraries and comes with powerful built-in integrations for large-scale data loading and distributed hyperparameter optimization.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Developer Velocity","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The number one request we heard among developers and data scientists was to reduce the iterative cycle of development from a local or Jupyter environment to a production environment.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Existing solutions will often make you wait for instances to start up in order to run your training script every time you make a code change or can be very complicated to integrate.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train is fast to integrate, easy to scale, and allows you to iterate very quickly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train is fast to integrate","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Quickly distribute your existing PyTorch, TensorFlow, or Horovod code with five simple steps.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For example, an existing PyTorch training script can be converted to run across ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"},{"type":"code"}],"value":"num_workers=4","nodeType":"text"},{"data":{},"marks":[],"value":" worker processes with the following changes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5qzveTSs9ZzECtRFB9bJe2","type":"Entry","createdAt":"2022-01-25T05:35:18.795Z","updatedAt":"2022-01-25T20:22:23.215Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"num_workers","body":"def train_func(): # 1. Wrap code in a function.\n    â¦ # Existing model and dataloader setup.\n    model = train.torch.prepare_model(model)  # 2. Sets up data parallelism.\n    dataloader = train.torch.prepare_data_loader(dataloader) # 2. Samples data.\n    for _ in range(num_epochs):\n        â¦ # Existing training loop.\n\nnum_workers = 4 # 3. Define your parallelization factor. \ntrainer = Trainer(backend=\"torch\", num_workers=num_workers) # 3. Initialize Trainer.\ntrainer.start() # 3. Setup worker processes.\nresults = trainer.run(train_func) # 4. Run training function.\ntrainer.shutdown() # 5. Tear down worker processes.","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Whatâs happening with these code changes?Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Wrap your code in a function: ","nodeType":"text"},{"data":{},"marks":[],"value":"Ray Train packages and sends this function to run on each worker process.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Use Ray Train âprepareâ utility functions: ","nodeType":"text"},{"data":{},"marks":[],"value":"These set up data parallelism when run on multiple workers (under the hood, this uses ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Distributed Data Parallelism","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":").","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Instantiate Ray Train trainer:","nodeType":"text"},{"data":{},"marks":[],"value":" Define your training backend and number of workers. Worker processes are started and the backend communication framework (e.g., Torch process group) is set up.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Run training function: ","nodeType":"text"},{"data":{},"marks":[],"value":"The training function is executed on each worker and results are communicated back to the Trainer.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Tear down work processes: ","nodeType":"text"},{"data":{},"marks":[],"value":"Clean up processes and release resources.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"To see this in action, see ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/ray-sgd"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for a simple example you can run directly on your laptop.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2ItHq3Ulyi5tawW4lpKGwx","type":"Entry","createdAt":"2022-01-25T20:37:50.063Z","updatedAt":"2022-01-25T20:37:50.063Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"inlineImage"}},"locale":"en-US"},"fields":{"name":"Scale to multiple machines","id":"scale-to-multiple-machines"}}},"content":[],"nodeType":"embedded-entry-inline"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train is easy to scale","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train provides simple solutions for three common scaling patterns: distributing across multiple machines, training on GPUs, and running on a remote cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scale to multiple machinesÂ ","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray, scaling Ray Train from your laptop to a multi-node setup is handled entirely by ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/quickstart.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"setting up your Ray cluster","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The same Ray Train script running locally can be run on a Ray cluster with multiple nodes without any additional modifications, just as if it were running on a single machine with more resources. You can further increase ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"num_workers","nodeType":"text"},{"data":{},"marks":[],"value":" to increase your training parallelism and utilize your cluster resources.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Training with multiple GPUs","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"GPU training can be toggled via the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/api.html#trainer"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Trainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". If your Ray cluster has GPUs, you can turn on GPU training by enabling the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"use_gpu","nodeType":"text"},{"data":{},"marks":[],"value":" flag when initializing the Trainer. Each worker process will be associated with a single GPU (which may be on the same or different machines), and by default it will use ","nodeType":"text"},{"data":{"uri":"https://developer.nvidia.com/nccl"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"NCCL","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for communication.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Tying this together, to scale your workload across 100 GPUs, you would update your Trainer:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3pywnZ0m9jIkIpAnwS2paa","type":"Entry","createdAt":"2022-01-25T05:37:14.980Z","updatedAt":"2022-01-25T20:24:28.828Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"trainer","body":"trainer = Trainer(backend=âtorchâ, num_workers=100, use_gpu=True)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Run on a remote cluster","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"Submit Ray Train jobs to a remote cluster from your laptop, another server, or a Jupyter notebook easily usingÂ  ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/ray-client.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Client","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (for interactive runs) or with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/job-submission.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Jobs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (for production-ready runs).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train is quick to iterateÂ ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"While distributed training infrastructure solutions such as ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/sagemaker/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://www.kubeflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Kubeflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" offer scaling, what really makes Ray Train stand out is its focus on developer productivity.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"When it comes to iterative development, it is invaluable to run code immediately after writing it.Â  Take a look at how long it takes to re-run a distributed training script after making a code change on two 8-GPU nodes.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2VnsApQWKvDMmo2yCfIkxO","type":"Asset","createdAt":"2022-01-25T05:38:40.412Z","updatedAt":"2022-01-25T17:55:09.509Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"locale":"en-US"},"fields":{"title":"Iteration time","description":"**Image:** Ray Train has minimal overhead for iterative development.","file":{"url":"//images.ctfassets.net/xjan103pcp94/2VnsApQWKvDMmo2yCfIkxO/ba09718eee51a427e4d5abadcfffb7cd/Iteration_time.png","details":{"size":13222,"image":{"width":512,"height":317}},"fileName":"Iteration time.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this example, we chose to use a ","nodeType":"text"},{"data":{"uri":"https://sagemaker-examples.readthedocs.io/en/latest/training/distributed_training/pytorch/data_parallel/mnist/pytorch_smdataparallel_mnist_demo.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"familiar MNIST training script","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" which has a training loop that executes in one minute. The difference in iteration time occurs during the startup period between writing code and running code -Â  with Ray Train there is ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"zero ","nodeType":"text"},{"data":{},"marks":[],"value":"additional overhead in starting up instances and minimal time spent setting up the distributed processes, so you can start your training function within ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"seconds","nodeType":"text"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The reason for the difference in startup time is that in Ray, you are able to reuse a cluster when iterating. While there still exists a one-time cost of starting up the cluster, subsequent runs are much faster. So not only can you ","nodeType":"text"},{"data":{"uri":"#scale-to-multiple-machines"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"run the same code on your laptop as your cluster","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", but you can also iteratively test your code directly on a distributed cluster!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Production Ready","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"When it comes to moving a training job to production, there are many additional aspects to consider such as the cost of long-running jobs, the ability to run on specific clouds or Kubernetes, and having the right monitoring and experiment tracking functionality. Ray Train was built with these production-level requirements in mind.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Save compute costs with spot instances and fault toleranceÂ ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"When training large or long running jobs, the cost of compute instances can easily grow to significant amounts. Ray Train solves this by providing built-in fault tolerance which allows you to run distributed training workers on spot instances, reducing the cost of your workflow by up to 90%. When a spot instance is interrupted, Ray Train will restore the worker on a new instance and resume training.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7lch3KnediIlze2Qvbn1RL","type":"Asset","createdAt":"2022-01-25T05:41:49.043Z","updatedAt":"2022-01-25T17:55:31.213Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"locale":"en-US"},"fields":{"title":"timeline of fault tolerance ","description":"**Image:** Timeline of fault tolerance handling logic.","file":{"url":"//images.ctfassets.net/xjan103pcp94/7lch3KnediIlze2Qvbn1RL/28c3ae9bf5b21289bba28c9b414a6f85/image_2.jpg","details":{"size":50886,"image":{"width":960,"height":528}},"fileName":"image 2.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Fault tolerance can be enabled by implementing logic to ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/user_guide.html#checkpointing"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"save and load checkpoints","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deploy anywhere: Multi-cloud and Kubernetes-ready","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Spending tons of time and developer resources to build out a ML training system that is tightly coupled to a proprietary cloud provider can limit flexibility in the future and/or lead to high cost of compute.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Developing on Ray with Ray Train allows teams to avoid cloud lock-in, and deploy their training or ML platforms with or without ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/kuberay"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Kubernetes anywhere,","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" including on-prem.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Monitor with integrated tools","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train has a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/api.html#trainingcallback"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"TrainingCallback","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" interface that can be used to process intermediate results (e.g., at the end of a training epoch). A few out-of-the-box callbacks are available, some of which integrate with your favorite monitoring tools.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1aEUHnH9e48UwVllKtNMFB","type":"Asset","createdAt":"2022-01-25T05:43:44.570Z","updatedAt":"2022-01-25T17:55:54.202Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"locale":"en-US"},"fields":{"title":"Callback","description":"**Image:** Using callbacks to integrate with different monitoring tools.","file":{"url":"//images.ctfassets.net/xjan103pcp94/1aEUHnH9e48UwVllKtNMFB/bd62c7b88410a130c61ac65adfaa8a21/image_3.jpg","details":{"size":24395,"image":{"width":960,"height":384}},"fileName":"image 3.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the near future, we plan to add additional callbacks for other integrations. In the meantime, the user can also extend the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"TrainingCallback","nodeType":"text"},{"data":{},"marks":[],"value":" API to define their own custom callback logic!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Batteries Included","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"A distributed training framework should allow developers to be flexible in incorporating tools and utilities, but also should not require them to code vital functionality from scratch. We wanted to let developers leverage as much of the open-source data ecosystem as possible.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed Data Loading with Ray Datasets","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Large-scale training jobs simply cannot be cost-effective without efficient data loading. To solve this problem, Ray Train integrates with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to perform distributed data loading. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5wnUyp19R8KFHBT0dNwlQ0","type":"Asset","createdAt":"2022-01-25T05:45:46.468Z","updatedAt":"2022-01-25T17:56:06.635Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"locale":"en-US"},"fields":{"title":"Ray dataset","description":"**Image:** Passing a Ray Dataset allows data to be sharded across the workers.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5wnUyp19R8KFHBT0dNwlQ0/4e908d0fa0b7f94d8bec87c8de327691/image_4.jpg","details":{"size":18858,"image":{"width":960,"height":288}},"fileName":"image 4.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This integration unlocks a number of features.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Sharded datasets:","nodeType":"text"},{"data":{},"marks":[],"value":" Only the fraction of the data that is needed by each worker will be transferred to and read by the worker, enabling you to train on large datasets that do not fit into a single nodeâs memory.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Windowed datasets:","nodeType":"text"},{"data":{},"marks":[],"value":" Only apply the chain of Dataset operations to a subset of the data at a time, limiting the working set size. This allows you to efficiently train on large datasets that do not fit into your clusterâs memory at the same time.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Pipelined execution","nodeType":"text"},{"data":{},"marks":[],"value":": When training on one batch of data, the next batch will be processed/loaded in parallel. This can reduce GPU idle time and decrease overall job training time.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Global shuffling: ","nodeType":"text"},{"data":{},"marks":[],"value":"Shuffling of the entire dataset between epochs to optimize training performance over no shuffling or local shuffling.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6e3xRK00DmcmW6LcwEqckr","type":"Asset","createdAt":"2022-01-25T05:47:13.103Z","updatedAt":"2022-01-25T17:56:22.060Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"Ray dataset 2","description":"**Image:** Ray Datasets provides an 8x improvement over other solutions.","file":{"url":"//images.ctfassets.net/xjan103pcp94/6e3xRK00DmcmW6LcwEqckr/906fad0ccdefc6fb75eeb6713686737a/Ray_train_ray_dataset.png","details":{"size":23147,"image":{"width":512,"height":303}},"fileName":"Ray train ray dataset.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"By pairing distributed training with distributed data, we see large improvements in the size, quality, and speed of large-scale ML data ingestion. For a more comprehensive walkthrough, see ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/deep-dive-data-ingest-in-a-third-generation-ml-architecture"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Deep Dive: Data Ingest in a Third Generation ML Architecture","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hyperparameter Optimization with Ray Tune","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train provides an integration with Ray Tune that allows you to perform ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/what-is-hyperparameter-tuning"},"content":[{"data":{},"marks":[],"value":"hyperparameter optimization","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in just a few lines of code.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2e900UukxFKzACy5Lt1Fsa","type":"Entry","createdAt":"2022-01-25T05:48:11.435Z","updatedAt":"2022-01-25T20:24:47.396Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"trainable","body":"trainable = trainer.to_tune_trainable(train_func)\nanalysis = tune.run(trainable, config=...)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Tune will create one Trial per hyperparameter configuration. In each Trial, a new Trainer will be initializedÂ  and run the training function with its generated configuration.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"SI7vjcdFwQ7RhRqXYHcaH","type":"Asset","createdAt":"2022-01-25T05:50:04.519Z","updatedAt":"2022-01-25T17:56:39.472Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"trainer trainable ","description":"**Image:** Using Ray Tune to conduct a distributed hyperparameter search.","file":{"url":"//images.ctfassets.net/xjan103pcp94/SI7vjcdFwQ7RhRqXYHcaH/8c6a449eff70340479d747835a17353e/image_5.jpg","details":{"size":20619,"image":{"width":960,"height":288}},"fileName":"image 5.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This native integration provides a few conveniences:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Minimal changes: ","nodeType":"text"},{"data":{},"marks":[],"value":"Training function utilities (","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"train.report()","nodeType":"text"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"train.checkpoint()","nodeType":"text"},{"data":{},"marks":[],"value":") are directly translated to their Tune equivalents (","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"tune.report()","nodeType":"text"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"tune.checkpoint()","nodeType":"text"},{"data":{},"marks":[],"value":").","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Resource management:","nodeType":"text"},{"data":{},"marks":[],"value":" Resource requirements are defined when initializing the Trainer, so you do not need to explicitly define any resource requests in your Tune experiment.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"For examples, see ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/examples.html#ray-tune-integration-examples"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train Examples - Ray Tune Integration Examples","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Flexibility: use anything in the Python ecosystemÂ ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Itâs important that a distributed training framework doesnât box you in or limit your flexibility.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray is and always will be open-source, and will work with anything in the Python ecosystem.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Loading data with Dask? No problem! Run Dask with the performance of Ray with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dask-on-ray.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Dask on Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ETLs with Apache Spark? Again, this is common. Run Spark with the speed of Ray with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/raydp.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Spark on Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", or simply call out to a Ray cluster from within your PySpark notebook with the interactive ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/ray-client.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Client","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or run a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/job-submission.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Job","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ultimately, we feel the ML ecosystem is so broad and multifaceted that the best solution for distributed training will be both open-source and flexible. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next Steps ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As of today, Deep Learning on Ray Train is officially in Beta.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"As development continues, Ray Train will be extended with a focus on integrating with both Ray and third party libraries. This includes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Dataset preprocessing and feature transformations to operate on Ray Datasets","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Streamlined model exporting to Ray Serve for model serving","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Expanding beyond deep learning to integrate with XGBoost-Ray, LightGBM-Ray, and HuggingFace Transformers","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Elastic training to support dynamically scaling the number of training workers for improved speed and performance","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Support for advanced deep learning models such as Graph Neural Networks and Embeddings","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Support for alternative distributed training strategies such as asynchronous parameter servers","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Numerous performance optimizations such as FP16 compression","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Improved profiling and monitoring with tools such as PyTorch Profiler","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Integrations with more of your favorite experimentation tracking tools","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"To learn more about Ray Train, you can visit the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". If you are already using Ray Train, weâd love to hear your feedback through the ","nodeType":"text"},{"data":{"uri":"https://forms.gle/PXFcJmHwszCwQhqX7"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"User Survey","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Lastly, If you have any questions about Ray Train, you can reach out to the Ray community on ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/c/raytrain/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discourse","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Slack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Contributions: ","nodeType":"text"},{"data":{},"marks":[],"value":"We are actively seeking development partners and open-source committers, so please ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/issues"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"drop a Github issue","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or get in contact if youâre interested!","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7kS5Mc1LeZRPqVlItSGMNI","type":"Asset","createdAt":"2022-01-25T05:17:10.580Z","updatedAt":"2022-06-06T23:16:27.147Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"locale":"en-US"},"fields":{"title":"Ray Train","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7kS5Mc1LeZRPqVlItSGMNI/28c234fac2d658dbca32e136b2030396/1382642_BlogImageIllustration-11_2_060322.jpg","details":{"size":545837,"image":{"width":1500,"height":1000}},"fileName":"1382642_BlogImageIllustration-11_2_060322.jpg","contentType":"image/jpeg"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}}],"recommendations":[]}},"thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"duY6EV2R5ypECLntNMPSm","type":"Asset","createdAt":"2022-03-21T15:49:23.115Z","updatedAt":"2022-03-21T22:35:23.764Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-recommended-content-gears-light2","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/duY6EV2R5ypECLntNMPSm/852cd8fd8d6ffecdaaf0a197684ac953/blog-recommended-content-gears-light2.jpg","details":{"size":46823,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-gears-light2.jpg","contentType":"image/jpeg"}}},"recommendations":null}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}}],"hideIntro":true,"showMainImage":false,"thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2sxKXnh9U3PHl8xypej4Ue","type":"Asset","createdAt":"2022-03-24T22:19:37.849Z","updatedAt":"2022-03-24T22:19:37.849Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-neural-net-dark","file":{"url":"//images.ctfassets.net/xjan103pcp94/2sxKXnh9U3PHl8xypej4Ue/c97c00f800206db3df12103d2c8fb827/blog-recommended-content-neural-net-dark.jpg","details":{"size":45208,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-neural-net-dark.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"SGSAdmkKVUNJneHpqtmmf","type":"Entry","createdAt":"2021-12-15T17:05:36.387Z","updatedAt":"2022-06-22T16:07:17.345Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"How to Speed Up XGBoost Model Training","slug":"how-to-speed-up-xgboost-model-training","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-12-15","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"XGBoost is an open-source implementation of gradient boosting designed for speed and performance. However, even XGBoost training can sometimes be slow. This post reviews some approaches for accelerating this process like changing tree construction method, leveraging cloud computing, distributed XGBoost on Ray.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Gradient boosting algorithms are widely used in supervised learning. While they are powerful, they can take a long time to train. Extreme gradient boosting, or ","nodeType":"text"},{"data":{"uri":"https://xgboost.readthedocs.io/en/latest/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"XGBoost","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", is an open-source implementation of gradient boosting designed for speed and performance. However, even XGBoost training can sometimes be slow.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are quite a few approaches to accelerating this process like:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Changing tree construction method","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Leveraging cloud computing","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/xgboost-ray.html#:~:text=XGBoost%2DRay%20integrates%20with%20Ray,training%20run%20parallelized%20by%20itself."},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Distributed XGBoost on Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"This article will review the advantages and disadvantages of each approach as well as go over how to get started. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Changing your tree construction algorithm","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"XGBoostâs ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"tree_method","nodeType":"text"},{"data":{},"marks":[],"value":" parameter allows you to specify which tree construction algorithm you want to use. Choosing an appropriate tree construction algorithm (","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"exact","nodeType":"text"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"approx","nodeType":"text"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"hist","nodeType":"text"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"gpu_hist","nodeType":"text"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"auto","nodeType":"text"},{"data":{},"marks":[],"value":") for your problem can help you produce an optimal model faster. Letâs now review the algorithms.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://xgboost.readthedocs.io/en/latest/treemethod.html#exact-solution"},"content":[{"data":{},"marks":[{"type":"bold"},{"type":"underline"}],"value":"exact","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"It is an accurate algorithm, but it is not very scalable as during each split find procedure it iterates over all entries of input data. In practice, this means long training times. It also doesnât support distributed training. You can learn more about this algorithm in the original ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1603.02754"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"XGBoost paper","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions"},"content":[{"data":{},"marks":[{"type":"bold"},{"type":"underline"}],"value":"approx","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While the exact algorithm is accurate, it is inefficient when the data does not completely fit into memory. The approximate tree method from the original ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1603.02754"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"XGBoost paper","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" uses quantile sketch and gradient histograms.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions"},"content":[{"data":{},"marks":[{"type":"bold"},{"type":"underline"}],"value":"hist","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"An approximation tree method used in ","nodeType":"text"},{"data":{"uri":"https://lightgbm.readthedocs.io/en/latest/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LightGBM","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with slight differences in implementation (uses some performance improvements such as bins caching) from ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"approx","nodeType":"text"},{"data":{},"marks":[],"value":". This is typically faster than ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"approx","nodeType":"text"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions"},"content":[{"data":{},"marks":[{"type":"bold"},{"type":"underline"}],"value":"gpu_hist","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"As GPUs are critical for many machine learning applications, XGBoost has a GPU implementation of the hist algorithm ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"gpu_hist","nodeType":"text"},{"data":{},"marks":[],"value":") that has support for external memory. ","nodeType":"text"},{"data":{"uri":"https://xgboost.readthedocs.io/en/latest/gpu/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"It is much faster and uses considerably less memory than hist","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Note that XGBoost doesnât have ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"native support","nodeType":"text"},{"data":{},"marks":[],"value":" for GPUs on some operating systems.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"47eFqX9KLtkgY6uBi1xx3f","type":"Asset","createdAt":"2021-12-15T03:57:05.108Z","updatedAt":"2021-12-15T03:57:53.293Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"GPUMac","description":"[XGBoost documentation](https://xgboost.readthedocs.io/en/latest/install.html#python)","file":{"url":"//images.ctfassets.net/xjan103pcp94/47eFqX9KLtkgY6uBi1xx3f/47b981107ea560ad2f1ff5ba5b79997a/GPUMac.png","details":{"size":39557,"image":{"width":858,"height":364}},"fileName":"GPUMac.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions"},"content":[{"data":{},"marks":[{"type":"bold"},{"type":"underline"}],"value":"auto","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is the default value for the parameter. Based on the dataset size, XGBoost will choose the âfastest methodâ. For small datasets, exact will be used. For larger datasets, approx will be used. Note that hist and gpu_hist arenât considered in this heuristic based approach even though they are often faster.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you run this ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/mGalarnyk/16d15183f691594bc2c256505a4c42b1"},"content":[{"data":{},"marks":[],"value":"code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", you will see how running models using gpu_hist can save a lot of time. On a relatively small dataset (100,000 rows, 1000 features) on my computer, changing from hist to gpu_hist decreased training time by about a factor of 2. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Leveraging cloud computing","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ETLioF3e6PLPYr5DggvUl","type":"Asset","createdAt":"2021-12-15T04:04:23.119Z","updatedAt":"2021-12-15T04:04:44.130Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"localMachineCloud","description":"Cloud computing allows you to not only utilize more cores and memory than your local machine, but can also give you access to specialized resources like GPUs.","file":{"url":"//images.ctfassets.net/xjan103pcp94/ETLioF3e6PLPYr5DggvUl/7f8247e1bf79ab49295882259bd5420d/localMachineCloud.png","details":{"size":149389,"image":{"width":2222,"height":1549}},"fileName":"localMachineCloud.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The last section was mostly about choosing more efficient algorithms in order to better use of available computational resources. However, sometimes available computational resources are not enough and you simply need more. For example, the MacBook shown in the image below only has 4 cores and 16GB memory. Furthermore, it runs on MacOS which at the time of this writing, XGBoost doesnât have GPU support for. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1KlGtgN1iAELLGtFdydxa5","type":"Asset","createdAt":"2021-12-15T04:06:20.879Z","updatedAt":"2021-12-15T04:06:20.879Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"laptop","description":" For the purpose of this post, you can think of the MacBook above as a single node with 4 cores.","file":{"url":"//images.ctfassets.net/xjan103pcp94/1KlGtgN1iAELLGtFdydxa5/5426fc9577edc3aeaa62f197196d51bb/laptop.png","details":{"size":146824,"image":{"width":806,"height":479}},"fileName":"laptop.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A way around this problem is to utilize more resources on the cloud. Utilizing cloud providers arenât free, but they often allow you to utilize more cores and memory than your local machine. Additionally, if XGBoost doesnât have support for your local machine, it is easy to choose an instance type that XGBoost has GPU support for.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you would like to try speeding up your training on the cloud, below is an overview of the steps from ","nodeType":"text"},{"data":{"uri":"https://machinelearningmastery.com/train-xgboost-models-cloud-amazon-web-services/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Jason Brownleeâs article","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on how to train an XGBoost model on an AWS EC2 instance:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"1. Setup an AWS account (if needed)","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"2. Launch an AWS Instance","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"3. Login and run the code","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"4. Train an XGBoost model","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"5. Close the AWS Instance (only pay for the instance when you are using it)","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you select a more powerful instance than what you have locally, youâll likely see that training on the cloud is faster. ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Note that multi-GPU training with XGBoost actually requires distributed training which means you need more than a single node/instance to accomplish this","nodeType":"text"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed XGBoost Training with Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"So far, this tutorial has covered speeding up training by changing the tree construction algorithm and by increasing computing resources through cloud computing. Another solution is to distribute XGBoost model training with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/xgboost-ray.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"XGBoost-Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" which leverages Ray.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What is Ray?","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a fast, simple distributed execution framework that makes it easy to scale your applications and to leverage state of the art machine learning libraries. Using Ray, you can take Python code that runs sequentially and with minimal code changes transform it into a distributed application. If you would like to learn about Ray and the ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Actor_model"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"actor model","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", you can learn about it ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2gIjsp5y2eK5kCOpXWXzvh","type":"Asset","createdAt":"2021-12-15T07:31:20.075Z","updatedAt":"2021-12-15T07:31:20.075Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray ecosystem","description":"While this tutorial explores how Ray makes it easy to parallelize and distribute XGBoost code, it is important to note that Ray and its ecosystem also make it easy to distribute plain Python code as well as existing libraries like [scikit-learn](https://www.anyscale.com/blog/how-to-speed-up-scikit-learn-model-training), [LightGBM](https://www.anyscale.com/blog/introducing-distributed-lightgbm-training-with-ray), [PyTorch](https://medium.com/pytorch/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-fd83c98fdead), and much more. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/2gIjsp5y2eK5kCOpXWXzvh/bc27ab46cb2860ac7c139c61e70ff771/RayEcosystem.png","details":{"size":181020,"image":{"width":1050,"height":400}},"fileName":"RayEcosystem.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"How to get started with XGBoost-Ray","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"To get started with XGBoost-Ray, ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/xgboost-ray.html#installation"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"you first need to install it","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"pip install \"xgboost_ray\"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Since it is fully compatible with the core XGBoost API, all you need is a few code changes to scale XGBoost training from a single machine to a cluster with hundreds of nodes. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4RM5nTOjDfaaKcFGF8VDzP","type":"Asset","createdAt":"2021-12-15T07:34:48.267Z","updatedAt":"2021-12-15T07:48:35.518Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"multiGPU","description":" XGBoost-Ray supports multi-node/multi-GPU training. On a machine, GPUs communicate gradients via NCCL2. Between nodes, they use Rabit instead ââ([learn more](https://www.anyscale.com/blog/distributed-xgboost-training-with-ray)).","file":{"url":"//images.ctfassets.net/xjan103pcp94/4RM5nTOjDfaaKcFGF8VDzP/83c8aadc9281e98e1c1b2e4ae7ce73b8/multiGPUBlog.png","details":{"size":94221,"image":{"width":836,"height":290}},"fileName":"multiGPUBlog.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"As you can see in the code below, the API is very similar to XGBoost. The highlighted portions are where the code is different than the normal XGBoost API.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1OzLI7K3hTgzkME4dW9Ci3","type":"Entry","createdAt":"2021-12-15T07:39:28.888Z","updatedAt":"2021-12-15T07:49:48.978Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"body":"from xgboost_ray import RayXGBClassifier, RayParams\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\nseed = 42\n\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=0.25, random_state=42\n)\n\nclf = RayXGBClassifier(\n    n_jobs=4,  # In XGBoost-Ray, n_jobs sets the number of actors\n    random_state=seed)\n\n# scikit-learn API will automatically convert the data\n# to RayDMatrix format as needed.\n# You can also pass X as a RayDMatrix, in which case\n# y will be ignored.\n\nclf.fit(X_train, y_train)\n\npred_ray = clf.predict(X_test)\nprint(pred_ray)\n\npred_proba_ray = clf.predict_proba(X_test)\nprint(pred_proba_ray)","highlightRows":["1","12","13","14"]}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The code above shows how little you need to change your code to use XGBoost-Ray. While you donât need XGboost-Ray to train the breast cancer dataset, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/distributed-xgboost-training-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"a previous post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" ran benchmarks on several dataset sizes (~1.5M to ~12M rows) across different amounts of workers (1 to 8) to show how it performs on bigger datasets on a single node.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5kAlbFNuCbmvSXoKV3dPnI","type":"Asset","createdAt":"2021-12-15T07:41:46.014Z","updatedAt":"2021-12-15T07:42:02.129Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"SingleNode_xgboost_dask_benchmark","description":"Training times for single node benchmarks (lower is better). XGBoost-Ray and XGBoost-Dask achieve similar performance on a single AWS m5.4xlarge instance with 16 cores and 64 GB memory.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5kAlbFNuCbmvSXoKV3dPnI/328721981263e15fba3895922088b5ed/SingleNode_xgboost_dask_benchmark.png","details":{"size":60658,"image":{"width":1600,"height":477}},"fileName":"SingleNode_xgboost_dask_benchmark.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"XGBoost-Ray is also performant in multi-node (distributed) settings as the image below shows.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RPKC3iqnWVvF72ZZzQQif","type":"Asset","createdAt":"2021-12-15T07:42:50.808Z","updatedAt":"2021-12-15T19:33:51.352Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Spark Benchmark","description":"Multi-node training times on several synthetic dataset sizes ranging from ~400k to ~2B rows (where lower is better). XGBoost-Ray and XGBoost-Spark achieve similar performance.","file":{"url":"//images.ctfassets.net/xjan103pcp94/3RPKC3iqnWVvF72ZZzQQif/c83a6b7c491e9d0d154ee2d3266eee57/spark_benchmark.png","details":{"size":103270,"image":{"width":1600,"height":737}},"fileName":"spark_benchmark.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you would like to learn more about XGBoost Ray, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/distributed-xgboost-training-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"check out this post on XGBoost-Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"This post went over a couple approaches you can use to speed up XGBoost model training like changing tree construction methods, leveraging cloud computing, and distributed XGBoost on Ray. If you have any questions or thoughts about XGBoost on Ray, please feel free to join our community through ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discourse","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Slack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". If you would like to keep up to date with all things Ray, ","nodeType":"text"},{"data":{"uri":"https://twitter.com/raydistributed"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"consider following @raydistributed on twitter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://anyscale.us5.list-manage.com/subscribe?u=524b25758d03ad7ec4f64105f\u0026id=d94e960a03"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"sign up for the Ray newsletter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2SMb72o8wuln9SIscqSazp","type":"Asset","createdAt":"2021-12-15T17:55:47.550Z","updatedAt":"2021-12-15T18:32:51.481Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Local Machine Cloud","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2SMb72o8wuln9SIscqSazp/7d7d57cb4a72db5033e610cdd515ccba/SpeedingXGBoostSmallLarge.png","details":{"size":158889,"image":{"width":2222,"height":1769}},"fileName":"SpeedingXGBoostSmallLarge.png","contentType":"image/png"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Zk7jixFrLqHf3IJLzrO2B","type":"Entry","createdAt":"2022-05-03T18:47:31.422Z","updatedAt":"2022-05-03T18:47:31.422Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Ray Summit: Aug 23-24 in SF!","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5LTWfPAScfUtzXj2KOUALz","type":"Entry","createdAt":"2022-04-12T15:29:42.674Z","updatedAt":"2022-10-04T22:06:07.976Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":17,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"summit"}},"locale":"en-US"},"fields":{"title":"Ray Summit 2022","slug":"ray-summit-2022","description":"Centered on Ray, the open-source distributed computing framework, Ray Summit brings together ML practitioners, data scientists, and more to discuss the growing demand for distributed computing.","sections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xIJZXdAqhOEizUCUuQaiV","type":"Entry","createdAt":"2022-03-31T16:04:49.746Z","updatedAt":"2022-09-12T20:58:36.991Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":20,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-hero","header":"The Place for _Everything Ray_","body":"Missed us in San Francisco for Ray Summit 2022? Don't fret, you can watch your favorite sessions on demand! ","ctaText":"View Sessions","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"9F9S7NLaIdPL4ThYwVlFW","type":"Asset","createdAt":"2022-09-06T14:49:38.936Z","updatedAt":"2022-09-06T14:53:02.350Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Presented by Anyscale","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/9F9S7NLaIdPL4ThYwVlFW/8ecd09b334942298f0b8c2d5567ec032/presented-by-anyscale.svg","details":{"size":22813,"image":{"width":289,"height":32}},"fileName":"presented-by-anyscale.svg","contentType":"image/svg+xml"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4A0d4TUfp0TCqUA60CXopR","type":"Asset","createdAt":"2022-09-06T14:53:48.720Z","updatedAt":"2022-09-06T14:53:48.720Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4A0d4TUfp0TCqUA60CXopR/2e4240c268d074d840f7a00d069a768a/rs-collage_2x.jpg","details":{"size":568547,"image":{"width":2400,"height":1376}},"fileName":"rs-collage@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6FGUP0cXunTbgs2UemeAU","type":"Asset","createdAt":"2022-09-06T14:54:09.975Z","updatedAt":"2022-09-06T14:54:09.975Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event, stacked vertically","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6FGUP0cXunTbgs2UemeAU/16ed175024bc9cc0a2fae3cb13538dff/rs-collage-mobile_2x.jpg","details":{"size":243282,"image":{"width":750,"height":1420}},"fileName":"rs-collage-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4RKa7qVhWoIaoNSa1ZH2SK"}}],"type":"2022 Summit Hero","styling":["transparent-header"]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6GL7ZzuSLadWdVFKHj7yIH","type":"Entry","createdAt":"2022-04-20T18:00:12.964Z","updatedAt":"2022-08-08T22:50:29.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-keynote-speakers","header":"Keynote Speakers","ctaText":"See all speakers","ctaLink":"/ray-summit-2022/speakers","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6VhewtcICU9SCXjwd5qhHJ"}}],"type":"2022 Summit Speakers","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JTi5fsyo0Q3YrF3SAQglg","type":"Entry","createdAt":"2022-08-08T22:49:55.163Z","updatedAt":"2022-08-09T07:03:38.615Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-ray-summit-selected-speakers","header":"Highlighted Talks","ctaText":"Full Agenda","ctaLink":"/ray-summit-2022/agenda","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"50eS4cKXBNE4GHezdZ8Rws"}},{"sys":{"type":"Link","linkType":"Entry","id":"5iioKfOgEqVzgQ1DMmA1Wy"}},{"sys":{"type":"Link","linkType":"Entry","id":"YCBUZOXJGB0qcJF8TKjfI"}}],"type":"2022 Summit Agenda Preview"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5cgHuoJ713DewhVY6iUYrG","type":"Entry","createdAt":"2022-05-03T21:57:45.147Z","updatedAt":"2022-05-09T13:41:03.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-trainings","header":"Exclusive Ray Training Sessions","body":"Ray Summit is a great opportunity to level up your Ray skills through hands-on training sessions with the builders and maintainers of Ray. Weâre offering several pre-conference training sessions on August 22. Space is limited â [reserve your spot now](https://www.myeventi.events/raysummit22/attendee/)!","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3sYQTUmD5X5ncvGRFyIRR6","type":"Asset","createdAt":"2022-05-03T04:40:25.040Z","updatedAt":"2022-05-03T04:40:25.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image desktop","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3sYQTUmD5X5ncvGRFyIRR6/a77c675b07d63e73dd79ada9d182498a/training-image_2x.jpg","details":{"size":313509,"image":{"width":1136,"height":1480}},"fileName":"training-image@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7rsC56a4YCgENLyxsHyeIb","type":"Asset","createdAt":"2022-05-03T04:40:54.089Z","updatedAt":"2022-05-03T04:40:54.089Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image mobile","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7rsC56a4YCgENLyxsHyeIb/e2592c9b4424048b3c916fbbbc313065/training-image-mobile_2x.jpg","details":{"size":153801,"image":{"width":830,"height":874}},"fileName":"training-image-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4Hb3J9SrCFTDXqIhIdSIwa"}},{"sys":{"type":"Link","linkType":"Entry","id":"hZ4Tl1wvj7CwF00slwpLg"}},{"sys":{"type":"Link","linkType":"Entry","id":"YJ5KWyjg4NGf29NrfHcJX"}}],"type":"2022 Summit Trainings"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mRJQtuDbIhAuEbwLT2V1K","type":"Entry","createdAt":"2022-03-31T21:42:56.117Z","updatedAt":"2022-04-11T23:46:07.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-reasons-to-attend","header":"Why Attend?","body":"As the annual user conference for the [open source Ray community](https://www.ray.io/community), Ray Summit is a unique opportunity to enhance your Ray skills, build your professional network, and learn what's on the Ray roadmap.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"5f6gxk4a4ujE2y6FZ8ATQ5"}},{"sys":{"type":"Link","linkType":"Entry","id":"20QwyiOmkHCEOkMvp0Jhtk"}},{"sys":{"type":"Link","linkType":"Entry","id":"6QxivsuhhLoVSloRQs9ZlQ"}}],"type":"2022 Summit Content Tiles"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"15k0SGqB2ZKOUfAsM3JnyV","type":"Entry","createdAt":"2022-03-31T16:05:55.739Z","updatedAt":"2022-08-05T00:25:20.225Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":16,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-about","header":"Celebrate the Ray community","body":"Get ready for our epic community bash at the Art of Tinkering exhibit at the San Francisco Exploratorium on August 23 from 5:30 to 9:30 p.m. Come explore, play, and celebrate with the Ray Community. This is an exclusive opportunity to experience this amazing interactive exhibition before it closes in September.","ctaText":"Check out the agenda","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UcZT4h1P7uFDz6cn6JH8l","type":"Asset","createdAt":"2022-06-02T20:37:22.299Z","updatedAt":"2022-06-02T20:37:22.299Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Man taking a photo on his phone","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2UcZT4h1P7uFDz6cn6JH8l/5040a9909f18ab015c27b220077f916e/taking-picture.jpg","details":{"size":107846,"image":{"width":980,"height":772}},"fileName":"taking-picture.jpg","contentType":"image/jpeg"}}}],"type":"2022 Summit Split Image","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4P8Jn0q7nBU05iD6DmcQCC","type":"Entry","createdAt":"2022-04-01T19:15:31.735Z","updatedAt":"2022-04-11T16:36:59.573Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-social-tiles","header":"Join the Conversation","body":"Ready to get involved in the Ray community before the conference? Ask a question in the forums. Open a pull request. Or share why youâre excited with the hashtag __#RaySummit__ on Twitter.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6OMTlXkJ1OHXNkEjyPrXTH"}},{"sys":{"type":"Link","linkType":"Entry","id":"7EE28ee1ATQtodoLloT6mg"}},{"sys":{"type":"Link","linkType":"Entry","id":"1CRAzFxndL37wiNg4x8WKV"}},{"sys":{"type":"Link","linkType":"Entry","id":"2qrec5rR8kabZxCHaPS7NH"}}],"type":"2022 Summit Social Tiles"}}],"headerLogo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70DIdxwT8XY96aw7vMUHOu","type":"Asset","createdAt":"2022-06-01T15:21:46.804Z","updatedAt":"2022-06-01T15:21:46.804Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo Dark Blue","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/70DIdxwT8XY96aw7vMUHOu/5ca0a8b7533c8df70bc0e032b889862a/rs-logo-nav.svg","details":{"size":9954,"image":{"width":108,"height":48}},"fileName":"rs-logo-nav.svg","contentType":"image/svg+xml"}}},"headerLogoWhite":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3qyLXKGgGfWhv4oPqYl0SE","type":"Asset","createdAt":"2022-04-20T21:50:58.774Z","updatedAt":"2022-04-20T21:50:58.774Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3qyLXKGgGfWhv4oPqYl0SE/c4dee9e7a344c39db11fefafbf255b5b/rs-logo-horizontal.svg","details":{"size":9975,"image":{"width":108,"height":48}},"fileName":"rs-logo-horizontal.svg","contentType":"image/svg+xml"}}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KUtRQbdqJo1RrWMrVyfEm","type":"Entry","createdAt":"2022-03-31T16:21:13.402Z","updatedAt":"2022-04-08T17:25:33.228Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Home","link":"/ray-summit-2022"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4MvQcRSkHLMqIJoO9O2cQe","type":"Entry","createdAt":"2022-03-31T16:17:27.336Z","updatedAt":"2022-09-12T16:59:51.221Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"On-Demand","link":"/ray-summit-2022/agenda","subLinks":[{"sys":{"type":"Link","linkType":"Entry","id":"3N9kRDz36tieTI4YGVWKBJ"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46B5diwEDDr3kB2XDoGfDP","type":"Entry","createdAt":"2022-05-11T17:13:42.174Z","updatedAt":"2022-05-11T17:13:42.174Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Speakers","link":"/ray-summit-2022/speakers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Dt5fy6A7rBpXdG7HeKMGd","type":"Entry","createdAt":"2022-03-31T16:17:43.592Z","updatedAt":"2022-03-31T16:17:43.592Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Sponsors","link":"/ray-summit-2022/sponsors"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SvRE9pnyq3BXUPGKRWx6W","type":"Entry","createdAt":"2022-06-03T19:29:31.009Z","updatedAt":"2022-06-03T19:29:31.009Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Who Attends","link":"/ray-summit-2022/who-attends"}}],"mainSponsors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"xMQIOU9n7gVS4mkX2Nlag","type":"Entry","createdAt":"2021-01-31T16:33:06.495Z","updatedAt":"2021-05-11T13:28:31.312Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Ray","url":"https://ray.io","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jLroqm98RCMLGASthyuPv","type":"Asset","createdAt":"2021-05-11T13:28:26.930Z","updatedAt":"2021-05-11T13:28:26.930Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/jLroqm98RCMLGASthyuPv/f76cc04827316de5d3f8b529f6a3f02c/Ray_BM.svg","details":{"size":4742,"image":{"width":93,"height":36}},"fileName":"Ray_BM.svg","contentType":"image/svg+xml"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11syIHhtX8JajSlXHSHE1O","type":"Entry","createdAt":"2021-01-31T16:31:24.509Z","updatedAt":"2021-05-11T13:28:00.309Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Anyscale","url":"https://www.anyscale.com","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UkDmTzmZHIOG7yxFP4sVL","type":"Asset","createdAt":"2021-05-11T13:27:55.219Z","updatedAt":"2021-05-11T13:27:55.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Anyscale","file":{"url":"//images.ctfassets.net/xjan103pcp94/5UkDmTzmZHIOG7yxFP4sVL/db10aebda9a9aad10903470781a6f2e6/ANSC_BM.svg","details":{"size":7431,"image":{"width":158,"height":36}},"fileName":"ANSC_BM.svg","contentType":"image/svg+xml"}}}}}],"footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cfwECWrOqJ5Fegr3YpReS","type":"Entry","createdAt":"2022-04-01T21:22:12.001Z","updatedAt":"2022-04-13T17:35:29.511Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-footer","body":"Â©Anyscale, Inc 2022\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/privacy-policy\"\u003ePrivacy Policy\u003c/a\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/code-of-conduct\"\u003eCode of Conduct\u003c/a\u003e"}},"recommendations":[]}},"ctaText":"Register now","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5nTlh9josVs5zpZqU3vTbn","type":"Asset","createdAt":"2022-05-03T18:47:25.125Z","updatedAt":"2022-05-03T18:47:25.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-ray-summit-2022","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5nTlh9josVs5zpZqU3vTbn/e66043f62ed4a4b53d7e182e50289ec3/blog-recommended-content-ray-summit-2022.png","details":{"size":17234,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-ray-summit-2022.png","contentType":"image/png"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6oRdKeOit1xdnRRjxmQqPB","type":"Entry","createdAt":"2022-03-24T23:14:36.802Z","updatedAt":"2022-03-24T23:14:36.802Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Blog series: XGBoost model training","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"55yHVx89gHSQH0jhiilUVr","type":"Entry","createdAt":"2022-02-17T16:09:06.264Z","updatedAt":"2022-06-22T15:57:31.139Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Three ways to speed up XGBoost model training","slug":"three-ways-to-speed-up-xgboost-model-training","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BGRfsYFKUgIpGjaXTUYml","type":"Entry","createdAt":"2021-08-09T15:38:43.390Z","updatedAt":"2021-08-09T15:38:43.390Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Antoni Baum","slug":"antoni-baum","link":"https://www.linkedin.com/in/yard1/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4W4CsBpJcrALb90x0HXkRQ","type":"Entry","createdAt":"2022-02-07T15:00:35.988Z","updatedAt":"2022-02-07T15:00:35.988Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Chandler Gibbons","slug":"chandler-gibbons"}}],"publishedDate":"2022-02-17","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"In a previous blog post, we covered the advantages and disadvantages of several approaches for speeding up XGBoost model training. In this article, weâll dive into three different approaches, with code snippets so you can follow along.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Extreme gradient boosting, or ","nodeType":"text"},{"data":{"uri":"https://xgboost.readthedocs.io/en/latest/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"XGBoost","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", is an efficient open-source implementation of the gradient boosting algorithm. This method is popular for classification and regression problems using tabular datasets because of its execution speed and model performance. But the XGBoost training process can be time consuming.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a previous blog post, we covered ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-speed-up-xgboost-model-training"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"the advantages and disadvantages of several approaches for speeding up XGBoost model training","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Check out that post for a high-level overview of each approach. In this article, weâll dive into three different approaches for reducing the training time of an XGBoost classifier, with code snippets so you can follow along:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Tree method (","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"gpu_hist","nodeType":"text"},{"data":{},"marks":[],"value":")","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Cloud training","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/xgboost-ray.html#:~:text=XGBoost%2DRay%20integrates%20with%20Ray,training%20run%20parallelized%20by%20itself."},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Distributed XGBoost on Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Before we jump in, letâs create an XGBoost model using the scikit-learn library to import the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"make-classification","nodeType":"text"},{"data":{},"marks":[],"value":" function. This allows you to create a synthetic dataset. Then, weâll define an XGBoost classifier to train on that dataset.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5a9uSqdEapOIjoFO3i5qKY","type":"Entry","createdAt":"2022-02-16T15:57:02.409Z","updatedAt":"2022-02-17T15:37:41.964Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Three ways to speed up XGBoost model training, code example 1","body":"from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\n# define dataset\nX, y = make_classification(\n    n_samples=100000,\n    n_features=1000,\n    n_informative=50,\n    n_redundant=0,\n    random_state=1)\n# summarize the dataset\nprint(X.shape, y.shape)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.50, random_state=1)\n\n# define the model\nmodel = XGBClassifier()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This creates an XGBoost classifier that is ready to be trained. Now, let's see how we can train it using the tree method.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Tree method","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The ","nodeType":"text"},{"data":{"uri":"https://xgboost.readthedocs.io/en/latest/treemethod.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"tree method","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" parameter sets the algorithm used by XGBoost to build boosted trees, as shown in the figure below. By default, an approx algorithm is used, which doesnât offer the best performance. Switching to the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"hist","nodeType":"text"},{"data":{},"marks":[],"value":" algorithm improves performance. However, because both of those algorithms only use the central processing unit (CPU), neither offers outstanding performance overall.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"By enabling the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"gpu_hist","nodeType":"text"},{"data":{},"marks":[],"value":" algorithm, as shown in the code sample below, you can train your XGBoost using the graphics processing unit (GPU). This is because running models on the GPU can save a great deal of time compared to running them on the CPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Note that when training XGBoost, you can set the depth to which the tree may grow. In the following diagram, the left tree has a depth of 2 and the right tree has a depth of 3. By default, the maximum depth is 6. Bigger trees can better model complex interactions between features, but if they are too deep, they may cause overfitting. Bigger trees also take longer to train. There are also multiple other hyperparameters that control the training process. Finding the best set of hyperparameters for your problem should be automated with a tool like HyperOpt, Optuna, or ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1uTgCLQFGTR9Pp9jcPqMi4","type":"Asset","createdAt":"2022-02-16T15:58:56.072Z","updatedAt":"2022-02-16T15:58:56.072Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-speed-up-xgboost-training-1","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1uTgCLQFGTR9Pp9jcPqMi4/01cd26b87a554ca22ff20e5f1500e36c/blog-speed-up-xgboost-training-1.png","details":{"size":2832,"image":{"width":342,"height":97}},"fileName":"blog-speed-up-xgboost-training-1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"LXSVWEOxundlXrhFvG5M2","type":"Entry","createdAt":"2022-02-16T15:59:24.983Z","updatedAt":"2022-02-16T23:33:41.318Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Three ways to speed up XGBoost training, code example 2","body":"# define the datasets to evaluate each iteration\nevalset = [(X_train, y_train), (X_test, y_test)]\n#################\nmodel = XGBClassifier(\n    learning_rate=0.02,\n    n_estimators=10,\n    objective=\"binary:logistic\",\n    nthread=3,\n    tree_method=\"gpu_hist\"  # this enables GPU.\n)\n\nimport time\nprint('Lets GO!')\nstart = time.ctime()\n# fit the model\nmodel.fit(X_train, y_train, eval_metric='logloss', eval_set=evalset)\n\nend = time.ctime()\nprint('all done!')\nprint('started', start)\nprint('finished', end)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The tree method learning curve of the XGBoost uses GPU. See the training time with CPU and GPU:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6rhZz0eLB0KcaaTHqN6bfy","type":"Asset","createdAt":"2022-02-16T16:00:13.608Z","updatedAt":"2022-02-16T16:00:13.608Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-speed-up-xgboost-training-2","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6rhZz0eLB0KcaaTHqN6bfy/c5a5b03f19bdd405a538a95db5e66749/blog-speed-up-xgboost-training-2.png","details":{"size":22407,"image":{"width":546,"height":201}},"fileName":"blog-speed-up-xgboost-training-2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Youâll notice in the figure above that when XGBoost is trained on a large dataset with ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"gpu_hist","nodeType":"text"},{"data":{},"marks":[],"value":" enabled, training speeds up dramatically with a decrease from 41 seconds (","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"hist","nodeType":"text"},{"data":{},"marks":[],"value":") to 23 seconds (","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"gpu_hist","nodeType":"text"},{"data":{},"marks":[],"value":").Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The cloud","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Tweaking the tree method is ideal for using our local GPUs to train XGBoost models, but other ways can be more effective. The solution hides in the cloud. Cloud computing allows us to access much more powerful GPUs and in greater numbers than we have available locally. However, this comes at a cost.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"These cloud GPU providers arenât free, but there are options for training on powerful GPUs, such as the pay-as-you-go solution. This gives you the right to shut down training instances when you finish training, meaning you only pay when youâre using them.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The XGBoost model used in this article is trained using AWS EC2 instances and checks out the training time results. The process is quite simple. Below is an overview of the steps used to train your XGBoost on AWS EC2 instances:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Set up an AWS account (if needed)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Launch an AWS Instance","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Log in and run the code","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Train an XGBoost model","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Close the AWS Instance","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"To make it simpler, after signing in, choose an Amazon Machine Image (AMI) to launch your virtual machine with EC2, on which you can run XGBoost.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"45FXYzqx3TTJtZGY8SQ1EQ","type":"Asset","createdAt":"2022-02-16T16:02:01.459Z","updatedAt":"2022-02-16T16:02:01.459Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-speed-up-xgboost-training-3","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/45FXYzqx3TTJtZGY8SQ1EQ/468b62580ed43a27734c0b41188bd856/blog-speed-up-xgboost-training-3.png","details":{"size":163369,"image":{"width":1811,"height":830}},"fileName":"blog-speed-up-xgboost-training-3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"68udHSEZdUF7G2aP3w2g8p","type":"Asset","createdAt":"2022-02-16T16:02:17.944Z","updatedAt":"2022-02-16T16:02:17.944Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-speed-up-xgboost-training-4","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/68udHSEZdUF7G2aP3w2g8p/401e035541e6bee6074af618b637cd94/blog-speed-up-xgboost-training-4.png","details":{"size":184246,"image":{"width":1825,"height":746}},"fileName":"blog-speed-up-xgboost-training-4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To learn more about how to start an EC2 instance, check out ","nodeType":"text"},{"data":{"uri":"https://machinelearningmastery.com/train-xgboost-models-cloud-amazon-web-services/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"How to Train XGBoost Models in the Cloud with Amazon Web Services","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Once you launch your instance, you can run the same code you created previously and train XGBoost, with the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"tree_method","nodeType":"text"},{"data":{},"marks":[],"value":" set to ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"gpu_hist","nodeType":"text"},{"data":{},"marks":[],"value":". Once trained, youâll notice that training with AWS EC2 is faster compared to using our local GPUs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed XGBoost training with Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"So far, youâve seen that itâs possible to speed up the training of XGBoost on a large dataset by either using a GPU-enabled tree method or a cloud-hosted solution like AWS or Google Cloud. In addition to these two options, thereâs a third â and better â solution: distributed XGBoost on Ray, or XGBoost-Ray for short.Â Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"XGBoost-Ray is a distributed learning Python library for XGBoost, built on a distributed computing framework called Ray. XGBoost-Ray allows the effortless distribution of training in a cluster with hundreds of nodes. It also provides various advanced features, such as fault tolerance, elastic training, and integration with Ray Tune for hyperparameter optimization.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The default implementation of XGBoost can only use one GPU and CPU on a single machine. In order to leverage more resources, itâs necessary to use a distributed training method like XGBoost-Ray. Furthermore, if you are working with datasets that are too big to fit in memory of a single machine, distributed training is necessary.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hereâs a diagram of XGBoost-Ray distributed learning with multi-nodes and multi-GPUs:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6joWwmvZUMEbX1HVVxDPiH","type":"Asset","createdAt":"2022-02-16T16:03:36.616Z","updatedAt":"2022-02-16T16:03:36.616Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-speed-up-xgboost-training-5","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6joWwmvZUMEbX1HVVxDPiH/0a7bd0e4d5f9920bf78c5dc51c992f92/blog-speed-up-xgboost-training-5.png","details":{"size":12391,"image":{"width":558,"height":183}},"fileName":"blog-speed-up-xgboost-training-5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"For training, XGBoost-Ray creates training actors for the whole cluster. Then, each of these actors trains on a separate piece of the data. This is called data-parallel training. Actors communicate their gradients using tree-based AllReduce as shown in the figure below.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3lZ86tW4p1p0YeDc4v86wW","type":"Asset","createdAt":"2022-02-16T16:04:23.968Z","updatedAt":"2022-02-16T16:04:23.968Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-speed-up-xgboost-training-6","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3lZ86tW4p1p0YeDc4v86wW/94011df05e39ff139c5c9897405a40cf/blog-speed-up-xgboost-training-6.png","details":{"size":27648,"image":{"width":721,"height":351}},"fileName":"blog-speed-up-xgboost-training-6.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hereâs what happens to the training time when you train XGBoost with Ray. First, start by getting the XGBoost code that you used before and import the XGBoost-Ray dependencies, such as train and ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"RayParams","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2fkTnBxn1Ai80gEvOM3bDh","type":"Entry","createdAt":"2022-02-16T16:05:15.404Z","updatedAt":"2022-02-17T15:38:19.682Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Three ways to speed up XGBoost model training, code example 3","body":"from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n# define dataset\nX, y = make_classification(\n    n_samples=100000,\n    n_features=1000,\n    n_informative=50,\n    n_redundant=0,\n    random_state=1)\n# summarize the dataset\nprint(X.shape, y.shape)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.50, random_state=1)\n\nfrom xgboost_ray import RayDMatrix, RayParams, train","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now that the classifier is ready, you can start configuring your distributed training framework to perform multi-GPU training â in other words, selecting how many actors you should use and the distribution of CPUs and GPUs amongst the actors. You do this with the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"RayParams","nodeType":"text"},{"data":{},"marks":[],"value":" object, which you use to divide CPUs and GPU among actors. Here, you train the XGBoost on a machine with six CPUs and two GPUs. For multi-CPU and GPU training, select the number of actors to be at least two with three CPUs and one GPU each. The actors will be automatically scheduled by Ray on your cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Because we are only using one machine in our example, both of those actors will be put on it, but we would be able to use the same code with a cluster of dozens or hundreds of machines. Note that the data is passed to XGBoost-Ray using a ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"RayDMatrix","nodeType":"text"},{"data":{},"marks":[],"value":" object. This object stores data in a sharded way so that each actor can access its part of the data to perform training.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1FRMnNR6FXJqWKSYW9IPGY","type":"Entry","createdAt":"2022-02-16T16:06:19.183Z","updatedAt":"2022-02-16T16:06:19.183Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Three ways to speed up XGBoost model training, code example 4","body":"train_set = RayDMatrix(X_train, y_train)\neval_set = RayDMatrix(X_test, y_test)\nevals_result = {}\nbst = train(\n    {\n        \"objective\": \"binary:logistic\",\n        \"eval_metric\": [\"logloss\", \"error\"],\n    },\n    train_set,\n    num_boost_round=10,\n    evals_result=evals_result,\n    evals=[(train_set, \"train\"), (eval_set, \"eval\")],\n    verbose_eval=True,\n    ray_params=RayParams(\n        num_actors=2,\n        gpus_per_actor=1,\n        cpus_per_actor=3,  # Divide evenly across actors per machine\n    ))\nbst.save_model(\"model.xgb\")\nprint(\"Final training error: {:.4f}\".format(\n    evals_result[\"train\"][\"error\"][-1]))\nprint(\"Final validation error: {:.4f}\".format(\n    evals_result[\"eval\"][\"error\"][-1]))"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The machine used for running the XGBoost training in this example has six cores:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"8g1bPCFg84LrKQqkF5alr","type":"Asset","createdAt":"2022-02-16T16:07:01.638Z","updatedAt":"2022-02-16T16:07:01.638Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-speed-up-xgboost-training-7","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/8g1bPCFg84LrKQqkF5alr/c254aa501fabbfe8f2b919cb6876af83/blog-speed-up-xgboost-training-7.png","details":{"size":97663,"image":{"width":511,"height":521}},"fileName":"blog-speed-up-xgboost-training-7.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing resultsÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, itâs time to compare the results to see which method speeds up the training of XGBoost the most. As you can see in the table below, distributed training with Ray was the most effective method for reducing the training time, thanks to the use of features like multi-CPU training, multi-GPU training, fault tolerance, and support for configurable parameters like the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"RayParam","nodeType":"text"},{"data":{},"marks":[],"value":" function. XGBoost-Ray could be used to reduce the training time even further by using a cluster of machines instead of one as in our example.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4MlRvjYSgajsb0un71L75Q","type":"Entry","createdAt":"2022-02-16T17:05:13.668Z","updatedAt":"2022-02-16T17:07:06.150Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"Three ways to speed up XGBoost model training, Table 1","body":"| XGBoost classifier                                              | Train time |\n|-----------------------------------------------------------------|------------|\n| Tree method (hist)                                              | 41 seconds |\n| Tree method (GPU-hist)                                          | 23 seconds |\n| EC2 instance                                                    | 19 seconds |\n| Distributed training with Ray (on a single multi-core computer) | 15 seconds |"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3Km8JHxxUV3WEqF7xVAKP1","type":"Asset","createdAt":"2022-02-16T16:08:47.040Z","updatedAt":"2022-02-16T16:08:47.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-speed-up-xgboost-training-9","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3Km8JHxxUV3WEqF7xVAKP1/c2c9a94785b89a81ffaf5bdeab8d99ac/blog-speed-up-xgboost-training-9.png","details":{"size":7509,"image":{"width":480,"height":334}},"fileName":"blog-speed-up-xgboost-training-9.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this article, we explored several methods that you can use to speed up the training of the XGBoost classifier. Ultimately, we found that XGBoost distributed training with Ray beats all other techniques in terms of training speed. This is because XGBoost-Ray includes multi-node training, full CPU support, full GPU support, and configurable parameters like ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"RayParam","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the next article in this series, weâll explore ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/deploying-xgboost-models-with-ray-serve"},"content":[{"data":{},"marks":[],"value":"how to use Ray Serve to deploy XGBoost models","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Or, if youâre interested in learning more about Ray and XGBoost, check out the additional resources below:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/events/2022/03/09/simplify-and-scale-your-xgboost-model-using-ray-on-anyscale"},"content":[{"data":{},"marks":[],"value":"Register for our upcoming webinar","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on how to simplify and scale your XGBoost model using Ray on Anyscale","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/building-an-end-to-end-ml-pipeline-using-mars-and-xgboost-on-ray"},"content":[{"data":{},"marks":[],"value":"Read how the Ray team at Ant Group used XGBoost on Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and other Ray machine learning libraries to implement an end-to-end AI pipeline in one job","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/xgboost-ray.html"},"content":[{"data":{},"marks":[],"value":"Explore the XGBoost-Ray documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5ptHMv1h6RgEkE7Mbezl3J","type":"Asset","createdAt":"2022-02-16T16:14:53.533Z","updatedAt":"2022-05-20T18:54:34.947Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-speed-up-xgboost-training-thumb","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5ptHMv1h6RgEkE7Mbezl3J/76831ad87880b4cc3c0d775eb7414716/1372329_BlogImageIllustration5_051922.jpg","details":{"size":286133,"image":{"width":1500,"height":1000}},"fileName":"1372329_BlogImageIllustration5_051922.jpg","contentType":"image/jpeg"}}},"recommendations":[],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}}],"hideIntro":true}},"thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2sxKXnh9U3PHl8xypej4Ue","type":"Asset","createdAt":"2022-03-24T22:19:37.849Z","updatedAt":"2022-03-24T22:19:37.849Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-neural-net-dark","file":{"url":"//images.ctfassets.net/xjan103pcp94/2sxKXnh9U3PHl8xypej4Ue/c97c00f800206db3df12103d2c8fb827/blog-recommended-content-neural-net-dark.jpg","details":{"size":45208,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-neural-net-dark.jpg","contentType":"image/jpeg"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6LR51NLq78j9J7jK2ULkMT","type":"Entry","createdAt":"2022-03-24T23:16:11.147Z","updatedAt":"2022-03-24T23:16:11.147Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"How to speed up scikit-learn model training","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4eagf98HnprtfrAyP9rqNp","type":"Entry","createdAt":"2021-02-03T17:39:56.659Z","updatedAt":"2022-06-22T17:19:57.531Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":13,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"How to Speed up Scikit-Learn ModelÂ Training","slug":"how-to-speed-up-scikit-learn-model-training","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-02-03","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This post gives an overview of different ways to speed up your scikit-learn models and discusses some limitations of each approach.","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6dzUnViTAOcBiI8fljtInt","type":"Asset","createdAt":"2021-11-17T20:46:54.605Z","updatedAt":"2021-11-17T20:46:54.605Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"scikit-learn resources","description":"Resources (dark blue) that scikit-learn can utilize for single core (A), multicore (B), and multinode training (C)","file":{"url":"//images.ctfassets.net/xjan103pcp94/6dzUnViTAOcBiI8fljtInt/2068c4218ddf7078c6d9086be2698add/scikit-learnNodeImage.png","details":{"size":129600,"image":{"width":1026,"height":530}},"fileName":"scikit-learnNodeImage.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scikit-Learn is an easy to use Python library for machine learning. However, sometimes scikit-learn models can take a long time to train. The question becomes, how do you create the best scikit-learn model in the least amount of time? There are quite a few approaches to solving this problem like:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Changing your optimization function (solver)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Using different hyperparameter optimization techniques (grid search, random search, early stopping)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Parallelize or distribute your training with ","nodeType":"text"},{"data":{"uri":"https://joblib.readthedocs.io/en/latest/"},"content":[{"data":{},"marks":[],"value":"joblib","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/index.html"},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"This post gives an overview of each approach, discusses some limitations, and offers resources to speed up your machine learning workflow!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Changing your optimization algorithm (solver)","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7L7wM7Ry74VHs2TF1J4ZKk","type":"Asset","createdAt":"2021-02-03T17:13:58.023Z","updatedAt":"2021-11-17T20:48:25.597Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"GaÃ«lVaroquauxTalk","description":"Some solvers can take longer to converge. Image from [GaÃ«l Varoquaux's talk](https://youtu.be/1s8RzWwMdqg?t=673)","file":{"url":"//images.ctfassets.net/xjan103pcp94/7L7wM7Ry74VHs2TF1J4ZKk/024d835a30d7bbcc93820c595aa4288a/Gae__lVaroquauxTalk.png","details":{"size":257625,"image":{"width":1200,"height":564}},"fileName":"GaeÌlVaroquauxTalk.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Better algorithms allow you to make better use of the same hardware. With a more efficient algorithm, you can produce an optimal model faster. One way to do this is to change your optimization algorithm (solver). For example, ","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"},"content":[{"data":{},"marks":[],"value":"scikit-learnâs logistic regression","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", allows you to choose between solvers like ânewton-cgâ, âlbfgsâ, âliblinearâ, âsagâ, and âsagaâ.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To understand how different solvers work, I encourage you to watch a talk by scikit-learn core contributor ","nodeType":"text"},{"data":{"uri":"https://youtu.be/1s8RzWwMdqg?t=671"},"content":[{"data":{},"marks":[],"value":"GaÃ«l Varoquaux","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". To paraphrase part of his talk, a full gradient algorithm (liblinear) converges rapidly, but each iteration (shown as a white +) can be prohibitively costly because it requires you to use all of the data. In a sub-sampled approach, each iteration is cheap to compute, but it can converge much more slowly. Some algorithms like âsagaâ achieve the best of both worlds. Each iteration is cheap to compute, and the algorithm converges rapidly because of a variance reduction technique. It is important to note that ","nodeType":"text"},{"data":{"uri":"https://leon.bottou.org/publications/pdf/nips-2007.pdf"},"content":[{"data":{},"marks":[],"value":"quick convergence doesnât always matter in practice","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and different solvers suit different problems.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1c5bXvbTpsw0L9tF0G7ox6","type":"Asset","createdAt":"2021-02-03T17:16:51.409Z","updatedAt":"2021-02-03T17:37:40.041Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"chooseSolver","file":{"url":"//images.ctfassets.net/xjan103pcp94/1c5bXvbTpsw0L9tF0G7ox6/cd03cd69563f2558d3dfb4de28267e9f/chooseSolver.png","details":{"size":229771,"image":{"width":1200,"height":652}},"fileName":"chooseSolver.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To determine which solver is right for your problem, you can check out the ","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/modules/linear_model.html"},"content":[{"data":{},"marks":[],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to learn more!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Different hyperparameter optimization techniques (grid search, random search, early stopping)","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"To achieve high performance for most scikit-learn algorithms, you need to tune a modelâs hyperparameters. Hyperparameters are the parameters of a model which are not updated during training. They can be used to configure the model or training function. Scikit-Learn natively contains a ","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/modules/grid_search.html"},"content":[{"data":{},"marks":[],"value":"couple techniques for hyperparameter tuning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" like grid search (","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV"},"content":[{"data":{},"marks":[],"value":"GridSearchCV","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") which exhaustively considers all parameter combinations and ","nodeType":"text"},{"data":{"uri":"https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf"},"content":[{"data":{},"marks":[],"value":"randomized search","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV"},"content":[{"data":{},"marks":[],"value":"RandomizedSearchCV","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") which samples a given number of candidates from a parameter space with a specified distribution. Recently, scikit-learn added the experimental hyperparameter search estimators halving grid search (","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html#sklearn.model_selection.HalvingGridSearchCV"},"content":[{"data":{},"marks":[],"value":"HalvingGridSearchCV","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") and halving random search (","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html#sklearn.model_selection.HalvingRandomSearchCV"},"content":[{"data":{},"marks":[],"value":"HalvingRandomSearch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":").","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UN5tMurSqAPzUknlYZeZG","type":"Asset","createdAt":"2021-02-03T17:19:27.036Z","updatedAt":"2021-11-17T20:50:42.743Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"Successive Halving","description":"Successive halving is an experimental new feature in scikit-learn version 0.24.1 (January 2021). Image from [documentation](https://medium.com/r/?url=https%3A%2F%2Fscikit-learn.org%2Fstable%2Fmodules%2Fgrid_search.html%23searching-for-optimal-parameters-with-successive-halving).","file":{"url":"//images.ctfassets.net/xjan103pcp94/2UN5tMurSqAPzUknlYZeZG/c507f8bb4666b96e62f0f2d95e36cd15/SuccessiveHalfing.png","details":{"size":38987,"image":{"width":640,"height":480}},"fileName":"SuccessiveHalfing.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"These techniques can be used to search the parameter space using ","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/modules/grid_search.html#searching-for-optimal-parameters-with-successive-halving"},"content":[{"data":{},"marks":[],"value":"successive halving","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". The image above shows that all hyperparameter candidates are evaluated with a small number of resources at the first iteration and the more promising candidates are selected and given more resources during each successive iteration.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While these new techniques are exciting, there is a library called ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/tune-sklearn"},"content":[{"data":{},"marks":[],"value":"Tune-sklearn","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" that provides cutting edge hyperparameter tuning techniques (bayesian optimization, early stopping, and distributed execution) that can provide significant speedups over grid search and random search.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4IGUajR5tJNXsndbGpFLV4","type":"Asset","createdAt":"2021-02-03T17:21:51.373Z","updatedAt":"2021-11-17T20:52:33.720Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"tune-sklearn","description":"Early stopping in action. Hyperparameter set 2 is a set of unpromising hyperparameters that would be detected by Tune-sklearn's early stopping mechanisms, and stopped early to avoid wasting time and resources. Image from [GridSearchCV 2.0](https://medium.com/distributed-computing-with-ray/gridsearchcv-2-0-new-and-improved-ee56644cbabf).","file":{"url":"//images.ctfassets.net/xjan103pcp94/4IGUajR5tJNXsndbGpFLV4/fc3609312bb4f6384a642ebf0373eca6/earlyStopping.gif","details":{"size":35906,"image":{"width":720,"height":540}},"fileName":"earlyStopping.gif","contentType":"image/gif"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Features of ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/tune-sklearn"},"content":[{"data":{},"marks":[],"value":"Tune-sklearn","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" include:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Consistency with the scikit-learn API: You usually only need to change a couple lines of code to use Tune-sklearn (","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/tune-sklearn/blob/master/examples/random_forest.py"},"content":[{"data":{},"marks":[],"value":"example","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":").","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Accessibility to modern hyperparameter tuning techniques: It is easy to change your code to utilize techniques like bayesian optimization, early stopping, and distributed execution","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Framework support: There is not only support for scikit-learn models, but other scikit-learn wrappers such as ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/tune-sklearn/blob/master/examples/torch_nn.py"},"content":[{"data":{},"marks":[],"value":"Skorch (PyTorch)","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/tune-sklearn/blob/master/examples/keras_example.py"},"content":[{"data":{},"marks":[],"value":", KerasClassifiers (Keras)","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/tune-sklearn/blob/master/examples/xgbclassifier.py"},"content":[{"data":{},"marks":[],"value":"XGBoostClassifiers (XGBoost)","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scalability: The library leverages ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/tune/index.html"},"content":[{"data":{},"marks":[],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a library for distributed hyperparameter tuning, to efficiently and transparently parallelize cross validation on multiple cores and even multiple machines.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Perhaps most importantly, tune-sklearn is fast as you can see in the image below.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5DdWV63Ggg59zA601XKDWs","type":"Asset","createdAt":"2021-02-03T17:26:51.460Z","updatedAt":"2021-11-17T20:53:29.459Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"FitTimesDualCoreGridSearch2.0","description":"You can see significant performance differences on an average laptop using tune-sklearn. Image from [GridSearchCV 2.0](https://medium.com/distributed-computing-with-ray/gridsearchcv-2-0-new-and-improved-ee56644cbabf).","file":{"url":"//images.ctfassets.net/xjan103pcp94/5DdWV63Ggg59zA601XKDWs/ba02d692b386b2224b5eb7da76ef3bb5/fitTimesDualCore.png","details":{"size":29401,"image":{"width":640,"height":480}},"fileName":"fitTimesDualCore.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you would like to learn more about tune-sklearn, you should check out this ","nodeType":"text"},{"data":{"uri":"https://medium.com/distributed-computing-with-ray/gridsearchcv-2-0-new-and-improved-ee56644cbabf"},"content":[{"data":{},"marks":[],"value":"blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For more on hyperparameters and various tuning approaches, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/what-is-hyperparameter-tuning"},"content":[{"data":{},"marks":[],"value":"read our three-part blog series on hyperparameter tuning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Parallelize or distribute your training with joblib andÂ Ray","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"15O810ZYIN3ydejKhTklYX","type":"Asset","createdAt":"2021-02-03T17:28:53.025Z","updatedAt":"2021-11-17T20:53:53.470Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"speedScikitLearnOriginal","description":"Resources (dark blue) that scikit-learn can utilize for single core (A), multicore (B), and multinode training (C)","file":{"url":"//images.ctfassets.net/xjan103pcp94/15O810ZYIN3ydejKhTklYX/7d12855e81556c9853ed256b6d5d77d8/speedScikitLearnOriginal.png","details":{"size":101783,"image":{"width":1200,"height":514}},"fileName":"speedScikitLearnOriginal.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another way to increase your model building speed is to parallelize or distribute your training with ","nodeType":"text"},{"data":{"uri":"https://joblib.readthedocs.io/en/latest/"},"content":[{"data":{},"marks":[],"value":"joblib","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/index.html"},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". By default, scikit-learn trains a model using a single core. It is important to note that virtually all computers today have multiple cores.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3ZsnFXaAlY0LCAYWLlUH6t","type":"Asset","createdAt":"2021-02-03T17:38:52.774Z","updatedAt":"2021-11-17T20:54:14.608Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Macbook4CoresImage","description":"For the purpose of this blog, you can think of the MacBook above as a single node with 4 cores.","file":{"url":"//images.ctfassets.net/xjan103pcp94/3ZsnFXaAlY0LCAYWLlUH6t/41ce9bd81086854e403bdb9f0a9c81f6/macbookImageCores.png","details":{"size":146824,"image":{"width":806,"height":479}},"fileName":"macbookImageCores.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Consequently, there is a lot of opportunity to speed up the training of your model by utilizing all the cores on your computer. This is especially true if your model has a high degree of high degree of parallelism like a random forestÂ®.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2rswjNxGeVpzFK8U6p9jpL","type":"Asset","createdAt":"2021-02-03T17:32:14.759Z","updatedAt":"2021-11-17T20:54:34.696Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"randomForestParallelizeDecisionTree","description":"A random forestÂ® is an easy model to parallelize as each decision tree is independent of the others.","file":{"url":"//images.ctfassets.net/xjan103pcp94/2rswjNxGeVpzFK8U6p9jpL/adb8ac53cc9f5eb72e90372794702934/randomForestParallelizeDecisionTree.png","details":{"size":87924,"image":{"width":1200,"height":703}},"fileName":"randomForestParallelizeDecisionTree.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scikit-Learn can parallelize training on a single node with ","nodeType":"text"},{"data":{"uri":"https://joblib.readthedocs.io/en/latest/parallel.html"},"content":[{"data":{},"marks":[],"value":"joblib which by default uses the âlokyâ backend","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Joblib allows you to choose between backends like âlokyâ, âmultiprocessingâ, âdaskâ, and ârayâ. This is a great feature as the âlokyâ backend is ","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/modules/generated/sklearn.utils.parallel_backend.html"},"content":[{"data":{},"marks":[],"value":"optimized for a single node and not for running distributed (multinode) applications","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Running distributed applications can introduce a host of complexities like:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scheduling tasks across multiple machines","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Transferring data efficiently","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Recovering from machine failures","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Fortunately, the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/joblib.html"},"content":[{"data":{},"marks":[],"value":"ârayâ backend","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" can handle these details for you, keep things simple, and give you better performance. The image below shows the normalized speedup in terms of execution time of Ray, Multiprocessing, and Dask relative to the default âlokyâ backend.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1lVrbr0wVJB0K4j8Os8Km3","type":"Asset","createdAt":"2021-02-03T17:33:27.465Z","updatedAt":"2021-11-17T20:55:20.912Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"distributedScikitLearnAmeer","description":"The performance was measured on one, five, and ten m5.8xlarge nodes with 32 cores each. The performance of Loky and Multiprocessing does not depend on the number of machines because they run on a single machine. [Image source](https://medium.com/distributed-computing-with-ray/easy-distributed-scikit-learn-training-with-ray-54ff8b643b33).","file":{"url":"//images.ctfassets.net/xjan103pcp94/1lVrbr0wVJB0K4j8Os8Km3/a9184b43e340cb9d99285f6ee0f00c09/distributedScikitLearnAmeer.png","details":{"size":100706,"image":{"width":1200,"height":395}},"fileName":"distributedScikitLearnAmeer.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you would like to learn about how to quickly parallelize or distribute your scikit-learn training, you can check out this ","nodeType":"text"},{"data":{"uri":"https://medium.com/distributed-computing-with-ray/easy-distributed-scikit-learn-training-with-ray-54ff8b643b33"},"content":[{"data":{},"marks":[],"value":"blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"This post went over a couple ways you can build the best scikit-learn model possible in the least amount of time. There are some ways that are native to scikit-learn like changing your optimization function (solver) or by utilizing experimental hyperparameter optimization techniques like ","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html#sklearn.model_selection.HalvingGridSearchCV"},"content":[{"data":{},"marks":[],"value":"HalvingGridSearchCV","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html#sklearn.model_selection.HalvingRandomSearchCV"},"content":[{"data":{},"marks":[],"value":"HalvingRandomSearch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". There are also libraries that you can use as plugins like ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/tune-sklearn"},"content":[{"data":{},"marks":[],"value":"Tune-sklearn","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to further speed up your model building. If you have any questions or thoughts about ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/tune-sklearn"},"content":[{"data":{},"marks":[],"value":"Tune-sklearn","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", please feel free to join our community through ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[],"value":"Discourse","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"data":{},"marks":[],"value":"Slack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6a34NsMPxkZFS9jgqGMlYP","type":"Asset","createdAt":"2021-02-03T23:32:17.740Z","updatedAt":"2021-02-03T23:32:17.740Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"SpeedUpScikitLearnNode","file":{"url":"//images.ctfassets.net/xjan103pcp94/6a34NsMPxkZFS9jgqGMlYP/9c7fca950e9983132a52ba48db50a41b/speedup.png","details":{"size":129600,"image":{"width":1026,"height":530}},"fileName":"speedup.png","contentType":"image/png"}}},"mainImageFit":"cover","recommendations":[],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true}},"thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4lI3EQhmRJaDEN5Vy2NrrS","type":"Asset","createdAt":"2022-03-21T15:45:45.593Z","updatedAt":"2022-03-21T15:46:44.402Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-recommended-content-clock-light","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4lI3EQhmRJaDEN5Vy2NrrS/237db33389cbc99d81664b828af613ed/blog-recommended-content-clock-light.jpg","details":{"size":36489,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-clock-light.jpg","contentType":"image/jpeg"}}},"recommendations":null}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}}],"hideIntro":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2BZkqc3tu6lgBkE995lNem","type":"Entry","createdAt":"2021-12-06T16:51:11.245Z","updatedAt":"2022-06-22T16:08:44.464Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Ray version 1.9 has been released","slug":"ray-version-1-9-has-been-released","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-12-06","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},"intro":"Ray version 1.9 has been released! Release highlights include: Ray Train is now in beta, Ray Datasets now supports groupby and aggregations, Ray Docker images for multiple CUDA versions, improved Windows support, and a Ray Job Submission server. ","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray version 1.9 has been released! Release highlights include:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train is now in beta!","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Datasets now supports groupby and aggregations! See the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/package-ref.html#ray.data.Dataset.groupby"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"groupby API","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/package-ref.html#groupeddataset-api"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GroupedDataset","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" docs for usage.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Docker images for multiple CUDA versions are now provided!","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"We are making continuing progress in improving Ray stability and usability on Windows. We encourage you to try it out and report feedback or issues at ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/issues"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://github.com/ray-project/ray/issues","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"We are launching a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-job-submission/overview.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Job Submission","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" server + CLI \u0026 SDK clients to make it easier to submit and monitor Ray applications","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can run ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"pip install -U ray","nodeType":"text"},{"data":{},"marks":[],"value":" to access these features and more. With that, letâs go over the highlights.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train is now in betaÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train is now in Beta! The beta version includes various usability improvements for distributed PyTorch training and checkpoint management, support for ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/ray-client.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Client","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and an ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/user_guide.html#distributed-data-ingest-ray-datasets"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"integration with Ray Datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for distributed data ingest.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"docs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to learn more. If you are using Ray Train, weâd love to hear your feedback ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfI3asn-m1cQSIbdrk_cd6qYenZvt-eNTVfTwba3SVhmHcHIg/viewform"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Datasets now supports groupby and aggregations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Datasets now supports groupby and aggregations! This includes multi-column/multi-lambda aggregations, making it much easier to do some aggregation on multiple columns.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"See the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/package-ref.html#ray.data.Dataset.groupby"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"groupby API","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/package-ref.html#groupeddataset-api"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GroupedDataset","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" docs for usage.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Docker images for multiple CUDA versions are now provided","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Docker images for multiple CUDA versions are now provided! You can specify a `-cuXXX` suffix to pick a specific version. Note that `ray-ml:cpu` images are now deprecated and that the `ray-ml` images are now only built for GPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Check out ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/pull/19505"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PR #19505","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to learn more.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Launching a Ray Job Submission server + CLI \u0026 SDK client","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We are launching a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-job-submission/overview.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Job Submission","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" server + CLI \u0026 SDK clients. The goal of Ray Job submission is to provide a lightweight mechanism for users to submit their locally developed and tested application to a running remote Ray cluster, thus enabling the user to package, deploy, and manage their Ray application as Jobs. These Jobs can be submitted by a Job manager of their choice.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is currently in alpha, so the APIs are subject to change, but please test it out and file issues on ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GitHub","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" \u0026 ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"discuss.ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Learn more","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"This post sums up just some of the release highlights. To learn about all the features and enhancements in this release including continuing progress in improving Ray stability and usability on Windows, visit the ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-1.9.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"release notes","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". If you would like to keep up to date with all things Ray, follow ","nodeType":"text"},{"data":{"uri":"https://twitter.com/raydistributed"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"@raydistributed","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on Twitter, and sign up for the ","nodeType":"text"},{"data":{"uri":"https://anyscale.us5.list-manage.com/subscribe?u=524b25758d03ad7ec4f64105f\u0026id=d94e960a03"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray newsletter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1vmSWNOBZZdbtROTXmSIA3","type":"Asset","createdAt":"2021-12-03T07:46:18.973Z","updatedAt":"2021-12-03T07:46:18.973Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray 1.9","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1vmSWNOBZZdbtROTXmSIA3/a85a5d4ffdb6548f94b37ddb35dafe4b/ray1.9.png","details":{"size":1573837,"image":{"width":1762,"height":988}},"fileName":"ray1.9.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true,"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4Shvq24HmMiYP6cT0dROLY","type":"Entry","createdAt":"2021-11-16T17:10:12.838Z","updatedAt":"2022-06-22T16:12:16.152Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Considerations for Deploying Machine Learning Models in Production","slug":"considerations-for-deploying-machine-learning-models-in-production","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-11-16","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"A common grumble among data science or machine learning researchers or practitioners is that putting a model in production is difficult. As a result, some claim that a large percentage, 87%, of models never see the light of the day in production. \n\nâI have a model, I spent considerable time developing it on my laptop. Whatâs next? How do I get it into our production environment? What should I consider for my ML stack and tooling?â","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"A common grumble among data science or machine learning researchers or practitioners is that putting a model in production is difficult. As a result, some claim that a large percentage, 87%, of models never see the","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/why-90-percent-of-all-machine-learning-models-never-make-it-into-production-ce7e250d5a4a"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":" light of the day in production","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"âI have a model, I spent considerable time developing it on my laptop. Whatâs next? How do I get it into our production environment? What should I consider for my ML stack and tooling?âÂ ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"These questions frequently emerge at meetups or conferences, after talks on machine learning operations (MLOps). There is no singular panacea or silver bullet for this nascent field of ","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/mlops-practices-for-data-scientists-dbb01be45dd8"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLOps Best Practices","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" that attempts to address and remedy this crucial problem.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, there are some acceptable and common technical considerations and pitfalls to keep in mind when considering your ML stack and tools. In this first part of a series on putting ML models in production, weâll discuss some common considerations and common pitfalls for tooling and best practices and ML model serving patterns that are an essential part of your journey from model development to deployment in production.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Developing with Ease","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Consider your development environment first. Most data scientists or ML engineers invariably use their laptops for development, testing or debugging code. Because of simplicity, easy to access and install the latest ML libraries, practitioners overwhelmingly prefer laptops over clusters for development. We are spoiled by IDEs and syntax-highlighted editors for good reason.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Python developers like to customize their environments to match their staging environment, with library dependencies using conda or Python virtual environments. Ideally, as a best practice, if the same code developed on their laptop can run with minimal changes on a staging or production environment on the cluster, it immensely improves the end-to-end developer productivity.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Consider your laptop as a preferred choice of development environment, with the possibility of extending or syncing your code to the cluster environment in the cloud.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"0i5dwwrx5iEKmB1i1lCNY","type":"Asset","createdAt":"2021-11-05T17:52:21.850Z","updatedAt":"2021-11-10T03:26:31.917Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"LaptopDeploy","description":"[image source](https://www.kctv5.com/jobs/us-wages-jump-by-the-most-in-records-dating-back-20-years/article_b07e82f7-4d92-5739-876b-5a1fd9063263.html)","file":{"url":"//images.ctfassets.net/xjan103pcp94/0i5dwwrx5iEKmB1i1lCNY/d27552d814334ed1c5dc651a3479f8ad/laptop.png","details":{"size":592457,"image":{"width":1156,"height":690}},"fileName":"laptop.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Consideration Number #1","nodeType":"text"},{"data":{},"marks":[],"value":": ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"Use your laptop for development as a best practice","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Training at Scale and Tracking Model Experiments","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlike the traditional software development cycle, the model development cycle paradigm is different. A number of factors influence an ML modelâs success in production. First, the outcome of a model is measured by its metrics, such as an acceptable accuracy.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Second, achieving an accuracy that satisfies the business goal means experimentation with not only one model or ML library but many models and many ML libraries while tracking each experiment runs: metrics, parameters, artifacts, etc. As vital as accuracy is, so is a developerâs choice of ML libraries to experiment with.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7oiARsfqccrDvMD12jlWJe","type":"Asset","createdAt":"2021-11-05T18:13:20.173Z","updatedAt":"2021-11-05T18:13:20.173Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Traditional Software vs Machine Learning","description":"[image source](https://github.com/dmatrix/mlflow-workshop-part-1/blob/master/slides/mlflow-workshop-series-part-1.pdf)","file":{"url":"//images.ctfassets.net/xjan103pcp94/7oiARsfqccrDvMD12jlWJe/daf859d2f1ad06bf931f5259bbc36255/TraditionalSoftware.png","details":{"size":396774,"image":{"width":1626,"height":566}},"fileName":"TraditionalSoftware.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Third, accuracy is directly linked to the quality of acquired data: bad data results in a bad model. As the diagram below shows data preparationâfeature extractions, feature selection, standardized or normalized features, data imputations and encodingâare all imperative steps before the cleansed data lands into a feature store, accessible to your model training and testing phase or inference in deployment.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6usr388mdxS5gFgfxoAMLm","type":"Asset","createdAt":"2021-11-05T18:17:52.891Z","updatedAt":"2021-11-16T17:11:39.120Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"Modern Data Lake","description":"Phases of the Model Development Cycle","file":{"url":"//images.ctfassets.net/xjan103pcp94/6usr388mdxS5gFgfxoAMLm/b6a4f57518af3904c77a90362271ddad/ModernDataLake.png","details":{"size":217302,"image":{"width":1404,"height":766}},"fileName":"ModernDataLake.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Fourth, a choice of programming language that is not only familiar to your data teamâdata analysts, data scientists, and ML engineersâbut also supported by many ML libraries employed during model experimentation and training phases. Python seems to be the ","nodeType":"text"},{"data":{"uri":"https://insights.stackoverflow.com/survey/2020#technology-most-loved-dreaded-and-wanted-languages-loved"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"de facto choice","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Alongside a choice of a programming language is the choice of an ML framework for taming compute-intensive ML workloads: deep learning, distributed training, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/what-is-hyperparameter-tuning"},"content":[{"data":{},"marks":[],"value":"hyperparameter optimization","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (HPO), and inferenceâall at horizontal scaleâfrom your laptop, single node multiple cores to multiple nodes, with multiple cores.Â Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"And finally, the ability to easily deploy models in diverse environments at scale: part of web applications, inside mobile devices, as a web service in the cloud. etc","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Consideration Number #2: Consider using model life cycle development and management platforms like ","nodeType":"text"},{"data":{"uri":"https://mlflow.org/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"MLflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://dvc.org/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"DVC","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://wandb.ai/site"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Weights \u0026 Biases","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", or ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/sagemaker/studio/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"SageMaker Studio","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":". And ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":",","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/raysgd/raysgd.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":" Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" (formerly Ray SGD), ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/tutorials/beginner/dist_overview.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" and ","nodeType":"text"},{"data":{"uri":"https://www.tensorflow.org/guide/distributed_training"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"TensorFlow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" for distributed, compute-intensive and deep learning ML workloads.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Managing Machine Learning Features","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.featurestore.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Feature stores","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" are emerging pivotal components in the modern machine learning development cycle. As more data scientists and engineers work together to successfully put models in production, having a singular store to persist cleaned and featurized data is becoming an increasing necessity as part of the model development cycle shown.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3ICMHtw1H093lIiCaZkuxG","type":"Asset","createdAt":"2021-11-10T03:31:55.753Z","updatedAt":"2021-11-10T03:39:59.411Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Feature Store Managing ML Features","description":"Feature Store for Managing ML Features","file":{"url":"//images.ctfassets.net/xjan103pcp94/3ICMHtw1H093lIiCaZkuxG/f25617cb1641fc2c4f5ec9cb079fc017/FeatureStore_ManagingMLFeatures.png","details":{"size":147609,"image":{"width":1078,"height":644}},"fileName":"FeatureStore ManagingMLFeatures.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Feature stores address operational challenges. They provide a consistent set of data between training and inference. They avoid any data skew or inadvertent data leakage. They offer both customized capability of writing feature transformations, both on batch and streaming data, during the feature extraction process while training. And they allow request augmentation with historical data at inference, which is common in large fraud and anomaly detection deployed models or recommendation systems.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Aside from challenges and considerations of putting models in production, operationalizing ML data is equally important. Model accuracy depends on good data, and feature stores help manage precomputed and cleansed features for your model training and production inference during model serving.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Consideration Number #3: Consider feature stores as part of your model development process. Look to ","nodeType":"text"},{"data":{"uri":"https://feast.dev/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Feast","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.tecton.ai/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Tecton","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_featurestore.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"SageMaker","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", and ","nodeType":"text"},{"data":{"uri":"https://databricks.com/product/feature-store"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Databricks","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" for feature stores.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deploying, Serving and Inferencing Models at Scale","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Once the model is trained and tested, with confidence that it met the business requirements for model accuracy, seven crucial requirements for scalable model serving frameworks to consider are:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Framework agnostic","nodeType":"text"},{"data":{},"marks":[],"value":": A model serving-elected framework should be ML framework agnostic. That is, it can deploy any common model built with common ML frameworks. For example, PyTorch, TensorFlow, XGBoost, or Scikit-learn, each with its own algorithms and model architectures.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Business Logic:","nodeType":"text"},{"data":{},"marks":[],"value":"Â  Model prediction often requires preprocessing, post processing or ability to augment request data by connecting to a feature store or any other data store for validation. Model serving should allow this as part of its inference.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Model Replication: ","nodeType":"text"},{"data":{},"marks":[],"value":"Some models are compute-intensive or network-bound. As such the elected framework can fan out requests over to model replicas, load balancing among replicas to support parallel request handling during peak traffic.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Request Batching: ","nodeType":"text"},{"data":{},"marks":[],"value":"Not all models in production are employed for real-time serving. Often, models are scored in large batches of requests. For example, for deep learning models, parallelizing these image requests to multiple cores, taking advantage of hardware accelerators, to expedite batch scoring and utilize hardware resources is worthy of consideration.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"High Concurrency and Low Latency: ","nodeType":"text"},{"data":{},"marks":[],"value":"Models in production require real-time inference with low latency while handling bursts of heavy traffic of requests. The consideration is crucial for best user experience to receive millisecond responses on prediction requests.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Model Deployment CLI and APIs: ","nodeType":"text"},{"data":{},"marks":[],"value":"An ML engineer responsible to deploy a model should be able to use model serverâs deployment APIs or command line interfaces (CLI) simply to deploy model artifacts into production. This allows model deployment from within an existing CI/CD pipeline or workflow.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Patterns of Models in Production","nodeType":"text"},{"data":{},"marks":[],"value":": As ML applications are increasingly becoming pervasive in all sectors of industry, models trained for these ML applications are complex and composite. They range from computer vision to natural language processing to recommendation systems and reinforcement learning.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"That is, they donât exist in isolation. Nor do they predict results singularly. Instead they operate jointly and often in ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"four model ML patterns","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": pipeline, ensemble, business logic, and online learning. Each pattern has its purpose and merit.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1xy0sgeTJzs9wGLt2C5rfE","type":"Asset","createdAt":"2021-11-05T18:30:46.397Z","updatedAt":"2021-11-05T18:30:46.397Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ML Patterns in Production","description":"ML Model Patterns in Production ([image source](https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns))","file":{"url":"//images.ctfassets.net/xjan103pcp94/1xy0sgeTJzs9wGLt2C5rfE/4715269a6f5f0a049f958a3b213325e4/MLPatternsProduction.png","details":{"size":203430,"image":{"width":1482,"height":696}},"fileName":"MLPatternsProduction.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Machine Learning engineers adopt ","nodeType":"text"},{"data":{"uri":"https://youtu.be/gV4YS4e1CXg?t=272"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"two common approaches ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"to deploy these patterns of models in production. One is to embed models into a web server, the other is to offload to an external service. Each approach has its own pros and cons, with respect to the above seven considerations.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Consideration Number #4: Look to ","nodeType":"text"},{"data":{"uri":"https://www.seldon.io/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Seldon","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.kubeflow.org/docs/components/kfserving/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"KFServing","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", or ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" for all these seven requirements.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Observing and Monitoring Model in Production","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Model monitoring, often an overlooked stage as part of model development lifecycle, is critical to modelâs viability in the post deployment production stage. It is often an afterthought, at an ML engineerâs peril.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Models have an afterlife of viability. That viable life in production needs a constant watchful or sentinel eye. In fact, monitoring as a phase is simply a continuation of the model serving, as depicted in the diagram below.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1B0lehLJcFXrfMsC6Rdu6f","type":"Asset","createdAt":"2021-11-05T18:33:35.806Z","updatedAt":"2021-11-05T18:33:35.806Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ML Model Monitoring in Production","description":"ML Model Monitoring in Production ([image source](https://evidentlyai.com/blog/machine-learning-monitoring-what-it-is-and-how-it-differs))","file":{"url":"//images.ctfassets.net/xjan103pcp94/1B0lehLJcFXrfMsC6Rdu6f/e0f90578ed433933118a36b2aeea5c39/ML_Model_Monitoring_in_Production.png","details":{"size":192050,"image":{"width":1040,"height":416}},"fileName":"ML Model Monitoring in Production.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Why consider model monitoring? For a number of practical reasons, this stage is pivotal. Let's briefly discuss them.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data drifts over time: ","nodeType":"text"},{"data":{},"marks":[],"value":"As we mentioned above, our quality and accuracy of the model depends on the quality of the data. Data is complex and never static, meaning what the original model was trained with the extracted features may not be as important over time. For example, a geo location for a credit application model may not be as important, as demographics evolve. Some new features may emerge that need to be taken into account. For example, seasonal data changes. Such ","nodeType":"text"},{"data":{"uri":"https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"features drifts","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in data require retraining and redeploying the model, because the distribution of the variables is no longer relevant.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Model concept changes over time: ","nodeType":"text"},{"data":{},"marks":[],"value":"Many practitioners refer to this as model ","nodeType":"text"},{"data":{"uri":"https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"decay or model staleness","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". When the patterns of trained models no longer hold with the drifting data, the model is no longer valid because the relationships of its input features may not necessarily produce the model's expected prediction. Hence, its accuracy degrades.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Â ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Models fail over time: ","nodeType":"text"},{"data":{},"marks":[],"value":"Models fail for inexplicable reasons: a system failure or bad network connection; an overloaded system; a bad input or corrupted request. Detecting these failuresâ root causes early or its frequency mitigates user bad experience or deters mistrust in the service if the user receives wrong or bogus outcomes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Systems degrade over load: ","nodeType":"text"},{"data":{},"marks":[],"value":"Constantly being vigilant of the health of your dedicated model servers or services deployed is just as important as monitoring the health of your data pipelines that transform data or your entire data infrastructureâs key components: data stores, web servers, routers, cluster nodesâ system health, etc.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Collectively, these aforementioned monitoring model concepts are called ","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/what-is-ml-observability-29e85e701688"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"model observability","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". This step is now an acceptable imperative in ","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/mlops-practices-for-data-scientists-dbb01be45dd8"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLOps best practices","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Monitoring the health of your data and models should never be an afterthought. Rather, it ought to be part and parcel of your model development cycle.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Consideration Number #5: For model observability look to ","nodeType":"text"},{"data":{"uri":"https://github.com/evidentlyai/evidently"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Evidently.ai","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://arize.com/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Arize.ai","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.arthur.ai/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Arthur.ai","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.fiddler.ai/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Fiddler.ai","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://valohai.com/model-monitoring/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Valohai.com","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":",","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":" or ","nodeType":"text"},{"data":{"uri":"https://whylabs.ai/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"whylabs.ai","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs recap. To avoid the common grumble of models not making it to production or having your model see the light of the day in production, take into account all the above considerations at heart if you want your models to journey to their desired destinationâand have a viable afterlife too.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Each consideration has its merits. Each consideration has either an open source solution addressing each problem or a managed solution from a vendor. Evaluate how each best fits and meets all the considerations into your existing machine learning tooling stack.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"But making it part and parcel of your ML model development tooling stack is crucial; it will significantly improve your end-to-end success in putting your models into production.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the next blog in this series, we will examine how you can implement consideration #1 and #2, focusing on some of the tools we suggested.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6pQJ9kqwgmpnC2IaMIDtoj","type":"Asset","createdAt":"2021-11-05T18:35:46.042Z","updatedAt":"2021-11-05T18:35:46.042Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"MLPatternsProduction","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6pQJ9kqwgmpnC2IaMIDtoj/c925f376d94117b1f1dd97b4bf84f72d/MLPatternsProduction.png","details":{"size":203430,"image":{"width":1482,"height":696}},"fileName":"MLPatternsProduction.png","contentType":"image/png"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2uweOvYql8znGqMh3XOkGX","type":"Entry","createdAt":"2022-03-24T22:49:07.304Z","updatedAt":"2022-03-24T22:52:23.190Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Considerations for deploying ML models in production: Part 2","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"391LSU2K2EsGRoj4Wp3EnU","type":"Entry","createdAt":"2022-02-02T23:48:27.802Z","updatedAt":"2022-06-22T16:01:46.793Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Considerations for deploying machine learning models in production: Part 2","slug":"considerations-for-deploying-machine-learning-models-in-production-part-2","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}}],"publishedDate":"2022-02-04","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"In an earlier blog post, we shared five considerations for deploying machine learning models in production. In this post, we'll explore how to tune and train at scale and track model experiments. ","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"In 1949 a British newspaper, ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"The Star","nodeType":"text"},{"data":{},"marks":[],"value":", confidently complained in a news article about the EDSAC computer: âThe âbrainâ [computer] may one day come down to our level [of the common people] and help with our income-tax and book-keeping calculations. But this is speculation, and there is no sign of it so far.â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Income-tax and book-keeping asideâcomputers perhaps were expected to help with demands of heavy calculations back thenâtodayâs top-of-the-line laptops or personal computers, armed with multiple cores and hardware accelerators and retina displays, can carry the burden of astronomical calculations at lightning speed. Not a hyperbole or speculation. But a fact. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/considerations-for-deploying-machine-learning-models-in-production"},"content":[{"data":{},"marks":[],"value":"part 1 of this blog series","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we shared five considerations for deploying machine learning models in production. This post will elaborate on two concerns: 1) Developing with Ease and 2) Tuning and Training at Scale and Tracking Model Experiments. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Developing with ease","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"For machine learning (ML) workloads, Python has become the ","nodeType":"text"},{"data":{"uri":"https://hackernoon.com/the-programming-language-for-machine-learning-projects-r9f73ycs"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"de-facto programming language","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" of choice, partly because of its flourishing PyData ecosystem and partly because it is easy to learn.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Python developers prefer to customize and isolate their developer environments to match their staging or production environment with library dependencies using ","nodeType":"text"},{"data":{"uri":"https://conda.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"conda","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://docs.python.org/3/tutorial/venv.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Python virtual environments","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Ideally, as best practice holds, if the same code developed on your laptop can run with minimal changes on a staging or production environment on a cluster, it can immensely improve end-to-end developer productivity.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Consider developing two machine learning models using two different frameworks: ","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"scikit-learn","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Both may have Python module dependencies the other may not need, and for development, you may wish not to conflate their environments. You may want to keep the settings distinct for model building, testing, debugging, and validating with small datasets. You may want to move or work in either or both environments concurrently seamlessly.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6aisUpIA8iEmrbI1TLezxt","type":"Asset","createdAt":"2022-01-25T01:26:04.162Z","updatedAt":"2022-02-02T23:10:43.073Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"python modules","description":"**Figure 1. Laptop with Python Modules and Conda Development Environments**","file":{"url":"//images.ctfassets.net/xjan103pcp94/6aisUpIA8iEmrbI1TLezxt/c4eade148d158808b08a9d3d1b3406d2/Figure_1.png","details":{"size":66075,"image":{"width":960,"height":540}},"fileName":"Figure 1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To achieve this setup, use ","nodeType":"text"},{"data":{"uri":"https://conda.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"conda","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://docs.python.org/3/tutorial/venv.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Python environments","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". We will use ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"conda","nodeType":"text"},{"data":{},"marks":[],"value":" here for familiarity:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Create two distinct ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"environments_\u003cconda_name\u003e.yml","nodeType":"text"},{"data":{},"marks":[],"value":" files","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Create two distinct conda environments and install the required dependencies.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"a. conda create -f environment_pytorch-dev-env.yml","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"b. conda create -f environments_scikit-learn-dev-env.yml","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Activate either of the conda environments ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"a. conda activate pytorch-dev-env","nodeType":"text"},{"data":{},"marks":[],"value":" ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use your IDE or Jupyter notebook to develop your models.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"An example of one of your conda-specific files, ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"environment_pytorch-dev-env.yml","nodeType":"text"},{"data":{},"marks":[],"value":", may look as follows:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3dHXvHagR4VZd5umNMBUbg","type":"Entry","createdAt":"2022-02-01T20:45:10.868Z","updatedAt":"2022-02-02T01:38:18.574Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"pytorch-dev-env","body":"name: pytorch-dev-env\nchannels:\n  - conda-forge\ndependencies:\n  - python \u003e= 3.8\n  - pip\n  - numpy \u003e= 1.18.5\n  - pandas\n  - torch\n  - torchvision\n  - tensorboard\n  - mlflow\n- pip:\n    - ray[default, tune]\n    - tensorboard \u003e= 2.3\n    - tensorflow \u003e= 2.3\n    - mlflow","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The simple steps above create two distinct environments on your laptop for development, which could match, in essence, all Python package dependencies on your staging or production environment as your laptop.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"When your model training requires more compute-intensive resources than your laptop can offer, you want to extend your laptop to an existing cluster to meet those resource requirements once past your experimental trials with small datasets.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How do you seamlessly do it?Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Here our advice is: turn to the ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" platform for three reasons. First,Â  ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/ray-client.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray client","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provides a simple client to attach or connect your laptop to an existing Ray cluster, taking advantage of all the compute and storage and Rayâs ML-specific native libraries. Second, your Ray cluster could be a current staging environment with similar or identical Python and other dependencies as specified in your conda-specific YAML files. Third, you can also ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/quickstart.html#"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"launch a Ray cluster","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with similar Python dependencies using a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/quickstart.html#launch-a-cluster-on-a-cloud-provider"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"YAML configuration ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"when creating a new Ray Cluster that meets your compute-intensive resource requirements as well as Python packages. And finally, all your development can be done on your laptop, allowing you to use your favorite IDE or development toolsâthat is, indeed, ease of development.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3zOwvJsJnnW3f1qNXbxos6","type":"Asset","createdAt":"2022-02-01T22:34:18.330Z","updatedAt":"2022-02-02T23:09:06.043Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"ray.init","description":"**Figure 2. The laptop extends or connects to a Ray cluster via a Ray Client**","file":{"url":"//images.ctfassets.net/xjan103pcp94/3zOwvJsJnnW3f1qNXbxos6/d52dbba631ae0d5a730987028adc2294/Fig_2_train_blog.png","details":{"size":46318,"image":{"width":960,"height":540}},"fileName":"Fig 2 train blog.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Although opinionated, Ray not only scales your Python applications. It also scales your machine learning workloads. This dual functionality to develop locally on a laptop (with small datasets and limited compute) and then extend remotely to a Ray cluster (with large datasets and elastic compute-resources) is a coveted yet opinionated consideration.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next, we examine ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for tuning and training at scale and an open-source ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-and-mlflow-taking-distributed-machine-learning-applications-to"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"ML lifecycle management tool integration with Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for tracking your ML experiments.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Tuning and training at scale and tracking model experiments","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As stated in ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/considerations-for-deploying-machine-learning-models-in-production"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"part 1","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", unlike the traditional software development cycle, the model development cycle paradigm is different. As a result, several factors influence an ML modelâs success in production. Of all aspects mentioned in part 1, letâs consider a few and elaborate on them: model tuning, distributed model training, and both model training and tuning together at scale and model experiment tracking.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Take model tuning at scale. Developing models with state-of-art ML frameworks, such as ","nodeType":"text"},{"data":{"uri":"https://xgboost.readthedocs.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"XGBoost","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.tensorflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"TensorFlow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", or ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"HuggingFace","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", are compute-intensive ML workloads; they use distributed tuning to reduce total tuning time, employ hyperparameter optimization (HPO) over its search space to obtain the best model parameter configs for best model accuracy, and may need batch inferenceâall at horizontal scaleâfrom your laptop or a single node with multiple cores to multiple nodes with multiple cores.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For all the above tuning compute-intensive tasks, ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is most suitable. At its core, Ray Tune leverages cluster resources to scale and distribute trials, each with its hyperparameter configuration and employs proven the state of the art (SOTA) algorithms. For example:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Launch a multi-core, single-node, or multi-core, multi-node hyperparameter sweep in a few lines of code","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use popular ML frameworks such as XGBoost, Scikit-learn, PyTorch, or TensorFlow","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Support state-of-the-art hyperparameter optimization algorithms such as Population Based Training (PBT), HyperBand, or Asynchronous Successive Halving (ASHA)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Provide cost-saving optimization techniques such as early-stopping, and use of spot instances","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6xUY0sVm7jRfMsXCep38LH","type":"Asset","createdAt":"2022-02-01T22:37:23.446Z","updatedAt":"2022-02-02T23:12:38.916Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"locale":"en-US"},"fields":{"title":"ray tune for distributed hpo","description":"**Figure 3. Ray Tune for tuning and hyperparameter optimization**\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/6xUY0sVm7jRfMsXCep38LH/14da25b5a017187a8e4c2bedcb230a24/FIg_3.png","details":{"size":79935,"image":{"width":960,"height":540}},"fileName":"FIg 3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"By capitalizing on Ray Tuneâs above feature functionality and its easy-to-use APIs, you can write tuning trials to run on a single node (multiple cores) or extend and scale it to the cluster with multiple nodes (multiple cores). Three simple steps are all you need in your Python code.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Step 1: ","nodeType":"text"},{"data":{},"marks":[],"value":"Define your objective function for training with Trainable APIsÂ ","nodeType":"text"}],"nodeType":"heading-4"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5eFkKki0HO5PS2FPucrOtb","type":"Entry","createdAt":"2022-02-02T01:06:36.157Z","updatedAt":"2022-02-02T01:38:34.500Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"objectable funciton","body":"import ray\nfrom ray import tune\n\ndef evaluation_fn(step, width, height):\n    time.sleep(0.1)\n    return (0.1 + width * step / 100)**(-1) + height * 0.1\n\ndef easy_objective(config):\n    # fetch our Hyperparameters sent as arguments\n    width, height = config[\"width\"], config[\"height\"]\n    # Iterate over number of steps\n    for step in range(config[\"steps\"]):\n        # Iterative training function - can be any arbitrary training procedure\n        # Here our objective function is the evaluation_fn\n        intermediate_score = evaluation_fn(step, width, height)\n        # Feed the score back back to Tune for each step.\n        tune.report(iterations=step, mean_loss=intermediate_score)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Step 2: Use Ray Tune APIs to execute tuning","nodeType":"text"}],"nodeType":"heading-4"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"LAWuCK4mSALk03AfkzJcD","type":"Entry","createdAt":"2022-02-02T01:07:39.603Z","updatedAt":"2022-02-02T20:02:55.490Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"tune.run","body":"analysis = tune.run(\n    easy_objective,\n    metric=\"mean_loss\",\n    mode=\"min\",\n    num_samples=5,\t\t# number of trials to parallelize \n    # Define our hyperparameter search space\n    config={\n        \"steps\": 5,      # this is like number of epochs\n        \"width\": tune.uniform(0, 20),\n        \"height\": tune.uniform(-100, 100),\n        \"activation\": tune.grid_search([\"relu\", \"tanh\"]),\n    },\n    verbose=1\n)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Step 3: Analyze the results","nodeType":"text"}],"nodeType":"heading-4"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1DXdTzQA7vcjp8bI8umnAc","type":"Entry","createdAt":"2022-02-02T01:08:29.486Z","updatedAt":"2022-02-02T01:38:56.879Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"analysis.results","body":"analysis.results_df.head(5)\nanalysis.trials\nanalysis.dataframe(metric=\"mean_loss\", mode=\"min\").head(5)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can explore complete ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/examples/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"code and examples ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"of using Ray Tune to tune and distribute your trials at scale ","nodeType":"text"},{"data":{"uri":"https://github.com/anyscale/academy/tree/main/ray-tune"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". All examples follow more or less the same three steps. First, you can execute code on your laptop for experimentation with small datasetsâand then extend and scale to the cluster with large datasets. More importantly, these code examples illustrate how to use efficient ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#tune-search-alg"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"search algorithms","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", combined with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/key-concepts.html#trial-schedulers"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"scheduling algorithms","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", along with integrations with popular tuning libraries such as ","nodeType":"text"},{"data":{"uri":"https://github.com/hyperopt/hyperopt"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Hyperopt","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://optuna.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Optuna","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", giving you the tools and techniques to do model training and tuning at scale.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In short, if you need to tune your model over a hyperparameter search space and you want to distribute its trials to produce the best config for the best model, use Ray Tune.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Aside from tuning, letâs consider distributed model training for deep learning. As a lightweight library for distributed deep learning,","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":" Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" allows you to:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scale your training code from a single node and multiple cores to multiple nodes and multi-cores in a Ray cluster","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Leverage distributed data-parallel deep learning training for PyTorch or TensorFlow models","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Offers composability and interoperability with Ray Tune to tune your distributed model and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html#datasets"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Datasets ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"to train with large amounts of dataÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Track experiments and training runs with MLflow, using callbacks functions","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3NuiadbGu91cZ21krqcgmU","type":"Asset","createdAt":"2022-02-02T01:10:54.953Z","updatedAt":"2022-02-02T23:13:52.816Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"deep learning","description":"**Figure 4. Ray Train for distributed deep learning training**\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/3NuiadbGu91cZ21krqcgmU/98a8dc5a9f2add364858fdd170bba04a/fig_4.png","details":{"size":83275,"image":{"width":960,"height":540}},"fileName":"fig 4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"As with Ray Tune, so with Ray Train, you can use simple, intuitive APIs and follow three steps to get going:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Step 1: Define your neural network","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this case, we use PyTorch, but you can just as easily use TensorFlow.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Vmgci8aM00nf30RdTeswE","type":"Entry","createdAt":"2022-02-02T01:15:01.722Z","updatedAt":"2022-02-02T01:39:12.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"import torch","body":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# import from Ray \nfrom ray import train\nfrom ray.train import Trainer\n\nNUM_SAMPLES = 20             # our dataset for training\nINPUT_SIZE = 20              # inputs or neurons into the first layer\nLAYER_SIZE = 15              # inputs or neurons to the hidden layer\nOUTPUT_SIZE = 5              # outputs to the last layer\n\n# In this example we use a randomly generated dataset.\ninput = torch.randn(NUM_SAMPLES, INPUT_SIZE)         # In normal ML parlance, X\nlabels = torch.randn(NUM_SAMPLES, OUTPUT_SIZE)       # In normal ML parlance, y\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.layer1 = nn.Linear(in_features=INPUT_SIZE, out_features=LAYER_SIZE)\n        # Our activation function\n        self.relu = nn.ReLU()           \n        self.layer2 = nn.Linear(in_features=LAYER_SIZE, out_features=OUTPUT_SIZE)\n\n    def forward(self, input):\n        return self.layer2(self.relu(self.layer1(input)))","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Step 2: Define your training function used by Ray Train","nodeType":"text"}],"nodeType":"heading-4"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5zGyPq874UboGW1kZaBip4","type":"Entry","createdAt":"2022-02-02T01:16:35.834Z","updatedAt":"2022-02-02T01:39:22.107Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"train func","body":"def train_func_distributed(config):\n\n    model = NeuralNetwork()\n    model = train.torch.prepare_model(model, move_to_device=True)\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.1)\n\n    # Iterate over the loop\n    epochs = config.get('NUM_EPOCHS', [20, 40, 60])\n    for epoch in epochs: \n        for e in tqdm(range(epoch)):\n            output = model(input)\n            loss = loss_fn(output, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            if e % epoch == 0:\n                print(f'epoch {epoch}, loss: {loss.item():.3f}')\n\n    # Return anything you want, here we just report back the PID \n    # on which this function runs on a remote or local distributed worker process \n   return os.getpid()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Step 3: Use Ray Train API to distribute trainingÂ ","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"We create a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/api.html#trainer"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Trainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", the main class. This trainer, in turn, will auto-connect to a Ray cluster without the code explicitly calling ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.init(...).","nodeType":"text"},{"data":{},"marks":[],"value":" If itâs running on a laptop, the trainer will connect to Ray locally, using four worker processes on four cores for distributed training. If the laptop is configured to connect to a cluster, the trainer will extend the laptop, via Ray client, to a remote Ray cluster, using four worker nodes and its cores. As your demands grow, where you need more compute, you can scale ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"num_workers","nodeType":"text"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"TqFL4uXalN0FtHjgqTh9x","type":"Entry","createdAt":"2022-02-02T01:20:28.303Z","updatedAt":"2022-02-02T01:39:31.427Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"trainer","body":"trainer = Trainer(backend='torch', num_workers=4)\ntrainer.start()\nresults = trainer.run(train_func_distributed, config={'NUM_EPOCHS': [20, 40, 60]})\ntrainer.shutdown()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can explore complete ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/examples.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"code and examples","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" using Ray Train for distributed training. All examples follow more or less the same three steps. And your training code can be executed on your laptop for experimentation with small datasets or extended to the cluster for horizontal scaling with large datasets.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Note","nodeType":"text"},{"data":{},"marks":[],"value":": Ray Train is still in beta in Ray 1.9 release. However, you can read its rich and extensive feature support in the ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/distributed-deep-learning-with-ray-train-is-now-in-beta"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"announcement blog","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In short, if you need to train your deep learning model using distributed data-parallel with PyTorch or TensorFlow, use Ray Train. But it does not stop you from using both (Ray Tune and Ray Train) when the need arises, as we explain next.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Step 4: Optionally, use Ray Tune with Ray Train APIÂ ","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Tune is interoperable with Ray Train, and it is natively supported in Ray Train. Such interoperability and composability are desirable design traits among Rayâs native libraries. In some use cases, where you want to have configurable epochs or network architectures, you can use both of these Rayâs native libraries together.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For brevity, we will skip the code here. Instead, an ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/user_guide.html#hyperparameter-tuning-ray-tune"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"example guide and steps","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" show how you can convert your ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/api.html#trainer"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Trainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" into a Tune trainableâand use it within Ray Tune.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Tracking model experiments with MLflow","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The last consideration we wish to elaborate on is how to track your experiments and how to examine their results from tuning trials and training runsâ metrics and parameters. Consider the popular open-source machine learning lifecycle management platform ","nodeType":"text"},{"data":{"uri":"https://mlflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". We recommend it for a few reasons. One is that MLflow supports the most popular ML frameworksâ ","nodeType":"text"},{"data":{"uri":"https://mlflow.org/docs/latest/models.html#built-in-model-flavors"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"model flavors","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". The second is that its APIs are fluent and Pythonic. And third, both Ray Tune and Ray Train have robust ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-and-mlflow-taking-distributed-machine-learning-applications-to"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"integrations","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with MLflow.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Whether tuning your models with Ray Tune or distributed training models with Ray Train, you can easily log all your metrics, parameters, and artifacts by providing respective callbacks: ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/tutorials/tune-mlflow.html?highlight=MLflowLoggerCallback#mlflow-logger-api"},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"code"}],"value":"MLflowLoggerCallback","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (for Tune) ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/api.html#mlflowloggercallback"},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"code"}],"value":"MLflowLoggerCallback","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (for Train) in Ray Tuneâs and Trainâs APIs. For example, to use it with Tune:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2JkJRpS9snv5ZMccGo7OnN","type":"Entry","createdAt":"2022-02-02T01:33:25.246Z","updatedAt":"2022-02-02T01:39:44.204Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"train model","body":"def train_model(config):\n    model = ConvNet(config)\n    for i in range(epochs):\n        current_loss = model.train()\n    tune.report(loss=current_loss)\n â¦\n\ntune.run(\n    train_model,\n    config={âlrâ: tune.uniform(0.001, 0.1)},\n    num_samples=100,\n    callbacks=[MLflowLoggerCallback(âmy_experimentâ)])","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This callback interacts with the ","nodeType":"text"},{"data":{"uri":"https://www.mlflow.org/docs/latest/tracking.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLflow Tracking Server","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and logs all its metrics, parameters, and artifacts under an MLflow experiment name â","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"my_experiment.","nodeType":"text"},{"data":{},"marks":[],"value":"â","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Similarly, with Ray Train, you too can provide a callback to log all metrics, parameters, or artifacts reported back to the train driver from the distributed trainers and logged by the train driver. You can peruse a complete code example, in its entirety, for distributed training on big data at scale using MLflow ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/auto_examples/datasets_train/datasets_train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, all the results logged to the ","nodeType":"text"},{"data":{"uri":"https://www.mlflow.org/docs/latest/tracking.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLflow Tracking Server","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" can be viewed in the MLflow UI.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4qT4tBwchhwjuaExKmEtPs","type":"Asset","createdAt":"2022-02-02T01:36:40.278Z","updatedAt":"2022-02-02T01:36:40.278Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ML flow","description":"**Figure 5: MLflow Integrations: Ray Tune \u0026 Train**\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/4qT4tBwchhwjuaExKmEtPs/3d697f7d69645aaba7d72b6aa3a916c7/annimation_train.gif","details":{"size":70111,"image":{"width":512,"height":288}},"fileName":"annimation train.gif","contentType":"image/gif"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Altogether, using a laptop for developing and extending to a Ray cluster, model tuning and distributed training at scale using Ray Tune and Ray Train, and instrumenting and tracking the results of all your trials and training runs with MLflow are vital considerations as part of ML modelâs journey to production.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs recap. In this second part, we elaborated and opinionated on two considerations for ML in production: 1) developing with ease and 2) tuning and training at scale and tracking model experiments. For the first, we shared methods how developers can replicate and isolate their staging or production environmentsâfor Python dependenciesâon their laptops using conda environments; how they can then build and test their ML models, using popular ML frameworks of their choice, with small datasets and then easily extend to a Ray cluster.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"As for the second, we elaborated how Ray Tune, Ray Train, and their respective MLflow integrations help you scale and distribute your model training, tuning, and tracking on your laptop and a Ray cluster. In both these considerations, we suggested starting with your laptop first using Ray and its ecosystem and then extending to a Ray cluster to scale your ML applications in production. One satisfied Ray ML engineer and ","nodeType":"text"},{"data":{"uri":"https://vishnudeva.medium.com/scaling-applications-on-kubernetes-with-ray-23692eb2e6f0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"blogger wrote","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"64NTVTR4AOLpaRiyxc2aEl","type":"Asset","createdAt":"2022-02-02T20:27:34.738Z","updatedAt":"2022-02-02T20:30:14.232Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"user quote","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/64NTVTR4AOLpaRiyxc2aEl/32320019840b2df531a0cb1fc0a0e685/train_quote.png","details":{"size":131021,"image":{"width":1800,"height":413}},"fileName":"train_quote.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Today, no one doubts that we have come a long way from the 1949 British newspaperâs pessimistic lament of the unrealized power of computers. Todayâs powerful laptops offer the best integrated, developer-friendly, and productive environments. And their effortless ability to extend (or connect) to the cluster is as normal as taking a daily-commuter flight and using WiFi at 30,000 feet. No speculation, just a fact.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the final blog in this series, we will examine","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/considerations-for-deploying-machine-learning-models-in-production"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":" consideration #4","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"underline"}],"value":":","nodeType":"text"},{"data":{},"marks":[],"value":" deploying, serving, and inferencing models at scale.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5nXVYNtAtqY3I8OBOoD5Vo","type":"Asset","createdAt":"2022-02-02T23:08:30.016Z","updatedAt":"2022-06-03T21:32:41.302Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Fig 2 ml models blog","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5nXVYNtAtqY3I8OBOoD5Vo/ce5fbd30d99e9e3f163691217c1ccf51/1382624_Blog_ImageIllustration10_Op1_060322.jpg","details":{"size":546031,"image":{"width":1500,"height":1000}},"fileName":"1382624_Blog ImageIllustration10_Op1_060322.jpg","contentType":"image/jpeg"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}}],"hideIntro":true,"recommendations":[]}},"ctaText":"Continue reading","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"61PvWi3ITpKEkCEWxOHWYP","type":"Asset","createdAt":"2022-03-24T22:19:37.859Z","updatedAt":"2022-03-24T22:19:37.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-nodes-light","file":{"url":"//images.ctfassets.net/xjan103pcp94/61PvWi3ITpKEkCEWxOHWYP/d459bdae1bf512a9664f404110d909b1/blog-recommended-content-nodes-light.jpg","details":{"size":41859,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-nodes-light.jpg","contentType":"image/jpeg"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"79MO0POTrTySavkSOhGbjT","type":"Entry","createdAt":"2022-03-24T22:49:50.185Z","updatedAt":"2022-03-24T22:56:17.734Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Serving ML models in production: Common patterns","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2HvnSZK5ssuHHocJvhm04l","type":"Entry","createdAt":"2021-10-01T16:29:03.786Z","updatedAt":"2022-06-22T16:21:03.872Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Serving ML Models in Production: Common Patterns","slug":"serving-ml-models-in-production-common-patterns","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"kQhxB9LxeWeB3noKuPXiJ","type":"Entry","createdAt":"2020-09-20T16:36:33.304Z","updatedAt":"2020-09-20T16:36:33.304Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Simon Mo","slug":"simon-mo","bio":"Simon Mo is a software engineer at Anyscale. Before Anyscale, he was a student at UC Berkeley participating in research at the RISELab. He focuses on studying and building systems for machine learning, in particular, how to make ML model serving systems more efficient, ergonomic, and scalable. He works on Ray Serve at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"fy7QSQIwnpMZWvM1ndmhx","type":"Asset","createdAt":"2020-09-20T16:36:28.219Z","updatedAt":"2020-09-20T16:36:28.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Simon Mo headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/fy7QSQIwnpMZWvM1ndmhx/cdef7ccbc81d0ea4903ed73336e42b48/Simon-Mo-headshot.png","details":{"size":365363,"image":{"width":550,"height":550}},"fileName":"Simon-Mo-headshot.png","contentType":"image/png"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7cbdj2z67h6KL6mmQH7yNJ","type":"Entry","createdAt":"2021-09-30T22:00:29.490Z","updatedAt":"2021-09-30T22:00:29.490Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Edward Oakes","slug":"edward-oakes","link":"https://www.linkedin.com/in/edward-oakes-8bb3b0a6/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-10-01","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Over the past couple years, we've listened to ML practitioners across many different industries to learn and improve the tooling around ML production use cases. Through this, we've seen 4 common patterns of machine learning in production: pipeline, ensemble, business logic, and online learning. In the ML serving space, implementing these patterns typically involves a tradeoff between ease of development and production readiness. Ray Serve was built to support these patterns by being both easy to develop and production ready.","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4xzqeZGkvoF6cBdEauda1I","type":"Entry","createdAt":"2021-09-30T22:02:40.713Z","updatedAt":"2021-09-30T22:02:40.713Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Serving ML Models in Production: Common Patterns","videoUrl":" https://www.youtube.com/watch?v=mM4hJLelzSw","caption":"This post is based on Simon Moâs âPatterns of Machine Learning in Productionâ [talk](https://www.youtube.com/watch?v=mM4hJLelzSw \"Serving ML Models in Production: Common Patterns\") from Ray Summit 2021. "}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Over the past couple years, we've listened to ML practitioners across many different industries to learn and improve the tooling around ML production use cases. Through this, we've seen 4 common patterns of machine learning in production: pipeline, ensemble, business logic, and online learning. In the ML serving space, implementing these patterns typically involves a tradeoff between ease of development and production readiness. ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" was built to support these patterns by being both easy to develop and production ready. It is a scalable and programmable serving framework built on top of ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to help you scale your microservices and ML models in production.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This post goes over:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"What is Ray Serve","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Where Ray Serve fits in the ML Serving Space","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Some common patterns of ML in production ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"How to implement these patterns using Ray Serve","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What is Ray Serve?\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1mw8vMERbD818wGhan6npt","type":"Asset","createdAt":"2021-09-30T22:13:54.942Z","updatedAt":"2021-09-30T23:58:30.276Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"Ray Ecosystem Serving ML Models in Production: Common Patterns","description":"Ray Serve is built on top of the Ray distributed computing platform, allowing it to easily scale to many machines, both in your datacenter and in the cloud. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/1mw8vMERbD818wGhan6npt/36b4a626338057b92520f43d85547154/RayEcosystem.png","details":{"size":494592,"image":{"width":2162,"height":918}},"fileName":"RayEcosystem.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve is an easy-to-use scalable model serving library built on Ray. Some advantages of the library include:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scalability: Horizontally scale across hundreds of processes or machines, while keeping the overhead in single-digit milliseconds","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Multi-model composition: Easily compose multiple models, mix model serving with business logic, and independently scale components, without complex microservices.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/batch.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Batching","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": Native support for batching requests to better utilize hardware and improve throughput.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/http-servehandle.html#serve-fastapi-http"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FastAPI Integration","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":":Â Scale an existing FastAPI server easily or define an HTTP interface for your model using its simple, elegant API.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Framework-agnostic: Use a single toolkit to serve everything from deep learning models built with frameworks like ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/serve/tutorials/pytorch.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Tensorflow and Keras","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", to ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/sklearn.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Scikit-Learn models","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-ikigai-labs-serves-interactive-ai-workflows-at-scale-using-ray-serve"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"arbitrary Python business logic","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can get started with Ray Serve by checking out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve Quickstart.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Where Ray Serve fits in the ML Serving Space","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6IcTIir1U1WBJdSbdygQ08","type":"Asset","createdAt":"2021-09-30T22:18:13.805Z","updatedAt":"2021-09-30T22:18:13.805Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"WhereRayServeFitsIn","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6IcTIir1U1WBJdSbdygQ08/70ceeb0e4f5c8b72b7007c61cb19eed8/WhereRayServeFitsIn.png","details":{"size":92883,"image":{"width":768,"height":444}},"fileName":"WhereRayServeFitsIn.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The image above shows that In the ML serving space, there is typically a tradeoff between ease of development and production readiness.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Web Frameworks","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To deploy a ML service, people typically start with the simplest systems out of the box like Flask or FastAPI. However, even though they can deliver a single prediction well and work well in proofs of concept, they cannot achieve high performance and scaling up is often costly.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Custom Tooling","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If web frameworks fail, teams typically transition to some sort of custom tooling by gluing together several tools to make the system ready for production. However, these custom toolings are typically hard to develop, deploy, and manage.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Specialized Systems","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There is a group of specialized systems for deploying and managing ML models in production. While these systems are great at managing and serving ML models, they often have less flexibility than web frameworks and often have a high learning curve.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve is a web framework specialized for ML model serving. It aspires to be easy to use, easy to deploy, and production ready.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What makes Ray Serve Different?","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"fZRYjZC4BoGvXCytyN3CQ","type":"Asset","createdAt":"2021-09-30T22:25:08.320Z","updatedAt":"2021-09-30T22:25:08.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Many Tools Run 1 Model Well","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/fZRYjZC4BoGvXCytyN3CQ/6e8fc978f76159db3638e3f98b30b0bf/ManyToolsRun1ModelWell.png","details":{"size":131094,"image":{"width":1004,"height":416}},"fileName":"ManyToolsRun1ModelWell.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are so many tools for training and serving one model. These tools help you run and deploy one model very well. The problem is that machine learning in real life is usually not that simple. In a production setting, you can encounter problems like:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Wrangling with infrastructure to scale beyond one copy of a model.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Having to work through complex YAML configuration files, learn custom tooling, and develop MLOps expertise.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Hit scalability or performance issues, unable to deliver business SLA objectives.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Many tools are very costly and can often lead to underutilization of resources. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling out a single model is hard enough. For many ML in production use cases, we observed that complex workloads require composing many different models together. Ray Serve is natively built for this kind of use case involving many models spanning multiple nodes. You can check out ","nodeType":"text"},{"data":{"uri":"https://youtu.be/mM4hJLelzSw?t=651"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"this part of the talk","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" where we go in depth about Ray Serveâs architectural components.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Patterns of ML Models in Production ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4RxLx15blgVAW8gQwQx8w9","type":"Asset","createdAt":"2021-09-30T22:27:41.224Z","updatedAt":"2021-09-30T22:27:41.224Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"13PatternsMLProduction","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4RxLx15blgVAW8gQwQx8w9/831258e5b2d865cba9bba4c00518768a/13PatternsMLProduction.png","details":{"size":64775,"image":{"width":882,"height":426}},"fileName":"13PatternsMLProduction.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A significant portion of ML applications in production follow 4 model patterns:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"pipeline","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"ensemble","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"business logic","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"online learningÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"This section will describe each of these patterns, show how they are used, go over how existing tools typically implement them, and show how Ray Serve can solve these challenges.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pipeline Pattern\n","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4JZHXITWwYcUhGidslJ14A","type":"Asset","createdAt":"2021-09-30T22:30:20.128Z","updatedAt":"2021-09-30T22:30:20.128Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ATypicalComputerVisionPipeline","description":"A typical computer vision pipeline","file":{"url":"//images.ctfassets.net/xjan103pcp94/4JZHXITWwYcUhGidslJ14A/a4bf91ad39086863ee3f31bbe2f9e038/ATypicalComputerVisionPipeline.png","details":{"size":451936,"image":{"width":1476,"height":452}},"fileName":"ATypicalComputerVisionPipeline.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The image above shows a typical computer vision pipeline that uses multiple deep learning models to caption the object in the picture. This pipeline consists of the following steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"1) The raw image goes through common preprocessing like image decoding, augmentation and clipping.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"2) A detection classifier model is used to identify the bounding box and the category. It's a cat.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"3) The image is passed into a keypoint detection model to identify the posture of the object. For the cat image, the model could identify key points like paws, neck, and head.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"4) Lastly, an NLP synthesis model generates a category of what the picture shows. In this case, a standing cat.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical pipeline rarely consists of just one model. To tackle real-life issues, ML applications often use many different models to perform even simple tasks. In general, pipelines break a specific task into many steps, where each step is conquered by a machine learning algorithm or some procedure. Letâs now go over a couple pipelines you might already be familiar with. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Scikit-Learn Pipeline","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"Pipeline([(âscalerâ, StandardScaler()), (âsvcâ, SVC())])","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"scikit-learnâs pipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" can be used to combine multiple âmodelsâ and âprocessing objectsâ together. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Recommendation Systems","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"[EmbeddingLookup(), FeatureInteraction(), NearestNeighbors(), Ranking()] ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are common pipeline patterns in recommendation systems. Item and video recommendations like those that you might see at ","nodeType":"text"},{"data":{"uri":"https://www.amazon.science/publications/two-decades-of-recommender-systems-at-amazon-com"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Amazon","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://research.google/pubs/pub45530/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"YouTube","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", respectively,Â  typically go through multiple stages like embedding lookup, feature interaction, nearest neighbor models, and ranking models.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Common Preprocessing","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"[HeavyWeightMLMegaModel(), DecisionTree()/BoostingModel()]","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are some very common use cases where some massive ML models are used to take care of common processing for text or images. For example, at Facebook, groups of ML researchers at ","nodeType":"text"},{"data":{"uri":"https://ai.facebook.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FAIR","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" create state of the art heavyweight models for vision and text. Then different product groups create downstream models to tackle their business use case (e.g. ","nodeType":"text"},{"data":{"uri":"https://ai.facebook.com/blog/under-the-hood-suicide-prevention-tools-powered-by-ai/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"suicide prevention","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") by implementing smaller models using random forest. The shared common preprocessing step oftentimes are materialized into a ","nodeType":"text"},{"data":{"uri":"https://www.tecton.ai/blog/what-is-a-feature-store/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"feature store pipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"General Pipeline Implementation Options","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2h69wovKC65iUOj3CSs7cu","type":"Asset","createdAt":"2021-09-30T22:35:27.708Z","updatedAt":"2021-09-30T22:35:27.708Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"17PipelineImplementation","description":"Before Ray Serve, implementing pipelines generally meant you had to choose between wrapping your models in a web server or using many specialized microservices. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/2h69wovKC65iUOj3CSs7cu/8462545f5593c27d61c7151fd069a56b/17PipelineImplementation.png","details":{"size":106807,"image":{"width":840,"height":414}},"fileName":"17PipelineImplementation.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In general, there are two approaches to implement a pipeline: wrap your models in a web server or use many specialized microservices.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Wrap Models in a Web Server","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The left side of the image above shows models that get run in a for loop during the web handling path. Whenever a request comes in, models get loaded (they can also be cached) and run through the pipeline. While this is simple and easy to implement, a major flaw is that this is hard to scale and not performant because each request gets handled sequentially.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Many Specialized Microservices","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The right side of the image above shows many specialized microservices where you essentially build and deploy one microservice per model. These microservices can be native ML platforms, ","nodeType":"text"},{"data":{"uri":"https://www.kubeflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Kubeflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", or even hosted services like AWS ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/sagemaker/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". However, as the number of models grow, the complexity and operational cost drastically increases. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Implementing Pipelines in Ray Serve","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11KQrEzA1s2n4S3SUKuH28","type":"Entry","createdAt":"2021-09-30T22:37:57.811Z","updatedAt":"2021-10-01T00:09:45.131Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"implementingPipelinesInRayServe","body":"@serve.deployment\nclass Featurizer: â¦\n\n@serve.deployment\nclass Predictor: â¦\n\n@serve.deployment\nclass Orchestrator\n   def __init__(self):\n      self.featurizer = Featurizer.get_handle()\n      self.predictor = Predictor.get_handle()\n\n   async def __call__(self, inp):\n      feat = await self.featurizer.remote(inp)\n      predicted = await self.predictor.remote(feat)\n      return predicted\n\nif __name__ == â__main__â:\n    Featurizer.deploy()\n    Predictor.deploy()\n    Orchestrator.deploy()","language":"python","caption":"Pseudocode showing how Ray Serve allows deployments to call other deployments"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In Ray Serve, you can directly call other deployments within your deployment.Â  In this code above, there are three deployments. ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Featurizer","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Predictor","nodeType":"text"},{"data":{},"marks":[],"value":" are just regular deployments containing the models. The ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Orchestrator","nodeType":"text"},{"data":{},"marks":[],"value":" receives the web input, passes it to the featurizer process via the featurizer handle, and then passes the computed feature to the predictor process. The interface is just Python and you donât need to learn any new framework or domain-specific language.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve achieves this with a mechanism called ServeHandle which gives you a similar flexibility to embed everything in the web server, without sacrificing performance or scalability. It allows you to directly call other deployments that live in other processes on other nodes. This allows you to scale out each deployment individually and load balance calls across the replicas.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you would like to get a deeper understanding of how this works, ","nodeType":"text"},{"data":{"uri":"https://youtu.be/mM4hJLelzSw?t=650"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"check out this section of Simon Moâs talk","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to learn about Ray Serveâs architecture. If you would like an example of a computer vision pipeline in production, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures#:~:text=A%20Tool%20for%20High%20Performance%20ML%20Systems"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"check out how Robovision used 5 ML models for vehicle detection","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ensemble Pattern","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3XfSBLbvH4TgteNQhSV3jC","type":"Asset","createdAt":"2021-09-30T22:39:02.077Z","updatedAt":"2021-09-30T22:39:02.077Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ensemble Pattern","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3XfSBLbvH4TgteNQhSV3jC/9754f37495ab90cbf5943494e2e13e7a/25Ensemble.png","details":{"size":32706,"image":{"width":730,"height":352}},"fileName":"25Ensemble.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a lot of production use cases, a pipeline is appropriate. However, one limitation of pipelines is that there can often be many upstream models for a given downstream model. This is where ensembles are useful. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"41I4eZnNsSRrneS0QO59Ss","type":"Asset","createdAt":"2021-09-30T22:41:17.502Z","updatedAt":"2021-09-30T22:41:17.502Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Serve Ensemble Usecase","description":"Ensemble Use Cases","file":{"url":"//images.ctfassets.net/xjan103pcp94/41I4eZnNsSRrneS0QO59Ss/aa86f27d79ef6a431743f8a8349a0343/26RayServeEnsembleUsecase.png","details":{"size":139926,"image":{"width":1364,"height":350}},"fileName":"26RayServeEnsembleUsecase.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ensemble patterns involve mixing output from one or more models. They are also called model stacking in some cases. Below are three use cases of ensemble patterns.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Model Update","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"New models are developed and trained over time. This means there will always be new versions of the model in production. The question becomes, how do you make sure the new models are valid and performant in live online traffic scenarios? One way to do this is by putting some portion of the traffic through the new model. You still select the output from the known good model, but you are also collecting live output from the newer version of the models in order to validate it.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Aggregation","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The most widely known use case is for aggregation. For regression models, outputs from multiple models are averaged. For classification models, the output will be a voted version of multiple modelsâ output. For example, if two models vote for cat and one model votes for dog, then the aggregated output will be cat. Aggregation helps combat inaccuracy in individual models and generally makes the output more accurate and âsaferâ.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dynamic Selection","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another use case for ensemble models is to dynamically perform model selection given input attributes. For example, if the input contains a cat, model A will be used because it is specialized for cats.Â  If the input contains a dog, model B will be used because it is specialized for dogs. Note that this dynamic selection doesnât necessarily mean the pipeline itself has to be static. It could also be selecting models given user feedback. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"General Ensemble Implementation Options","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"58Z7G6vwRfpDBSIepG0fQZ","type":"Asset","createdAt":"2021-09-30T23:40:47.037Z","updatedAt":"2021-09-30T23:41:09.505Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"General Ensemble Implementation","description":"Before Ray Serve, implementing ensembles generally meant you had to choose between wrapping your models in a web server or using many specialized microservices. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/58Z7G6vwRfpDBSIepG0fQZ/7e666548b9cef471ae4194991784f455/27EnsembleDeployment.png","details":{"size":167575,"image":{"width":1332,"height":510}},"fileName":"27EnsembleDeployment.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ensemble implementations suffer the same sort of issues as pipelines. It is simple to wrap models in a web server, but it is not performant. When you use specialized microservices, you end up having a lot of operational overhead as the number of microservices scale with the number of models. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71qMXFLqM4EeyAIHCMnVzY","type":"Asset","createdAt":"2021-09-30T23:44:12.374Z","updatedAt":"2021-09-30T23:44:12.374Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2020 Anyscale demo","description":"Ensemble example from the [2020 Anyscale demo](https://youtu.be/8GTd8Y_JGTQ)","file":{"url":"//images.ctfassets.net/xjan103pcp94/71qMXFLqM4EeyAIHCMnVzY/a76ac67159a965c670a53a944404b39a/28RayServeEnsembleRaySummit.png","details":{"size":371197,"image":{"width":1341,"height":600}},"fileName":"28RayServeEnsembleRaySummit.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray Serve, the kind of pattern is incredibly simple. You can look at the ","nodeType":"text"},{"data":{"uri":"https://youtu.be/8GTd8Y_JGTQ"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"2020 Anyscale demo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to see how to utilize Ray Serveâs handle mechanism to perform dynamic model selection.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another example of using Ray Serve for ensembling is Wildlife Studios combining output of many classifiers for a single prediction. You can check out how they were able to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/wildlife-studios-serves-in-game-offers-3x-faster-at-1-10th-the-cost-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"serve in-game offers 3x faster with Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Business Logic Pattern","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Productionizing machine learning will always involve business logic. No models can stand-alone and serve requests by themselves. Business logic patterns involve everything thatâs involved in a common ML task that is not ML model inference. This includes:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Database lookups for relational records","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Web API calls for external services","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Feature store lookup for pre-compute feature vectors","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Feature transformations like data validation, encoding, and decoding.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"General Business Logic Implementation Options","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ixwTFsSoaWNza9Mp3OPjj","type":"Asset","createdAt":"2021-09-30T23:45:47.219Z","updatedAt":"2021-09-30T23:45:47.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"General Business Logic Implementation Options","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6ixwTFsSoaWNza9Mp3OPjj/8144cd2bea5d42a6333ad1d1f55270c0/31BusinessLogicInAction.png","details":{"size":111620,"image":{"width":984,"height":462}},"fileName":"31BusinessLogicInAction.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The pseudocode for the web handler above does the following things:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"It loads the model (letâs say from S3)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Validates the input from the database","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Looks up some pre-computed features from the feature store.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Only after the web handler completes these business logic steps are the inputs passed through to ML models. The problem is that the requirements of model inference and business logic lead to the server being ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"both network bounded and compute bounded","nodeType":"text"},{"data":{},"marks":[],"value":". This is due to the model loading step, database lookup, and feature store lookups being network bounded and I/O heavy as well as the model inference being compute bound and memory hungry. The combination of these factors lead to an inefficient utilization of resources. Scaling will be expensive.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2tzQhijrmJQ1i1dtp8SeF","type":"Asset","createdAt":"2021-09-30T23:47:03.551Z","updatedAt":"2021-09-30T23:47:03.551Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Web handler approach (left) and microservices approach (right)","description":"Web handler approach (left) and microservices approach (right)","file":{"url":"//images.ctfassets.net/xjan103pcp94/2tzQhijrmJQ1i1dtp8SeF/de61106c82ef5dd503d6200ecdaebd30/32WheretoRunBusinessLogic.png","details":{"size":80148,"image":{"width":1337,"height":464}},"fileName":"32WheretoRunBusinessLogic.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A common way to increase utilization is to split models out into model servers or microservices.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The web app is purely network bounded while the model servers are compute bounded. However, a common problem is the interface between the two. If you put too much business logic into the model server, then the model servers become a mix of network bounded and compute bounded calls.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you let the model servers be pure model servers, then you have the â","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"tensor-in, tensor-out","nodeType":"text"},{"data":{},"marks":[],"value":"â interface problem. The input types for model servers are typically very constrained to just tensors or some alternate form of it. This makes it hard to keep the pre-processing, post-processing, and business logic in sync with the model itself.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"It becomes hard to reason about the interaction between the processing logic and the model itself because during training, the processing logic and models are tightly coupled, but when serving, they are split across two servers and two implementations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Neither the web handler approach nor the microservices approach is satisfactory.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Implementing Business Logic in Ray Serve","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4WEB4OkdjgTl7azrJctx4A","type":"Asset","createdAt":"2021-09-30T23:48:49.625Z","updatedAt":"2021-09-30T23:48:49.625Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Business Logic in Ray Serve","description":"Business Logic in Ray Serve","file":{"url":"//images.ctfassets.net/xjan103pcp94/4WEB4OkdjgTl7azrJctx4A/703ef612cc6e50aaa48433deef6ba985/33BusinessLogicInRayServe.png","details":{"size":114370,"image":{"width":856,"height":398}},"fileName":"33BusinessLogicInRayServe.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray Serve, you just have to make some simple changes to the old web server to alleviate the issues described above. Instead of loading the model directly, you can retrieve a ServeHandle that wraps the model, and offload the computation to another deployment. All the data types are preserved and there is no need to write âtensor-in, tensor-outâ API calls--you can just pass in regular Python types. Additionally, the model deployment class can stay in the same file, and be deployed together with the prediction handler. This makes it easy to understand and debug the code.Â ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"model.remote","nodeType":"text"},{"data":{},"marks":[],"value":" looks like just a function and you can easily trace it to the model deployment class.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this way, Ray Serve helps you split up the business logic and inference into two separation components, one I/O heavy and the other compute heavy. This allows you to scale each piece individually, without losing the ease of deployment. Additionally, because ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"model.remote","nodeType":"text"},{"data":{},"marks":[],"value":" is just a function call, itâs a lot easier to test and debug than separate external services.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve FastAPI Integration","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"HStDeyRVrI7LM2WdUP2s6","type":"Asset","createdAt":"2021-09-30T23:50:19.687Z","updatedAt":"2021-09-30T23:50:19.687Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Serve: Ingress with FastAPI","description":"Ray Serve: Ingress with FastAPI","file":{"url":"//images.ctfassets.net/xjan103pcp94/HStDeyRVrI7LM2WdUP2s6/4ff62672c24788b1d07d59736b85817b/34RayServeIngress.png","details":{"size":297542,"image":{"width":1656,"height":838}},"fileName":"34RayServeIngress.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"An important part of implementing business logic and other patterns is authentication and input validation. ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/http-servehandle.html#serve-fastapi-http"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve natively integrates with FastAPI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which is a type safe and ergonomic web framework. ","nodeType":"text"},{"data":{"uri":"https://fastapi.tiangolo.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FastAPI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" has features like automatic dependency injection, type checking and validation, and OpenAPI doc generation.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray Serve, you can directly pass the FastAPI app object into it with ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"@serve.ingress","nodeType":"text"},{"data":{},"marks":[],"value":". This decorator makes sure that all existing FastAPI routes still work and that you can attach new routes with the deployment class so states like loaded models, and networked database connections can easily be managed. Architecturally, we just made sure that your FastAPI app is correctly embedded into the replica actor and the FastAPI app can scale out across many Ray nodes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Online LearningÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Online learning is an emerging pattern thatâs become more and more widely used. It refers to a model running in production that is constantly being updated, trained, validated and deployed. Below are three use cases of online learning patterns.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dynamically Learn Model Weights","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are use cases for dynamically learning model weights online. As users interact with your services, these updated model weights can contribute to a personalized model for each user or group. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6J7nynhCYa2vV5Ud57cn0m","type":"Asset","createdAt":"2021-09-30T23:53:34.472Z","updatedAt":"2021-09-30T23:53:34.472Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Online Learning example at Ant Group (image courtesy of Ant Group) ","description":"[Online Learning example at Ant Group](https://www.anyscale.com/blog/online-resource-allocation-with-ray-at-ant-group) (image courtesy of Ant Group) ","file":{"url":"//images.ctfassets.net/xjan103pcp94/6J7nynhCYa2vV5Ud57cn0m/0362b38997ac5c4e8c1db756cf4a1dcd/38OnlineLearningAntGroup.png","details":{"size":311903,"image":{"width":1208,"height":533}},"fileName":"38OnlineLearningAntGroup.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"One case study of online learning consists of an online resource allocation business solution at Ant Group. The model is trained from offline data, then combined with real time streaming data source, and then served live traffic. One thing to note is that online learning systems are drastically more complex than their static serving counterparts. In this case, putting models in the web server, or even splitting it up into multiple microservices, would not help with the implementation. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dynamically Learn Parameters to Orchestrate Models","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are also use cases for learning parameters to orchestrate or compose models, for example, ","nodeType":"text"},{"data":{"uri":"https://dl.acm.org/doi/fullHtml/10.1145/3383313.3411550"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"learning which model a user prefers","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". This manifests often in model selection scenarios or contextual bandit algorithms.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Reinforcement Learning","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Reinforcement learning is the branch of machine learning that trains agents to interact with the environment. The environment can be the physical world or a simulated environment. You can learn about reinforcement learning ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and see how you can deploy a RL model using Ray Serve ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/rllib.html#serve-rllib-tutorial"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â  ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ConclusionÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Zmo0bB3BCXWqX3bfhtxlg","type":"Asset","createdAt":"2021-09-30T23:55:14.299Z","updatedAt":"2021-09-30T23:55:14.299Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"WhereRayServeFitsIn","description":"Ray Serve is easy to develop and production ready.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Zmo0bB3BCXWqX3bfhtxlg/f6a8378f2cbb372c458904651e7bd3c1/WhereRayServeFitsIn.png","details":{"size":92883,"image":{"width":768,"height":444}},"fileName":"WhereRayServeFitsIn.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This post went over 4 main patterns of machine learning in production, how Ray Serve can help you natively scale and work with complex architectures, and how ML in production often means many models in production. Ray Serve is built with all of this in mind on top of the distributed runtime Ray. If youâre interested in learning more about Ray, you can check out the ","nodeType":"text"},{"data":{"uri":"https://ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", join us on ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discourse","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and check out the ","nodeType":"text"},{"data":{"uri":"http://tinyurl.com/ray-white-paper"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"whitepaper","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"! If you're interested in working with us to make it easier to leverage Ray, we're ","nodeType":"text"},{"data":{"uri":"https://jobs.lever.co/anyscale"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"hiring","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"!","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"BBDpqdGlEcQZHPoQxytXf","type":"Asset","createdAt":"2021-10-01T00:19:41.125Z","updatedAt":"2021-10-01T00:19:41.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Where Ray Serve Fits In","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/BBDpqdGlEcQZHPoQxytXf/7dff7d7f06b7a5d55cfd255cc3cd88e6/WhereRayServeFitsIn.png","details":{"size":92883,"image":{"width":768,"height":444}},"fileName":"WhereRayServeFitsIn.png","contentType":"image/png"}}},"recommendations":[],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BzSmyQ2ojATOq6qOpD4PX","type":"Entry","createdAt":"2021-10-02T19:52:20.790Z","updatedAt":"2021-10-02T19:52:20.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Technology","identifier":"technology"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}}],"hideIntro":true}},"thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"FaPihBS7H8opDps1E8dXE","type":"Asset","createdAt":"2022-03-21T15:11:27.810Z","updatedAt":"2022-03-21T15:28:52.364Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"blog-recommended-content-gears-dark","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/FaPihBS7H8opDps1E8dXE/20dca1470a6453ffd33c172009cc488a/blog-recommended-content-gears-dark.jpg","details":{"size":40281,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-gears-dark.jpg","contentType":"image/jpeg"}}},"recommendations":null}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"64NABlk75VOHsG6QhKyhZ7","type":"Entry","createdAt":"2021-11-04T16:08:54.107Z","updatedAt":"2022-06-22T16:15:42.611Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Ray version 1.8 has been released","slug":"ray-version-1-8-has-been-released","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-11-04","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},"intro":"Ray version 1.8 has been released! Release highlights include: Ray SGD has been renamed to Ray Train, Ray Datasets is now in beta, and experimental support for Ray on Apple Silicon (M1 Macs)\n","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray version 1.8 has been released! Release highlights include:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray SGD has been renamed to Ray Train.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is now in beta! The beta release includes a new integration with Ray Train yielding scalable ML ingest for distributed training.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Experimental support for Ray on Apple Silicon (M1 Macs)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can run ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"pip install -U ray","nodeType":"text"},{"data":{},"marks":[],"value":" to access these features and more. With that, letâs go over the highlights.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray SGD has been renamed to Ray TrainÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The distributed deep learning library Ray SGD has been rebranded to Ray Train! Ray Train, like its predecessor, allows you to easily scale your model training using Pytorch, Tensorflow, or Horovod. The new documentation landing page can be found ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Also in this release, weâve developed a tight integration with Ray Datasets for distributed data loading while training. See the section below for more info on this integration!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Datasets is now in beta","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Datasets is now in beta! The beta release includes a new integration with Ray Train yielding scalable ML ingest for distributed training. This integration is meant to support use cases like:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Large Datasets","nodeType":"text"},{"data":{},"marks":[],"value":": With Ray Datasets, you can easily work with datasets that are too big to fit on a single node. Ray Datasets will distribute the dataset across the Ray Cluster and allow you to perform dataset operations (map, filter, etc.) on the distributed dataset.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Automatic locality-aware sharding","nodeType":"text"},{"data":{},"marks":[],"value":": If provided a Ray Dataset, Ray Train will automatically shard the dataset and assign each shard to a training worker while minimizing cross-node data transfer. Unlike with standard Torch or TensorFlow datasets, each training worker will only load its assigned shard into memory rather than the entire Dataset.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Pipelined Execution","nodeType":"text"},{"data":{},"marks":[],"value":": Ray Datasets also supports pipelining, meaning that data processing operations can be run concurrently with training. Training is no longer blocked on expensive data processing operations (such as global shuffling) and this minimizes the amount of time your GPUs are idle. See ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/dataset-pipeline.html#dataset-pipeline"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Dataset Pipelines","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for more information.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Check out the docs ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", try it out for your ML ingest and batch inference workloads, and let us know how it goes!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Experimental support for Ray on Apple Silicon (M1 Macs)","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Apple is transitioning their laptops and desktop from using Intel chips to their own M1 chip. The M1 chip promises to be faster because it is optimized for Mac systems in which small size and power efficiency are critically important.Â  From a CPU architecture perspective, ","nodeType":"text"},{"data":{"uri":"https://www.anaconda.com/blog/apple-silicon-transition"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"the M1 has a couple significant differences from previous Intel CPUs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The CPU architecture went from x86 to ARM","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The on-chip GPU is now made by Apple","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Custom Apple accelerators are now available such as the Apple Neural Engine (ANE)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"System RAM is shared by the CPU, GPU, and ANE cores","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"These changes mean that for optimal performance on M1 macs, software needs to be built to support these chips. In Ray version 1.8, we are adding experimental support for Ray on M1 Macs. To try it out without having dependency issues, make sure to check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/installation.html#apple-silicon-support"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"installation instructions","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Learn more","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"That sums up the release highlights. To learn about all the features and enhancements in this release, visit the ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-1.8.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"release notes","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". If you would like to keep up to date with all things Ray, follow ","nodeType":"text"},{"data":{"uri":"https://twitter.com/raydistributed"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"@raydistributed","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on Twitter, and sign up for the ","nodeType":"text"},{"data":{"uri":"https://anyscale.us5.list-manage.com/subscribe?u=524b25758d03ad7ec4f64105f\u0026id=d94e960a03"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray newsletter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2kFSOyAxnApA6wjknCVaqF","type":"Asset","createdAt":"2021-11-04T16:08:44.379Z","updatedAt":"2021-11-04T16:08:44.379Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray 1.8","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2kFSOyAxnApA6wjknCVaqF/7a975ede4eb8b90a8e75c53f9a0add4b/option_1__1_.png","details":{"size":1569691,"image":{"width":1762,"height":988}},"fileName":"option 1 (1).png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true,"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3NSDhfdYQA0CjW1Cbv4qw1","type":"Entry","createdAt":"2021-10-11T15:56:26.073Z","updatedAt":"2022-06-22T16:19:07.830Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Ray version 1.7 has been released","slug":"ray-version-1-7-has-been-released","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-10-11","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},"intro":"Ray version 1.7 is here. Highlights include: Ray SGD v2 and  is in alpha, Ray Workflows is in alpha, and major enhancements to the C++ API","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray version 1.7 has been released. Release highlights include:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray SGD v2 is now in alpha. The v2 version introduces APIs that focus on ease of use and composability.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Workflows is now in alpha. Try it out for your large-scale data science, ML, and long-running business workflows.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Major enhancements to the C++ API which enables you to build a C++ distributed system easily, just like the Python and the Java API.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can run ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"pip install -U ray","nodeType":"text"},{"data":{},"marks":[],"value":" to access these features and more. With that, letâs dive into the highlights.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray SGD v2 is now in alpha","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In Ray 1.7, we are rolling out a new and more streamlined version of Ray SGD that focuses on usability and composability. Ray SGD v2 has a much simpler API, has support for more deep learning backends, integrates better with other libraries in the Ray ecosystem, and will continue to be actively developed with more features.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"So, what is changing with the API and how does it affect you?Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"1. There is now a single Trainer interface for all backends (torch, tensorflow, horovod), and the backend is simply specified via an argument. Any features that we add to Ray SGD will be supported for all backends, and there wonât be any API divergence like there was with a separate TorchTrainer and TFTrainer.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"2. You no longer have to make your training logic fit into a restrictive interface as ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"TrainingOperator","nodeType":"text"},{"data":{},"marks":[],"value":" and creator functions have been replaced by a more natural user-defined training function. This allows you to have more flexibility over your training code and reduces the overhead to move to Ray SGD.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"3. Rather than iteratively calling ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"trainer.train()","nodeType":"text"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"trainer.validate()","nodeType":"text"},{"data":{},"marks":[],"value":" for each epoch, the training function now defines the full training execution and can be run via ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"trainer.run(train_func)","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Most importantly, ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-1.7.0/raysgd/v2/raysgd.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray SGD v2","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" will continue to have all the great features of v1, such as out of the box fault tolerance and checkpoint management, with more features on the roadmap. It is in alpha in version 1.7 and is quickly maturing; we would love your feedback. To learn more about migrating from Ray SGD v1, visit the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-1.7.0/raysgd/v2/migration-guide.html#migrating-from-ray-sgd-v1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"migration guide","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Workflows is now in alpha!Â ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5IjkL5UFrhez09PDXfcYia","type":"Asset","createdAt":"2021-10-08T17:24:47.211Z","updatedAt":"2021-10-08T17:24:47.211Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"RayWorkflows","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5IjkL5UFrhez09PDXfcYia/698d52c6360b1f3d1f345fc71ce60910/RayWorkflows.png","details":{"size":238237,"image":{"width":2854,"height":924}},"fileName":"RayWorkflows.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Workflow engines often impose trade-offs in flexibility or performance. By building on Ray core and adding durability, Ray Workflows aims to be the best of both worlds: very high performance/flexibility and durability. It provides high-performance, durable application workflows using Ray tasks as the underlying execution engine. It is intended to support both large-scale workflows (e.g., ML and data pipelines) and long-running business workflows (when used together with Ray Serve). More specifically, benefits of Workflows include:Â Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Flexibility:","nodeType":"text"},{"data":{},"marks":[],"value":" Combine the flexibility of Rayâs dynamic task graphs with strong durability guarantees. Branch or loop conditionally based on runtime data. Use Ray distributed libraries seamlessly within workflow steps.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Performance:","nodeType":"text"},{"data":{},"marks":[],"value":" Workflows offers sub-second overheads for task launch and supports workflows with hundreds of thousands of steps. Take advantage of the Ray object store to pass distributed datasets between steps with zero-copy overhead.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dependency management:","nodeType":"text"},{"data":{},"marks":[],"value":" Workflows leverages Rayâs runtime environment feature to snapshot the code dependencies of a workflow. This enables management of workflows and virtual actors as code is upgraded over time.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Workflows is available as alpha in Ray 1.7. You can learn more about the library in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-1.7.0/workflows/concepts.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and by giving it a try! Let us know what you think.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Major enhancements to C++ API!","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Originally, Ray only supported the Python API. In mid-2018, ","nodeType":"text"},{"data":{"uri":"https://www.youtube.com/watch?v=AFHyIEIfQYw"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ant Group contributed the Java API to the Ray Project","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Recently, Ant Group contributed the ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"experimental","nodeType":"text"},{"data":{},"marks":[],"value":" C++ API to Ray. Since Ray Core is primarily C++, this new API can be used to seamlessly connect the user layer and the core layer so that there can be no inter-language overhead.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Besides allowing you to build C++ distributed systems easily, we envision that the C++ API can be used to better meet business needs in some high-performance scenarios due to a lack of inter-language overhead and itâs lightweight characteristics.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The C++ API can be installed by runningÂ  ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"pip install -U ray[cpp]","nodeType":"text"},{"data":{},"marks":[],"value":". You can also learn how to use the API by running Â ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray cpp --help","nodeType":"text"},{"data":{},"marks":[],"value":". If you would like to be notified of an upcoming deep-dive blog on this topic, you can follow us on ","nodeType":"text"},{"data":{"uri":"https://twitter.com/raydistributed"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Twitter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or sign up for the ","nodeType":"text"},{"data":{"uri":"https://anyscale.us5.list-manage.com/subscribe?u=524b25758d03ad7ec4f64105f\u0026id=d94e960a03"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Newsletter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Learn more","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"That sums up the release highlights. To see all the features and enhancements in this release, visit the ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-1.7.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"release notes","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38DUPPalvavUgTl9AIbYvy","type":"Asset","createdAt":"2021-10-11T15:56:17.691Z","updatedAt":"2021-10-11T15:56:17.691Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray 1.7","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/38DUPPalvavUgTl9AIbYvy/afb23a92a6cf3a102fcab412e52fd78b/Ray_1.7.png","details":{"size":1377872,"image":{"width":1760,"height":986}},"fileName":"Ray 1.7.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true,"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2HvnSZK5ssuHHocJvhm04l","type":"Entry","createdAt":"2021-10-01T16:29:03.786Z","updatedAt":"2022-06-22T16:21:03.872Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Serving ML Models in Production: Common Patterns","slug":"serving-ml-models-in-production-common-patterns","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"kQhxB9LxeWeB3noKuPXiJ","type":"Entry","createdAt":"2020-09-20T16:36:33.304Z","updatedAt":"2020-09-20T16:36:33.304Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Simon Mo","slug":"simon-mo","bio":"Simon Mo is a software engineer at Anyscale. Before Anyscale, he was a student at UC Berkeley participating in research at the RISELab. He focuses on studying and building systems for machine learning, in particular, how to make ML model serving systems more efficient, ergonomic, and scalable. He works on Ray Serve at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"fy7QSQIwnpMZWvM1ndmhx","type":"Asset","createdAt":"2020-09-20T16:36:28.219Z","updatedAt":"2020-09-20T16:36:28.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Simon Mo headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/fy7QSQIwnpMZWvM1ndmhx/cdef7ccbc81d0ea4903ed73336e42b48/Simon-Mo-headshot.png","details":{"size":365363,"image":{"width":550,"height":550}},"fileName":"Simon-Mo-headshot.png","contentType":"image/png"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7cbdj2z67h6KL6mmQH7yNJ","type":"Entry","createdAt":"2021-09-30T22:00:29.490Z","updatedAt":"2021-09-30T22:00:29.490Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Edward Oakes","slug":"edward-oakes","link":"https://www.linkedin.com/in/edward-oakes-8bb3b0a6/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-10-01","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Over the past couple years, we've listened to ML practitioners across many different industries to learn and improve the tooling around ML production use cases. Through this, we've seen 4 common patterns of machine learning in production: pipeline, ensemble, business logic, and online learning. In the ML serving space, implementing these patterns typically involves a tradeoff between ease of development and production readiness. Ray Serve was built to support these patterns by being both easy to develop and production ready.","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4xzqeZGkvoF6cBdEauda1I","type":"Entry","createdAt":"2021-09-30T22:02:40.713Z","updatedAt":"2021-09-30T22:02:40.713Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Serving ML Models in Production: Common Patterns","videoUrl":" https://www.youtube.com/watch?v=mM4hJLelzSw","caption":"This post is based on Simon Moâs âPatterns of Machine Learning in Productionâ [talk](https://www.youtube.com/watch?v=mM4hJLelzSw \"Serving ML Models in Production: Common Patterns\") from Ray Summit 2021. "}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Over the past couple years, we've listened to ML practitioners across many different industries to learn and improve the tooling around ML production use cases. Through this, we've seen 4 common patterns of machine learning in production: pipeline, ensemble, business logic, and online learning. In the ML serving space, implementing these patterns typically involves a tradeoff between ease of development and production readiness. ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" was built to support these patterns by being both easy to develop and production ready. It is a scalable and programmable serving framework built on top of ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to help you scale your microservices and ML models in production.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This post goes over:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"What is Ray Serve","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Where Ray Serve fits in the ML Serving Space","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Some common patterns of ML in production ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"How to implement these patterns using Ray Serve","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What is Ray Serve?\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1mw8vMERbD818wGhan6npt","type":"Asset","createdAt":"2021-09-30T22:13:54.942Z","updatedAt":"2021-09-30T23:58:30.276Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"Ray Ecosystem Serving ML Models in Production: Common Patterns","description":"Ray Serve is built on top of the Ray distributed computing platform, allowing it to easily scale to many machines, both in your datacenter and in the cloud. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/1mw8vMERbD818wGhan6npt/36b4a626338057b92520f43d85547154/RayEcosystem.png","details":{"size":494592,"image":{"width":2162,"height":918}},"fileName":"RayEcosystem.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve is an easy-to-use scalable model serving library built on Ray. Some advantages of the library include:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scalability: Horizontally scale across hundreds of processes or machines, while keeping the overhead in single-digit milliseconds","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Multi-model composition: Easily compose multiple models, mix model serving with business logic, and independently scale components, without complex microservices.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/batch.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Batching","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": Native support for batching requests to better utilize hardware and improve throughput.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/http-servehandle.html#serve-fastapi-http"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FastAPI Integration","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":":Â Scale an existing FastAPI server easily or define an HTTP interface for your model using its simple, elegant API.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Framework-agnostic: Use a single toolkit to serve everything from deep learning models built with frameworks like ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/serve/tutorials/pytorch.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Tensorflow and Keras","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", to ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/sklearn.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Scikit-Learn models","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-ikigai-labs-serves-interactive-ai-workflows-at-scale-using-ray-serve"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"arbitrary Python business logic","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can get started with Ray Serve by checking out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve Quickstart.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Where Ray Serve fits in the ML Serving Space","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6IcTIir1U1WBJdSbdygQ08","type":"Asset","createdAt":"2021-09-30T22:18:13.805Z","updatedAt":"2021-09-30T22:18:13.805Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"WhereRayServeFitsIn","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6IcTIir1U1WBJdSbdygQ08/70ceeb0e4f5c8b72b7007c61cb19eed8/WhereRayServeFitsIn.png","details":{"size":92883,"image":{"width":768,"height":444}},"fileName":"WhereRayServeFitsIn.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The image above shows that In the ML serving space, there is typically a tradeoff between ease of development and production readiness.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Web Frameworks","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To deploy a ML service, people typically start with the simplest systems out of the box like Flask or FastAPI. However, even though they can deliver a single prediction well and work well in proofs of concept, they cannot achieve high performance and scaling up is often costly.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Custom Tooling","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If web frameworks fail, teams typically transition to some sort of custom tooling by gluing together several tools to make the system ready for production. However, these custom toolings are typically hard to develop, deploy, and manage.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Specialized Systems","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There is a group of specialized systems for deploying and managing ML models in production. While these systems are great at managing and serving ML models, they often have less flexibility than web frameworks and often have a high learning curve.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve is a web framework specialized for ML model serving. It aspires to be easy to use, easy to deploy, and production ready.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What makes Ray Serve Different?","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"fZRYjZC4BoGvXCytyN3CQ","type":"Asset","createdAt":"2021-09-30T22:25:08.320Z","updatedAt":"2021-09-30T22:25:08.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Many Tools Run 1 Model Well","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/fZRYjZC4BoGvXCytyN3CQ/6e8fc978f76159db3638e3f98b30b0bf/ManyToolsRun1ModelWell.png","details":{"size":131094,"image":{"width":1004,"height":416}},"fileName":"ManyToolsRun1ModelWell.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are so many tools for training and serving one model. These tools help you run and deploy one model very well. The problem is that machine learning in real life is usually not that simple. In a production setting, you can encounter problems like:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Wrangling with infrastructure to scale beyond one copy of a model.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Having to work through complex YAML configuration files, learn custom tooling, and develop MLOps expertise.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Hit scalability or performance issues, unable to deliver business SLA objectives.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Many tools are very costly and can often lead to underutilization of resources. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling out a single model is hard enough. For many ML in production use cases, we observed that complex workloads require composing many different models together. Ray Serve is natively built for this kind of use case involving many models spanning multiple nodes. You can check out ","nodeType":"text"},{"data":{"uri":"https://youtu.be/mM4hJLelzSw?t=651"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"this part of the talk","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" where we go in depth about Ray Serveâs architectural components.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Patterns of ML Models in Production ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4RxLx15blgVAW8gQwQx8w9","type":"Asset","createdAt":"2021-09-30T22:27:41.224Z","updatedAt":"2021-09-30T22:27:41.224Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"13PatternsMLProduction","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4RxLx15blgVAW8gQwQx8w9/831258e5b2d865cba9bba4c00518768a/13PatternsMLProduction.png","details":{"size":64775,"image":{"width":882,"height":426}},"fileName":"13PatternsMLProduction.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A significant portion of ML applications in production follow 4 model patterns:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"pipeline","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"ensemble","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"business logic","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"online learningÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"This section will describe each of these patterns, show how they are used, go over how existing tools typically implement them, and show how Ray Serve can solve these challenges.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pipeline Pattern\n","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4JZHXITWwYcUhGidslJ14A","type":"Asset","createdAt":"2021-09-30T22:30:20.128Z","updatedAt":"2021-09-30T22:30:20.128Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ATypicalComputerVisionPipeline","description":"A typical computer vision pipeline","file":{"url":"//images.ctfassets.net/xjan103pcp94/4JZHXITWwYcUhGidslJ14A/a4bf91ad39086863ee3f31bbe2f9e038/ATypicalComputerVisionPipeline.png","details":{"size":451936,"image":{"width":1476,"height":452}},"fileName":"ATypicalComputerVisionPipeline.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The image above shows a typical computer vision pipeline that uses multiple deep learning models to caption the object in the picture. This pipeline consists of the following steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"1) The raw image goes through common preprocessing like image decoding, augmentation and clipping.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"2) A detection classifier model is used to identify the bounding box and the category. It's a cat.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"3) The image is passed into a keypoint detection model to identify the posture of the object. For the cat image, the model could identify key points like paws, neck, and head.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"4) Lastly, an NLP synthesis model generates a category of what the picture shows. In this case, a standing cat.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical pipeline rarely consists of just one model. To tackle real-life issues, ML applications often use many different models to perform even simple tasks. In general, pipelines break a specific task into many steps, where each step is conquered by a machine learning algorithm or some procedure. Letâs now go over a couple pipelines you might already be familiar with. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Scikit-Learn Pipeline","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"Pipeline([(âscalerâ, StandardScaler()), (âsvcâ, SVC())])","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"scikit-learnâs pipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" can be used to combine multiple âmodelsâ and âprocessing objectsâ together. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Recommendation Systems","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"[EmbeddingLookup(), FeatureInteraction(), NearestNeighbors(), Ranking()] ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are common pipeline patterns in recommendation systems. Item and video recommendations like those that you might see at ","nodeType":"text"},{"data":{"uri":"https://www.amazon.science/publications/two-decades-of-recommender-systems-at-amazon-com"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Amazon","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://research.google/pubs/pub45530/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"YouTube","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", respectively,Â  typically go through multiple stages like embedding lookup, feature interaction, nearest neighbor models, and ranking models.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Common Preprocessing","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"[HeavyWeightMLMegaModel(), DecisionTree()/BoostingModel()]","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are some very common use cases where some massive ML models are used to take care of common processing for text or images. For example, at Facebook, groups of ML researchers at ","nodeType":"text"},{"data":{"uri":"https://ai.facebook.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FAIR","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" create state of the art heavyweight models for vision and text. Then different product groups create downstream models to tackle their business use case (e.g. ","nodeType":"text"},{"data":{"uri":"https://ai.facebook.com/blog/under-the-hood-suicide-prevention-tools-powered-by-ai/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"suicide prevention","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") by implementing smaller models using random forest. The shared common preprocessing step oftentimes are materialized into a ","nodeType":"text"},{"data":{"uri":"https://www.tecton.ai/blog/what-is-a-feature-store/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"feature store pipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"General Pipeline Implementation Options","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2h69wovKC65iUOj3CSs7cu","type":"Asset","createdAt":"2021-09-30T22:35:27.708Z","updatedAt":"2021-09-30T22:35:27.708Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"17PipelineImplementation","description":"Before Ray Serve, implementing pipelines generally meant you had to choose between wrapping your models in a web server or using many specialized microservices. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/2h69wovKC65iUOj3CSs7cu/8462545f5593c27d61c7151fd069a56b/17PipelineImplementation.png","details":{"size":106807,"image":{"width":840,"height":414}},"fileName":"17PipelineImplementation.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In general, there are two approaches to implement a pipeline: wrap your models in a web server or use many specialized microservices.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Wrap Models in a Web Server","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The left side of the image above shows models that get run in a for loop during the web handling path. Whenever a request comes in, models get loaded (they can also be cached) and run through the pipeline. While this is simple and easy to implement, a major flaw is that this is hard to scale and not performant because each request gets handled sequentially.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Many Specialized Microservices","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The right side of the image above shows many specialized microservices where you essentially build and deploy one microservice per model. These microservices can be native ML platforms, ","nodeType":"text"},{"data":{"uri":"https://www.kubeflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Kubeflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", or even hosted services like AWS ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/sagemaker/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". However, as the number of models grow, the complexity and operational cost drastically increases. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Implementing Pipelines in Ray Serve","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11KQrEzA1s2n4S3SUKuH28","type":"Entry","createdAt":"2021-09-30T22:37:57.811Z","updatedAt":"2021-10-01T00:09:45.131Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"implementingPipelinesInRayServe","body":"@serve.deployment\nclass Featurizer: â¦\n\n@serve.deployment\nclass Predictor: â¦\n\n@serve.deployment\nclass Orchestrator\n   def __init__(self):\n      self.featurizer = Featurizer.get_handle()\n      self.predictor = Predictor.get_handle()\n\n   async def __call__(self, inp):\n      feat = await self.featurizer.remote(inp)\n      predicted = await self.predictor.remote(feat)\n      return predicted\n\nif __name__ == â__main__â:\n    Featurizer.deploy()\n    Predictor.deploy()\n    Orchestrator.deploy()","language":"python","caption":"Pseudocode showing how Ray Serve allows deployments to call other deployments"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In Ray Serve, you can directly call other deployments within your deployment.Â  In this code above, there are three deployments. ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Featurizer","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Predictor","nodeType":"text"},{"data":{},"marks":[],"value":" are just regular deployments containing the models. The ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Orchestrator","nodeType":"text"},{"data":{},"marks":[],"value":" receives the web input, passes it to the featurizer process via the featurizer handle, and then passes the computed feature to the predictor process. The interface is just Python and you donât need to learn any new framework or domain-specific language.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve achieves this with a mechanism called ServeHandle which gives you a similar flexibility to embed everything in the web server, without sacrificing performance or scalability. It allows you to directly call other deployments that live in other processes on other nodes. This allows you to scale out each deployment individually and load balance calls across the replicas.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you would like to get a deeper understanding of how this works, ","nodeType":"text"},{"data":{"uri":"https://youtu.be/mM4hJLelzSw?t=650"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"check out this section of Simon Moâs talk","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to learn about Ray Serveâs architecture. If you would like an example of a computer vision pipeline in production, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures#:~:text=A%20Tool%20for%20High%20Performance%20ML%20Systems"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"check out how Robovision used 5 ML models for vehicle detection","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ensemble Pattern","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3XfSBLbvH4TgteNQhSV3jC","type":"Asset","createdAt":"2021-09-30T22:39:02.077Z","updatedAt":"2021-09-30T22:39:02.077Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ensemble Pattern","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3XfSBLbvH4TgteNQhSV3jC/9754f37495ab90cbf5943494e2e13e7a/25Ensemble.png","details":{"size":32706,"image":{"width":730,"height":352}},"fileName":"25Ensemble.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a lot of production use cases, a pipeline is appropriate. However, one limitation of pipelines is that there can often be many upstream models for a given downstream model. This is where ensembles are useful. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"41I4eZnNsSRrneS0QO59Ss","type":"Asset","createdAt":"2021-09-30T22:41:17.502Z","updatedAt":"2021-09-30T22:41:17.502Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Serve Ensemble Usecase","description":"Ensemble Use Cases","file":{"url":"//images.ctfassets.net/xjan103pcp94/41I4eZnNsSRrneS0QO59Ss/aa86f27d79ef6a431743f8a8349a0343/26RayServeEnsembleUsecase.png","details":{"size":139926,"image":{"width":1364,"height":350}},"fileName":"26RayServeEnsembleUsecase.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ensemble patterns involve mixing output from one or more models. They are also called model stacking in some cases. Below are three use cases of ensemble patterns.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Model Update","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"New models are developed and trained over time. This means there will always be new versions of the model in production. The question becomes, how do you make sure the new models are valid and performant in live online traffic scenarios? One way to do this is by putting some portion of the traffic through the new model. You still select the output from the known good model, but you are also collecting live output from the newer version of the models in order to validate it.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Aggregation","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The most widely known use case is for aggregation. For regression models, outputs from multiple models are averaged. For classification models, the output will be a voted version of multiple modelsâ output. For example, if two models vote for cat and one model votes for dog, then the aggregated output will be cat. Aggregation helps combat inaccuracy in individual models and generally makes the output more accurate and âsaferâ.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dynamic Selection","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another use case for ensemble models is to dynamically perform model selection given input attributes. For example, if the input contains a cat, model A will be used because it is specialized for cats.Â  If the input contains a dog, model B will be used because it is specialized for dogs. Note that this dynamic selection doesnât necessarily mean the pipeline itself has to be static. It could also be selecting models given user feedback. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"General Ensemble Implementation Options","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"58Z7G6vwRfpDBSIepG0fQZ","type":"Asset","createdAt":"2021-09-30T23:40:47.037Z","updatedAt":"2021-09-30T23:41:09.505Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"General Ensemble Implementation","description":"Before Ray Serve, implementing ensembles generally meant you had to choose between wrapping your models in a web server or using many specialized microservices. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/58Z7G6vwRfpDBSIepG0fQZ/7e666548b9cef471ae4194991784f455/27EnsembleDeployment.png","details":{"size":167575,"image":{"width":1332,"height":510}},"fileName":"27EnsembleDeployment.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ensemble implementations suffer the same sort of issues as pipelines. It is simple to wrap models in a web server, but it is not performant. When you use specialized microservices, you end up having a lot of operational overhead as the number of microservices scale with the number of models. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71qMXFLqM4EeyAIHCMnVzY","type":"Asset","createdAt":"2021-09-30T23:44:12.374Z","updatedAt":"2021-09-30T23:44:12.374Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2020 Anyscale demo","description":"Ensemble example from the [2020 Anyscale demo](https://youtu.be/8GTd8Y_JGTQ)","file":{"url":"//images.ctfassets.net/xjan103pcp94/71qMXFLqM4EeyAIHCMnVzY/a76ac67159a965c670a53a944404b39a/28RayServeEnsembleRaySummit.png","details":{"size":371197,"image":{"width":1341,"height":600}},"fileName":"28RayServeEnsembleRaySummit.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray Serve, the kind of pattern is incredibly simple. You can look at the ","nodeType":"text"},{"data":{"uri":"https://youtu.be/8GTd8Y_JGTQ"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"2020 Anyscale demo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to see how to utilize Ray Serveâs handle mechanism to perform dynamic model selection.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another example of using Ray Serve for ensembling is Wildlife Studios combining output of many classifiers for a single prediction. You can check out how they were able to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/wildlife-studios-serves-in-game-offers-3x-faster-at-1-10th-the-cost-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"serve in-game offers 3x faster with Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Business Logic Pattern","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Productionizing machine learning will always involve business logic. No models can stand-alone and serve requests by themselves. Business logic patterns involve everything thatâs involved in a common ML task that is not ML model inference. This includes:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Database lookups for relational records","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Web API calls for external services","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Feature store lookup for pre-compute feature vectors","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Feature transformations like data validation, encoding, and decoding.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"General Business Logic Implementation Options","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ixwTFsSoaWNza9Mp3OPjj","type":"Asset","createdAt":"2021-09-30T23:45:47.219Z","updatedAt":"2021-09-30T23:45:47.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"General Business Logic Implementation Options","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6ixwTFsSoaWNza9Mp3OPjj/8144cd2bea5d42a6333ad1d1f55270c0/31BusinessLogicInAction.png","details":{"size":111620,"image":{"width":984,"height":462}},"fileName":"31BusinessLogicInAction.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The pseudocode for the web handler above does the following things:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"It loads the model (letâs say from S3)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Validates the input from the database","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Looks up some pre-computed features from the feature store.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Only after the web handler completes these business logic steps are the inputs passed through to ML models. The problem is that the requirements of model inference and business logic lead to the server being ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"both network bounded and compute bounded","nodeType":"text"},{"data":{},"marks":[],"value":". This is due to the model loading step, database lookup, and feature store lookups being network bounded and I/O heavy as well as the model inference being compute bound and memory hungry. The combination of these factors lead to an inefficient utilization of resources. Scaling will be expensive.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2tzQhijrmJQ1i1dtp8SeF","type":"Asset","createdAt":"2021-09-30T23:47:03.551Z","updatedAt":"2021-09-30T23:47:03.551Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Web handler approach (left) and microservices approach (right)","description":"Web handler approach (left) and microservices approach (right)","file":{"url":"//images.ctfassets.net/xjan103pcp94/2tzQhijrmJQ1i1dtp8SeF/de61106c82ef5dd503d6200ecdaebd30/32WheretoRunBusinessLogic.png","details":{"size":80148,"image":{"width":1337,"height":464}},"fileName":"32WheretoRunBusinessLogic.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A common way to increase utilization is to split models out into model servers or microservices.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The web app is purely network bounded while the model servers are compute bounded. However, a common problem is the interface between the two. If you put too much business logic into the model server, then the model servers become a mix of network bounded and compute bounded calls.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you let the model servers be pure model servers, then you have the â","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"tensor-in, tensor-out","nodeType":"text"},{"data":{},"marks":[],"value":"â interface problem. The input types for model servers are typically very constrained to just tensors or some alternate form of it. This makes it hard to keep the pre-processing, post-processing, and business logic in sync with the model itself.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"It becomes hard to reason about the interaction between the processing logic and the model itself because during training, the processing logic and models are tightly coupled, but when serving, they are split across two servers and two implementations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Neither the web handler approach nor the microservices approach is satisfactory.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Implementing Business Logic in Ray Serve","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4WEB4OkdjgTl7azrJctx4A","type":"Asset","createdAt":"2021-09-30T23:48:49.625Z","updatedAt":"2021-09-30T23:48:49.625Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Business Logic in Ray Serve","description":"Business Logic in Ray Serve","file":{"url":"//images.ctfassets.net/xjan103pcp94/4WEB4OkdjgTl7azrJctx4A/703ef612cc6e50aaa48433deef6ba985/33BusinessLogicInRayServe.png","details":{"size":114370,"image":{"width":856,"height":398}},"fileName":"33BusinessLogicInRayServe.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray Serve, you just have to make some simple changes to the old web server to alleviate the issues described above. Instead of loading the model directly, you can retrieve a ServeHandle that wraps the model, and offload the computation to another deployment. All the data types are preserved and there is no need to write âtensor-in, tensor-outâ API calls--you can just pass in regular Python types. Additionally, the model deployment class can stay in the same file, and be deployed together with the prediction handler. This makes it easy to understand and debug the code.Â ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"model.remote","nodeType":"text"},{"data":{},"marks":[],"value":" looks like just a function and you can easily trace it to the model deployment class.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this way, Ray Serve helps you split up the business logic and inference into two separation components, one I/O heavy and the other compute heavy. This allows you to scale each piece individually, without losing the ease of deployment. Additionally, because ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"model.remote","nodeType":"text"},{"data":{},"marks":[],"value":" is just a function call, itâs a lot easier to test and debug than separate external services.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve FastAPI Integration","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"HStDeyRVrI7LM2WdUP2s6","type":"Asset","createdAt":"2021-09-30T23:50:19.687Z","updatedAt":"2021-09-30T23:50:19.687Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Serve: Ingress with FastAPI","description":"Ray Serve: Ingress with FastAPI","file":{"url":"//images.ctfassets.net/xjan103pcp94/HStDeyRVrI7LM2WdUP2s6/4ff62672c24788b1d07d59736b85817b/34RayServeIngress.png","details":{"size":297542,"image":{"width":1656,"height":838}},"fileName":"34RayServeIngress.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"An important part of implementing business logic and other patterns is authentication and input validation. ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/http-servehandle.html#serve-fastapi-http"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve natively integrates with FastAPI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which is a type safe and ergonomic web framework. ","nodeType":"text"},{"data":{"uri":"https://fastapi.tiangolo.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FastAPI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" has features like automatic dependency injection, type checking and validation, and OpenAPI doc generation.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray Serve, you can directly pass the FastAPI app object into it with ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"@serve.ingress","nodeType":"text"},{"data":{},"marks":[],"value":". This decorator makes sure that all existing FastAPI routes still work and that you can attach new routes with the deployment class so states like loaded models, and networked database connections can easily be managed. Architecturally, we just made sure that your FastAPI app is correctly embedded into the replica actor and the FastAPI app can scale out across many Ray nodes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Online LearningÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Online learning is an emerging pattern thatâs become more and more widely used. It refers to a model running in production that is constantly being updated, trained, validated and deployed. Below are three use cases of online learning patterns.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dynamically Learn Model Weights","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are use cases for dynamically learning model weights online. As users interact with your services, these updated model weights can contribute to a personalized model for each user or group. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6J7nynhCYa2vV5Ud57cn0m","type":"Asset","createdAt":"2021-09-30T23:53:34.472Z","updatedAt":"2021-09-30T23:53:34.472Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Online Learning example at Ant Group (image courtesy of Ant Group) ","description":"[Online Learning example at Ant Group](https://www.anyscale.com/blog/online-resource-allocation-with-ray-at-ant-group) (image courtesy of Ant Group) ","file":{"url":"//images.ctfassets.net/xjan103pcp94/6J7nynhCYa2vV5Ud57cn0m/0362b38997ac5c4e8c1db756cf4a1dcd/38OnlineLearningAntGroup.png","details":{"size":311903,"image":{"width":1208,"height":533}},"fileName":"38OnlineLearningAntGroup.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"One case study of online learning consists of an online resource allocation business solution at Ant Group. The model is trained from offline data, then combined with real time streaming data source, and then served live traffic. One thing to note is that online learning systems are drastically more complex than their static serving counterparts. In this case, putting models in the web server, or even splitting it up into multiple microservices, would not help with the implementation. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dynamically Learn Parameters to Orchestrate Models","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are also use cases for learning parameters to orchestrate or compose models, for example, ","nodeType":"text"},{"data":{"uri":"https://dl.acm.org/doi/fullHtml/10.1145/3383313.3411550"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"learning which model a user prefers","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". This manifests often in model selection scenarios or contextual bandit algorithms.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Reinforcement Learning","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Reinforcement learning is the branch of machine learning that trains agents to interact with the environment. The environment can be the physical world or a simulated environment. You can learn about reinforcement learning ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and see how you can deploy a RL model using Ray Serve ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/rllib.html#serve-rllib-tutorial"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â  ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ConclusionÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Zmo0bB3BCXWqX3bfhtxlg","type":"Asset","createdAt":"2021-09-30T23:55:14.299Z","updatedAt":"2021-09-30T23:55:14.299Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"WhereRayServeFitsIn","description":"Ray Serve is easy to develop and production ready.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Zmo0bB3BCXWqX3bfhtxlg/f6a8378f2cbb372c458904651e7bd3c1/WhereRayServeFitsIn.png","details":{"size":92883,"image":{"width":768,"height":444}},"fileName":"WhereRayServeFitsIn.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This post went over 4 main patterns of machine learning in production, how Ray Serve can help you natively scale and work with complex architectures, and how ML in production often means many models in production. Ray Serve is built with all of this in mind on top of the distributed runtime Ray. If youâre interested in learning more about Ray, you can check out the ","nodeType":"text"},{"data":{"uri":"https://ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", join us on ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discourse","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and check out the ","nodeType":"text"},{"data":{"uri":"http://tinyurl.com/ray-white-paper"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"whitepaper","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"! If you're interested in working with us to make it easier to leverage Ray, we're ","nodeType":"text"},{"data":{"uri":"https://jobs.lever.co/anyscale"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"hiring","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"!","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"BBDpqdGlEcQZHPoQxytXf","type":"Asset","createdAt":"2021-10-01T00:19:41.125Z","updatedAt":"2021-10-01T00:19:41.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Where Ray Serve Fits In","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/BBDpqdGlEcQZHPoQxytXf/7dff7d7f06b7a5d55cfd255cc3cd88e6/WhereRayServeFitsIn.png","details":{"size":92883,"image":{"width":768,"height":444}},"fileName":"WhereRayServeFitsIn.png","contentType":"image/png"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Zk7jixFrLqHf3IJLzrO2B","type":"Entry","createdAt":"2022-05-03T18:47:31.422Z","updatedAt":"2022-05-03T18:47:31.422Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Ray Summit: Aug 23-24 in SF!","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5LTWfPAScfUtzXj2KOUALz","type":"Entry","createdAt":"2022-04-12T15:29:42.674Z","updatedAt":"2022-10-04T22:06:07.976Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":17,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"summit"}},"locale":"en-US"},"fields":{"title":"Ray Summit 2022","slug":"ray-summit-2022","description":"Centered on Ray, the open-source distributed computing framework, Ray Summit brings together ML practitioners, data scientists, and more to discuss the growing demand for distributed computing.","sections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xIJZXdAqhOEizUCUuQaiV","type":"Entry","createdAt":"2022-03-31T16:04:49.746Z","updatedAt":"2022-09-12T20:58:36.991Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":20,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-hero","header":"The Place for _Everything Ray_","body":"Missed us in San Francisco for Ray Summit 2022? Don't fret, you can watch your favorite sessions on demand! ","ctaText":"View Sessions","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"9F9S7NLaIdPL4ThYwVlFW","type":"Asset","createdAt":"2022-09-06T14:49:38.936Z","updatedAt":"2022-09-06T14:53:02.350Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Presented by Anyscale","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/9F9S7NLaIdPL4ThYwVlFW/8ecd09b334942298f0b8c2d5567ec032/presented-by-anyscale.svg","details":{"size":22813,"image":{"width":289,"height":32}},"fileName":"presented-by-anyscale.svg","contentType":"image/svg+xml"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4A0d4TUfp0TCqUA60CXopR","type":"Asset","createdAt":"2022-09-06T14:53:48.720Z","updatedAt":"2022-09-06T14:53:48.720Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4A0d4TUfp0TCqUA60CXopR/2e4240c268d074d840f7a00d069a768a/rs-collage_2x.jpg","details":{"size":568547,"image":{"width":2400,"height":1376}},"fileName":"rs-collage@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6FGUP0cXunTbgs2UemeAU","type":"Asset","createdAt":"2022-09-06T14:54:09.975Z","updatedAt":"2022-09-06T14:54:09.975Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event, stacked vertically","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6FGUP0cXunTbgs2UemeAU/16ed175024bc9cc0a2fae3cb13538dff/rs-collage-mobile_2x.jpg","details":{"size":243282,"image":{"width":750,"height":1420}},"fileName":"rs-collage-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4RKa7qVhWoIaoNSa1ZH2SK"}}],"type":"2022 Summit Hero","styling":["transparent-header"]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6GL7ZzuSLadWdVFKHj7yIH","type":"Entry","createdAt":"2022-04-20T18:00:12.964Z","updatedAt":"2022-08-08T22:50:29.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-keynote-speakers","header":"Keynote Speakers","ctaText":"See all speakers","ctaLink":"/ray-summit-2022/speakers","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6VhewtcICU9SCXjwd5qhHJ"}}],"type":"2022 Summit Speakers","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JTi5fsyo0Q3YrF3SAQglg","type":"Entry","createdAt":"2022-08-08T22:49:55.163Z","updatedAt":"2022-08-09T07:03:38.615Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-ray-summit-selected-speakers","header":"Highlighted Talks","ctaText":"Full Agenda","ctaLink":"/ray-summit-2022/agenda","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"50eS4cKXBNE4GHezdZ8Rws"}},{"sys":{"type":"Link","linkType":"Entry","id":"5iioKfOgEqVzgQ1DMmA1Wy"}},{"sys":{"type":"Link","linkType":"Entry","id":"YCBUZOXJGB0qcJF8TKjfI"}}],"type":"2022 Summit Agenda Preview"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5cgHuoJ713DewhVY6iUYrG","type":"Entry","createdAt":"2022-05-03T21:57:45.147Z","updatedAt":"2022-05-09T13:41:03.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-trainings","header":"Exclusive Ray Training Sessions","body":"Ray Summit is a great opportunity to level up your Ray skills through hands-on training sessions with the builders and maintainers of Ray. Weâre offering several pre-conference training sessions on August 22. Space is limited â [reserve your spot now](https://www.myeventi.events/raysummit22/attendee/)!","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3sYQTUmD5X5ncvGRFyIRR6","type":"Asset","createdAt":"2022-05-03T04:40:25.040Z","updatedAt":"2022-05-03T04:40:25.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image desktop","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3sYQTUmD5X5ncvGRFyIRR6/a77c675b07d63e73dd79ada9d182498a/training-image_2x.jpg","details":{"size":313509,"image":{"width":1136,"height":1480}},"fileName":"training-image@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7rsC56a4YCgENLyxsHyeIb","type":"Asset","createdAt":"2022-05-03T04:40:54.089Z","updatedAt":"2022-05-03T04:40:54.089Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image mobile","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7rsC56a4YCgENLyxsHyeIb/e2592c9b4424048b3c916fbbbc313065/training-image-mobile_2x.jpg","details":{"size":153801,"image":{"width":830,"height":874}},"fileName":"training-image-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4Hb3J9SrCFTDXqIhIdSIwa"}},{"sys":{"type":"Link","linkType":"Entry","id":"hZ4Tl1wvj7CwF00slwpLg"}},{"sys":{"type":"Link","linkType":"Entry","id":"YJ5KWyjg4NGf29NrfHcJX"}}],"type":"2022 Summit Trainings"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mRJQtuDbIhAuEbwLT2V1K","type":"Entry","createdAt":"2022-03-31T21:42:56.117Z","updatedAt":"2022-04-11T23:46:07.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-reasons-to-attend","header":"Why Attend?","body":"As the annual user conference for the [open source Ray community](https://www.ray.io/community), Ray Summit is a unique opportunity to enhance your Ray skills, build your professional network, and learn what's on the Ray roadmap.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"5f6gxk4a4ujE2y6FZ8ATQ5"}},{"sys":{"type":"Link","linkType":"Entry","id":"20QwyiOmkHCEOkMvp0Jhtk"}},{"sys":{"type":"Link","linkType":"Entry","id":"6QxivsuhhLoVSloRQs9ZlQ"}}],"type":"2022 Summit Content Tiles"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"15k0SGqB2ZKOUfAsM3JnyV","type":"Entry","createdAt":"2022-03-31T16:05:55.739Z","updatedAt":"2022-08-05T00:25:20.225Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":16,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-about","header":"Celebrate the Ray community","body":"Get ready for our epic community bash at the Art of Tinkering exhibit at the San Francisco Exploratorium on August 23 from 5:30 to 9:30 p.m. Come explore, play, and celebrate with the Ray Community. This is an exclusive opportunity to experience this amazing interactive exhibition before it closes in September.","ctaText":"Check out the agenda","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UcZT4h1P7uFDz6cn6JH8l","type":"Asset","createdAt":"2022-06-02T20:37:22.299Z","updatedAt":"2022-06-02T20:37:22.299Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Man taking a photo on his phone","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2UcZT4h1P7uFDz6cn6JH8l/5040a9909f18ab015c27b220077f916e/taking-picture.jpg","details":{"size":107846,"image":{"width":980,"height":772}},"fileName":"taking-picture.jpg","contentType":"image/jpeg"}}}],"type":"2022 Summit Split Image","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4P8Jn0q7nBU05iD6DmcQCC","type":"Entry","createdAt":"2022-04-01T19:15:31.735Z","updatedAt":"2022-04-11T16:36:59.573Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-social-tiles","header":"Join the Conversation","body":"Ready to get involved in the Ray community before the conference? Ask a question in the forums. Open a pull request. Or share why youâre excited with the hashtag __#RaySummit__ on Twitter.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6OMTlXkJ1OHXNkEjyPrXTH"}},{"sys":{"type":"Link","linkType":"Entry","id":"7EE28ee1ATQtodoLloT6mg"}},{"sys":{"type":"Link","linkType":"Entry","id":"1CRAzFxndL37wiNg4x8WKV"}},{"sys":{"type":"Link","linkType":"Entry","id":"2qrec5rR8kabZxCHaPS7NH"}}],"type":"2022 Summit Social Tiles"}}],"headerLogo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70DIdxwT8XY96aw7vMUHOu","type":"Asset","createdAt":"2022-06-01T15:21:46.804Z","updatedAt":"2022-06-01T15:21:46.804Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo Dark Blue","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/70DIdxwT8XY96aw7vMUHOu/5ca0a8b7533c8df70bc0e032b889862a/rs-logo-nav.svg","details":{"size":9954,"image":{"width":108,"height":48}},"fileName":"rs-logo-nav.svg","contentType":"image/svg+xml"}}},"headerLogoWhite":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3qyLXKGgGfWhv4oPqYl0SE","type":"Asset","createdAt":"2022-04-20T21:50:58.774Z","updatedAt":"2022-04-20T21:50:58.774Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3qyLXKGgGfWhv4oPqYl0SE/c4dee9e7a344c39db11fefafbf255b5b/rs-logo-horizontal.svg","details":{"size":9975,"image":{"width":108,"height":48}},"fileName":"rs-logo-horizontal.svg","contentType":"image/svg+xml"}}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KUtRQbdqJo1RrWMrVyfEm","type":"Entry","createdAt":"2022-03-31T16:21:13.402Z","updatedAt":"2022-04-08T17:25:33.228Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Home","link":"/ray-summit-2022"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4MvQcRSkHLMqIJoO9O2cQe","type":"Entry","createdAt":"2022-03-31T16:17:27.336Z","updatedAt":"2022-09-12T16:59:51.221Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"On-Demand","link":"/ray-summit-2022/agenda","subLinks":[{"sys":{"type":"Link","linkType":"Entry","id":"3N9kRDz36tieTI4YGVWKBJ"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46B5diwEDDr3kB2XDoGfDP","type":"Entry","createdAt":"2022-05-11T17:13:42.174Z","updatedAt":"2022-05-11T17:13:42.174Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Speakers","link":"/ray-summit-2022/speakers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Dt5fy6A7rBpXdG7HeKMGd","type":"Entry","createdAt":"2022-03-31T16:17:43.592Z","updatedAt":"2022-03-31T16:17:43.592Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Sponsors","link":"/ray-summit-2022/sponsors"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SvRE9pnyq3BXUPGKRWx6W","type":"Entry","createdAt":"2022-06-03T19:29:31.009Z","updatedAt":"2022-06-03T19:29:31.009Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Who Attends","link":"/ray-summit-2022/who-attends"}}],"mainSponsors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"xMQIOU9n7gVS4mkX2Nlag","type":"Entry","createdAt":"2021-01-31T16:33:06.495Z","updatedAt":"2021-05-11T13:28:31.312Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Ray","url":"https://ray.io","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jLroqm98RCMLGASthyuPv","type":"Asset","createdAt":"2021-05-11T13:28:26.930Z","updatedAt":"2021-05-11T13:28:26.930Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/jLroqm98RCMLGASthyuPv/f76cc04827316de5d3f8b529f6a3f02c/Ray_BM.svg","details":{"size":4742,"image":{"width":93,"height":36}},"fileName":"Ray_BM.svg","contentType":"image/svg+xml"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11syIHhtX8JajSlXHSHE1O","type":"Entry","createdAt":"2021-01-31T16:31:24.509Z","updatedAt":"2021-05-11T13:28:00.309Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Anyscale","url":"https://www.anyscale.com","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UkDmTzmZHIOG7yxFP4sVL","type":"Asset","createdAt":"2021-05-11T13:27:55.219Z","updatedAt":"2021-05-11T13:27:55.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Anyscale","file":{"url":"//images.ctfassets.net/xjan103pcp94/5UkDmTzmZHIOG7yxFP4sVL/db10aebda9a9aad10903470781a6f2e6/ANSC_BM.svg","details":{"size":7431,"image":{"width":158,"height":36}},"fileName":"ANSC_BM.svg","contentType":"image/svg+xml"}}}}}],"footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cfwECWrOqJ5Fegr3YpReS","type":"Entry","createdAt":"2022-04-01T21:22:12.001Z","updatedAt":"2022-04-13T17:35:29.511Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-footer","body":"Â©Anyscale, Inc 2022\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/privacy-policy\"\u003ePrivacy Policy\u003c/a\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/code-of-conduct\"\u003eCode of Conduct\u003c/a\u003e"}},"recommendations":[]}},"ctaText":"Register now","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5nTlh9josVs5zpZqU3vTbn","type":"Asset","createdAt":"2022-05-03T18:47:25.125Z","updatedAt":"2022-05-03T18:47:25.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-ray-summit-2022","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5nTlh9josVs5zpZqU3vTbn/e66043f62ed4a4b53d7e182e50289ec3/blog-recommended-content-ray-summit-2022.png","details":{"size":17234,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-ray-summit-2022.png","contentType":"image/png"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"po86u7oYjYZvGsh15b2oO","type":"Entry","createdAt":"2022-03-24T22:42:36.065Z","updatedAt":"2022-03-24T22:42:36.065Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Considerations for deploying ML models in production","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4Shvq24HmMiYP6cT0dROLY","type":"Entry","createdAt":"2021-11-16T17:10:12.838Z","updatedAt":"2022-06-22T16:12:16.152Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Considerations for Deploying Machine Learning Models in Production","slug":"considerations-for-deploying-machine-learning-models-in-production","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-11-16","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"A common grumble among data science or machine learning researchers or practitioners is that putting a model in production is difficult. As a result, some claim that a large percentage, 87%, of models never see the light of the day in production. \n\nâI have a model, I spent considerable time developing it on my laptop. Whatâs next? How do I get it into our production environment? What should I consider for my ML stack and tooling?â","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"A common grumble among data science or machine learning researchers or practitioners is that putting a model in production is difficult. As a result, some claim that a large percentage, 87%, of models never see the","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/why-90-percent-of-all-machine-learning-models-never-make-it-into-production-ce7e250d5a4a"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":" light of the day in production","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"âI have a model, I spent considerable time developing it on my laptop. Whatâs next? How do I get it into our production environment? What should I consider for my ML stack and tooling?âÂ ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"These questions frequently emerge at meetups or conferences, after talks on machine learning operations (MLOps). There is no singular panacea or silver bullet for this nascent field of ","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/mlops-practices-for-data-scientists-dbb01be45dd8"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLOps Best Practices","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" that attempts to address and remedy this crucial problem.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, there are some acceptable and common technical considerations and pitfalls to keep in mind when considering your ML stack and tools. In this first part of a series on putting ML models in production, weâll discuss some common considerations and common pitfalls for tooling and best practices and ML model serving patterns that are an essential part of your journey from model development to deployment in production.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Developing with Ease","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Consider your development environment first. Most data scientists or ML engineers invariably use their laptops for development, testing or debugging code. Because of simplicity, easy to access and install the latest ML libraries, practitioners overwhelmingly prefer laptops over clusters for development. We are spoiled by IDEs and syntax-highlighted editors for good reason.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Python developers like to customize their environments to match their staging environment, with library dependencies using conda or Python virtual environments. Ideally, as a best practice, if the same code developed on their laptop can run with minimal changes on a staging or production environment on the cluster, it immensely improves the end-to-end developer productivity.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Consider your laptop as a preferred choice of development environment, with the possibility of extending or syncing your code to the cluster environment in the cloud.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"0i5dwwrx5iEKmB1i1lCNY","type":"Asset","createdAt":"2021-11-05T17:52:21.850Z","updatedAt":"2021-11-10T03:26:31.917Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"LaptopDeploy","description":"[image source](https://www.kctv5.com/jobs/us-wages-jump-by-the-most-in-records-dating-back-20-years/article_b07e82f7-4d92-5739-876b-5a1fd9063263.html)","file":{"url":"//images.ctfassets.net/xjan103pcp94/0i5dwwrx5iEKmB1i1lCNY/d27552d814334ed1c5dc651a3479f8ad/laptop.png","details":{"size":592457,"image":{"width":1156,"height":690}},"fileName":"laptop.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Consideration Number #1","nodeType":"text"},{"data":{},"marks":[],"value":": ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"Use your laptop for development as a best practice","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Training at Scale and Tracking Model Experiments","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlike the traditional software development cycle, the model development cycle paradigm is different. A number of factors influence an ML modelâs success in production. First, the outcome of a model is measured by its metrics, such as an acceptable accuracy.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Second, achieving an accuracy that satisfies the business goal means experimentation with not only one model or ML library but many models and many ML libraries while tracking each experiment runs: metrics, parameters, artifacts, etc. As vital as accuracy is, so is a developerâs choice of ML libraries to experiment with.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7oiARsfqccrDvMD12jlWJe","type":"Asset","createdAt":"2021-11-05T18:13:20.173Z","updatedAt":"2021-11-05T18:13:20.173Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Traditional Software vs Machine Learning","description":"[image source](https://github.com/dmatrix/mlflow-workshop-part-1/blob/master/slides/mlflow-workshop-series-part-1.pdf)","file":{"url":"//images.ctfassets.net/xjan103pcp94/7oiARsfqccrDvMD12jlWJe/daf859d2f1ad06bf931f5259bbc36255/TraditionalSoftware.png","details":{"size":396774,"image":{"width":1626,"height":566}},"fileName":"TraditionalSoftware.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Third, accuracy is directly linked to the quality of acquired data: bad data results in a bad model. As the diagram below shows data preparationâfeature extractions, feature selection, standardized or normalized features, data imputations and encodingâare all imperative steps before the cleansed data lands into a feature store, accessible to your model training and testing phase or inference in deployment.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6usr388mdxS5gFgfxoAMLm","type":"Asset","createdAt":"2021-11-05T18:17:52.891Z","updatedAt":"2021-11-16T17:11:39.120Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"Modern Data Lake","description":"Phases of the Model Development Cycle","file":{"url":"//images.ctfassets.net/xjan103pcp94/6usr388mdxS5gFgfxoAMLm/b6a4f57518af3904c77a90362271ddad/ModernDataLake.png","details":{"size":217302,"image":{"width":1404,"height":766}},"fileName":"ModernDataLake.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Fourth, a choice of programming language that is not only familiar to your data teamâdata analysts, data scientists, and ML engineersâbut also supported by many ML libraries employed during model experimentation and training phases. Python seems to be the ","nodeType":"text"},{"data":{"uri":"https://insights.stackoverflow.com/survey/2020#technology-most-loved-dreaded-and-wanted-languages-loved"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"de facto choice","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Alongside a choice of a programming language is the choice of an ML framework for taming compute-intensive ML workloads: deep learning, distributed training, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/what-is-hyperparameter-tuning"},"content":[{"data":{},"marks":[],"value":"hyperparameter optimization","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (HPO), and inferenceâall at horizontal scaleâfrom your laptop, single node multiple cores to multiple nodes, with multiple cores.Â Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"And finally, the ability to easily deploy models in diverse environments at scale: part of web applications, inside mobile devices, as a web service in the cloud. etc","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Consideration Number #2: Consider using model life cycle development and management platforms like ","nodeType":"text"},{"data":{"uri":"https://mlflow.org/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"MLflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://dvc.org/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"DVC","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://wandb.ai/site"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Weights \u0026 Biases","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", or ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/sagemaker/studio/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"SageMaker Studio","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":". And ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":",","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/raysgd/raysgd.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":" Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" (formerly Ray SGD), ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/tutorials/beginner/dist_overview.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" and ","nodeType":"text"},{"data":{"uri":"https://www.tensorflow.org/guide/distributed_training"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"TensorFlow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" for distributed, compute-intensive and deep learning ML workloads.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Managing Machine Learning Features","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.featurestore.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Feature stores","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" are emerging pivotal components in the modern machine learning development cycle. As more data scientists and engineers work together to successfully put models in production, having a singular store to persist cleaned and featurized data is becoming an increasing necessity as part of the model development cycle shown.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3ICMHtw1H093lIiCaZkuxG","type":"Asset","createdAt":"2021-11-10T03:31:55.753Z","updatedAt":"2021-11-10T03:39:59.411Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Feature Store Managing ML Features","description":"Feature Store for Managing ML Features","file":{"url":"//images.ctfassets.net/xjan103pcp94/3ICMHtw1H093lIiCaZkuxG/f25617cb1641fc2c4f5ec9cb079fc017/FeatureStore_ManagingMLFeatures.png","details":{"size":147609,"image":{"width":1078,"height":644}},"fileName":"FeatureStore ManagingMLFeatures.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Feature stores address operational challenges. They provide a consistent set of data between training and inference. They avoid any data skew or inadvertent data leakage. They offer both customized capability of writing feature transformations, both on batch and streaming data, during the feature extraction process while training. And they allow request augmentation with historical data at inference, which is common in large fraud and anomaly detection deployed models or recommendation systems.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Aside from challenges and considerations of putting models in production, operationalizing ML data is equally important. Model accuracy depends on good data, and feature stores help manage precomputed and cleansed features for your model training and production inference during model serving.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Consideration Number #3: Consider feature stores as part of your model development process. Look to ","nodeType":"text"},{"data":{"uri":"https://feast.dev/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Feast","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.tecton.ai/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Tecton","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_featurestore.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"SageMaker","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", and ","nodeType":"text"},{"data":{"uri":"https://databricks.com/product/feature-store"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Databricks","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" for feature stores.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deploying, Serving and Inferencing Models at Scale","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Once the model is trained and tested, with confidence that it met the business requirements for model accuracy, seven crucial requirements for scalable model serving frameworks to consider are:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Framework agnostic","nodeType":"text"},{"data":{},"marks":[],"value":": A model serving-elected framework should be ML framework agnostic. That is, it can deploy any common model built with common ML frameworks. For example, PyTorch, TensorFlow, XGBoost, or Scikit-learn, each with its own algorithms and model architectures.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Business Logic:","nodeType":"text"},{"data":{},"marks":[],"value":"Â  Model prediction often requires preprocessing, post processing or ability to augment request data by connecting to a feature store or any other data store for validation. Model serving should allow this as part of its inference.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Model Replication: ","nodeType":"text"},{"data":{},"marks":[],"value":"Some models are compute-intensive or network-bound. As such the elected framework can fan out requests over to model replicas, load balancing among replicas to support parallel request handling during peak traffic.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Request Batching: ","nodeType":"text"},{"data":{},"marks":[],"value":"Not all models in production are employed for real-time serving. Often, models are scored in large batches of requests. For example, for deep learning models, parallelizing these image requests to multiple cores, taking advantage of hardware accelerators, to expedite batch scoring and utilize hardware resources is worthy of consideration.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"High Concurrency and Low Latency: ","nodeType":"text"},{"data":{},"marks":[],"value":"Models in production require real-time inference with low latency while handling bursts of heavy traffic of requests. The consideration is crucial for best user experience to receive millisecond responses on prediction requests.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Model Deployment CLI and APIs: ","nodeType":"text"},{"data":{},"marks":[],"value":"An ML engineer responsible to deploy a model should be able to use model serverâs deployment APIs or command line interfaces (CLI) simply to deploy model artifacts into production. This allows model deployment from within an existing CI/CD pipeline or workflow.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Patterns of Models in Production","nodeType":"text"},{"data":{},"marks":[],"value":": As ML applications are increasingly becoming pervasive in all sectors of industry, models trained for these ML applications are complex and composite. They range from computer vision to natural language processing to recommendation systems and reinforcement learning.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"That is, they donât exist in isolation. Nor do they predict results singularly. Instead they operate jointly and often in ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"four model ML patterns","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": pipeline, ensemble, business logic, and online learning. Each pattern has its purpose and merit.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1xy0sgeTJzs9wGLt2C5rfE","type":"Asset","createdAt":"2021-11-05T18:30:46.397Z","updatedAt":"2021-11-05T18:30:46.397Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ML Patterns in Production","description":"ML Model Patterns in Production ([image source](https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns))","file":{"url":"//images.ctfassets.net/xjan103pcp94/1xy0sgeTJzs9wGLt2C5rfE/4715269a6f5f0a049f958a3b213325e4/MLPatternsProduction.png","details":{"size":203430,"image":{"width":1482,"height":696}},"fileName":"MLPatternsProduction.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Machine Learning engineers adopt ","nodeType":"text"},{"data":{"uri":"https://youtu.be/gV4YS4e1CXg?t=272"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"two common approaches ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"to deploy these patterns of models in production. One is to embed models into a web server, the other is to offload to an external service. Each approach has its own pros and cons, with respect to the above seven considerations.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Consideration Number #4: Look to ","nodeType":"text"},{"data":{"uri":"https://www.seldon.io/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Seldon","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.kubeflow.org/docs/components/kfserving/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"KFServing","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", or ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" for all these seven requirements.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Observing and Monitoring Model in Production","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Model monitoring, often an overlooked stage as part of model development lifecycle, is critical to modelâs viability in the post deployment production stage. It is often an afterthought, at an ML engineerâs peril.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Models have an afterlife of viability. That viable life in production needs a constant watchful or sentinel eye. In fact, monitoring as a phase is simply a continuation of the model serving, as depicted in the diagram below.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1B0lehLJcFXrfMsC6Rdu6f","type":"Asset","createdAt":"2021-11-05T18:33:35.806Z","updatedAt":"2021-11-05T18:33:35.806Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ML Model Monitoring in Production","description":"ML Model Monitoring in Production ([image source](https://evidentlyai.com/blog/machine-learning-monitoring-what-it-is-and-how-it-differs))","file":{"url":"//images.ctfassets.net/xjan103pcp94/1B0lehLJcFXrfMsC6Rdu6f/e0f90578ed433933118a36b2aeea5c39/ML_Model_Monitoring_in_Production.png","details":{"size":192050,"image":{"width":1040,"height":416}},"fileName":"ML Model Monitoring in Production.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Why consider model monitoring? For a number of practical reasons, this stage is pivotal. Let's briefly discuss them.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data drifts over time: ","nodeType":"text"},{"data":{},"marks":[],"value":"As we mentioned above, our quality and accuracy of the model depends on the quality of the data. Data is complex and never static, meaning what the original model was trained with the extracted features may not be as important over time. For example, a geo location for a credit application model may not be as important, as demographics evolve. Some new features may emerge that need to be taken into account. For example, seasonal data changes. Such ","nodeType":"text"},{"data":{"uri":"https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"features drifts","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in data require retraining and redeploying the model, because the distribution of the variables is no longer relevant.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Model concept changes over time: ","nodeType":"text"},{"data":{},"marks":[],"value":"Many practitioners refer to this as model ","nodeType":"text"},{"data":{"uri":"https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"decay or model staleness","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". When the patterns of trained models no longer hold with the drifting data, the model is no longer valid because the relationships of its input features may not necessarily produce the model's expected prediction. Hence, its accuracy degrades.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Â ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Models fail over time: ","nodeType":"text"},{"data":{},"marks":[],"value":"Models fail for inexplicable reasons: a system failure or bad network connection; an overloaded system; a bad input or corrupted request. Detecting these failuresâ root causes early or its frequency mitigates user bad experience or deters mistrust in the service if the user receives wrong or bogus outcomes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Systems degrade over load: ","nodeType":"text"},{"data":{},"marks":[],"value":"Constantly being vigilant of the health of your dedicated model servers or services deployed is just as important as monitoring the health of your data pipelines that transform data or your entire data infrastructureâs key components: data stores, web servers, routers, cluster nodesâ system health, etc.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Collectively, these aforementioned monitoring model concepts are called ","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/what-is-ml-observability-29e85e701688"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"model observability","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". This step is now an acceptable imperative in ","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/mlops-practices-for-data-scientists-dbb01be45dd8"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLOps best practices","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Monitoring the health of your data and models should never be an afterthought. Rather, it ought to be part and parcel of your model development cycle.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Consideration Number #5: For model observability look to ","nodeType":"text"},{"data":{"uri":"https://github.com/evidentlyai/evidently"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Evidently.ai","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://arize.com/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Arize.ai","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.arthur.ai/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Arthur.ai","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.fiddler.ai/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Fiddler.ai","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://valohai.com/model-monitoring/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Valohai.com","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":",","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":" or ","nodeType":"text"},{"data":{"uri":"https://whylabs.ai/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"whylabs.ai","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs recap. To avoid the common grumble of models not making it to production or having your model see the light of the day in production, take into account all the above considerations at heart if you want your models to journey to their desired destinationâand have a viable afterlife too.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Each consideration has its merits. Each consideration has either an open source solution addressing each problem or a managed solution from a vendor. Evaluate how each best fits and meets all the considerations into your existing machine learning tooling stack.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"But making it part and parcel of your ML model development tooling stack is crucial; it will significantly improve your end-to-end success in putting your models into production.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the next blog in this series, we will examine how you can implement consideration #1 and #2, focusing on some of the tools we suggested.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6pQJ9kqwgmpnC2IaMIDtoj","type":"Asset","createdAt":"2021-11-05T18:35:46.042Z","updatedAt":"2021-11-05T18:35:46.042Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"MLPatternsProduction","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6pQJ9kqwgmpnC2IaMIDtoj/c925f376d94117b1f1dd97b4bf84f72d/MLPatternsProduction.png","details":{"size":203430,"image":{"width":1482,"height":696}},"fileName":"MLPatternsProduction.png","contentType":"image/png"}}},"recommendations":[],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true}},"thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"wSQM6naQrXBOvxNfv5iYe","type":"Asset","createdAt":"2022-03-24T22:19:37.934Z","updatedAt":"2022-03-24T22:19:37.934Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-nodes-dark","file":{"url":"//images.ctfassets.net/xjan103pcp94/wSQM6naQrXBOvxNfv5iYe/ed3965226bd4e0dc9d39da26d95d6e14/blog-recommended-content-nodes-dark.jpg","details":{"size":41506,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-nodes-dark.jpg","contentType":"image/jpeg"}}},"recommendations":null}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BzSmyQ2ojATOq6qOpD4PX","type":"Entry","createdAt":"2021-10-02T19:52:20.790Z","updatedAt":"2021-10-02T19:52:20.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Technology","identifier":"technology"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}}],"hideIntro":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6dwKBw7V5sw7cl5lfEmDlo","type":"Entry","createdAt":"2021-09-02T16:39:30.889Z","updatedAt":"2022-06-22T16:55:57.303Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":10,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Parallelizing Python Code ","slug":"parallelizing-python-code","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7xsDT3qwgziVxkIpEDqo5Q","type":"Entry","createdAt":"2021-09-01T21:13:33.309Z","updatedAt":"2021-09-01T21:13:33.309Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Dawid Borycki","slug":"dawid-borycki","link":"https://www.linkedin.com/in/dawidborycki/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-09-02","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This article reviews some common options for parallelizing Python code including process-based parallelism, specialized libraries, ipython parallel, and Ray.\n","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Python is great for tasks like training machine learning models, performing numerical simulations, and quickly developing proof-of-concept solutions without setting up development tools and installing several dependencies. When performing these tasks, you also want to use your underlying hardware as much as possible for quick results. Parallelizing Python code enables this. However, using the standard CPython implementation means you cannot fully use the underlying hardware because of the global interpreter lock (GIL) that prevents running the bytecode from multiple threads simultaneously.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This article reviews some common options for parallelizing Python code including:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.python.org/3/library/multiprocessing.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Process-based parallelism","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Specialized libraries","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://ipython.readthedocs.io/en/stable/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"IPython Parallel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"For each technique, this article lists some advantages and disadvantages and shows a code sample to help you understand what itâs like to use.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How to Parallelize Python Code","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are several common ways to parallelize Python code. You can launch several application instances or a script to perform jobs in parallel. This approach is great when you donât need to exchange data between parallel jobs. Otherwise, sharing data between processes significantly reduces performance when aggregating data.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Starting multiple threads within the same process allows you to share data between jobs more efficiently. In this case, thread-based parallelization can offload some work to the background. However, the standard CPython implementationâs global interpreter lock (GIL) prevents running the bytecode in multiple threads simultaneously.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The sample function below simulates complex calculations (meant to mimic activation functions)Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ZhjxMjb34NNejPqZr3m6x","type":"Entry","createdAt":"2021-09-01T21:17:58.733Z","updatedAt":"2021-09-01T21:17:58.733Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"iterations_count = round(1e7)","body":"iterations_count = round(1e7)\ndef complex_operation(input_index):\n   print(\"Complex operation. Input index: {:2d}\".format(input_index))\n   [math.exp(i) * math.sinh(i) for i in [1] * iterations_count]\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"complex_operation","nodeType":"text"},{"data":{},"marks":[],"value":" executes several times to better estimate the processing time. It divides the long-running operation into a batch of smaller ones. It does this by dividing the input values into several subsets and then processing the inputs from those subsets in parallel.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hereâs the code that runs ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"complex_operation","nodeType":"text"},{"data":{},"marks":[],"value":" several times (input range of ten) and measures the execution time with the timebudget package:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2jhikURwzrxs4RqmCKpPEr","type":"Entry","createdAt":"2021-09-01T22:04:46.592Z","updatedAt":"2021-09-01T22:04:46.592Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"@timebudget","body":"@timebudget\ndef run_complex_operations(operation, input):\n   for i in input:\n      operation(i) \n\ninput = range(10)\nrun_complex_operations(complex_operation, input) \n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"After executing ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/mGalarnyk/8c491fbdfe6ce3e498a7f62f03fa9ca4"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"this script","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", you will get an output similar to the one below:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1lIyKj3DsJzMsJBW6YuBlD","type":"Asset","createdAt":"2021-09-01T22:06:02.993Z","updatedAt":"2021-09-01T22:14:52.851Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"parallelizing blog 1","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1lIyKj3DsJzMsJBW6YuBlD/e4e011eeb7c1db8d6c90d00b07690204/parallyzing_blog_1.png","details":{"size":77487,"image":{"width":512,"height":236}},"fileName":"parallyzing blog 1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"As you can see, it took about 39 seconds to execute this code on the laptop used in this tutorial. Letâs see how to improve this result.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Process-Based Parallelism","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The first approach is to use process-based parallelism. With this approach, it is possible to start several processes at the same time (concurrently). This way, they can concurrently perform calculations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Starting from Python 3, the","nodeType":"text"},{"data":{"uri":"https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing"},"content":[{"data":{},"marks":[],"value":" multiprocessing package","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is preinstalled and gives us a convenient syntax for launching concurrent processes. It provides the Pool object, which automatically divides input into subsets and distributes them among many processes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://gist.github.com/mGalarnyk/b5455b0454815b04363ef9994f22fbf3"},"content":[{"data":{},"marks":[],"value":"Here is an example","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" of how to use a Pool object to launch ten processes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1abfJSxSAm73kPyLcr2YYC","type":"Entry","createdAt":"2021-09-01T22:06:59.165Z","updatedAt":"2021-09-01T22:12:02.079Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"import math","body":"import math\nimport numpy as np\nfrom timebudget import timebudget\nfrom multiprocessing import Pool\n\niterations_count = round(1e7)\n\ndef complex_operation(input_index):\n    print(\"Complex operation. Input index: {:2d}\\n\".format(input_index))\n    [math.exp(i) * math.sinh(i) for i in [1] * iterations_count]\n\n@timebudget\ndef run_complex_operations(operation, input, pool):\n    pool.map(operation, input)\n\nprocesses_count = 10\n\nif __name__ == '__main__':\n    processes_pool = Pool(processes_count)\n    run_complex_operations(complex_operation, range(10), processes_pool)   \n","language":"python","caption":"[Code sample](https://gist.github.com/mGalarnyk/b5455b0454815b04363ef9994f22fbf3)"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Each process concurrently performs the complex operation. So, the code could theoretically reduce the total execution time by up to ten times. However, the output from the code below only shows about a fourfold improvement (39 seconds in the previous section vs 9.4 in this section).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6B7ewbHMg9JmRmf397uhEK","type":"Asset","createdAt":"2021-09-01T22:08:30.198Z","updatedAt":"2021-09-01T22:15:10.841Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"parallelizing blog 2","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6B7ewbHMg9JmRmf397uhEK/0ed9effb3ca1ac369f82dc2057e44a1e/parallyzing_blog_2.png","details":{"size":63680,"image":{"width":512,"height":364}},"fileName":"parallyzing blog 2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are a couple reasons why the improvement is not tenfold. First, the maximum number of processes that can run concurrently depends on the the number of CPUs in the system. You can find out how many CPUs your system has by using the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"os.cpu_count()","nodeType":"text"},{"data":{},"marks":[],"value":" method.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3a52SVCuKhfB5cfURStgO5","type":"Entry","createdAt":"2021-09-01T22:08:56.719Z","updatedAt":"2021-09-09T22:30:56.730Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"import os (parallelizing python)","body":"import os\nprint('Number of CPUs in the system: {}'.format(os.cpu_count()))\n"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3hFb0PY8xY6qi8hrzNlVY5","type":"Asset","createdAt":"2021-09-01T22:09:39.529Z","updatedAt":"2021-09-09T22:31:46.280Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"locale":"en-US"},"fields":{"title":"parallelizing blog 3","description":"The machine used in this tutorial has eight CPUs","file":{"url":"//images.ctfassets.net/xjan103pcp94/3hFb0PY8xY6qi8hrzNlVY5/26af309e69d23edb05774db80dd83f77/importOS.png","details":{"size":17081,"image":{"width":776,"height":60}},"fileName":"importOS.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The next reason why the improvement is not more is that the computations in this tutorial are relatively small. Finally, it is important to note that there is usually some overhead when parallelizing computation as processes that want to communicate must utilize ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Inter-process_communication"},"content":[{"data":{},"marks":[],"value":"interprocess communication mechanisms","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". This means that for very small tasks parallelizing computation is often slower than serial computation (normal Python). If you are interested in learning more about multiprocessing, Selva Prabhakaran has an ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray"},"content":[{"data":{},"marks":[],"value":"excellent blog","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" which inspired this section of the tutorial. If you would like to learn about some more of the trade-offs in parallel/distributed computing, ","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/writing-your-first-distributed-python-application-with-ray-4248ebc07f41"},"content":[{"data":{},"marks":[],"value":"check out this tutorial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4ggc9Rzv8uvnLwQM6PO56F","type":"Asset","createdAt":"2021-09-01T22:14:08.712Z","updatedAt":"2021-09-01T22:14:08.712Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"parallelizing blog 4","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4ggc9Rzv8uvnLwQM6PO56F/05d2176bf19135f4ca6414c1e6847cf5/parallyzing_blog_4.png","details":{"size":41104,"image":{"width":1136,"height":344}},"fileName":"parallyzing blog 4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Specialized Libraries","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://stackoverflow.com/questions/36479159/why-are-numpy-calculations-not-affected-by-the-global-interpreter-lock"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Many calculations for specialized libraries like NumPy are unaffected by the GIL","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and can use threads and other techniques to work in parallel. This section of the tutorial goes over the benefits of combining NumPy and multiprocessing","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To demonstrate the differences between the naÃ¯ve implementation and the NumPy-based implementation, an additional function needs to be implemented:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3IEvF1mTH3N4nknYheGorV","type":"Entry","createdAt":"2021-09-01T22:16:14.653Z","updatedAt":"2021-09-01T22:16:14.653Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"def complex_operation_numpy(input_index):","body":"def complex_operation_numpy(input_index):\n      print(\"Complex operation (numpy). Input index: {:2d}\".format(input_index))\n\n      data = np.ones(iterations_count)\n      np.exp(data) * np.sinh(data)\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The code now uses the NumPy exp and sinh functions to perform calculations on the input sequence. Then, the code executes complex_operation and complex_operation_numpy ten times using the processes pool to compare their performance:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"qOivysVk6k2lrlzdOARHt","type":"Entry","createdAt":"2021-09-01T22:16:54.024Z","updatedAt":"2021-09-01T22:16:54.024Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"processes_count = 10","body":"processes_count = 10\ninput = range(10)\n\nif __name__ == '__main__':\n    processes_pool = Pool(processes_count)\n    print(âWithout NumPyâ)\n    run_complex_operations(complex_operation, input, processes_pool)\n    print(âNumPyâ)\n    run_complex_operations(complex_operation_numpy, input, processes_pool)\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The output below shows the performance with and without NumPy for ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/mGalarnyk/703c53bb98aa94d66bb6c49d48ce5c09"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"this script","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3VOltZ8c6mAkocPZIjc7LR","type":"Asset","createdAt":"2021-09-01T22:17:45.820Z","updatedAt":"2021-09-01T22:17:45.820Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"parallelizing blog 5","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3VOltZ8c6mAkocPZIjc7LR/ec5caa75c801b673706848fa2b661f18/parallelizing_blog_5.png","details":{"size":92946,"image":{"width":512,"height":341}},"fileName":"parallelizing blog 5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"NumPy offers a rapid boost in performance. Here, NumPy reduced the computation time to about 10 percent of the original time (859ms vs 9.515sec). One reason why it is faster is because most processing in NumPy is vectorized. With vectorization, the underlying code is effectively âparallelizedâ because the operation can calculate multiple array elements at once, rather than looping through them one at a time. If you are interested in learning more about this, Jake Vanderplas gave an excellent talk on the subject ","nodeType":"text"},{"data":{"uri":"https://youtu.be/EEUXKG97YRw?t=613"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"13k36Q3LmF673vVrbUPqZa","type":"Asset","createdAt":"2021-09-01T22:19:18.055Z","updatedAt":"2021-09-01T22:19:18.055Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"parallelizing blog 6","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/13k36Q3LmF673vVrbUPqZa/28396aaa33aa8618afabc0eca99a163b/parallelizing_blog_6.png","details":{"size":58112,"image":{"width":1138,"height":372}},"fileName":"parallelizing blog 6.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"IPython Parallel","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The IPython shell supports interactive parallel and distributed computing across multiple IPython instances. IPython Parallel was developed (almost) together with IPython.Â  When IPython was renamed to Jupyter, they split out IPython Parallel into its own package. ","nodeType":"text"},{"data":{"uri":"https://ipython.org/ipython-doc/3/parallel/parallel_intro.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"IPython Parallel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" has a number of advantages, but perhaps the biggest advantage is that it enables parallel applications to be developed, executed, and monitored interactively. When using IPython Parallel for parallel computing, you typically start with the ipcluster command.Â Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"64uuCAseEQroqm32TkSDNX","type":"Entry","createdAt":"2021-09-01T22:35:55.681Z","updatedAt":"2021-09-01T22:35:55.681Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ipcluster start -n 10","body":"ipcluster start -n 10","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The last parameter controls the number of engines (nodes) to launch. The command above becomes available after ","nodeType":"text"},{"data":{"uri":"https://ipyparallel.readthedocs.io/en/latest/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"installing the ipyparallel Python package","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Below is a sample output:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ebalmkxu1ukvpH4poPoFr","type":"Asset","createdAt":"2021-09-01T22:39:31.719Z","updatedAt":"2021-09-01T22:39:31.719Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"parallelizing blog 7","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6ebalmkxu1ukvpH4poPoFr/f2aa39aa743664d2e931b3dbf860b6f0/parallelizing_blog_7.png","details":{"size":67784,"image":{"width":512,"height":319}},"fileName":"parallelizing blog 7.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The next step is to provide Python code that should connect to ipcluster and start parallel jobs. Fortunately, IPython provides a convenient API for doing this. The code looks like process-based parallelism based on the Pool object:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"aYoLda5Xxm7gqSbr8KNYc","type":"Entry","createdAt":"2021-09-01T22:40:21.451Z","updatedAt":"2021-09-01T22:40:53.474Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"import math import numpy as np","body":"import math\nimport numpy as np\nfrom timebudget import timebudget\nimport ipyparallel as ipp\n\niterations_count = round(1e7)\n\ndef complex_operation(input_index):\n    print(\"Complex operation. Input index: {:2d}\".format(input_index))\n\n    [math.exp(i) * math.sinh(i) for i in [1] * iterations_count]\n\ndef complex_operation_numpy(input_index):\n    print(\"Complex operation (numpy). Input index: {:2d}\".format(input_index))\n\n    data = np.ones(iterations_count)\n    np.exp(data) * np.sinh(data)\n\n@timebudget\ndef run_complex_operations(operation, input, pool):\n    pool.map(operation, input)\n\nclient_ids = ipp.Client()\npool = client_ids[:]\n\ninput = range(10)\nprint('Without NumPy')\nrun_complex_operations(complex_operation, input, pool)\nprint('NumPy')\nrun_complex_operations(complex_operation_numpy, input, pool)\n","language":"python","caption":"[Code sample](https://gist.github.com/mGalarnyk/6dab23cc6485f145d2b148fc64d34b3c)"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The code above executed in a new tab in the terminal produces the output shown below:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3oOlwnkO2edLArTuWAimXk","type":"Asset","createdAt":"2021-09-01T22:41:48.786Z","updatedAt":"2021-09-01T22:41:48.786Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"parallelizing blog 8","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3oOlwnkO2edLArTuWAimXk/318561ba4a59504e34f9a7b54850ea16/parallelizing_blog_8.png","details":{"size":37920,"image":{"width":512,"height":313}},"fileName":"parallelizing blog 8.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The execution times with and without NumPy for IPython Parallel are 13.88 ms and 9.98 ms, respectively. Note, there are no logs included in the standard output, however they can be assessed with additional commands.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"25yTzWsBo114khG50lNYoY","type":"Asset","createdAt":"2021-09-01T22:47:23.369Z","updatedAt":"2021-09-01T22:47:23.369Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"parallelizing blog 9","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/25yTzWsBo114khG50lNYoY/0fc1f36d15924685e4a8cdd4dce1b2e9/parallelizing_blog_9.png","details":{"size":49289,"image":{"width":1136,"height":326}},"fileName":"parallelizing blog 9.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Like IPython Parallel, ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" can be used for parallel ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"and","nodeType":"text"},{"data":{},"marks":[],"value":" distributed computing. Ray is a fast, simple distributed execution framework that makes it easy to scale your applications and to leverage state of the art machine learning libraries. Using Ray, you can take Python code that runs sequentially and transform it into a distributed application with minimal code changes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"JRSqxrziyanjUq8ba6Rz6","type":"Asset","createdAt":"2021-09-01T22:49:57.493Z","updatedAt":"2021-09-01T22:51:28.882Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"parallelizing blog 10","description":"While this tutorial briefly goes over how Ray makes it easy to parallelize plain Python code, it is important to note that Ray and its ecosystem also make it easy to parallelize existing libraries like [scikit-learn](https://medium.com/distributed-computing-with-ray/how-to-speed-up-scikit-learn-model-training-aaf17e2d1e1), [XGBoost](https://www.anyscale.com/blog/distributed-xgboost-training-with-ray), [LightGBM](https://www.anyscale.com/blog/introducing-distributed-lightgbm-training-with-ray), [PyTorch](https://medium.com/pytorch/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-fd83c98fdead), and much more. \n","file":{"url":"//images.ctfassets.net/xjan103pcp94/JRSqxrziyanjUq8ba6Rz6/d24278ef959def650efd27d118da370c/parallelizing_blog_10.png","details":{"size":60895,"image":{"width":512,"height":195}},"fileName":"parallelizing blog 10.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To use Ray, ray.init() is needed to start all of the relevant Ray processes. By default, Ray creates one worker process per CPU core. If you would want to run Ray on a cluster, you would need to pass in a cluster address with something like ray.init(address='insertAddressHere').","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1j7jrZ01ulNOiMfLrz0UN7","type":"Entry","createdAt":"2021-09-01T22:52:20.163Z","updatedAt":"2021-09-01T22:52:20.163Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray.init()","body":"ray.init()\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The next step is to create a Ray task. This can be done by decorating a normal Python function with the @ray.remote decorator. This creates a task which can be scheduled across your laptopâs CPU cores (or Ray cluster). Hereâs an example for the previously created complex_operation_numpy:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6LuOb8rKdgjTmziDCfJy1g","type":"Entry","createdAt":"2021-09-01T22:52:46.541Z","updatedAt":"2021-09-02T19:20:34.793Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"@ray.remote","body":"@ray.remote\ndef complex_operation_numpy(input_index):\n   print(\"Complex operation (numpy). Input index: {:2d}\".format(input_index))\n   data = np.ones(iterations_count)\n   np.exp(data) * np.sinh(data)\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the last step, execute these functions within the ray runtime, like so:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1UnnCgrctCnKFm44YKOQdB","type":"Entry","createdAt":"2021-09-01T22:53:21.661Z","updatedAt":"2021-09-02T19:22:26.195Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"@timebudget","body":"@timebudget\ndef run_complex_operations(operation, input):\n   ray.get([operation.remote(i) for i in input])\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"After executing ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/mGalarnyk/30c8672620c8655a37940be935899a57"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"this script","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", you will get an output similar to the one below:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1W223ROJ7pJBUXHzhgXDjG","type":"Asset","createdAt":"2021-09-01T22:53:57.373Z","updatedAt":"2021-09-01T22:53:57.373Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"parallelizing blog 11","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1W223ROJ7pJBUXHzhgXDjG/df1d586e64955c7dfa540a8d4fc2780e/parallelizing_blog_11.png","details":{"size":115794,"image":{"width":512,"height":333}},"fileName":"parallelizing blog 11.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The execution times with and without NumPy for Ray are 3.382sec and 419.98ms, respectively. It is important to remember that the performance benefits of Ray will be more pronounced when executing long-running tasks like the graph below shows.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42cIJnjlhuFKgQSjE1ieAP","type":"Asset","createdAt":"2021-09-01T22:54:28.139Z","updatedAt":"2021-09-01T22:55:14.329Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"parallelizing blog 12","description":"Ray has more pronounced benefits when running bigger jobs [(image source)](https://towardsdatascience.com/10x-faster-parallel-python-without-python-multiprocessing-e5017c93cce1)\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/42cIJnjlhuFKgQSjE1ieAP/6b46d3912acd4a912bbb155ae326be9b/parallelizing_blog_12.jpeg","details":{"size":20292,"image":{"width":512,"height":256}},"fileName":"parallelizing blog 12.jpeg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you would like to learn about Rayâs syntax, there is an introductory tutorial on it ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"25l9VaeFvbiyo58XfLjxHD","type":"Asset","createdAt":"2021-09-01T22:55:55.041Z","updatedAt":"2021-09-01T22:55:55.041Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"parallelizing blog 13","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/25l9VaeFvbiyo58XfLjxHD/e5c33468431e2b5029def040d5354bac/parallelizing_blog_13.png","details":{"size":104718,"image":{"width":1136,"height":610}},"fileName":"parallelizing blog 13.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Alternative Python Implementations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"One final consideration is that you can apply multithreading using other Python implementations. Examples include IronPython for .NET and Jython for Java. In such cases, you could use the low-level threading support from the underlying frameworks. This approach is beneficial if you already have experience with the multi-processing capabilities of .NET or Java.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"This article reviewed common approaches for parallelizing Python through code samples and by highlighting some of their advantages and disadvantages. We performed tests using benchmarks on simple numerical data. It is important to keep in mind that parallelized code often introduces some overhead and that the benefits of parallelization are more pronounced with bigger jobs rather than the short computations in this tutorial.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Keep in mind that the parallelization can be more powerful for other applications. Especially when dealing with typical AI-based tasks in which you must perform repetitive fine-tuning of your models. In such cases, ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" offers the best support due to its rich ecosystem, autoscaling, fault tolerance, and capability of using remote machines.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6JUJVFL5DeeHcjyZICDWor","type":"Asset","createdAt":"2021-09-02T16:11:05.027Z","updatedAt":"2021-09-02T16:11:05.027Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"With and without NumPy Ray","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6JUJVFL5DeeHcjyZICDWor/625fbfc21738362e5d468da8036a5506/withWithoutNumPy.png","details":{"size":115794,"image":{"width":512,"height":333}},"fileName":"withWithoutNumPy.png","contentType":"image/png"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Zk7jixFrLqHf3IJLzrO2B","type":"Entry","createdAt":"2022-05-03T18:47:31.422Z","updatedAt":"2022-05-03T18:47:31.422Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Ray Summit: Aug 23-24 in SF!","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5LTWfPAScfUtzXj2KOUALz","type":"Entry","createdAt":"2022-04-12T15:29:42.674Z","updatedAt":"2022-10-04T22:06:07.976Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":17,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"summit"}},"locale":"en-US"},"fields":{"title":"Ray Summit 2022","slug":"ray-summit-2022","description":"Centered on Ray, the open-source distributed computing framework, Ray Summit brings together ML practitioners, data scientists, and more to discuss the growing demand for distributed computing.","sections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xIJZXdAqhOEizUCUuQaiV","type":"Entry","createdAt":"2022-03-31T16:04:49.746Z","updatedAt":"2022-09-12T20:58:36.991Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":20,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-hero","header":"The Place for _Everything Ray_","body":"Missed us in San Francisco for Ray Summit 2022? Don't fret, you can watch your favorite sessions on demand! ","ctaText":"View Sessions","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"9F9S7NLaIdPL4ThYwVlFW","type":"Asset","createdAt":"2022-09-06T14:49:38.936Z","updatedAt":"2022-09-06T14:53:02.350Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Presented by Anyscale","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/9F9S7NLaIdPL4ThYwVlFW/8ecd09b334942298f0b8c2d5567ec032/presented-by-anyscale.svg","details":{"size":22813,"image":{"width":289,"height":32}},"fileName":"presented-by-anyscale.svg","contentType":"image/svg+xml"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4A0d4TUfp0TCqUA60CXopR","type":"Asset","createdAt":"2022-09-06T14:53:48.720Z","updatedAt":"2022-09-06T14:53:48.720Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4A0d4TUfp0TCqUA60CXopR/2e4240c268d074d840f7a00d069a768a/rs-collage_2x.jpg","details":{"size":568547,"image":{"width":2400,"height":1376}},"fileName":"rs-collage@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6FGUP0cXunTbgs2UemeAU","type":"Asset","createdAt":"2022-09-06T14:54:09.975Z","updatedAt":"2022-09-06T14:54:09.975Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event, stacked vertically","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6FGUP0cXunTbgs2UemeAU/16ed175024bc9cc0a2fae3cb13538dff/rs-collage-mobile_2x.jpg","details":{"size":243282,"image":{"width":750,"height":1420}},"fileName":"rs-collage-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4RKa7qVhWoIaoNSa1ZH2SK"}}],"type":"2022 Summit Hero","styling":["transparent-header"]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6GL7ZzuSLadWdVFKHj7yIH","type":"Entry","createdAt":"2022-04-20T18:00:12.964Z","updatedAt":"2022-08-08T22:50:29.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-keynote-speakers","header":"Keynote Speakers","ctaText":"See all speakers","ctaLink":"/ray-summit-2022/speakers","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6VhewtcICU9SCXjwd5qhHJ"}}],"type":"2022 Summit Speakers","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JTi5fsyo0Q3YrF3SAQglg","type":"Entry","createdAt":"2022-08-08T22:49:55.163Z","updatedAt":"2022-08-09T07:03:38.615Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-ray-summit-selected-speakers","header":"Highlighted Talks","ctaText":"Full Agenda","ctaLink":"/ray-summit-2022/agenda","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"50eS4cKXBNE4GHezdZ8Rws"}},{"sys":{"type":"Link","linkType":"Entry","id":"5iioKfOgEqVzgQ1DMmA1Wy"}},{"sys":{"type":"Link","linkType":"Entry","id":"YCBUZOXJGB0qcJF8TKjfI"}}],"type":"2022 Summit Agenda Preview"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5cgHuoJ713DewhVY6iUYrG","type":"Entry","createdAt":"2022-05-03T21:57:45.147Z","updatedAt":"2022-05-09T13:41:03.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-trainings","header":"Exclusive Ray Training Sessions","body":"Ray Summit is a great opportunity to level up your Ray skills through hands-on training sessions with the builders and maintainers of Ray. Weâre offering several pre-conference training sessions on August 22. Space is limited â [reserve your spot now](https://www.myeventi.events/raysummit22/attendee/)!","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3sYQTUmD5X5ncvGRFyIRR6","type":"Asset","createdAt":"2022-05-03T04:40:25.040Z","updatedAt":"2022-05-03T04:40:25.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image desktop","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3sYQTUmD5X5ncvGRFyIRR6/a77c675b07d63e73dd79ada9d182498a/training-image_2x.jpg","details":{"size":313509,"image":{"width":1136,"height":1480}},"fileName":"training-image@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7rsC56a4YCgENLyxsHyeIb","type":"Asset","createdAt":"2022-05-03T04:40:54.089Z","updatedAt":"2022-05-03T04:40:54.089Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image mobile","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7rsC56a4YCgENLyxsHyeIb/e2592c9b4424048b3c916fbbbc313065/training-image-mobile_2x.jpg","details":{"size":153801,"image":{"width":830,"height":874}},"fileName":"training-image-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4Hb3J9SrCFTDXqIhIdSIwa"}},{"sys":{"type":"Link","linkType":"Entry","id":"hZ4Tl1wvj7CwF00slwpLg"}},{"sys":{"type":"Link","linkType":"Entry","id":"YJ5KWyjg4NGf29NrfHcJX"}}],"type":"2022 Summit Trainings"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mRJQtuDbIhAuEbwLT2V1K","type":"Entry","createdAt":"2022-03-31T21:42:56.117Z","updatedAt":"2022-04-11T23:46:07.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-reasons-to-attend","header":"Why Attend?","body":"As the annual user conference for the [open source Ray community](https://www.ray.io/community), Ray Summit is a unique opportunity to enhance your Ray skills, build your professional network, and learn what's on the Ray roadmap.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"5f6gxk4a4ujE2y6FZ8ATQ5"}},{"sys":{"type":"Link","linkType":"Entry","id":"20QwyiOmkHCEOkMvp0Jhtk"}},{"sys":{"type":"Link","linkType":"Entry","id":"6QxivsuhhLoVSloRQs9ZlQ"}}],"type":"2022 Summit Content Tiles"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"15k0SGqB2ZKOUfAsM3JnyV","type":"Entry","createdAt":"2022-03-31T16:05:55.739Z","updatedAt":"2022-08-05T00:25:20.225Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":16,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-about","header":"Celebrate the Ray community","body":"Get ready for our epic community bash at the Art of Tinkering exhibit at the San Francisco Exploratorium on August 23 from 5:30 to 9:30 p.m. Come explore, play, and celebrate with the Ray Community. This is an exclusive opportunity to experience this amazing interactive exhibition before it closes in September.","ctaText":"Check out the agenda","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UcZT4h1P7uFDz6cn6JH8l","type":"Asset","createdAt":"2022-06-02T20:37:22.299Z","updatedAt":"2022-06-02T20:37:22.299Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Man taking a photo on his phone","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2UcZT4h1P7uFDz6cn6JH8l/5040a9909f18ab015c27b220077f916e/taking-picture.jpg","details":{"size":107846,"image":{"width":980,"height":772}},"fileName":"taking-picture.jpg","contentType":"image/jpeg"}}}],"type":"2022 Summit Split Image","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4P8Jn0q7nBU05iD6DmcQCC","type":"Entry","createdAt":"2022-04-01T19:15:31.735Z","updatedAt":"2022-04-11T16:36:59.573Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-social-tiles","header":"Join the Conversation","body":"Ready to get involved in the Ray community before the conference? Ask a question in the forums. Open a pull request. Or share why youâre excited with the hashtag __#RaySummit__ on Twitter.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6OMTlXkJ1OHXNkEjyPrXTH"}},{"sys":{"type":"Link","linkType":"Entry","id":"7EE28ee1ATQtodoLloT6mg"}},{"sys":{"type":"Link","linkType":"Entry","id":"1CRAzFxndL37wiNg4x8WKV"}},{"sys":{"type":"Link","linkType":"Entry","id":"2qrec5rR8kabZxCHaPS7NH"}}],"type":"2022 Summit Social Tiles"}}],"headerLogo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70DIdxwT8XY96aw7vMUHOu","type":"Asset","createdAt":"2022-06-01T15:21:46.804Z","updatedAt":"2022-06-01T15:21:46.804Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo Dark Blue","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/70DIdxwT8XY96aw7vMUHOu/5ca0a8b7533c8df70bc0e032b889862a/rs-logo-nav.svg","details":{"size":9954,"image":{"width":108,"height":48}},"fileName":"rs-logo-nav.svg","contentType":"image/svg+xml"}}},"headerLogoWhite":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3qyLXKGgGfWhv4oPqYl0SE","type":"Asset","createdAt":"2022-04-20T21:50:58.774Z","updatedAt":"2022-04-20T21:50:58.774Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3qyLXKGgGfWhv4oPqYl0SE/c4dee9e7a344c39db11fefafbf255b5b/rs-logo-horizontal.svg","details":{"size":9975,"image":{"width":108,"height":48}},"fileName":"rs-logo-horizontal.svg","contentType":"image/svg+xml"}}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KUtRQbdqJo1RrWMrVyfEm","type":"Entry","createdAt":"2022-03-31T16:21:13.402Z","updatedAt":"2022-04-08T17:25:33.228Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Home","link":"/ray-summit-2022"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4MvQcRSkHLMqIJoO9O2cQe","type":"Entry","createdAt":"2022-03-31T16:17:27.336Z","updatedAt":"2022-09-12T16:59:51.221Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"On-Demand","link":"/ray-summit-2022/agenda","subLinks":[{"sys":{"type":"Link","linkType":"Entry","id":"3N9kRDz36tieTI4YGVWKBJ"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46B5diwEDDr3kB2XDoGfDP","type":"Entry","createdAt":"2022-05-11T17:13:42.174Z","updatedAt":"2022-05-11T17:13:42.174Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Speakers","link":"/ray-summit-2022/speakers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Dt5fy6A7rBpXdG7HeKMGd","type":"Entry","createdAt":"2022-03-31T16:17:43.592Z","updatedAt":"2022-03-31T16:17:43.592Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Sponsors","link":"/ray-summit-2022/sponsors"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SvRE9pnyq3BXUPGKRWx6W","type":"Entry","createdAt":"2022-06-03T19:29:31.009Z","updatedAt":"2022-06-03T19:29:31.009Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Who Attends","link":"/ray-summit-2022/who-attends"}}],"mainSponsors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"xMQIOU9n7gVS4mkX2Nlag","type":"Entry","createdAt":"2021-01-31T16:33:06.495Z","updatedAt":"2021-05-11T13:28:31.312Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Ray","url":"https://ray.io","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jLroqm98RCMLGASthyuPv","type":"Asset","createdAt":"2021-05-11T13:28:26.930Z","updatedAt":"2021-05-11T13:28:26.930Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/jLroqm98RCMLGASthyuPv/f76cc04827316de5d3f8b529f6a3f02c/Ray_BM.svg","details":{"size":4742,"image":{"width":93,"height":36}},"fileName":"Ray_BM.svg","contentType":"image/svg+xml"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11syIHhtX8JajSlXHSHE1O","type":"Entry","createdAt":"2021-01-31T16:31:24.509Z","updatedAt":"2021-05-11T13:28:00.309Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Anyscale","url":"https://www.anyscale.com","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UkDmTzmZHIOG7yxFP4sVL","type":"Asset","createdAt":"2021-05-11T13:27:55.219Z","updatedAt":"2021-05-11T13:27:55.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Anyscale","file":{"url":"//images.ctfassets.net/xjan103pcp94/5UkDmTzmZHIOG7yxFP4sVL/db10aebda9a9aad10903470781a6f2e6/ANSC_BM.svg","details":{"size":7431,"image":{"width":158,"height":36}},"fileName":"ANSC_BM.svg","contentType":"image/svg+xml"}}}}}],"footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cfwECWrOqJ5Fegr3YpReS","type":"Entry","createdAt":"2022-04-01T21:22:12.001Z","updatedAt":"2022-04-13T17:35:29.511Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-footer","body":"Â©Anyscale, Inc 2022\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/privacy-policy\"\u003ePrivacy Policy\u003c/a\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/code-of-conduct\"\u003eCode of Conduct\u003c/a\u003e"}},"recommendations":[]}},"ctaText":"Register now","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5nTlh9josVs5zpZqU3vTbn","type":"Asset","createdAt":"2022-05-03T18:47:25.125Z","updatedAt":"2022-05-03T18:47:25.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-ray-summit-2022","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5nTlh9josVs5zpZqU3vTbn/e66043f62ed4a4b53d7e182e50289ec3/blog-recommended-content-ray-summit-2022.png","details":{"size":17234,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-ray-summit-2022.png","contentType":"image/png"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"13cizi53rMXokAKIXc7OOC","type":"Entry","createdAt":"2022-03-24T21:30:45.286Z","updatedAt":"2022-03-24T21:30:45.286Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Writing your first distributed Python application with Ray","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4mDD9iGuZZbkGgK5fUfQcH","type":"Entry","createdAt":"2021-08-12T21:33:46.544Z","updatedAt":"2022-06-22T17:00:48.654Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Writing your First Distributed Python Application with Ray ","slug":"writing-your-first-distributed-python-application-with-ray","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-08-12","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Ray is a fast, simple distributed execution framework that makes it easy to scale your applications and to leverage state of the art machine learning libraries. Using Ray, you can take Python code that runs sequentially and transform it into a distributed application with minimal code changes.Â \nThe goal of this tutorial is to explore how to get started with Ray as well as some common trade-offs in distributed computing (compute cost, memory, I/O, etc).","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1XwPRevPAwseF7Xn59pehK","type":"Asset","createdAt":"2021-08-12T18:10:09.695Z","updatedAt":"2021-08-12T18:27:25.776Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Ray makes parallel and distributed computing work more like you would hope","description":"Ray makes parallel and distributed computing work more like you would hope ([image source](https://www.reddit.com/r/aww/comments/2oagj8/multithreaded_programming_theory_and_practice/))","file":{"url":"//images.ctfassets.net/xjan103pcp94/1XwPRevPAwseF7Xn59pehK/41eac0cf3497c936660874d79b2e8a7c/RayPuppies.png","details":{"size":623811,"image":{"width":891,"height":433}},"fileName":"RayPuppies.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a fast, simple distributed execution framework that makes it easy to scale your applications and to leverage state of the art machine learning libraries. Using Ray, you can take Python code that runs sequentially and transform it into a distributed application with minimal code changes.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The goal of this tutorial is to explore the following:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Why should you parallelize and distribute with RayÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"How to get started with Ray","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Trade-offs in distributed computing (compute cost, memory, I/O, etc)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Why should you parallelize and distribute with Ray?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As a ","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/modern-parallel-and-distributed-python-a-quick-tutorial-on-ray-99f8d70369b8"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"previous post pointed out","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", parallel and distributed computing are a staple of modern applications. The problem is that taking existing Python code and trying to parallelize or distribute it can mean rewriting existing code, sometimes from scratch. Additionally modern applications have requirements that existing modules like ","nodeType":"text"},{"data":{"uri":"https://docs.python.org/3/library/multiprocessing.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"multiprocessing","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" lack. These requirements include:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Running the same code on more than one machine","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Building microservices and actors that have state and can communicate","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Graceful handling of machine failures and preemption","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Efficient handling of large objects and numerical data","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The Ray library satisfies these requirements and allows you to scale your applications without rewriting them. In order to make parallel \u0026 distributed computing simple, Ray takes functions and classes and translates them to the distributed setting as tasks and actors. The rest of this tutorial explores these concepts as well as some important things to consider when building parallel \u0026 distributed applications.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7fYvNgpXdywkUVdBs3rc2o","type":"Asset","createdAt":"2021-08-12T18:16:08.498Z","updatedAt":"2021-09-30T02:41:22.139Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Ray Ecosystem","description":"While this tutorial explores how Ray makes it easy to parallelize plain Python code, it is important to note that Ray and its ecosystem also make it easy to parallelize [python code](https://www.anyscale.com/blog/parallelizing-python-code) as well as existing libraries like [scikit-learn](https://medium.com/distributed-computing-with-ray/how-to-speed-up-scikit-learn-model-training-aaf17e2d1e1), [XGBoost](https://www.anyscale.com/blog/distributed-xgboost-training-with-ray), [LightGBM](https://www.anyscale.com/blog/introducing-distributed-lightgbm-training-with-ray), [PyTorch](https://medium.com/pytorch/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-fd83c98fdead), and much more.","file":{"url":"//images.ctfassets.net/xjan103pcp94/7fYvNgpXdywkUVdBs3rc2o/a3fcae9248f53b1e714d418ceda2af96/RayEcosystem.png","details":{"size":181020,"image":{"width":1050,"height":400}},"fileName":"RayEcosystem.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"How to get started with Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Turning Python Functions into Remote Functions (Ray Tasks)","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray can be installed through pip. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"77bS2q6cHNNDFMWcxk4rrA","type":"Entry","createdAt":"2021-08-12T18:17:09.481Z","updatedAt":"2021-08-12T18:17:09.481Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"pip install 'ray[default]'","body":"pip install 'ray[default]'"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs begin our Ray journey by creating a Ray task. This can be done by decorating a normal Python function with @ray.remote. This creates a task which can be scheduled across your laptop's CPU cores (or Ray cluster).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Consider the two functions below which generate Fibonacci sequences (integer sequence characterized by the fact that every number after the first two is the sum of the two preceding ones). The first is a normal python function and the second is a Ray task.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"22VwJrYkF1TYE9N1FS5eXL","type":"Entry","createdAt":"2021-08-12T18:18:18.201Z","updatedAt":"2021-08-12T18:18:18.201Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Fibonacci local and remote","body":"import os\nimport time\nimport ray\n\n# Normal Python\ndef fibonacci_local(sequence_size):\n    fibonacci = []\n    for i in range(0, sequence_size):\n        if i \u003c 2:\n            fibonacci.append(i)\n            continue\n        fibonacci.append(fibonacci[i-1]+fibonacci[i-2])\n    return sequence_size\n\n# Ray task\n@ray.remote\ndef fibonacci_distributed(sequence_size):\n    fibonacci = []\n    for i in range(0, sequence_size):\n        if i \u003c 2:\n            fibonacci.append(i)\n            continue\n        fibonacci.append(fibonacci[i-1]+fibonacci[i-2])\n    return sequence_size\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are a couple of things to note regarding these two functions. First, they are identical except for the @ray.remote decorator on the fibonacci_distributed function.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The second thing to note is the small return value. They are not returning the Fibonacci sequences themselves, but the sequence size, which is an integer.Â  This is important, because it might lessen the value of a distributed function by designing it so that it requires or returns a lot of data (parameters). Engineers often refer to this as the input/output (IO) of a distributed function.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing Local vs Remote Performance","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The functions in this section will allow us to compare how long it takes to generate multiple long Fibonacci sequences both locally and in parallel. It is important to note that both functions below utilize os.cpu_count() which returns the number of CPUs in the system.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"54SXSBTuKUayBitHV0SrKc","type":"Entry","createdAt":"2021-08-12T18:20:03.593Z","updatedAt":"2021-08-12T21:18:17.799Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"os.cpu_count()","body":"os.cpu_count()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"paIFMkTXJ1wDurn4EtxUM","type":"Asset","createdAt":"2021-08-12T21:18:55.296Z","updatedAt":"2021-08-12T21:31:05.140Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"os cpucount","description":"The machine used in this tutorial has eight CPUs which means that each function below will generate 8 Fibonacci sequences.","file":{"url":"//images.ctfassets.net/xjan103pcp94/paIFMkTXJ1wDurn4EtxUM/da4d62db6720bafae6b79732267fa8c1/os.cpu_countBigger.png","details":{"size":10542,"image":{"width":652,"height":134}},"fileName":"os.cpu_countBigger.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"43HStbGxOqxYXSH1ZiYCUB","type":"Entry","createdAt":"2021-08-12T18:29:12.449Z","updatedAt":"2021-08-12T18:29:12.449Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"normal python vs run_remote Ray","body":"\n# Normal Python\ndef run_local(sequence_size):\n    start_time = time.time()\n    results = [fibonacci_local(sequence_size) for _ in range(os.cpu_count())]\n    duration = time.time() - start_time\n    print('Sequence size: {}, Local execution time: {}'.format(sequence_size, duration))\n\n# Ray\ndef run_remote(sequence_size):\n    # Starting Ray\n    ray.init()\n    start_time = time.time()\n    results = ray.get([fibonacci_distributed.remote(sequence_size) for _ in range(os.cpu_count())])\n    duration = time.time() - start_time\n    print('Sequence size: {}, Remote execution time: {}'.format(sequence_size, duration))  \n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Before getting into how the code for run_local and run_remote work, let's run both of these functions to see how long it takes to generate multiple 100000 number Fibonacci sequences both locally and remotely.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4cMVjiUB2AE87FBezGtfwP","type":"Entry","createdAt":"2021-08-12T18:29:55.125Z","updatedAt":"2021-08-12T18:29:55.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"run local vs run remote","body":"run_local(100000)\nrun_remote(100000)"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5R29PpjjLWmpy47q0ZcaSm","type":"Asset","createdAt":"2021-08-12T18:31:20.666Z","updatedAt":"2021-08-12T18:31:20.666Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"first distributed run_local run_remote","file":{"url":"//images.ctfassets.net/xjan103pcp94/5R29PpjjLWmpy47q0ZcaSm/e8e31d311963e266377017bde7bba81e/localremoteOutput.png","details":{"size":56866,"image":{"width":1160,"height":340}},"fileName":"localremoteOutput.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The run_remote function parallelized the computation across multiple cpus which resulted in a smaller processing time (1.76s vs 4.20s).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The Ray APIÂ ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"In order to better understand why run_remote was faster, let's briefly go over the code and along the way explain how the Ray API works.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xL53Bs7d4tDw0tRAlwP7y","type":"Asset","createdAt":"2021-08-12T18:32:53.495Z","updatedAt":"2021-08-12T18:32:53.495Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"run_remote yellow","file":{"url":"//images.ctfassets.net/xjan103pcp94/2xL53Bs7d4tDw0tRAlwP7y/e480af08c8a919a16c5d4f7eb8736277/run_remote_init_yellow.png","details":{"size":112667,"image":{"width":1600,"height":373}},"fileName":"run_remote_init_yellow.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The ray.init() command starts all of the relevant Ray processes. By default, Ray creates one worker process per CPU core. If you would want to run Ray on a cluster, you would need to pass in a cluster address with something like ray.init(address= 'InsertAddressHere').","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2YKe8I5CcCFWz7AMP3hCeg","type":"Asset","createdAt":"2021-08-12T18:38:26.784Z","updatedAt":"2021-08-12T18:38:26.784Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"run_remote remote fibonacci_distributed.remote","file":{"url":"//images.ctfassets.net/xjan103pcp94/2YKe8I5CcCFWz7AMP3hCeg/a8176d41153ea3950da968de71f20be2/ray_remote_remote.png","details":{"size":127368,"image":{"width":2695,"height":629}},"fileName":"ray_remote_remote.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"fibonacci_distributed.remote(100000)","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"WVgyApqF1JHhHmW12j9qQ","type":"Asset","createdAt":"2021-08-12T20:59:12.103Z","updatedAt":"2021-08-12T20:59:12.103Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"fibonacci_distributed.remote(100000)","file":{"url":"//images.ctfassets.net/xjan103pcp94/WVgyApqF1JHhHmW12j9qQ/0871bb81486d25f2acea6965a505ba88/fibonacci_distributed_remote.png","details":{"size":22343,"image":{"width":796,"height":114}},"fileName":"fibonacci_distributed_remote.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Calling fibonacci_distributed.remote(sequence_size) immediately returns a future and not the return value of the function. The actual function execution will take place in the background. Since it returns immediately, each function call can be executed in parallel. This makes generating those multiple 100000 long fibonacci sequences take less time.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4CgUHJZN6Tn0nBwgwykJae","type":"Asset","createdAt":"2021-08-12T21:00:13.491Z","updatedAt":"2021-08-12T21:00:13.491Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray.get","file":{"url":"//images.ctfassets.net/xjan103pcp94/4CgUHJZN6Tn0nBwgwykJae/a4485d5f26c191ede3a503577f51f352/ray_remote_get.png","details":{"size":125763,"image":{"width":2695,"height":629}},"fileName":"ray_remote_get.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3MfuX62xtaw4TBEe7KBE8U","type":"Asset","createdAt":"2021-08-12T21:01:41.965Z","updatedAt":"2021-08-12T21:01:41.965Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray get results","file":{"url":"//images.ctfassets.net/xjan103pcp94/3MfuX62xtaw4TBEe7KBE8U/6023be4619b7c76b869a761ee38de18c/ray_get_results.png","details":{"size":23349,"image":{"width":1204,"height":126}},"fileName":"ray get results.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"ray.get retrieves the resulting value from the task when it completes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, it is important to note that when the process calling ray.init() terminates, the Ray runtime will also terminate. Note that if you try and run ray.init() more than once you may get a RuntimeError (Maybe you called ray.init twice by accident?). This can be solved by using ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.shutdown()","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"f6zrwMtxDInCHI7rtsCtz","type":"Entry","createdAt":"2021-08-12T21:02:32.237Z","updatedAt":"2021-08-12T21:02:32.237Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray.shutdown 1st distributed application","body":"# To explicitly stop or restart Ray, use the shutdown API\nray.shutdown()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Dashboard","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray comes with a dashboard that is available at http://127.0.0.1:8265 after you call the ray.init function.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Among ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-dashboard.html#ray-dashboard"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"other things","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", the dashboard lets you:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Understand Ray memory utilization and debug memory errors.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"See per-actor resource usage, executed tasks, logs, and more.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"View cluster metrics.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Kill actors and profile your Ray jobs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"See errors and exceptions at a glance.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"View logs across many machines in a single pane.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"See ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" jobs and trial information.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The dashboard below shows the resource utilization on a per-node and per-worker basis after running run_remote(200000). Notice how the dashboard shows the function fibonacci_distributed thatâs running in each worker. Itâs a good idea to observe your distributed functions while they are running. That way, if you see one worker doing all the work, then you may be using the ray.get function incorrectly. Also, if you see your total CPU utilization getting close to 100 percent, you may be doing too much.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7slD3bTCoh7Wsd0LIScROV","type":"Asset","createdAt":"2021-08-12T21:03:54.003Z","updatedAt":"2021-08-12T21:03:54.003Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray dashboard 8 core","file":{"url":"//images.ctfassets.net/xjan103pcp94/7slD3bTCoh7Wsd0LIScROV/29127717361206b65fab68981b56e59d/8CoreMichael.png","details":{"size":369929,"image":{"width":2106,"height":1326}},"fileName":"8CoreMichael.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Trade-offs in distributed computing","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"This tutorial used Fibonacci sequences because they provide several options for tweaking computing and IO. You can alter the amount of computing that each function call requires by increasing and decreasing the sequence size. The greater the sequence size, the more computing you need to generate the sequence, whereas the smaller the sequence size, the less computing you need. If the computation you distribute is too small, the overhead of Ray would dominate the total processing time, and you wouldnât get any value out of distributing our functions.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"IO is also essential when distributing functions. If you modified these functions to return the sequences they calculate, the IO would increase as the sequence size increased. At some point, the time needed to transmit the data would dominate the total time required to complete the multiple calls to the distributed function. This is important if you are distributing your functions over a cluster. This would require the use of a network, and network calls are more costly than the interprocess communication used in this tutorial.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Therefore, it is recommended that you try to experiment with both the distributed Fibonacci function and the local Fibonacci function. Try to determine the minimum sequence size needed to benefit from a remote function. Once you figure out the computing, play with the IO to see what happens to overall performance. Distributed architectures, regardless of the tool you use, work best when they donât have to move a lot of data around.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Fortunately, a major benefit of Ray is the ability to maintain entire objects remotely. This helps mitigate the IO problem. Letâs look at that next.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Remote Objects as Actors","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Just as Ray translates Python functions to the distributed setting as tasks, Ray translates Python classes to the distributed setting as actors. Ray provides actors to allow you to parallelize an instance of a class. Code wise, all you need to add to a Python class is the @ray.remote decorator to make it an actor. When you make an instance of that class, Ray creates a new actor which is a process that runs in the cluster and holds a copy of the object.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Since they are remote objects, they can hold data, and their methods can manipulate that data. This helps cut down on interprocess communication. Consider using an actor if you find yourself writing too many tasks that return data, which in turn are sent to other tasks.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs now look at the actor below.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"TwIHe3DkT9AQ6nocBiax0","type":"Entry","createdAt":"2021-08-12T21:05:46.558Z","updatedAt":"2021-08-12T21:05:46.558Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Ray Actor","body":"from collections import namedtuple\nimport csv\nimport tarfile\nimport time\n\nimport ray\n\n@ray.remote\nclass GSODActor():\n\n    def __init__(self, year, high_temp):\n        self.high_temp = float(high_temp)\n        self.high_temp_count = None\n        self.rows = []\n        self.stations = None\n        self.year = year\n\n    def get_row_count(self):\n        return len(self.rows)\n\n    def get_high_temp_count(self):\n        if self.high_temp_count is None:\n            filtered = [l for l in self.rows if float(l.TEMP) \u003e= self.high_temp]\n            self.high_temp_count = len(filtered)\n        return self.high_temp_count\n\n    def get_station_count(self):\n        return len(self.stations)\n\n    def get_stations(self):\n        return self.stations\n\n    def get_high_temp_count(self, stations):\n        filtered_rows = [l for l in self.rows if float(l.TEMP) \u003e= self.high_temp and l.STATION in stations]\n        return len(filtered_rows)\n\n    def load_data(self):\n        file_name = self.year + '.tar.gz'\n        row = namedtuple('Row', ('STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'NAME', 'TEMP', 'TEMP_ATTRIBUTES', 'DEWP',\n                                 'DEWP_ATTRIBUTES', 'SLP', 'SLP_ATTRIBUTES', 'STP', 'STP_ATTRIBUTES', 'VISIB', 'VISIB_ATTRIBUTES',\n                                 'WDSP', 'WDSP_ATTRIBUTES', 'MXSPD', \n                                 'GUST', 'MAX', 'MAX_ATTRIBUTES', 'MIN', 'MIN_ATTRIBUTES', 'PRCP',\n                                 'PRCP_ATTRIBUTES', 'SNDP', 'FRSHTT'))\n\n        tar = tarfile.open(file_name, 'r:gz')\n        for member in tar.getmembers():\n            member_handle = tar.extractfile(member)\n            byte_data = member_handle.read()\n            decoded_string = byte_data.decode()\n            lines = decoded_string.splitlines()\n            reader = csv.reader(lines, delimiter=',')\n\n            # Get all the rows in the member. Skip the header.\n            _ = next(reader)\n            file_rows = [row(*l) for l in reader]\n            self.rows += file_rows\n\n        self.stations = {l.STATION for l in self.rows}\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The code above can be used to load and manipulate data from a public dataset known as the Global Surface Summary of the Day (GSOD). The dataset is managed by the National Oceanic and Atmospheric Administration (NOAA) and it is freely available on their ","nodeType":"text"},{"data":{"uri":"https://www.ncei.noaa.gov/data/global-summary-of-the-day/archive/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"site","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". NOAA currently maintains data from over 9,000 stations worldwide and the GSOD dataset contains daily summary information from these stations. There is one gzip file for each year from 1929 to 2020. For this tutorial, you only need to download the files for ","nodeType":"text"},{"data":{"uri":"https://www.ncei.noaa.gov/data/global-summary-of-the-day/archive/1980.tar.gz"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"1980","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://www.ncei.noaa.gov/data/global-summary-of-the-day/archive/2020.tar.gz"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"2020","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The goal of this actor experiment is to compute how many readings from 1980 and 2020 were 100 degrees or greater and determine if 2020 had more extreme temperatures than 1980. In order to implement a fair comparison, only stations that existed in both 1980 and 2020 should be considered. So, the logic of this experiment looks like this:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Load 1980 data.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Load 2020 data.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Get a list of stations that existed in 1980.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Get a list of stations that existed in 2020.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Determine the intersection of stations.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Get the number of readings that were 100 degrees or greater from the intersection of stations during 1980.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Get the number of readings that were 100 degrees or greater from the intersection of stations during 2020.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Print the results.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The problem is that this logic is completely sequential; one thing only happens after another. With Ray, a lot of this logic can be done in parallel.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The table below shows a more parallelizable logic.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"68pZlG1nraPtehugiEkHS0","type":"Asset","createdAt":"2021-08-12T21:07:50.037Z","updatedAt":"2021-08-12T21:07:50.037Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Actor Logic","file":{"url":"//images.ctfassets.net/xjan103pcp94/68pZlG1nraPtehugiEkHS0/c6cca4f793189d8cac6a1705b667f83e/RayActorLogic.png","details":{"size":41120,"image":{"width":850,"height":278}},"fileName":"RayActorLogic.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Writing out the logic in this fashion is an excellent way of making sure you are executing everything that you can in a parallelizable way. The code below implements this logic. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3KKnRHGruKzanOKTyoGWe5","type":"Entry","createdAt":"2021-08-12T21:08:53.055Z","updatedAt":"2021-08-12T21:08:53.055Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"compare_years actor","body":"# Code assumes you have the 1980.tar.gz and 2020.tar.gz files in your current working directory.\ndef compare_years(year1, year2, high_temp):\n\n    # if you know that you need fewer than the default number of workers,\n    # you can modify the num_cpus parameter\n    ray.init(num_cpus=2)\n\n    # Create actor processes\n    gsod_y1 = GSODActor.remote(year1, high_temp)\n    gsod_y2 = GSODActor.remote(year2, high_temp)\n\n    ray.get([gsod_y1.load_data.remote(), gsod_y2.load_data.remote()])\n\n    y1_stations, y2_stations = ray.get([gsod_y1.get_stations.remote(),\n               \t                    gsod_y2.get_stations.remote()])\n\n    intersection = set.intersection(y1_stations, y2_stations)\n\n    y1_count, y2_count = ray.get([gsod_y1.get_high_temp_count.remote(intersection),\n                                  gsod_y2.get_high_temp_count.remote(intersection)])\n\n    print('Number of stations in common: {}'.format(len(intersection)))\n    print('{} - High temp count for common stations: {}'.format(year1, y1_count))\n    print('{} - High temp count for common stations: {}'.format(year2, y2_count))\n\n#Running the code below will output which year had more extreme temperatures\ncompare_years('1980', '2020', 100)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2ByyLQUfrMmGJY26r8i7zf","type":"Asset","createdAt":"2021-08-12T21:09:46.703Z","updatedAt":"2021-08-12T21:09:46.703Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"compare years","file":{"url":"//images.ctfassets.net/xjan103pcp94/2ByyLQUfrMmGJY26r8i7zf/4acbc0305931d25c6e33533afcfa3ebe/compare_years.png","details":{"size":78224,"image":{"width":1460,"height":296}},"fileName":"compare_years.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are a couple important things to mention about the code above. First, putting the @ray.remote decorator at the class level enabled all class methods to be called remotely. Second, the code above utilizes two actor processes (gsod_y1 and gsod_y2) which can execute methods in parallel (though each actor can only execute one method at a time). This is what enabled the loading and processing of the 1980 and 2020 data at the same time.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a fast, simple distributed execution framework that makes it easy to scale your applications and to leverage state of the art machine learning libraries.Â  This tutorial showed how using Ray makes it easy to take your existing Python code that runs sequentially and transform it into a distributed application with minimal code changes. While the experiments here were all performed on the same machine, ","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/how-to-scale-python-on-every-major-cloud-provider-5e5df3e88274"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray also makes it easy to scale your Python code on every major cloud provider","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". If youâre interested in learning more about Ray, check out the ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray project on GitHub","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", follow ","nodeType":"text"},{"data":{"uri":"https://twitter.com/raydistributed"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"@raydistributed on twitter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and sign up for the ","nodeType":"text"},{"data":{"uri":"https://anyscale.us5.list-manage.com/subscribe?u=524b25758d03ad7ec4f64105f\u0026id=d94e960a03"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray newsletter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3w1vm8nJgJ1YKcBvy8eHXU","type":"Asset","createdAt":"2021-08-12T21:35:52.571Z","updatedAt":"2021-08-12T21:35:52.571Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Dashboard 8 Core","file":{"url":"//images.ctfassets.net/xjan103pcp94/3w1vm8nJgJ1YKcBvy8eHXU/eb733efbaf26b0c1c31378f9eb349e22/8CPUMichael.png","details":{"size":373794,"image":{"width":2274,"height":1322}},"fileName":"8CPUMichael.png","contentType":"image/png"}}},"recommendations":[],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true}},"thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"qyo3m0dc8CvMNCfhxDy5e","type":"Asset","createdAt":"2022-03-24T21:30:36.648Z","updatedAt":"2022-03-24T21:30:36.648Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-code-dark","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/qyo3m0dc8CvMNCfhxDy5e/409ed6f79aea5822b10aaa365318a005/blog-recommended-content-code-dark.jpg","details":{"size":39745,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-code-dark.jpg","contentType":"image/jpeg"}}},"recommendations":null}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"RDchDwEES1b4bxXTiRthS","type":"Entry","createdAt":"2021-08-26T15:47:04.783Z","updatedAt":"2022-06-22T16:57:04.024Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":15,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"An Introduction to Reinforcement Learning with OpenAI Gym, RLlib, and Google Colab","slug":"an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4MRdnDXbzFHCnXPJLKM6yu","type":"Entry","createdAt":"2021-01-19T01:14:31.681Z","updatedAt":"2021-01-19T01:14:31.681Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Sven Mika","slug":"sven-mika","link":"https://twitter.com/sven_mika"}}],"publishedDate":"2021-08-26","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"An introductory tutorial on reinforcement learning with OpenAI Gym, RLlib, and Google Colab. ","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4vx7G22RlSS9WfUsYRnWXP","type":"Entry","createdAt":"2021-08-26T02:23:06.709Z","updatedAt":"2021-08-26T02:23:06.709Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"CartPole Video Embed","videoUrl":"https://www.youtube.com/watch?v=XiigTGKZfks\u0026t=106s ","caption":"This tutorial will use reinforcement learning (RL) to help balance a virtual CartPole. The [video](https://www.youtube.com/watch?v=XiigTGKZfks\u0026t=106s ) above from PilcoLearner shows the results of using RL in a real-life CartPole environment."}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"One possible definition of reinforcement learning (RL) is a computational approach to learning how to maximize the total sum of rewards when interacting with an environment. While a definition is useful, this tutorial aims to illustrate what reinforcement learning is through images, code, and video examples and along the way introduce reinforcement learning terms like agents and environments.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In particular, this tutorial explores:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"What is Reinforcement LearningÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The OpenAI Gym CartPole Environment","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The Role of Agents in Reinforcement Learning","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"How to Train an Agent by using the Python Library RLlib","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"How to use a GPU to Speed Up Training","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Hyperparameter Tuning with Ray Tune","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What is Reinforcement Learning","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/reinforcement-learning-with-rllib-in-the-unity-game-engine"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"previous post noted","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", machine learning (ML), a sub-field of AI, uses neural networks or other types of mathematical models to learn how to interpret complex patterns. Two areas of ML that have recently become very popular due to their high level of maturity are supervised learning (SL), in which neural networks learn to make predictions based on large amounts of data, and reinforcement learning (RL), where the networks learn to make good action decisions in a trial-and-error fashion, using a simulator.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1uWlGSCvzjk6hY3PNw5NdA","type":"Asset","createdAt":"2021-08-25T20:35:29.798Z","updatedAt":"2021-08-25T20:35:29.798Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"intro-to-rl-1","file":{"url":"//images.ctfassets.net/xjan103pcp94/1uWlGSCvzjk6hY3PNw5NdA/92e09a3d2351f8672b3bb46df9144cc7/Blog_-_intro_reinforcement.png","details":{"size":102498,"image":{"width":512,"height":417}},"fileName":"Blog - intro reinforcement.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"RL is the tech behind mind-boggling successes such as DeepMindâs AlphaGo Zero and the StarCraft II AI (AlphaStar) or OpenAIâs DOTA 2 AI (âOpenAI Fiveâ). Note that there are ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"many impressive uses of reinforcement learning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the reason why it is so powerful and promising for real-life decision making problems is because RL is capable of learning continuously â sometimes even in ever changing environments â starting with no knowledge of which decisions to make whatsoever (random behavior). ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1lBFXrIrR4PZT2kwg1ewQW","type":"Entry","createdAt":"2022-02-17T18:22:16.717Z","updatedAt":"2022-02-17T23:55:42.067Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"rl-summit-cta","body":"\u003ca href=\"/production-rl-summit#agenda?utm_source=anyscale\u0026utm_medium=blog\u0026utm_content=openaigym-blog\u0026utm_campaign=rl_summit\"\u003e\u003cimg src=\"//images.ctfassets.net/xjan103pcp94/1hu1Zwg1RQ0ifg9Lfp98bA/7d913772436d45666a80e83f629b4815/RL_Summit__12_.png\" style=\"width: 100%\"\u003e\u003c/a\u003e"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Agents and Environments","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5h5ZwNAqLHAIRZ9jPGvRU1","type":"Asset","createdAt":"2021-08-25T20:48:58.153Z","updatedAt":"2021-08-25T20:48:58.153Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"agents-and-environments","file":{"url":"//images.ctfassets.net/xjan103pcp94/5h5ZwNAqLHAIRZ9jPGvRU1/6ceb65f718883cf2e7b8ca9dcd0a5fc4/Blog_-_intro_reinforcement_2.png","details":{"size":81263,"image":{"width":512,"height":309}},"fileName":"Blog - intro reinforcement 2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The diagram above shows the interactions and communications between an agent and an environment. In reinforcement learning, one or more agents interact within an environment which may be either a simulation like CartPole in this tutorial or a connection to real-world sensors and actuators. At each step, the agent receives an observation (i.e., the state of the environment), takes an action, and usually receives a reward (the frequency at which an agent receives a reward depends on a given task or problem). Agents learn from repeated trials, and a sequence of those is called an episode â the sequence of actions from an initial observation up to either a âsuccessâ or âfailureâ causing the environment to reach its âdoneâ state. The learning portion of an RL framework trains a policy about which actions (i.e., sequential decisions) cause agents to maximize their long-term, cumulative rewards. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The OpenAI Gym Cartpole Environment","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4hLHnMXJN2EwwAXq2yYx9v","type":"Asset","createdAt":"2021-09-02T17:53:23.032Z","updatedAt":"2021-09-02T17:54:36.660Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"CartPole Remade","description":"CartPole","file":{"url":"//images.ctfassets.net/xjan103pcp94/4hLHnMXJN2EwwAXq2yYx9v/41b16121290d6c46b6b85492a572a4cf/cartPoleRemade.png","details":{"size":29009,"image":{"width":1052,"height":781}},"fileName":"cartPoleRemade.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The problem we are trying to solve is trying to keep a pole upright. Specifically, the pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1g6s6IMlS4IqUCZrpXZGUM","type":"Asset","createdAt":"2021-08-25T20:56:09.733Z","updatedAt":"2021-08-25T20:56:09.733Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"policy ","file":{"url":"//images.ctfassets.net/xjan103pcp94/1g6s6IMlS4IqUCZrpXZGUM/2493f114da86c08108f3b763343e83a9/Blog_-_intro_reinforcement_4.png","details":{"size":81222,"image":{"width":512,"height":310}},"fileName":"Blog - intro reinforcement 4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Rather than code this environment from scratch, this tutorial will use ","nodeType":"text"},{"data":{"uri":"http://gym.openai.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"OpenAI Gym","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" which is a toolkit that provides a wide variety of simulated environments (Atari games, board games, 2D and 3D physical simulations, and so on). Gym makes no assumptions about the structure of your agent (what pushes the cart left or right in this cartpole example), and is compatible with any numerical computation library, such as numpy.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The code below loads the cartpole environment.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"248NFtYGwnp83dCT5z5g2O","type":"Entry","createdAt":"2021-08-26T02:26:23.264Z","updatedAt":"2021-08-26T02:26:23.264Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"import gym rllib","body":"import gym\nenv = gym.make(\"CartPole-v0\")","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's now start to understand this environment by looking at the action space.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3tOSAPhzPsECwFrbpl9rmr","type":"Entry","createdAt":"2021-08-26T02:27:02.351Z","updatedAt":"2021-08-26T02:27:26.263Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"env.action_space","body":"env.action_space","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4h49FDvVX7yC4W132pIojC","type":"Asset","createdAt":"2021-08-25T20:59:15.616Z","updatedAt":"2021-08-25T20:59:15.616Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"env.action_space","file":{"url":"//images.ctfassets.net/xjan103pcp94/4h49FDvVX7yC4W132pIojC/d0299dd7ceae93f7ef70b12a2b9b5b44/Blog_-_intro_reinforcement_5.png","details":{"size":11312,"image":{"width":290,"height":118}},"fileName":"Blog - intro reinforcement 5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The output Discrete(2) means that there are two actions. In cartpole, 0 corresponds to \"push cart to the left\" and 1 corresponds to \"push cart to the right\". Note that in this particular example, standing still is not an option. In reinforcement learning, the agent produces an action output and this action is sent to an environment which then reacts. The environment produces an observation (along with a reward signal, not shown here) which we can see below:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1tocyYPjnLSMgmc0g6CBcq","type":"Entry","createdAt":"2021-08-26T02:27:56.995Z","updatedAt":"2021-08-26T02:27:56.995Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"env.reset()","body":"env.reset()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"rxba5zIqvEqMBkOs8CFEF","type":"Asset","createdAt":"2021-08-25T21:00:08.346Z","updatedAt":"2021-08-25T21:00:08.346Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"env.reset ()","file":{"url":"//images.ctfassets.net/xjan103pcp94/rxba5zIqvEqMBkOs8CFEF/532a32e7eb27763d8d656bf2b8747f0f/Blog_-_intro_reinforcement_6.png","details":{"size":15292,"image":{"width":512,"height":140}},"fileName":"Blog - intro reinforcement 6.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The observation is a vector of dim=4, containing the cart's x position, cart x velocity, the pole angle in radians (1 radian = 57.295 degrees), and the angular velocity of the pole. The numbers shown above are the initial observation after starting a new episode (`env.reset()`). With each timestep (and action), the observation values will change, depending on the state of the cart and pole.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Training an Agent","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In reinforcement learning, the goal of the agent is to produce smarter and smarter actions over time. It does so with a policy. In deep reinforcement learning, this policy is represented with a neural network. Let's first interact with the gym environment without a neural network or machine learning algorithm of any kind. Instead we'll start with random movement (left or right). This is just to understand the mechanisms.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"52y6RzXWOM5ntjn1BZwUYP","type":"Asset","createdAt":"2021-08-25T21:01:08.772Z","updatedAt":"2021-08-25T21:01:08.772Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"policy random movement","file":{"url":"//images.ctfassets.net/xjan103pcp94/52y6RzXWOM5ntjn1BZwUYP/94044c6727443aab584f449dfb419cf0/Blog_-_intro_reinforcement_7.png","details":{"size":73994,"image":{"width":512,"height":312}},"fileName":"Blog - intro reinforcement 7.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The code below resets the environment and takes 20 steps (20 cycles), always taking a random action and printing the results.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1po0JCIIyh2uhzuI0QtWIl","type":"Entry","createdAt":"2021-08-26T02:33:42.667Z","updatedAt":"2021-08-26T02:33:42.667Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"CartPole 20 steps and take random actions","body":"# returns an initial observation\nenv.reset()\n\nfor i in range(20):\n\n  # env.action_space.sample() produces either 0 (left) or 1 (right).\n  observation, reward, done, info = env.step(env.action_space.sample())\n\n  print(\"step\", i, observation, reward, done, info)\n\nenv.close()\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"74TmunIlHtkSEvH0nxtnjg","type":"Asset","createdAt":"2021-08-25T21:03:22.070Z","updatedAt":"2021-08-25T21:03:36.648Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"steps","description":"Sample output. There are multiple conditions for episode termination in cartpole. In the image, the episode is terminated because it is over 12 degrees (0.20944 rad). Other conditions for episode termination are cart position is more than 2.4 (center of the cart reaches the edge of the display), episode length is greater than 200, or the solved requirement which is when the average return is greater than or equal to 195.0 over 100 consecutive trials.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/74TmunIlHtkSEvH0nxtnjg/44ed9a28256c676eb01b365afd499328/Blog_-_intro_reinforcement_8.png","details":{"size":147352,"image":{"width":512,"height":280}},"fileName":"Blog - intro reinforcement 8.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The printed output above shows the following things:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"step (how many times it has cycled through the environment). In each timestep, an agent chooses an action, and the environment returns an observation and a reward","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"observation of the environment [x cart position, x cart velocity, pole angle (rad), pole angular velocity]","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward. The reward is 1 for every step taken for cartpole, including the termination step. After it is 0 (step 18 and 19 in the image).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"done is a boolean. It indicates whether it's time to reset the environment again. Most tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. In cart pole, it could be that the pole tipped too far (more than 12 degrees/0.20944 radians), position is more than 2.4 meaning the center of the cart reaches the edge of the display, episode length is greater than 200, or the solved requirement which is when the average return is greater than or equal to 195.0 over 100 consecutive trials.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"info which is diagnostic information useful for debugging. It is empty for this cartpole environment.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"While these numbers are useful outputs, a video might be clearer. If you are running this code in Google Colab,Â  it is important to note that there is no display driver available for generating videos. However, it is possible to install a virtual display driver to get it to work.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"BMC1nfuvsamFESKvCOZjK","type":"Entry","createdAt":"2021-08-26T02:34:55.826Z","updatedAt":"2021-08-26T02:34:55.826Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"install dependencies for recording videos in jupyter","body":"# install dependencies needed for recording videos\n!apt-get install -y xvfb x11-utils\n!pip install pyvirtualdisplay==0.2.*\n"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The next step is to start an instance of the virtual display.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Z83g1VInTf4XsjvJKtCsW","type":"Entry","createdAt":"2021-08-26T02:35:55.777Z","updatedAt":"2021-08-26T02:35:55.777Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"from pyvirtualdisplay import Display","body":"from pyvirtualdisplay import Display\ndisplay = Display(visible=False, size=(1400, 900))\n_ = display.start()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"OpenAI gym has a VideoRecorder wrapper that can record a video of the running environment in MP4 format. The code below is the same as before except that it is for 200 steps and is recording.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3JvjQ0rhBqiacofcsb0cCZ","type":"Entry","createdAt":"2021-08-26T02:36:57.451Z","updatedAt":"2021-08-26T02:37:11.181Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"from gym.wrappers.monitoring.video_recorder import VideoRecorder","body":"from gym.wrappers.monitoring.video_recorder import VideoRecorder\nbefore_training = \"before_training.mp4\"\n\nvideo = VideoRecorder(env, before_training)\n# returns an initial observation\nenv.reset()\nfor i in range(200):\nÂ Â env.render()\nÂ Â video.capture_frame()\nÂ Â # env.action_space.sample() produces either 0 (left) or 1 (right).\nÂ Â observation, reward, done, info = env.step(env.action_space.sample())\nÂ Â # Not printing this time\nÂ Â #print(\"step\", i, observation, reward, done, info)\n\nvideo.close()\nenv.close()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1mDEqsrizwjyoV3YSBWZkH","type":"Asset","createdAt":"2021-08-25T21:06:40.749Z","updatedAt":"2021-08-25T21:06:58.240Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"user warning","description":"Usually you end the simulation when done is 1 (True). The code above let the environment keep on going after a termination condition was reached. For example, in CartPole, this could be when the pole tips over, pole goes off-screen, or reaches other termination conditions.","file":{"url":"//images.ctfassets.net/xjan103pcp94/1mDEqsrizwjyoV3YSBWZkH/5867b700eb021a594fa0b1d34120ebe6/Blog_-_intro_reinforcement_9.png","details":{"size":8732,"image":{"width":512,"height":11}},"fileName":"Blog - intro reinforcement 9.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The code above saved the video file into the Colab disk. In order to display it in the notebook, you need a helper function.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4FGtJPhCDOla8Eeg1uff2q","type":"Entry","createdAt":"2021-08-26T02:39:16.921Z","updatedAt":"2021-08-26T02:39:16.921Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"from base64 import b64encode","body":"from base64 import b64encode\ndef render_mp4(videopath: str) -\u003e str:\nÂ Â \"\"\"\nÂ Â Gets a string containing a b4-encoded version of the MP4 video\nÂ Â at the specified path.\nÂ Â \"\"\"\nÂ Â mp4 = open(videopath, 'rb').read()\nÂ Â base64_encoded_mp4 = b64encode(mp4).decode()\nÂ Â return f'\u003cvideo width=400 controls\u003e\u003csource src=\"data:video/mp4;' \\\nÂ Â Â Â Â Â Â Â Â f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"\u003e\u003c/video\u003e'\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The code below renders the results. You should get a video similar to the one below.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"17lz8NAyS0WFEMHGEGeRtH","type":"Entry","createdAt":"2021-08-26T18:22:43.337Z","updatedAt":"2021-08-26T18:22:43.337Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Render Results","body":"from IPython.display import HTML\nhtml = render_mp4(before_training)\nHTML(html)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"25DKSpb5hbLqZyjB3kx1YE","type":"Entry","createdAt":"2021-08-26T02:41:53.727Z","updatedAt":"2021-08-26T02:41:53.727Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"after training cartpole","videoUrl":"https://www.youtube.com/watch?v=ZF4IJotq4ac"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Playing the video demonstrates that randomly choosing an action is not a good policy for keeping the CartPole upright.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How to Train an Agent using Ray's RLlib","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The previous section of the tutorial had our agent make random actions disregarding the observations and rewards from the environment. The goal of having an agent is to produce smarter and smarter actions over time and random actions don't accomplish that. To make an agent make smarter actions over time, itl needs a better policy. In deep reinforcement learning, the policy is represented with a neural network.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3su83sKnDVpzt0r1OJO1DD","type":"Asset","createdAt":"2021-08-25T21:08:18.790Z","updatedAt":"2021-08-25T21:08:18.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"policy deep-rl","file":{"url":"//images.ctfassets.net/xjan103pcp94/3su83sKnDVpzt0r1OJO1DD/8a59845c574723c1b9ec0d5ebf9cacc4/Blog_-_intro_reinforcement_10.png","details":{"size":91950,"image":{"width":512,"height":298}},"fileName":"Blog - intro reinforcement 10.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This tutorial will use the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/rllib.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RLlib library","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to train a smarter agent. RLlib has many advantages like:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Extreme flexibility. It allows you to customize every aspect of the RL cycle. For instance, this section of the tutorial will make a custom neural network policy using PyTorch (RLlib also has native support for TensorFlow).Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scalability. Reinforcement learning applications can be quite compute intensive and often need to scale-out to a cluster for faster training. RLlib not only has first-class support for GPUs, but it is also built onÂ ","nodeType":"text"},{"data":{"uri":"https://ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â which is an open source library forÂ ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/parallelizing-python-code"},"content":[{"data":{},"marks":[],"value":"parallel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â andÂ ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray"},"content":[{"data":{},"marks":[],"value":"distributed","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â Python. This makes scaling Python programs from a laptop to a cluster easy.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"A unified API and support for offline, model-based, model-free, multi-agent algorithms, and more (these algorithms wonât be explored in this tutorial).Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Being part of the ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Project ecosystem","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". One advantage of this is that RLlib can be run with other libraries in the ecosystem like ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a library for experiment execution and hyperparameter tuning at any scale (more on this later).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"While some of these features won't be fully utilized in this post, they are highly useful for when you want to do something more complicated and solve real world problems. You can learn about some impressive use-cases of RLlib ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To get started with RLlib, you need to first install it.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"llPmdpicPEh4sktQLClkN","type":"Entry","createdAt":"2021-08-26T02:43:18.133Z","updatedAt":"2021-08-26T02:44:10.138Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"pip install 'ray[rllib]'==1.6","body":"!pip install 'ray[rllib]'==1.6","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now you can train a PyTorch model using the Proximal Policy Optimization (PPO) algorithm. It is a very well rounded, one size fits all type of algorithm which you can learn more about ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/rllib-algorithms.html#proximal-policy-optimization-ppo"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". The code below uses a neural network consisting of a single hidden layer of 32 neurons and linear activation functions.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3CEIlpc4TZNTT5rhjNXXmk","type":"Entry","createdAt":"2021-08-26T02:45:30.639Z","updatedAt":"2021-08-26T02:45:30.639Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"PPOTrainer RLlib ","body":"import ray\nfrom ray.rllib.agents.ppo import PPOTrainer\nconfig = {\nÂ Â Â Â \"env\": \"CartPole-v0\",\nÂ Â Â Â # Change the following line to `âframeworkâ: âtfâ` to use tensorflow\nÂ Â Â Â \"framework\": \"torch\",\nÂ Â Â Â \"model\": {\nÂ Â Â Â Â Â \"fcnet_hiddens\": [32],\nÂ Â Â Â Â Â \"fcnet_activation\": \"linear\",\nÂ Â Â Â },\n}\nÂ stop = {\"episode_reward_mean\": 195}\nÂ ray.shutdown()\nray.init(\nÂ Â num_cpus=3,\nÂ Â include_dashboard=False,\nÂ Â ignore_reinit_error=True,\nÂ Â log_to_driver=False,\n)\n# execute trainingÂ \nanalysis = ray.tune.run(\nÂ Â \"PPO\",\nÂ Â config=config,\nÂ Â stop=stop,\nÂ Â checkpoint_at_end=True,\n)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This code should produce quite a bit of output. The final entry should look something like this:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1ABB5vepqo4Iy7UOOCJawT","type":"Asset","createdAt":"2021-08-25T21:11:05.878Z","updatedAt":"2021-08-25T21:11:05.878Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"status","file":{"url":"//images.ctfassets.net/xjan103pcp94/1ABB5vepqo4Iy7UOOCJawT/906249ef28e0138ab98df13b79dbe955/Blog_-_intro_reinforcement_13.png","details":{"size":40211,"image":{"width":512,"height":128}},"fileName":"Blog - intro reinforcement 13.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The entry shows it took 35 iterations, running over 258 seconds, to solve the environment. This will be different each time, but will probably be about 7 seconds per iteration (258 / 35 = 7.3). Note that if you like to learn the Ray API and see what commands like ray.shutdown and ray.init do, you can ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"check out this tutorial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How to use a GPU to Speed Up Training","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"While the rest of the tutorial utilizes CPUs, it is important to note that you can speed up model training by using a GPU in Google Colab. This can be done by selecting ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Runtime \u003e Change runtime type","nodeType":"text"},{"data":{},"marks":[],"value":" and set hardware accelerator to ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"GPU","nodeType":"text"},{"data":{},"marks":[],"value":". Then select ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Runtime \u003e Restart and run all","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"67hYTruOvuO7Y8L0rKwG8g","type":"Asset","createdAt":"2021-08-25T21:14:07.578Z","updatedAt":"2021-08-25T21:14:07.578Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"status 2","file":{"url":"//images.ctfassets.net/xjan103pcp94/67hYTruOvuO7Y8L0rKwG8g/c6f8e508ce2c0c5a5d6db907edb2c9f7/Blog_-_intro_reinforcement_14.png","details":{"size":38211,"image":{"width":512,"height":95}},"fileName":"Blog - intro reinforcement 14.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Notice that, although the number of training iterations might be about the same, the time per iteration has come down significantly (from 7 seconds to 5.5 seconds).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Creating a Video of the Trained Model in Action","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"RLlib provides a Trainer class which holds a policy for environment interaction. Through the trainer interface, a policy can be trained, action computed, and checkpointed. While the analysis object returned from ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"ray.tune.run","nodeType":"text"},{"data":{},"marks":[],"value":" earlier did not contain any trainer instances, it has all the information needed to reconstruct one from a saved checkpoint becauseÂ  ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"checkpoint_at_end=True","nodeType":"text"},{"data":{},"marks":[],"value":" was passed as a parameter. The code below shows this.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"53jTRj0oqUQCU2FqdhRQR1","type":"Entry","createdAt":"2021-08-26T02:46:16.825Z","updatedAt":"2021-08-26T02:46:16.825Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"restore a trainer from the last checkpoint","body":"# restore a trainer from the last checkpoint\ntrial = analysis.get_best_logdir(\"episode_reward_mean\", \"max\")\ncheckpoint = analysis.get_best_checkpoint(\nÂ Â trial,\nÂ Â \"training_iteration\",\nÂ Â \"max\",\n)\ntrainer = PPOTrainer(config=config)\ntrainer.restore(checkpoint)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs now create another video, but this time choose the action recommended by the trained model instead of acting randomly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Poa61wCX6geBXS9dKLRdy","type":"Entry","createdAt":"2021-08-26T02:47:13.048Z","updatedAt":"2021-08-26T02:47:13.048Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"after_training = \"after_training.mp4\"","body":"after_training = \"after_training.mp4\"\nafter_video = VideoRecorder(env, after_training)\nobservation = env.reset()\ndone = False\nwhile not done:\nÂ Â env.render()\nÂ Â after_video.capture_frame()\nÂ Â action = trainer.compute_action(observation)\nÂ Â observation, reward, done, info = env.step(action)\nafter_video.close()\nenv.close()\n# You should get a video similar to the one below.Â \nhtml = render_mp4(after_training)\nHTML(html)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7j1W4H6wFrdoem88Ff0pM2","type":"Entry","createdAt":"2021-08-26T02:48:20.776Z","updatedAt":"2021-08-26T02:48:20.776Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"After Training CartPole","videoUrl":"https://youtu.be/6VJvXM-KSYY"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This time, the pole balances nicely which means the agent has solved the cartpole environment!Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hyperparameter Tuning with Ray Tune","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1EGuORfezOpLR5ywb0Hjlq","type":"Asset","createdAt":"2021-08-25T21:20:39.972Z","updatedAt":"2021-08-25T21:20:39.972Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"libraries","description":"The Ray Ecosystem","file":{"url":"//images.ctfassets.net/xjan103pcp94/1EGuORfezOpLR5ywb0Hjlq/86b3d2ba725ba7a355597b5055659794/Blog_-_intro_reinforcement_11.jpeg","details":{"size":41774,"image":{"width":512,"height":288}},"fileName":"Blog - intro reinforcement 11.jpeg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"For a deep-dive on hyperparameter tuning, from the basics to how to distribute hyperparameter tuning using Ray Tune, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/what-is-hyperparameter-tuning"},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"check our our blog series on hyperparameter optimization","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"RLlib is a reinforcement learning library that is part of the Ray Ecosystem. Ray is a highly scalable universal framework for parallel and distributed python. It is very general and that generality is important for supporting its library ecosystem. The ecosystem covers everything from ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"training","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", to ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"production serving","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/data-processing-support-in-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"data processing","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and more. You can use multiple libraries together and build applications that do all of these things.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This part of the tutorial utilizes ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" which is another library in the Ray Ecosystem. It is a library for experiment execution and hyperparameter tuning at any scale. While this tutorial will only use grid search, note that Ray Tune also gives you access to more efficient hyperparameter tuning algorithms like population based training, BayesOptSearch, and HyperBand/ASHA.Â Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs now try to find hyperparameters that can solve the CartPole environment in the fewest timesteps.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Enter the following code, and be prepared for it to take a while to run:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"57rjoEkvVk1pYlLGGoQImw","type":"Entry","createdAt":"2021-08-26T02:51:39.184Z","updatedAt":"2021-08-26T02:51:39.184Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"parameter_search_config = {     \"env\": \"CartPole-v0\",","body":"parameter_search_config = {\n    \"env\": \"CartPole-v0\",\n    \"framework\": \"torch\",\n\n    # Hyperparameter tuning\n    \"model\": {\n      \"fcnet_hiddens\": ray.tune.grid_search([[32], [64]]),\n      \"fcnet_activation\": ray.tune.grid_search([\"linear\", \"relu\"]),\n    },\n    \"lr\": ray.tune.uniform(1e-7, 1e-2)\n}\n\n# To explicitly stop or restart Ray, use the shutdown API.\nray.shutdown()\n\nray.init(\n  num_cpus=12,\n  include_dashboard=False,\n  ignore_reinit_error=True,\n  log_to_driver=False,\n)\n\nparameter_search_analysis = ray.tune.run(\n  \"PPO\",\n  config=parameter_search_config,\n  stop=stop,\n  num_samples=5,\n  metric=\"timesteps_total\",\n  mode=\"min\",\n)\n\nprint(\n  \"Best hyperparameters found:\",\n  parameter_search_analysis.best_config,\n)\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"By asking for 12 CPU cores by passing in num_cpus=12 to ray.init, four trials get run in parallel across three cpus each. If this doesnât work, perhaps Google has changed the VMs available on Colab. Any value of three or more should work. If Colab errors by running out of RAM, you might need to do ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Runtime \u003e Factory reset runtime","nodeType":"text"},{"data":{},"marks":[],"value":", followed by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Runtime \u003e Run all","nodeType":"text"},{"data":{},"marks":[],"value":". Note that there is an area in the top right of the Colab notebook showing the RAM and disk use.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Specifying num_samples=5 means that you will get five random samples for the learning rate. For each of those, there are two values for the size of the hidden layer, and two values for the activation function. So, there will be 5 * 2 * 2 = 20 trials, shown with their statuses in the output of the cell as the calculation runs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Note that Ray prints the current best configuration as it goes. This includes all the default values that have been set, which is a good place to find other parameters that could be tweaked.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"After running this, the final output might be similar to the following output:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"INFO tune.py:549 -- Total run time: 3658.24 seconds (3657.45 seconds for the tuning loop).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Best hyperparameters found: {'env': 'CartPole-v0', 'framework': 'torch', 'model': {'fcnet_hiddens': [64], 'fcnet_activation': 'relu'}, 'lr': 0.006733929096170726};'''","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"So, of the twenty sets of hyperparameters, the one with 64 neurons, the ReLU activation function, and a learning rate around 6.7e-3 performed best.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"601dfdJWuaH4znfvrjIDRI","type":"Asset","createdAt":"2021-08-25T21:22:37.638Z","updatedAt":"2021-08-25T21:23:15.054Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"neural mmo","description":"Neural MMO is an environment modeled from Massively Multiplayer Online games â a genre supporting hundreds to thousands of concurrent players. You can learn how Ray and RLlib help enable some key features of this and other projects [here](https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021). ","file":{"url":"//images.ctfassets.net/xjan103pcp94/601dfdJWuaH4znfvrjIDRI/31fb646652ae4332b2fe0c75fb76f1cd/Blog_-_intro_reinforcement_12.jpeg","details":{"size":97412,"image":{"width":512,"height":288}},"fileName":"Blog - intro reinforcement 12.jpeg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This tutorial illustrated what reinforcement learning is by introducing reinforcement learning terminology, by showing how agents and environments interact, and by demonstrating these concepts through code and video examples. If you would like to learn more about reinforcement learning, check out the ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/events/2021/06/24/hands-on-reinforcement-learning-with-rays-rllib"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RLlib tutorial by Sven Mika","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". It is a great way to learn about RLlibâs best practices, multi-agent algorithms, and much more. If you would like to keep up to date with all things RLlib and Ray, consider ","nodeType":"text"},{"data":{"uri":"https://twitter.com/raydistributed"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"following @raydistributed on twitter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://anyscale.us5.list-manage.com/subscribe?u=524b25758d03ad7ec4f64105f\u0026id=d94e960a03"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"sign up for the Ray newsletter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3ies13ZkanQdfdD5onpL6C","type":"Asset","createdAt":"2021-08-26T15:50:13.877Z","updatedAt":"2021-08-26T15:50:13.877Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Introduction to Reinforcement Learning ","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3ies13ZkanQdfdD5onpL6C/88151cd3a715a9dabc4f5adff8c66b30/InteractionCommunicationEnvironment.jpg","details":{"size":61078,"image":{"width":782,"height":471}},"fileName":"InteractionCommunicationEnvironment.jpg","contentType":"image/jpeg"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Zk7jixFrLqHf3IJLzrO2B","type":"Entry","createdAt":"2022-05-03T18:47:31.422Z","updatedAt":"2022-05-03T18:47:31.422Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Ray Summit: Aug 23-24 in SF!","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5LTWfPAScfUtzXj2KOUALz","type":"Entry","createdAt":"2022-04-12T15:29:42.674Z","updatedAt":"2022-10-04T22:06:07.976Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":17,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"summit"}},"locale":"en-US"},"fields":{"title":"Ray Summit 2022","slug":"ray-summit-2022","description":"Centered on Ray, the open-source distributed computing framework, Ray Summit brings together ML practitioners, data scientists, and more to discuss the growing demand for distributed computing.","sections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xIJZXdAqhOEizUCUuQaiV","type":"Entry","createdAt":"2022-03-31T16:04:49.746Z","updatedAt":"2022-09-12T20:58:36.991Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":20,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-hero","header":"The Place for _Everything Ray_","body":"Missed us in San Francisco for Ray Summit 2022? Don't fret, you can watch your favorite sessions on demand! ","ctaText":"View Sessions","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"9F9S7NLaIdPL4ThYwVlFW","type":"Asset","createdAt":"2022-09-06T14:49:38.936Z","updatedAt":"2022-09-06T14:53:02.350Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Presented by Anyscale","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/9F9S7NLaIdPL4ThYwVlFW/8ecd09b334942298f0b8c2d5567ec032/presented-by-anyscale.svg","details":{"size":22813,"image":{"width":289,"height":32}},"fileName":"presented-by-anyscale.svg","contentType":"image/svg+xml"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4A0d4TUfp0TCqUA60CXopR","type":"Asset","createdAt":"2022-09-06T14:53:48.720Z","updatedAt":"2022-09-06T14:53:48.720Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4A0d4TUfp0TCqUA60CXopR/2e4240c268d074d840f7a00d069a768a/rs-collage_2x.jpg","details":{"size":568547,"image":{"width":2400,"height":1376}},"fileName":"rs-collage@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6FGUP0cXunTbgs2UemeAU","type":"Asset","createdAt":"2022-09-06T14:54:09.975Z","updatedAt":"2022-09-06T14:54:09.975Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event, stacked vertically","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6FGUP0cXunTbgs2UemeAU/16ed175024bc9cc0a2fae3cb13538dff/rs-collage-mobile_2x.jpg","details":{"size":243282,"image":{"width":750,"height":1420}},"fileName":"rs-collage-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4RKa7qVhWoIaoNSa1ZH2SK"}}],"type":"2022 Summit Hero","styling":["transparent-header"]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6GL7ZzuSLadWdVFKHj7yIH","type":"Entry","createdAt":"2022-04-20T18:00:12.964Z","updatedAt":"2022-08-08T22:50:29.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-keynote-speakers","header":"Keynote Speakers","ctaText":"See all speakers","ctaLink":"/ray-summit-2022/speakers","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6VhewtcICU9SCXjwd5qhHJ"}}],"type":"2022 Summit Speakers","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JTi5fsyo0Q3YrF3SAQglg","type":"Entry","createdAt":"2022-08-08T22:49:55.163Z","updatedAt":"2022-08-09T07:03:38.615Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-ray-summit-selected-speakers","header":"Highlighted Talks","ctaText":"Full Agenda","ctaLink":"/ray-summit-2022/agenda","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"50eS4cKXBNE4GHezdZ8Rws"}},{"sys":{"type":"Link","linkType":"Entry","id":"5iioKfOgEqVzgQ1DMmA1Wy"}},{"sys":{"type":"Link","linkType":"Entry","id":"YCBUZOXJGB0qcJF8TKjfI"}}],"type":"2022 Summit Agenda Preview"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5cgHuoJ713DewhVY6iUYrG","type":"Entry","createdAt":"2022-05-03T21:57:45.147Z","updatedAt":"2022-05-09T13:41:03.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-trainings","header":"Exclusive Ray Training Sessions","body":"Ray Summit is a great opportunity to level up your Ray skills through hands-on training sessions with the builders and maintainers of Ray. Weâre offering several pre-conference training sessions on August 22. Space is limited â [reserve your spot now](https://www.myeventi.events/raysummit22/attendee/)!","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3sYQTUmD5X5ncvGRFyIRR6","type":"Asset","createdAt":"2022-05-03T04:40:25.040Z","updatedAt":"2022-05-03T04:40:25.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image desktop","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3sYQTUmD5X5ncvGRFyIRR6/a77c675b07d63e73dd79ada9d182498a/training-image_2x.jpg","details":{"size":313509,"image":{"width":1136,"height":1480}},"fileName":"training-image@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7rsC56a4YCgENLyxsHyeIb","type":"Asset","createdAt":"2022-05-03T04:40:54.089Z","updatedAt":"2022-05-03T04:40:54.089Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image mobile","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7rsC56a4YCgENLyxsHyeIb/e2592c9b4424048b3c916fbbbc313065/training-image-mobile_2x.jpg","details":{"size":153801,"image":{"width":830,"height":874}},"fileName":"training-image-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4Hb3J9SrCFTDXqIhIdSIwa"}},{"sys":{"type":"Link","linkType":"Entry","id":"hZ4Tl1wvj7CwF00slwpLg"}},{"sys":{"type":"Link","linkType":"Entry","id":"YJ5KWyjg4NGf29NrfHcJX"}}],"type":"2022 Summit Trainings"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mRJQtuDbIhAuEbwLT2V1K","type":"Entry","createdAt":"2022-03-31T21:42:56.117Z","updatedAt":"2022-04-11T23:46:07.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-reasons-to-attend","header":"Why Attend?","body":"As the annual user conference for the [open source Ray community](https://www.ray.io/community), Ray Summit is a unique opportunity to enhance your Ray skills, build your professional network, and learn what's on the Ray roadmap.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"5f6gxk4a4ujE2y6FZ8ATQ5"}},{"sys":{"type":"Link","linkType":"Entry","id":"20QwyiOmkHCEOkMvp0Jhtk"}},{"sys":{"type":"Link","linkType":"Entry","id":"6QxivsuhhLoVSloRQs9ZlQ"}}],"type":"2022 Summit Content Tiles"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"15k0SGqB2ZKOUfAsM3JnyV","type":"Entry","createdAt":"2022-03-31T16:05:55.739Z","updatedAt":"2022-08-05T00:25:20.225Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":16,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-about","header":"Celebrate the Ray community","body":"Get ready for our epic community bash at the Art of Tinkering exhibit at the San Francisco Exploratorium on August 23 from 5:30 to 9:30 p.m. Come explore, play, and celebrate with the Ray Community. This is an exclusive opportunity to experience this amazing interactive exhibition before it closes in September.","ctaText":"Check out the agenda","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UcZT4h1P7uFDz6cn6JH8l","type":"Asset","createdAt":"2022-06-02T20:37:22.299Z","updatedAt":"2022-06-02T20:37:22.299Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Man taking a photo on his phone","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2UcZT4h1P7uFDz6cn6JH8l/5040a9909f18ab015c27b220077f916e/taking-picture.jpg","details":{"size":107846,"image":{"width":980,"height":772}},"fileName":"taking-picture.jpg","contentType":"image/jpeg"}}}],"type":"2022 Summit Split Image","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4P8Jn0q7nBU05iD6DmcQCC","type":"Entry","createdAt":"2022-04-01T19:15:31.735Z","updatedAt":"2022-04-11T16:36:59.573Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-social-tiles","header":"Join the Conversation","body":"Ready to get involved in the Ray community before the conference? Ask a question in the forums. Open a pull request. Or share why youâre excited with the hashtag __#RaySummit__ on Twitter.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6OMTlXkJ1OHXNkEjyPrXTH"}},{"sys":{"type":"Link","linkType":"Entry","id":"7EE28ee1ATQtodoLloT6mg"}},{"sys":{"type":"Link","linkType":"Entry","id":"1CRAzFxndL37wiNg4x8WKV"}},{"sys":{"type":"Link","linkType":"Entry","id":"2qrec5rR8kabZxCHaPS7NH"}}],"type":"2022 Summit Social Tiles"}}],"headerLogo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70DIdxwT8XY96aw7vMUHOu","type":"Asset","createdAt":"2022-06-01T15:21:46.804Z","updatedAt":"2022-06-01T15:21:46.804Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo Dark Blue","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/70DIdxwT8XY96aw7vMUHOu/5ca0a8b7533c8df70bc0e032b889862a/rs-logo-nav.svg","details":{"size":9954,"image":{"width":108,"height":48}},"fileName":"rs-logo-nav.svg","contentType":"image/svg+xml"}}},"headerLogoWhite":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3qyLXKGgGfWhv4oPqYl0SE","type":"Asset","createdAt":"2022-04-20T21:50:58.774Z","updatedAt":"2022-04-20T21:50:58.774Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3qyLXKGgGfWhv4oPqYl0SE/c4dee9e7a344c39db11fefafbf255b5b/rs-logo-horizontal.svg","details":{"size":9975,"image":{"width":108,"height":48}},"fileName":"rs-logo-horizontal.svg","contentType":"image/svg+xml"}}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KUtRQbdqJo1RrWMrVyfEm","type":"Entry","createdAt":"2022-03-31T16:21:13.402Z","updatedAt":"2022-04-08T17:25:33.228Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Home","link":"/ray-summit-2022"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4MvQcRSkHLMqIJoO9O2cQe","type":"Entry","createdAt":"2022-03-31T16:17:27.336Z","updatedAt":"2022-09-12T16:59:51.221Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"On-Demand","link":"/ray-summit-2022/agenda","subLinks":[{"sys":{"type":"Link","linkType":"Entry","id":"3N9kRDz36tieTI4YGVWKBJ"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46B5diwEDDr3kB2XDoGfDP","type":"Entry","createdAt":"2022-05-11T17:13:42.174Z","updatedAt":"2022-05-11T17:13:42.174Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Speakers","link":"/ray-summit-2022/speakers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Dt5fy6A7rBpXdG7HeKMGd","type":"Entry","createdAt":"2022-03-31T16:17:43.592Z","updatedAt":"2022-03-31T16:17:43.592Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Sponsors","link":"/ray-summit-2022/sponsors"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SvRE9pnyq3BXUPGKRWx6W","type":"Entry","createdAt":"2022-06-03T19:29:31.009Z","updatedAt":"2022-06-03T19:29:31.009Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Who Attends","link":"/ray-summit-2022/who-attends"}}],"mainSponsors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"xMQIOU9n7gVS4mkX2Nlag","type":"Entry","createdAt":"2021-01-31T16:33:06.495Z","updatedAt":"2021-05-11T13:28:31.312Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Ray","url":"https://ray.io","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jLroqm98RCMLGASthyuPv","type":"Asset","createdAt":"2021-05-11T13:28:26.930Z","updatedAt":"2021-05-11T13:28:26.930Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/jLroqm98RCMLGASthyuPv/f76cc04827316de5d3f8b529f6a3f02c/Ray_BM.svg","details":{"size":4742,"image":{"width":93,"height":36}},"fileName":"Ray_BM.svg","contentType":"image/svg+xml"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11syIHhtX8JajSlXHSHE1O","type":"Entry","createdAt":"2021-01-31T16:31:24.509Z","updatedAt":"2021-05-11T13:28:00.309Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Anyscale","url":"https://www.anyscale.com","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UkDmTzmZHIOG7yxFP4sVL","type":"Asset","createdAt":"2021-05-11T13:27:55.219Z","updatedAt":"2021-05-11T13:27:55.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Anyscale","file":{"url":"//images.ctfassets.net/xjan103pcp94/5UkDmTzmZHIOG7yxFP4sVL/db10aebda9a9aad10903470781a6f2e6/ANSC_BM.svg","details":{"size":7431,"image":{"width":158,"height":36}},"fileName":"ANSC_BM.svg","contentType":"image/svg+xml"}}}}}],"footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cfwECWrOqJ5Fegr3YpReS","type":"Entry","createdAt":"2022-04-01T21:22:12.001Z","updatedAt":"2022-04-13T17:35:29.511Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-footer","body":"Â©Anyscale, Inc 2022\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/privacy-policy\"\u003ePrivacy Policy\u003c/a\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/code-of-conduct\"\u003eCode of Conduct\u003c/a\u003e"}},"recommendations":[]}},"ctaText":"Register now","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5nTlh9josVs5zpZqU3vTbn","type":"Asset","createdAt":"2022-05-03T18:47:25.125Z","updatedAt":"2022-05-03T18:47:25.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-ray-summit-2022","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5nTlh9josVs5zpZqU3vTbn/e66043f62ed4a4b53d7e182e50289ec3/blog-recommended-content-ray-summit-2022.png","details":{"size":17234,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-ray-summit-2022.png","contentType":"image/png"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2EQzVvFklHtrDGOZF9gmhJ","type":"Entry","createdAt":"2022-03-21T17:46:21.879Z","updatedAt":"2022-03-21T17:47:09.167Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"An informal introduction to RL","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2q2uFmxMeiYtjxODaW61uy","type":"Entry","createdAt":"2022-02-22T16:24:28.106Z","updatedAt":"2022-06-22T15:56:22.597Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"An informal introduction to reinforcement learning","slug":"an-informal-introduction-to-reinforcement-learning","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2AgYlFYAtLGaS7VgbwcoVa","type":"Entry","createdAt":"2022-02-15T19:00:59.347Z","updatedAt":"2022-02-15T19:00:59.347Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Misha Laskin","slug":"misha-laskin"}}],"publishedDate":"2022-02-22","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Reinforcement learning (RL) has played a critical role in the rapid pace of AI advances over the last decade. In this post, we'll cover what RL is and why it's important, both as a research subject and for a diverse set of practical applications.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"This series on reinforcement learning was guest-authored by ","nodeType":"text"},{"data":{"uri":"https://twitter.com/MishaLaskin"},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Misha Laskin","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" while he was at UC Berkeley. Misha's focus areas are unsupervised learning and reinforcement learning.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Reinforcement learning (RL) has played a critical role in the rapid pace of AI advances over the last decade. In this post, we will explain in simple terms what RL is and why it is important, not only as a research subject but also for a diverse set of practical applications.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What is reinforcement learning?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To understand what RL is, itâs easier to first understand what it is not. Most well-known machine learning algorithms make predictions but do not need to reason over a long time period or interact with the world. For example, given an image of an object, an image classifier will make a prediction about the identity of the object (e.g., an apple or an orange). This image classifier is passive. It doesnât interact with the world and it doesnât need to reason about how previous images influence its current prediction. Image classification is categorized as supervised learning but it is not RL.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlike passive predictive models like image classifiers, RL algorithms both interact with the world and need to reason over periods of time to achieve their goals. An RL algorithmâs goal is to maximize rewards over a given time period. Intuitively, it works similar to classical conditioning ideas from psychology. For example, a common way to train a dog to learn a new trick is by rewarding it with a treat when it does something right. The dog wants to maximize the number of treats it gets and will learn new skills to maximize its reward.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1KRtNT9VBGTuSUYZJuat8m","type":"Asset","createdAt":"2022-02-15T19:10:38.146Z","updatedAt":"2022-02-15T19:21:05.973Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"blog-intro-to-rl-1","description":"RL agents continually learn from experience, whereas supervised learning agents learn to make predictions based on a static dataset.","file":{"url":"//images.ctfassets.net/xjan103pcp94/1KRtNT9VBGTuSUYZJuat8m/83c09bb7810cb3ca4ccb0efb19c45723/blog-intro-to-rl-1.png","details":{"size":1494705,"image":{"width":3306,"height":1908}},"fileName":"blog-intro-to-rl-1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5l1F2FF0i2PCHFy9mG040i","type":"Entry","createdAt":"2022-02-17T22:42:00.588Z","updatedAt":"2022-02-17T23:17:30.524Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"RL Summit CTA (Misha Laskin RL blog series)","body":"\u003ca href=\"/production-rl-summit#agenda?utm_source=anyscale\u0026utm_medium=blog\u0026utm_content=rl-blog-series\u0026utm_campaign=rl_summit\"\u003e\u003cimg src=\"//images.ctfassets.net/xjan103pcp94/1hu1Zwg1RQ0ifg9Lfp98bA/7d913772436d45666a80e83f629b4815/RL_Summit__12_.png\" style=\"width: 100%\"\u003e\u003c/a\u003e"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Apple-picking robot","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"RL algorithms work in the same way as training dogs to learn new tricks. Instead of food treats, RL agents receive numerical rewards. For example, suppose we drop a robot into a maze and want it to maximize the number of apples it picks up. We can assign a +1 reward each time the robot picks up an apple. At the end of a fixed time period, we count all the apples the robot picked up to determine its total reward. RL algorithms are a mathematical framework for maximizing the total reward. An RL algorithm, given some set of assumptions about its environment, will  guarantee that the robot picks up the maximal possible number of apples in its environment.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"With this apple-picking robot example we can also illustrate a fundamental challenge with RL algorithms: the exploration problem. Suppose the robot is in a room with a moderate number of apples, but there exists another room down a long corridor with an entire apple tree. The best place for the robot to be is in the room with the apple tree, but it first has to traverse a long corridor with no rewards to get there. RL algorithms will discover the optimal solution on their own, but they can take a long time to train if exploration is hard.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Li6bNTzAGrhzQM6YbsJlE","type":"Asset","createdAt":"2022-02-15T19:14:41.480Z","updatedAt":"2022-02-15T19:17:58.196Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"blog-intro-to-rl-2","description":"The exploration problem in RL: Room A has few apples, but it's nearby. Room B has many more apples, but it's far away. How does the robot know that it's better to explore to get to Room B?","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Li6bNTzAGrhzQM6YbsJlE/2ca06c8f8e0f76e0f1e356860d38b211/blog-intro-to-rl-2.png","details":{"size":661464,"image":{"width":2903,"height":1096}},"fileName":"blog-intro-to-rl-2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A brief history of reinforcement learning","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"While parts of RL theory started to be developed as early as the mid 1950s, the ideas that have shaped modern RL werenât brought together until 1989 when Chris Watkins formalized Q-learning â the mathematical framework for training RL agents. But it wasnât until after the breakthrough AlexNet paper in 2012 that RL took off in the modern era. The ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"ImageNet moment","nodeType":"text"},{"data":{},"marks":[],"value":" marked the first time a deep neural network substantially outperformed prior approaches to image classification on complex real-world images.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Shortly after in 2013, Vlad Mnih and collaborators at DeepMind released the Deep Q Network (DQN), which was the first system to play Atari video games autonomously from pixel inputs. This groundbreaking achievement paved the way for modern RL called Deep RL because it utilized deep neural networks to estimate how likely particular actions are to lead to high rewards. The DQN algorithm was at the core of subsequent advances like AlphaGo and AlphaStar.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Practical applications of reinforcement learning","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"While most RL research today focuses on algorithms to develop reward maximizing agents in simulated settings, the RL framework is quite general and is already starting to be applied to solve a variety of practical problems.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Recommender systems:","nodeType":"text"},{"data":{},"marks":[],"value":" Recommender systems aim to recommend products or content to a user that will have higher likelihood to convert or engage. The number of possible products a system could recommend is quite large and efficient algorithms are needed to not only select products a user will like most, but also make sure these selections will increase the companyâs desired long-term metrics. Examples of RL being used to power recommender systems include ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/pdf/2008.09369.pdf"},"content":[{"data":{},"marks":[],"value":"product recommendation at Alibaba","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/pdf/1812.02353.pdf"},"content":[{"data":{},"marks":[],"value":"video recommendations at YouTube","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Over the last few years, new RL algorithms have been developed specifically tailored for recommender systems. One example is ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1905.12767"},"content":[{"data":{},"marks":[],"value":"SlateQ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which uses RL to present a slate of items to the user. SlateQ is designed to deal with the combinatorially large space of potential product recommendations to drive overall user satisfaction.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4CHdygpw6pWLrmvkKT5Z86","type":"Asset","createdAt":"2022-02-15T19:30:06.069Z","updatedAt":"2022-02-15T19:30:06.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-intro-to-rl-5","description":"SlateQ increases user engagement over time. Source: https://arxiv.org/abs/1905.12767","file":{"url":"//images.ctfassets.net/xjan103pcp94/4CHdygpw6pWLrmvkKT5Z86/873aaff5f08eedbb4ad3cdef46fe43c7/blog-intro-to-rl-5.png","details":{"size":362621,"image":{"width":1200,"height":696}},"fileName":"blog-intro-to-rl-5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"One important aspect to keep in mind when deploying RL algorithms is to make sure that the reward function is set up to maximize the companyâs long-term metrics, such as satisfaction with the companyâs product. For example, in the mid 2010s YouTube learned that while their recommendations were increasing user engagement, they were not increasing long-term user satisfaction. This was due to implicit bias, when it is unclear if a user is clicking on a video because it is their intent or simply because the recommendation was present on the screen. Since then Google released ","nodeType":"text"},{"data":{"uri":"https://dl.acm.org/doi/10.1145/3298689.3346997"},"content":[{"data":{},"marks":[],"value":"a new RL algorithm","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" that addresses implicit bias.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Autonomous vehicle navigation:","nodeType":"text"},{"data":{},"marks":[],"value":" RL is being used today to plan paths and control autonomous vehicles (AVs) at Tesla. One example of RL usage for AVs is to plan a path to autonomously park a car. The number of possible paths a car could explore is exponential, which results in slow and inefficient path planning with traditional methods. At AI day, ","nodeType":"text"},{"data":{"uri":"https://www.youtube.com/watch?v=j0z4FweCy4M"},"content":[{"data":{},"marks":[],"value":"Tesla showcased","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" an algorithm based on AlphaGo to find faster and more efficient plans.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Cooling data centers","nodeType":"text"},{"data":{},"marks":[],"value":": Another useful application of RL has been ","nodeType":"text"},{"data":{"uri":"https://deepmind.com/blog/article/deepmind-ai-reduces-google-data-centre-cooling-bill-40"},"content":[{"data":{},"marks":[],"value":"cooling data centers at Google","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Data centers consume a lot of energy and the computers they house dissipate it as heat. Maintaining an operational data center requires sophisticated cooling systems, which are expensive to run. With RL, Google was able to deploy cooling on-demand to eliminate wasteful periods where cooling was not needed.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this post, we covered an informal introduction to what is RL and why itâs useful. These are just three of many practical applications of RL. Itâs exciting that one algorithmic framework is general enough to apply to such a broad range of problems.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next up, we'll explore ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-reinforcement-learning-framework"},"content":[{"data":{},"marks":[],"value":"how the mathematical framework for RL actually works","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Or, check out our other resources below for more on RL:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/production-rl-summit#agenda?utm_source=anyscale\u0026utm_medium=blog\u0026utm_content=rl-blog-series\u0026utm_campaign=rl_summit"},"content":[{"data":{},"marks":[],"value":"Register for the upcoming Production RL Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a free virtual event that brings together ML engineers, data scientists, and researchers pioneering the use of RL to solve real-world business problems","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/index.html"},"content":[{"data":{},"marks":[],"value":"Learn more about RLlib: industry-grade reinforcement learning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google"},"content":[{"data":{},"marks":[],"value":"Check out our introduction to reinforcement learning with OpenAI Gym, RLlib, and Google Colab","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021"},"content":[{"data":{},"marks":[],"value":"Get an overview of some of the best reinforcement learning talks presented at Ray Summit 2021","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ZV5flSIzNMWJXtg09FEZM","type":"Asset","createdAt":"2022-02-18T15:14:44.998Z","updatedAt":"2022-05-24T19:56:36.761Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-intro-to-rl-thumb","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/ZV5flSIzNMWJXtg09FEZM/19745e2831a018f600a4e3eba68389e1/1371476_Blog_Image-Illustration_-4_052022.jpg","details":{"size":817851,"image":{"width":1500,"height":1000}},"fileName":"1371476_Blog Image-Illustration -4_052022.jpg","contentType":"image/jpeg"}}},"mainImageFit":"cover","recommendations":[],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}}],"hideIntro":true}},"ctaText":"Read blog series","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3tbbqYD9I2NRmPfTlTH1Io","type":"Asset","createdAt":"2022-03-21T17:46:16.833Z","updatedAt":"2022-03-23T18:51:22.476Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"locale":"en-US"},"fields":{"title":"blog-recommended-content-rl-robot-gear-light","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3tbbqYD9I2NRmPfTlTH1Io/d4d8b09181c1d59d7f64c6ee3d009ba2/blog-recommended-content-rl-robot-gear-light.jpg","details":{"size":50047,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-rl-robot-gear-light.jpg","contentType":"image/jpeg"}}},"recommendations":null}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}}],"hideIntro":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5cfb4aW2tabel64uQU75DN","type":"Entry","createdAt":"2021-08-24T15:56:03.332Z","updatedAt":"2022-06-22T16:57:43.260Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Fast AutoML with FLAML + Ray Tune","slug":"fast-automl-with-flaml-ray-tune","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6c89m7Vgb7ydlCiRjXj45C","type":"Entry","createdAt":"2021-08-23T22:25:49.634Z","updatedAt":"2021-08-23T22:25:49.634Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Qingyun Wu","slug":"qingyun-wu","link":"https://www.linkedin.com/in/qingyun-wu-183019a6/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"286oPte4Z9yDAJO6urQOxc","type":"Entry","createdAt":"2021-08-23T22:26:59.650Z","updatedAt":"2021-08-23T22:26:59.650Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Chi Wang","slug":"chi-wang","link":"https://www.linkedin.com/in/chi-wang-49b15b16/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BGRfsYFKUgIpGjaXTUYml","type":"Entry","createdAt":"2021-08-09T15:38:43.390Z","updatedAt":"2021-08-09T15:38:43.390Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Antoni Baum","slug":"antoni-baum","link":"https://www.linkedin.com/in/yard1/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-08-24","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"FLAML is a lightweight Python library from Microsoft Research that finds accurate machine learning models in an efficient and economical way using cutting edge algorithms designed to be resource-efficient and easily parallelizable. FLAML can also utilize Ray Tune for distributed hyperparameter tuning to scale up these AutoML methods across a cluster. ","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/microsoft/FLAML"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FLAML","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a lightweight Python library from Microsoft Research that finds accurate machine learning models in an efficient and economical way using ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/2005.01571"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"cutting edge","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" algorithms designed to be resource-efficient and easily parallelizable. FLAML can also utilize Ray Tune for ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-distribute-hyperparameter-tuning-using-ray-tune"},"content":[{"data":{},"marks":[],"value":"distributed hyperparameter tuning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to scale up these AutoML methods across a cluster.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This blog highlights:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The need for economical AutoML methods","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Economical AutoML with FLAML","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"How to scale FLAMLâs optimization algorithms with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"79NYcdlVAjEpuwgPA08UQ9","type":"Asset","createdAt":"2021-08-23T20:49:00.153Z","updatedAt":"2021-08-23T20:49:00.153Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"FLAML + Ray","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/79NYcdlVAjEpuwgPA08UQ9/d99006050d666660e2fce764957da941/FLAMLRay.png","details":{"size":33036,"image":{"width":512,"height":220}},"fileName":"FLAMLRay.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The need of economical AutoML methods","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"AutoML is known to be a resource and time consuming operation as it involves trials and errors to find a hyperparameter configuration with good performance. Since the space of possible configuration values is often very large, there is a need for an economical AutoML method that can more effectively search them.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The high resource and time consumption of hyperparameter search in AutoML boils down to the following two factors:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"large number of candidate hyperparameter configurations (trial) needed to find a configuration with good performance","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"high âevaluationâ cost of each hyperparameter as the evaluation involves training and validating a machine learning model with the given training data.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"To address both of these factors, Microsoft Researchers have developed ","nodeType":"text"},{"data":{"uri":"https://github.com/microsoft/FLAML"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FLAML","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (Fast Lightweight AutoML).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"WHAT IS FLAML?","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"FLAML is a newly released library containing state-of-the-art hyperparameter optimization algorithms. FLAML leverages the structure of the search space to optimize for both cost and model performance simultaneously. It contains two new methods developed by Microsoft Research:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Cost-Frugal Optimization (CFO)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"BlendSearch","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Cost-Frugal Optimization (CFO) is a method that conducts its search process in a cost-aware fashion. The search method starts from a low-cost initial point and gradually moves towards a higher cost region while optimizing the given objective (like model loss or accuracy).Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Blendsearch is an extension of CFO that combines the frugality of CFO and the exploration ability of Bayesian optimization. Like CFO, BlendSearch requires a low-cost initial point as input if such point exists, and starts the search from there. However, unlike CFO, BlendSearch will not wait for the local search to fully converge before trying new start points.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The economical HPO methods in FLAML are inspired by two key insights:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Many machine learning algorithms have hyperparameters that can cause a large variation in the training cost. For example, an XGBoost model with 10 trees will train much quicker than a model with 1000 trees.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The âcostâ for parameters is often âcontinuous and consistentâ -- evaluating trees=10 is cheaper than evaluating trees=100, which itself is cheaper than evaluating trees=500.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Together, these insights provide useful","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":" structural information","nodeType":"text"},{"data":{},"marks":[],"value":" about the hyperparameters in the cost space. The methods, i.e., CFO and BlendSearch, are able to effectively leverage these insights to reduce the cost incurred along the way without affecting the convergence to the optimal solution.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"DOES FLAML WORK?","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the latest ","nodeType":"text"},{"data":{"uri":"https://openml.github.io/automlbenchmark/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"AutoML benchmark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/pdf/1911.04706.pdf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FLAML","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is able to achieve the same or better performance as the state-of-the-art AutoML solutions using only 10% of the computation resource on over 62% of the tasks.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"FLAMLâs performance is attributed to its economical optimization methods. The new HPO methods (CFO, BlendSearch) leverage the structure of the search space to choose search orders optimized for both good performance and low cost. This can make a big difference in search efficiency under budget constraints.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Figure 1 shows a typical result obtained from FLAML and a state-of-the-art hyperparameter tuning library ","nodeType":"text"},{"data":{"uri":"https://github.com/optuna/optuna"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Optuna","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for tuning LightGBM with 9-dimensional hyperparameters. You can see that FLAML is able to achieve a better solution in a much shorter amount of time.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"76PbQqxJBdwCGurVPyOb1v","type":"Asset","createdAt":"2021-08-23T20:51:36.856Z","updatedAt":"2021-08-24T08:00:12.840Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"FLAMLOptuna","description":"Figure 1. Validation loss (1-auc) curve for tuning LightGBM on a [classification dataset](https://www.openml.org/d/23517). The lines and shaded area show the mean and standard deviation of validation loss over 10 runs. Results in this figure are obtained from experiments with 1 cpu (without parallelization).","file":{"url":"//images.ctfassets.net/xjan103pcp94/76PbQqxJBdwCGurVPyOb1v/9178a458df0f320631d2bc1f4e124c1c/FLAMLOptuna.png","details":{"size":39101,"image":{"width":512,"height":448}},"fileName":"FLAMLOptuna.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The following code example shows how to get started with FLAML with just several lines of code (assuming the training dataset is provided and saved as ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"X_train","nodeType":"text"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"y_train","nodeType":"text"},{"data":{},"marks":[],"value":"). The task is to tune hyperparameters of the LightGBM model with a time budget of 60 seconds. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5cW8LVxxuIuXHFuyMaD1gx","type":"Entry","createdAt":"2021-08-23T20:52:30.140Z","updatedAt":"2021-08-23T20:52:30.140Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"from flaml import AutoML","body":"from flaml import AutoML\nautoml = AutoML()\nautoml.fit(X_train=X_train, y_train=y_train, time_budget=60, estimator_list=['lgbm'])\n\n''' retrieve best model and best configuration found'''\nprint('Best ML model:', automl.model)\nprint('Best hyperparameter config:', automl.best_config)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this example, we search over a default search space for LightGBM, which is already provided in FLAML. FLAML provides rich customization options about one's concerned task, such as the learner class, the search space, the evaluation metric, etc. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A walkthrough example","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now we use a toy example to demonstrate cost-frugal behavior of CFO in tuning XGBoost with two hyperparameters: # of trees and # of leaves.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7jzx4HlVCtxvJDJGc2FjRY","type":"Entry","createdAt":"2021-08-23T20:53:56.229Z","updatedAt":"2021-08-23T20:53:56.229Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"from flaml.model import XGBoostSklearnEstimator","body":"'''create an XGBoost learner class with a customized search space'''\nfrom flaml.model import XGBoostSklearnEstimator\nfrom flaml import tune\n\nclass MyXGB(XGBoostSklearnEstimator):\nââ    '''XGBoostSklearnEstimator with a customized search space'''\n    @classmethod\n    def search_space(cls, data_size, **params):\n        upper = min(2**15, int(data_size))\n        return {\n            'n_estimators': {\n                'domain': tune.lograndint(lower=4, upper=upper),\n                'low_cost_init_value': 4,\n            },\n            'max_leaves': {\n                'domain': tune.lograndint(lower=4, upper=upper),\n                'low_cost_init_value': 4,\n            },\n        }\n\n'''Use CFO in FLAML to tune XGBoost'''\nfrom flaml import AutoML\nautoml = AutoML()\nautoml.add_learner(learner_name='my_xgboost', learner_class=MyXGB)\nautoml.fit(X_train=X_train, y_train=y_train, time_budget=15, estimator_list=['my_xgboost'], hpo_method='cfo')","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"How CFO and BlendSearch WorkÂ ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The two GIFs below demonstrate the search trajectory of CFO in the loss and evaluation cost (i.e., the evaluation time ) space respectively. CFO begins with a low-cost initial point (specified through ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"low_cost_init_value","nodeType":"text"},{"data":{},"marks":[],"value":" in the search space) and performs local updates following its randomized local search strategy. With such a strategy, CFO can quickly move toward the low-loss region, showing a good convergence property. Additionally, CFO tends to avoid exploring the high-cost region until necessary. This search strategy is further grounded with a ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/2005.01571"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"provable convergence rate and bounded cost in expectation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4RdqfCCVX22q4hqaLyn7I8","type":"Asset","createdAt":"2021-08-24T07:42:36.290Z","updatedAt":"2021-08-24T07:42:36.290Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"FLAML Blog GIF","description":"Figure 2. CFO in tuning the # of leaves and the # of trees for XGBoost. The two heatmaps show the loss and cost distribution of all configurations. The black dots are the points evaluated in CFO. Black dots connected by lines are points that yield better loss performance when evaluated.","file":{"url":"//images.ctfassets.net/xjan103pcp94/4RdqfCCVX22q4hqaLyn7I8/8e964d4beefc51b36fd23fa75dbc9dfc/FLAML-blog.gif","details":{"size":713901,"image":{"width":1500,"height":600}},"fileName":"FLAML-blog.gif","contentType":"image/gif"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"BlendSearch further combines this local search strategy used in CFO with global search. It leverages the frugality of CFO and the space exploration capability of global search methods such as Bayesian optimization. Specifically, BlendSearch maintains one global search model, and gradually creates local search threads over time based on the hyperparameter configurations proposed by the global model. It further prioritizes the global search thread and multiple local search threads depending on their real-time performance and cost. It can further improve the efficiency of CFO in tasks with complicated search space, e.g., a search space that contains multiple disjoint, non-continuous subspaces.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"FLAML vs Bayesian Optimization Performance","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Figure 3 shows typical behaviors of the economical HPO methods in FLAML (CFO is labeled `LSâ in this figure) versus a Bayesian Optimization (BO) method for tuning XGBoost with 11 hyperparameters.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"From Figure 3(a), we observe that the evaluation time for the proposed configurations in BO can be very long. When the total resource is limited, e.g., 1 cpu-hour (or less), BO is not able to give a satisfying result (Figure 3(b)).Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"FLAMLâs CFO (labeled LS) and BlendSearch have clear advantages in finding good configurations quickly: they are able to concentrate on configurations that have low evaluation time, while navigating ones with good performance, i.e., low loss.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1WPEkjLf4QHEFe6ECYnkfc","type":"Asset","createdAt":"2021-08-24T07:50:42.764Z","updatedAt":"2021-08-24T18:02:16.708Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"FLAML Loss vs Evaluation Time","description":"Figure 3. (a) is a scatter plot of the hyperparameter configurations proposed by different methods, with x-axis and y-axis being the evaluation time and loss. The evaluation time of a hyperparameter configuration  is the time taken for training a machine learning model with the hyperparameter configuration on the training data and validating its performance on a validation dataset. The loss is the validation loss.  (b) shows the best loss obtained by different methods over wall-clock time. ([image source](https://openreview.net/pdf?id=VbLH04pRA3))","file":{"url":"//images.ctfassets.net/xjan103pcp94/1WPEkjLf4QHEFe6ECYnkfc/7420e5ca286dd2b31a06e711f8162488/lossvsEvaluationTime.png","details":{"size":313080,"image":{"width":2351,"height":687}},"fileName":"lossvsEvaluationTime.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"How to scale up CFO and BlendSearch with Ray Tuneâs distributed tuning","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To speed up hyperparameter optimization, you may want to parallelize","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" ","nodeType":"text"},{"data":{},"marks":[],"value":"your hyperparameter search. For example, BlendSearch is able to work well in a parallel setting: It leverages multiple search threads that can be independently executed without obvious degradation of performance. This desirable property is not always true for existing optimization algorithms such as Bayesian Optimization.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To achieve parallelization, FLAML is integrated with Ray Tune. Ray Tune is a Python library that accelerates hyperparameter tuning by allowing you to leverage cutting edge optimization algorithms at scale. Ray Tune also allows you to scale out hyperparameter search from your laptop to a cluster without changing your code. You can either use Ray Tune in FLAML or run the hyperparameter search methods from FLAML in Ray Tune to parallelize your search. The following code example shows the former usage, which is achieved by simply configuring the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"n_concurrent_trials","nodeType":"text"},{"data":{},"marks":[],"value":" argument in FLAML.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"55bh6GlJSMSBm5sj952T59","type":"Entry","createdAt":"2021-08-23T22:10:40.860Z","updatedAt":"2021-08-23T22:10:40.860Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Use BlendSearch for hyperparameter search","body":"'''Use BlendSearch for hyperparameter search, and Ray Tune for parallelizing concurrent trials (when n_concurrent_trials \u003e 1) in FLAML to tune XGBoost'''\nfrom flaml import AutoML\nautoml = AutoML()\nautoml.add_learner(learner_name='my_xgboost', learner_class=MyXGB)\nautoml.fit(X_train=X_train, y_train=y_train, time_budget=15, estimator_list=['my_xgboost'], hpo_method='bs', n_concurrent_trials=8)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"d6T9VtjnL5U53ZrGAp03n","type":"Asset","createdAt":"2021-08-23T22:12:00.204Z","updatedAt":"2021-08-23T22:12:00.204Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"train_xgboost FLAML","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/d6T9VtjnL5U53ZrGAp03n/efb193da72f70f5faec916ce0c06be81/xgboostFLAML.png","details":{"size":21543,"image":{"width":512,"height":235}},"fileName":"xgboostFLAML.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The code below shows the latter usage, an end-to-end example of how to use BlendSearch with Ray Tune.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DHvIpW25vRKdfKZr8xDnL","type":"Entry","createdAt":"2021-08-23T22:12:58.056Z","updatedAt":"2021-08-23T22:12:58.056Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"from flaml import CFO, BlendSearch","body":"from ray import tune \nfrom flaml import CFO, BlendSearch\nimport time\n\ndef training_func(config):\n    '''evaluate a hyperparameter configuration'''\n    # we use a toy example with 2 hyperparameters\n    metric = (round(config['x'])-85000)**2 - config['x']/config['y']\n\n    # usually the evaluation takes a non-neglible cost\n    # and the cost could be related to certain hyperparameters\n    # in this example, we assume it's proportional to x\n    time.sleep(config['x']/100000)\n    # use tune.report to report the metric to optimize    \n    tune.report(metric=metric) \n\n# provide the search space\nsearch_space = {\n        'x': tune.lograndint(lower=1, upper=100000),\n        'y': tune.randint(lower=1, upper=100000)\n    }\n\n# provide the low cost partial config\nlow_cost_partial_config={'x':1}\n\n# set up BlendSearch\nblendsearch = BlendSearch(\n    metric=\"metric\", mode=\"min\",\n    space=search_space,\n    low_cost_partial_config=low_cost_partial_config)\n\nblendsearch.set_search_properties(config={\"time_budget_s\": 60})\n\nanalysis = tune.run(\n    training_func,    # the function to evaluate a config\n    config=search_space,\n    metric='metric',    # the name of the metric used for optimization\n    mode='min',         # the optimization mode, 'min' or 'max'\n    num_samples=-1,    # the maximal number of configs to try, -1 means infinite\n    time_budget_s=60,   # the time budget in seconds\n    local_dir='logs/',  # the local directory to store logs\n    search_alg=blendsearch  # or cfo\n    )\n\nprint(analysis.best_trial.last_result)  # the best trial's result\nprint(analysis.best_config)  # the best config","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Other key Ray Tune features include:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Automatic integration with experiment tracking tools like Tensorboard and Weights/Biases","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Support for GPUs","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Early stopping","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"A scikit-learn API to easily integrate with ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/distributed-xgboost-training-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"XGBoost","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/introducing-distributed-lightgbm-training-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LightGBM","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/tune-sklearn"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Scikit-Learn","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", etc.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Benchmark results","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We have conducted an experiment to check how well BlendSearch stacks up to Optuna (with multivariate TPE sampler) and random search in a highly parallelized setting. We have used a subset of 12 datasets from the ","nodeType":"text"},{"data":{"uri":"https://www.openml.org/s/218"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"AutoML Benchmark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Each optimization run was conducted with 16 trials in parallel for 20 minutes, using 3-fold cross-validation, using ROC-AUC (weighted one-vs-rest for multiclass datasets). The runs were repeated three times with different random seeds. Reproduction code can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/Yard1/Blendsearch-on-Ray-benchmark"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"14l8atAVkDi2YVH95lGeLZ","type":"Asset","createdAt":"2021-08-23T22:14:43.226Z","updatedAt":"2021-08-23T22:14:43.226Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Number of Wins BlendSearch Optuna","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/14l8atAVkDi2YVH95lGeLZ/a9a7e1922935beecdfd98f934545ae2d/numberofWins.png","details":{"size":10987,"image":{"width":512,"height":317}},"fileName":"numberofWins.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"BlendSearch achieved the best cross-validation score in 6 out of 12 datasets. Furthermore, BlendSearch had an average of 2.52% improvement over random search, compared to Optunaâs 1.96%. It is worth noting that BlendSearch was using univariate Optuna-TPE as its global searcher - using multivariate TPE would most likely improve the scores further.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"491TmHU8fKrthaT1y8ATNi","type":"Asset","createdAt":"2021-08-23T22:17:44.147Z","updatedAt":"2021-08-23T22:17:44.147Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"FLAML CV Results","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/491TmHU8fKrthaT1y8ATNi/ea8541cb123ef9e3fe71c8879325eb15/CVResults3Seeds.png","details":{"size":393384,"image":{"width":2117,"height":1306}},"fileName":"CVResults3Seeds.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In addition, thanks to its cost-frugal approach, BlendSearch evaluated, on average, twice the number of trials than the other searchers in the same time limit. This shows that the gap between BlendSearch and the other algorithms will increase with bigger time budgets.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ConclusionÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"FLAML is a newly released library containing ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/2005.01571"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"state-of-the-art","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" hyperparameter optimization algorithms that leverages the structure of the search space to optimize for both cost and model performance simultaneously. FLAML can also utilize ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for distributed hyperparameter tuning to scale up these economical AutoML methods across a cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For more information on FLAML, please see the ","nodeType":"text"},{"data":{"uri":"https://github.com/microsoft/FLAML"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GitHub repository","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the ","nodeType":"text"},{"data":{"uri":"http://aka.ms/flaml"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"project page","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". If you would like to keep up to date with all things Ray, consider ","nodeType":"text"},{"data":{"uri":"https://twitter.com/raydistributed"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"following @raydistributed on twitter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://anyscale.us5.list-manage.com/subscribe?u=524b25758d03ad7ec4f64105f\u0026id=d94e960a03"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"sign up for the newsletter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3MGjtAfNgVLpsUoRRe6StY","type":"Asset","createdAt":"2021-08-24T07:58:38.559Z","updatedAt":"2021-08-24T07:58:38.559Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"FLAML XGBOOST Ray Tune","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3MGjtAfNgVLpsUoRRe6StY/4a621beddf288b4b8c081f92c9877bbf/xgboostFLAML.png","details":{"size":21543,"image":{"width":512,"height":235}},"fileName":"xgboostFLAML.png","contentType":"image/png"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"667O8uBOBT8lFmThm48TTv","type":"Entry","createdAt":"2022-03-29T00:40:28.108Z","updatedAt":"2022-03-29T00:40:28.108Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"What is hyperparameter tuning?","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6tA3BkveY3Qsz4kUDVCFUo","type":"Entry","createdAt":"2022-02-08T15:57:57.427Z","updatedAt":"2022-06-22T16:00:08.556Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"What is hyperparameter tuning?","slug":"what-is-hyperparameter-tuning","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Njah6Xtnm7bwiyiwTrQ4V","type":"Entry","createdAt":"2022-02-04T18:35:38.461Z","updatedAt":"2022-02-04T18:35:38.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Juan Navas","slug":"juan-navas"}}],"publishedDate":"2022-02-08","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Hyperparameter tuning is an essential part of controlling the behavior of a machine learning model. In this article, weâll explore some examples of hyperparameters and delve into a few models for tuning hyperparameters.\n","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"ð¡ ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"This blog post is part 1 in our series on hyperparameter tuning. If you're looking for a hands-on look at different tuning methods, be sure to check out part 2, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-tune-hyperparameters-on-xgboost"},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"How to tune hyperparameters on XGBoost","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", and part 3, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-distribute-hyperparameter-tuning-using-ray-tune"},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"How to distribute hyperparameter tuning using Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hyperparameter tuning is an essential part of controlling the behavior of a machine learning model. If we donât correctly tune our hyperparameters, our estimated model parameters produce suboptimal results, as they donât minimize the loss function. This means our model makes more errors. In practice, key indicators like the accuracy or the confusion matrix will be worse.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this article, weâll explore some examples of hyperparameters and delve into a few models for tuning hyperparameters. Then, in the following two articles of this series, weâll demonstrate ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-tune-hyperparameters-on-xgboost"},"content":[{"data":{},"marks":[],"value":"how to tune hyperparameters on XGBoost","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-distribute-hyperparameter-tuning-using-ray-tune"},"content":[{"data":{},"marks":[],"value":"how to perform distributed hyperparameter tuning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What are hyperparameters?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In machine learning, we need to differentiate between parameters and hyperparameters. A learning algorithm learns or estimates model parameters for the given data set, then continues updating these values as it continues to learn. After learning is complete, these parameters become part of the model. For example, each weight and bias in a neural network is a parameter.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hyperparameters, on the other hand, are specific to the algorithm itself, so we canât calculate their values from the data. We use hyperparameters to calculate the model parameters. Different hyperparameter values produce different model parameter values for a given data set.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hyperparameter tuning consists of finding a set of optimal hyperparameter values for a learning algorithm while applying this optimized algorithm to any data set. That combination of hyperparameters maximizes the modelâs performance, minimizing a predefined loss function to produce better results with fewer errors. Note that the learning algorithm optimizes the loss based on the input data and tries to find an optimal solution within the given setting. However, hyperparameters describe this setting exactly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For instance, if we work on natural language processing (NLP) models, we probably use neural networks, support-vector machines (SVMs), Bayesian networks, and Extreme Gradient Boosting (XGB) for tuning parameters.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Weâll discuss how to perform hyperparameter tuning in detail later.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hyperparameter types","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Some important hyperparameters that require tuning in neural networks are:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Number of hidden layers","nodeType":"text"},{"data":{},"marks":[],"value":": Itâs a trade-off between keeping our neural network as simple as possible (fast and generalized) and classifying our input data correctly. We can start with values of four to six and check our dataâs prediction accuracy when we increase or decrease this hyperparameter.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Number of nodes/neurons per layer","nodeType":"text"},{"data":{},"marks":[],"value":": More isn't always better when determining how many neurons to use per layer. Increasing neuron count can help, up to a point. But layers that are too wide may memorize the training dataset, causing the network to be less accurate on new data.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Learning rate","nodeType":"text"},{"data":{},"marks":[],"value":": Model parameters are adjusted iteratively â and the learning rate controls the size of the adjustment at each step. The lower the learning rate, the lower the changes to parameter estimates are. This means that it takes a longer time (and more data) to fit the model â but it also means that it is more likely that we actually find the minimum loss.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Momentum","nodeType":"text"},{"data":{},"marks":[],"value":": Momentum helps us avoid falling into local minima by resisting rapid changes to parameter values. It encourages parameters to keep changing in the direction they were ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"already","nodeType":"text"},{"data":{},"marks":[],"value":" changing, which helps prevent zig-zagging on every iteration. Aim to start with low momentum values and adjust upward as needed.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"We consider these essential hyperparameters for tuning SVMs:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"C","nodeType":"text"},{"data":{},"marks":[],"value":": A trade-off between a smooth decision boundary (more generic) and a neat decision boundary (more accurate for the training data). A low value may cause the model to incorrectly classify some training data, while a high value may cause the model to incur overfitting. Overfitting creates an analysis too specific for the current data set and possibly unfit for future data and unreliable for future observations.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Gamma","nodeType":"text"},{"data":{},"marks":[],"value":": The inverse of the influence radius of data samples we selected as support vectors. High values indicate the small radius of influence and small decision boundaries that do not consider relatively close data samples. These high values cause overfitting. Low values indicate the significant effect of distant data samples, so the model canât capture the correct decision boundaries from the data set.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Important hyperparameters that need tuning for XGBoost are:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"},{"type":"code"}],"value":"max_depth","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"},{"type":"code"}],"value":"min_child_weight","nodeType":"text"},{"data":{},"marks":[],"value":": This controls the tree architecture. ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"max_depth","nodeType":"text"},{"data":{},"marks":[],"value":" defines the maximum number of nodes from the root to the farthest leaf (the default number is 6). ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"min_child_weight ","nodeType":"text"},{"data":{},"marks":[],"value":"is the minimum weight required to create a new node in the tree.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"},{"type":"code"}],"value":"learning_rate","nodeType":"text"},{"data":{},"marks":[],"value":": This determines the amount of correction at each step, given that each boosting round corrects the previous roundâs errors. learning_rate takes values from 0 to 1, and the default value is 0.3.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"},{"type":"code"}],"value":"n_estimators","nodeType":"text"},{"data":{},"marks":[],"value":": This defines the number of trees in the ensemble. The default value is 100. Note that if we were using vanilla XGBoost instead of scikit-learn, we'd use ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"num_boost_rounds","nodeType":"text"},{"data":{},"marks":[],"value":" instead of ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"n_estimators","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"},{"type":"code"}],"value":"colsample_bytree","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"},{"type":"code"}],"value":"subsample","nodeType":"text"},{"data":{},"marks":[],"value":": This controls the data set samples that each round uses. These hyperparameters are helpful to avoid overfitting. ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"subsample","nodeType":"text"},{"data":{},"marks":[],"value":" is the fraction of samples used, with a value from 0 to 1 and a default value of 1. ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"colsample_bytree","nodeType":"text"},{"data":{},"marks":[],"value":" defines the fraction of columns (features) and takes numbers from 0 to 1, with a default value of 1.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In these examples, hyperparameters tending toward one extreme or the other can negatively affect our modelâs ability to make predictions. The trick is to find just the right value for each hyperparameter, so our model performs well and produces the best possible results.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Methods for tuning hyperparameters","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now that we understand what hyperparameters are and the importance of tuning them, we need to know how to choose their optimal values. We can find these optimal hyperparameter values using manual or automated methods.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"When tuning hyperparameters manually, we typically start using the default recommended values or rules of thumb, then search through a range of values using trial-and-error. But manual tuning is a tedious and time-consuming approach. It isnât practical when there are many hyperparameters with a wide range.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Automated hyperparameter tuning methods use an algorithm to search for the optimal values. Some of todayâs most popular automated methods are grid search, random search, and Bayesian optimization. Letâs explore these methods in detail.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Grid search","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Grid search is a sort of âbrute forceâ hyperparameter tuning method. We create a grid of possible discrete hyperparameter values then fit the model with every possible combination. We record the model performance for each set then select the combination that has produced the best performance.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7uzzMdsywkjZ5qk8qDZpey","type":"Asset","createdAt":"2022-02-03T16:34:27.369Z","updatedAt":"2022-02-03T16:34:27.369Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"hyperparameter-tuning-grid-search","description":"Grid search is a hyperparameter tuning method in which we create a grid of possible discrete hyperparameter values, then fit the model with every possible combination.","file":{"url":"//images.ctfassets.net/xjan103pcp94/7uzzMdsywkjZ5qk8qDZpey/904f5f2fcb7dba5232c488b504d4cd68/hyperparameter-tuning-grid-search.png","details":{"size":43838,"image":{"width":544,"height":418}},"fileName":"hyperparameter-tuning-grid-search.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Grid search is an exhaustive algorithm that can find the best combination of hyperparameters. However, the drawback is that itâs slow. Fitting the model with every possible combination usually requires a high computation capacity and significant time, which may not be available.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Random search","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The random search method (as its name implies) chooses values randomly rather than using a predefined set of values like the grid search method.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Random search tries a random combination of hyperparameters in each iteration and records the model performance. After several iterations, it returns the mix that produced the best result.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5DTA6ixwjbGYE2veJVaFcL","type":"Asset","createdAt":"2022-02-03T16:35:09.269Z","updatedAt":"2022-02-03T16:35:09.269Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"hyperparameter-tuning-random-search","description":"Random search tries a random combination of hyperparameters in each iteration and records the model performance. After several iterations, it returns the mix that produced the best result.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5DTA6ixwjbGYE2veJVaFcL/0af9813a2bf88151a03ce7bbf4dc292a/hyperparameter-tuning-random-search.png","details":{"size":36556,"image":{"width":544,"height":418}},"fileName":"hyperparameter-tuning-random-search.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Random search is appropriate when we have several hyperparameters with relatively large search domains. We can make discrete ranges (for instance, [5-100] in steps of 5) and still get a reasonably good set of combinations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The benefit is that random search typically requires less time than grid search to return a comparable result. It also ensures we don't end up with a model that's biased toward value sets arbitrarily chosen by users. Its drawback is that the result may not be the best possible hyperparameter combination.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Bayesian optimization","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Grid search and random search are relatively inefficient because they often evaluate many unsuitable hyperparameter combinations. They donât take into account the previous iterationsâ results when choosing the next hyperparameters.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The Bayesian optimization method takes a different approach. This method treats the search for the optimal hyperparameters as an optimization problem. When choosing the next hyperparameter combination, this method considers the previous evaluation results. It then applies a probabilistic function to select the combination that will probably yield the best results. This method discovers a fairly good hyperparameter combination in relatively few iterations.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Data scientists choose a probabilistic model when the objective function is unknown. That is, there is no analytical expression to maximize or minimize. The data scientists apply the learning algorithm to a data set, use the algorithmâs results to define the objective function, and take the various hyperparameter combinations as the input domain.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The probabilistic model is based on past evaluation results. It estimates the probability of a hyperparameter combinationâs objective function result:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"P( result | hyperparameters )","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"blockquote"},{"data":{},"content":[{"data":{},"marks":[],"value":"This probabilistic model is a âsurrogateâ of the objective function. The objective function can be, for instance, the root-mean-square error (RMSE). We calculate the objective function using the training data with the hyperparameter combination. We try to optimize it (maximize or minimize, depending on the objective function selected).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Applying the probabilistic model to the hyperparameters is computationally inexpensive compared to the objective function, so this method typically updates and improves the surrogate probability model every time the objective function runs. Better hyperparameter predictions decrease the number of objective function evaluations we need to achieve a good result.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Gaussian processes, random forest regression, and tree-structured Parzen estimators (TPE) are surrogate model examples.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The figure below shows how a surrogate function finds the minimum of the âobjectiveâ function, where the âobjectiveâ function is unknown.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4SHIioexvFyT7VrxIZoXqS","type":"Asset","createdAt":"2022-02-03T16:35:58.836Z","updatedAt":"2022-06-02T23:23:21.409Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"hyperparameter-tuning-bayesian-optimization","description":"The Bayesian optimization method treats the search for the optimal hyperparameters as an optimization problem. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/4SHIioexvFyT7VrxIZoXqS/a73f41d18104cf0e28b77f7a003dd443/1382615_Blog_ImageIllustration-9_3_060222.png","details":{"size":101690,"image":{"width":1500,"height":1000}},"fileName":"1382615_Blog ImageIllustration-9_3_060222.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"After a few iterations, with the evaluations (observations) obtained from the âobjectiveâ function, the surrogate model finds the minimum at the coordinates x=-0.35 and y=-0.8. Note that there are several evaluations close to that point, corresponding to the latest iterations, as the model has almost found the minimum and produces similar values (different color points correspond to different iterations).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The Bayesian optimization model is complex to implement. Fortunately, we can use off-the-shelf libraries like ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#bayesian-optimization-tune-suggest-bayesopt-bayesoptsearch"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to simplify the process. Itâs worthwhile to use this type of model because it finds an adequate hyperparameter combination in relatively few iterations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Bayesian optimization is helpful when the objective function is costly in computing resources and time. A drawback compared to grid search or random search is that we must compute Bayesian optimization sequentially (where the next iteration depends on the previous one), so it doesnât allow distributed processing. So, Bayesian optimization takes longer yet uses fewer computational resources.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next steps","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We should perform model hyperparameter tuning to ensure good results from our machine learning model and data. We can choose from three hyperparameter tuning methods â grid search, random search, and Bayesian optimization. If evaluating our model with training data will be quick, we can choose the grid search method. Otherwise, we should select random search or Bayesian optimization to save time and computing resources.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the next article in this series, weâll demonstrate hands-on ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-tune-hyperparameters-on-xgboost"},"content":[{"data":{},"marks":[],"value":"how to tune hyperparameters on XGBoost","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Weâll build and optimize a machine learning model to identify images of digits.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4SHIioexvFyT7VrxIZoXqS","type":"Asset","createdAt":"2022-02-03T16:35:58.836Z","updatedAt":"2022-06-02T23:23:21.409Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"hyperparameter-tuning-bayesian-optimization","description":"The Bayesian optimization method treats the search for the optimal hyperparameters as an optimization problem. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/4SHIioexvFyT7VrxIZoXqS/a73f41d18104cf0e28b77f7a003dd443/1382615_Blog_ImageIllustration-9_3_060222.png","details":{"size":101690,"image":{"width":1500,"height":1000}},"fileName":"1382615_Blog ImageIllustration-9_3_060222.png","contentType":"image/png"}}},"recommendations":[],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}}],"hideIntro":true}},"url":"https://www.anyscale.com/blog/what-is-hyperparameter-tuning","ctaText":"Read more","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tRzIapKjPrMXXKHCy84A5","type":"Asset","createdAt":"2022-03-24T22:19:37.827Z","updatedAt":"2022-03-24T22:19:37.827Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-rl-robot-gear-dark","file":{"url":"//images.ctfassets.net/xjan103pcp94/4tRzIapKjPrMXXKHCy84A5/6db744f330c85c7d33d1db721b9d4653/blog-recommended-content-rl-robot-gear-dark.jpg","details":{"size":51103,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-rl-robot-gear-dark.jpg","contentType":"image/jpeg"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2SE0kDJN6mdzagMlzv6bV9","type":"Entry","createdAt":"2022-03-29T00:50:01.362Z","updatedAt":"2022-03-29T00:50:01.362Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"How to tune hyperparameters on XGBoost","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3VHYwnZLqT7r9Y6kd7U7sv","type":"Entry","createdAt":"2022-02-09T18:28:07.790Z","updatedAt":"2022-06-22T15:59:49.486Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"How to tune hyperparameters on XGBoost","slug":"how-to-tune-hyperparameters-on-xgboost","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Njah6Xtnm7bwiyiwTrQ4V","type":"Entry","createdAt":"2022-02-04T18:35:38.461Z","updatedAt":"2022-02-04T18:35:38.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Juan Navas","slug":"juan-navas"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}}],"publishedDate":"2022-02-09","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"In this hands-on article, weâll explain how to tune hyperparameters on XGBoost. You just need to know some Python to follow along, and weâll show you how to easily deploy machine learning models and then optimize their performance.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"ð¡ ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"This blog post is part 2 in our series on hyperparameter tuning. If you're just getting started, check out part 1, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/what-is-hyperparameter-tuning"},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"What is hyperparameter tuning?","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":". In part 3, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-distribute-hyperparameter-tuning-using-ray-tune"},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"How to distribute hyperparameter tuning using Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", we'll dive into a hands-on example of how to speed up the tuning task.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/what-is-hyperparameter-tuning"},"content":[{"data":{},"marks":[],"value":"In the first article of this series","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we learned what hyperparameter tuning is, its importance, and our various options. In this hands-on article, weâll explore a practical case to explain how to tune hyperparameters on XGBoost. You just need to know some Python to follow along, and weâll show you how to easily deploy machine learning models and then optimize their performance.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Weâll use the Modified National Institute of Standards and Technology (","nodeType":"text"},{"data":{"uri":"http://yann.lecun.com/exdb/mnist/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MNIST","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") database, including 60,000 training samples and 10,000 test samples. Each sample is an image of a handwritten digit, normalized to a 28 by 28-pixel box and anti-aliased (grayscale levels).Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The figures below are a couple of samples from the MNIST database:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2hrxzFsCUSjTwEXRrUKF7M","type":"Asset","createdAt":"2022-02-03T17:36:19.337Z","updatedAt":"2022-05-24T20:06:51.710Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"hands-on-hyperparameter-tuning-MNIST-samples","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2hrxzFsCUSjTwEXRrUKF7M/1a2afecafb9d69c8ae3afe0be2075d36/1372491_Blog_image_illustration_-8_1_052322.jpg","details":{"size":162817,"image":{"width":1500,"height":1000}},"fileName":"1372491_Blog image illustration -8_1_052322.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To build our digit identification model, weâll use the popular library, XGBoost. Then to tune it, we will use the scikit-learn library, which provides a relatively easy and uniform hyperparameter tuning method. Letâs get started.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pre-processing the original MNIST files","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"First, we download the four files in the ","nodeType":"text"},{"data":{"uri":"http://yann.lecun.com/exdb/mnist/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MNIST data set","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"train-images-idx3-ubyte","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"train-labels-idx1-ubyte","nodeType":"text"},{"data":{},"marks":[],"value":" for the training, and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"t10k-images-idx3-ubyte","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"t10k-labels-idx1-ubyte","nodeType":"text"},{"data":{},"marks":[],"value":" for the test data.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Then, we convert the ubyte files to comma-separated values (CSV) files to input them into the machine learning algorithm.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The function below performs the conversion:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"68BHpcRknDR5pyhcnWt3br","type":"Entry","createdAt":"2022-02-03T17:38:40.163Z","updatedAt":"2022-02-09T18:24:30.961Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hands-on hyperparameter tuning, code step 1","body":"def convert(imgf, labelf, outf, n):\n    f = open(imgf, \"rb\")\n    o = open(outf, \"w\")\n    l = open(labelf, \"rb\")\n\n    f.read(16)\n    l.read(8)\n    images = []\n\n    for i in range(n):\n      image = [ord(l.read(1))]\n      for j in range(28*28):\n        image.append(ord(f.read(1)))\n        images.append(image)\n\n    for image in images:\n      o.write(\",\".join(str(pix) for pix in image)+\"\\n\")\n\n    f.close()\n    o.close()\n    l.close()\n\nconvert(\"train-images-idx3-ubyte\", \n        \"train-labels-idx1-ubyte\",\n        \"mnist_train.csv\", 60000)\nconvert(\"t10k-images-idx3-ubyte\", \n        \"t10k-labels-idx1-ubyte\",\n        \"mnist_test.csv\", 10000)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now we need to adjust the label column for the machine learning algorithm. The training set is in ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"column 5,","nodeType":"text"},{"data":{},"marks":[],"value":" while the test set is in ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"column 7","nodeType":"text"},{"data":{},"marks":[],"value":". We need to rename those columns and generate new CSV files.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"xyNjmxianga0wxTAiYo3a","type":"Entry","createdAt":"2022-02-03T17:40:36.534Z","updatedAt":"2022-02-03T17:40:42.601Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hands-on hyperparameter tuning, code step 2","body":"import pandas as pd\n\n# read the converted files\ndf_orig_train = pd.read_csv('mnist_train.csv')\ndf_orig_test = pd.read_csv('mnist_test.csv')\n\n# rename columns\ndf_orig_train.rename(columns={'5':'label'}, inplace=True)\ndf_orig_test.rename(columns={'7':'label'}, inplace=True)\n\n# write final version of the csv files\ndf_orig_train.to_csv('mnist_train_final.csv', index=False)\ndf_orig_test.to_csv('mnist_test_final.csv', index=False)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We have now finished pre-processing the data set and can start building the model.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Building model parameters without tuning hyperparameters","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs start by building the model without any hyperparameter tuning. Instead, we will use typically recommended values for our hyperparameters.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To run this code yourself, youâll need to install NumPy, sklearn (scikit-learn), pandas, and XGBoost using pip, Conda, or another Python package management tool. Note that this isn't intended to be a comprehensive XGBoost tutorial. If youâre new to XGBoost, we recommend starting with ","nodeType":"text"},{"data":{"uri":"https://xgboost.readthedocs.io/en/latest/get_started.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"the guides and tutorials","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in the XGBoost documentation.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"First, weâll define a ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"model_mnist","nodeType":"text"},{"data":{},"marks":[],"value":" function that takes a hyperparameter list as input. This makes it easy to quickly recreate the model with different hyperparameters.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Also, for practical reasons, we will work with the first 1,000 samples from the training dataset. The MNIST training dataset has 60,000 samples with high dimensionality (784 features), which means about 47 million data points. Building the model for the complete dataset takes time (in the range of 10-15 minutes for an 8-core CPU), so it will take many hours, or even days, to perform hyperparameter tuning on a single machine. So, using a smaller dataset while weâre learning allows us to experiment with different tuning techniques more quickly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"Kt65lmVSlUT1AyCSTB8Tt","type":"Entry","createdAt":"2022-02-03T17:42:50.692Z","updatedAt":"2022-02-03T17:42:50.692Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hands-on hyperparameter tuning, code step 3","body":"import numpy as np\nimport pandas as pd   # data processing, CSV file I/O (e.g. pd.read_csv)\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\ndef model_mnist(params):\n\n    # Input data files are available in the \"./data/\" directory.\n    train_df = pd.read_csv(\"./data/mnist_train_final.csv\")\n    test_df = pd.read_csv(\"./data/mnist_test_final.csv\")\n\n    # limit the dataset size to 1000 samples\n    dataset_size = 1000\n    train_df = train_df.iloc[0:dataset_size, :]\n    test_df = test_df.iloc[0:dataset_size, :]\n\n    y = train_df.label.values\n    X = train_df.drop('label', axis=1).values\n\n    # build train and validation datasets\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15)\n    print(\"Shapes - X_train: \", X_train.shape,\n          \", X_val: \", X_val.shape, \", y_train: \",\n          y_train.shape, \", y_val: \", y_val.shape)\n\n    train_data = xgb.DMatrix(X_train, label=y_train)\n    val_data = xgb.DMatrix(X_val, label=y_val)\n\n    n_rounds = 600\n    early_stopping = 50\n\n    # build the model\n    results = {}\n\n    model = xgb.train(params, \n                      train_data, \n                      num_boost_round=100, \n                      evals=[(val_data, 'val')], \n                      evals_result=results)\n\n    # letâs check how good the model is\n    x_test = test_df.drop('label', axis=1).values\n    test_labels = test_df.label.values\n    test_data = xgb.DMatrix(x_test)\n\n    predictions = model.predict(test_data)\n\n    accuracy = metrics.accuracy_score(test_labels, predictions)\n\n    return accuracy\n\nif __name__ == '__main__':\n\n    # define number of classes = 10 (digits) \n    # and the metric as merror (multi-class error classification rate)\n    default_params = [\n        (\"num_class\", 10), (\"eval_metric\", \"merror\")\n    ]\n\n    accuracy_result = model_mnist(default_params)\n\n    print(\"accuracy: \", accuracy_result)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Save the above Python code in a .py file (for instance, ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"mnist_model.py","nodeType":"text"},{"data":{},"marks":[],"value":") and run it from the command line:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"$ python mnist_model.py","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We should see the following output:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"UMZMbuIOBYrrGil4g3gqT","type":"Entry","createdAt":"2022-02-03T17:44:39.709Z","updatedAt":"2022-02-03T17:44:39.709Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hands-on hyperparameter tuning, code step 4","body":"Shapes - X_train:  (850, 784) , X_val:  (150, 784) , y_train:  (850,) , y_val:  (150,)\n[0]\tval-merror:0.326667\n[1]\tval-merror:0.233333\n .\n .\n[98]\tval-merror:0.166667\n[99]\tval-merror:0.166667\naccuracy:  0.826","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The output shows we obtained an accuracy of 82.6 percent (that is, our model correctly classified 82.6 percent of test samples).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Tuning hyperparameters","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now weâll tune our hyperparameters using the random search method. For that, weâll use the sklearn library, which provides a function specifically for this purpose: RandomizedSearchCV.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"First, we save the Python code below in a .py file (for instance, ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"random_search.py","nodeType":"text"},{"data":{},"marks":[],"value":").","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4eZTfBiqfcQmssVca45hl0","type":"Entry","createdAt":"2022-02-03T17:45:29.803Z","updatedAt":"2022-02-03T17:45:29.803Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hands-on hyperparameter tuning, code step 5","body":"import numpy as np\nimport pandas as pd   \nimport xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\n\ndef random_search_tuning():\n    # Input data files are available in the \"./data/\" directory.\n    train_df = pd.read_csv(\"./data/mnist_train_final.csv\")\n    test_df = pd.read_csv(\"./data/mnist_test_final.csv\")\n    print (train_df.shape, test_df.shape)\n\n    y = train_df.label.values\n    x = train_df.drop('label', axis=1).values\n\n    # define the train set and test set\n    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.05)\n    print(\"Shapes - X_train: \", x_train.shape,\n          \", X_val: \", x_val.shape, \", y_train: \",\n          y_train.shape, \", y_val: \", y_val.shape)\n\n    params = {'max_depth': [3, 6, 10, 15],\n              'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4],\n              'subsample': np.arange(0.5, 1.0, 0.1),\n              'colsample_bytree': np.arange(0.5, 1.0, 0.1),\n              'colsample_bylevel': np.arange(0.5, 1.0, 0.1),\n              'n_estimators': [100, 250, 500, 750],\n              'num_class': [10]\n              }\n\n    xgbclf = xgb.XGBClassifier(objective=\"multi:softmax\", tree_method='hist')\n    clf = RandomizedSearchCV(estimator=xgbclf,\n                             param_distributions=params,\n                             scoring='accuracy',\n                             n_iter=25,\n                             n_jobs=4,\n                             verbose=1)\n\n    clf.fit(x_train, y_train)\n\n    best_combination = clf.best_params_\n\n    return best_combination\n\nif __name__ == '__main__':\n\n    best_params = random_search_tuning()\n\n    print(\"Best hyperparameter combination: \", best_params)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Our expected output looks something like this:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Sua3h0847cfiPXC4PqGMr","type":"Entry","createdAt":"2022-02-03T17:46:06.370Z","updatedAt":"2022-02-03T17:46:06.370Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hands-on hyperparameter tuning, code step 6","body":"Fitting 5 folds for each of 25 candidates, totaling 125 fits\nAccuracy:  0.858\nBest hyperparameter combination:  {'subsample': 0.9, \n                                   'num_class': 10, \n                                   'n_estimators': 500, \n                                   'max_depth': 10, \n                                   'learning_rate': 0.1, \n                                   'colsample_bytree': 0.6, \n                                   'colsample_bylevel': 0.7}","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The accuracy has improved to 85.8 percent. Weâve now found the best hyperparameter combination for our model.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next steps","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"After running our untuned and tuned models, we discovered that our model with tuned hyperparameters has better accuracy. The tuned model can make better predictions on new data, so it can more accurately identify images of digits beyond those in the training set.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the final article of our three-part series, weâll explore hands-on ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-distribute-hyperparameter-tuning-using-ray-tune"},"content":[{"data":{},"marks":[],"value":"how distributed hyperparameter tuning improves our results even more","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Weâll use Ray to deploy our Python machine learning model to the cloud for distributed tuning using Ray Tune.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2hrxzFsCUSjTwEXRrUKF7M","type":"Asset","createdAt":"2022-02-03T17:36:19.337Z","updatedAt":"2022-05-24T20:06:51.710Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"hands-on-hyperparameter-tuning-MNIST-samples","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2hrxzFsCUSjTwEXRrUKF7M/1a2afecafb9d69c8ae3afe0be2075d36/1372491_Blog_image_illustration_-8_1_052322.jpg","details":{"size":162817,"image":{"width":1500,"height":1000}},"fileName":"1372491_Blog image illustration -8_1_052322.jpg","contentType":"image/jpeg"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}}],"hideIntro":true,"recommendations":[]}},"url":"https://www.anyscale.com/blog/how-to-tune-hyperparameters-on-xgboost","ctaText":"Read more","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Cf9zMp25hZJpUm4pk9veA","type":"Asset","createdAt":"2022-03-21T17:16:32.337Z","updatedAt":"2022-03-21T17:16:32.337Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-gears-bubble","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6Cf9zMp25hZJpUm4pk9veA/59d25fadae5d6ae2b5e5090b0b3c5de8/blog-recommended-content-gears-bubble.jpg","details":{"size":45436,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-gears-bubble.jpg","contentType":"image/jpeg"}}},"recommendations":null}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}}],"hideIntro":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4mDD9iGuZZbkGgK5fUfQcH","type":"Entry","createdAt":"2021-08-12T21:33:46.544Z","updatedAt":"2022-06-22T17:00:48.654Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Writing your First Distributed Python Application with Ray ","slug":"writing-your-first-distributed-python-application-with-ray","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-08-12","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Ray is a fast, simple distributed execution framework that makes it easy to scale your applications and to leverage state of the art machine learning libraries. Using Ray, you can take Python code that runs sequentially and transform it into a distributed application with minimal code changes.Â \nThe goal of this tutorial is to explore how to get started with Ray as well as some common trade-offs in distributed computing (compute cost, memory, I/O, etc).","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1XwPRevPAwseF7Xn59pehK","type":"Asset","createdAt":"2021-08-12T18:10:09.695Z","updatedAt":"2021-08-12T18:27:25.776Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Ray makes parallel and distributed computing work more like you would hope","description":"Ray makes parallel and distributed computing work more like you would hope ([image source](https://www.reddit.com/r/aww/comments/2oagj8/multithreaded_programming_theory_and_practice/))","file":{"url":"//images.ctfassets.net/xjan103pcp94/1XwPRevPAwseF7Xn59pehK/41eac0cf3497c936660874d79b2e8a7c/RayPuppies.png","details":{"size":623811,"image":{"width":891,"height":433}},"fileName":"RayPuppies.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a fast, simple distributed execution framework that makes it easy to scale your applications and to leverage state of the art machine learning libraries. Using Ray, you can take Python code that runs sequentially and transform it into a distributed application with minimal code changes.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The goal of this tutorial is to explore the following:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Why should you parallelize and distribute with RayÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"How to get started with Ray","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Trade-offs in distributed computing (compute cost, memory, I/O, etc)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Why should you parallelize and distribute with Ray?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As a ","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/modern-parallel-and-distributed-python-a-quick-tutorial-on-ray-99f8d70369b8"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"previous post pointed out","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", parallel and distributed computing are a staple of modern applications. The problem is that taking existing Python code and trying to parallelize or distribute it can mean rewriting existing code, sometimes from scratch. Additionally modern applications have requirements that existing modules like ","nodeType":"text"},{"data":{"uri":"https://docs.python.org/3/library/multiprocessing.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"multiprocessing","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" lack. These requirements include:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Running the same code on more than one machine","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Building microservices and actors that have state and can communicate","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Graceful handling of machine failures and preemption","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Efficient handling of large objects and numerical data","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The Ray library satisfies these requirements and allows you to scale your applications without rewriting them. In order to make parallel \u0026 distributed computing simple, Ray takes functions and classes and translates them to the distributed setting as tasks and actors. The rest of this tutorial explores these concepts as well as some important things to consider when building parallel \u0026 distributed applications.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7fYvNgpXdywkUVdBs3rc2o","type":"Asset","createdAt":"2021-08-12T18:16:08.498Z","updatedAt":"2021-09-30T02:41:22.139Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Ray Ecosystem","description":"While this tutorial explores how Ray makes it easy to parallelize plain Python code, it is important to note that Ray and its ecosystem also make it easy to parallelize [python code](https://www.anyscale.com/blog/parallelizing-python-code) as well as existing libraries like [scikit-learn](https://medium.com/distributed-computing-with-ray/how-to-speed-up-scikit-learn-model-training-aaf17e2d1e1), [XGBoost](https://www.anyscale.com/blog/distributed-xgboost-training-with-ray), [LightGBM](https://www.anyscale.com/blog/introducing-distributed-lightgbm-training-with-ray), [PyTorch](https://medium.com/pytorch/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-fd83c98fdead), and much more.","file":{"url":"//images.ctfassets.net/xjan103pcp94/7fYvNgpXdywkUVdBs3rc2o/a3fcae9248f53b1e714d418ceda2af96/RayEcosystem.png","details":{"size":181020,"image":{"width":1050,"height":400}},"fileName":"RayEcosystem.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"How to get started with Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Turning Python Functions into Remote Functions (Ray Tasks)","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray can be installed through pip. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"77bS2q6cHNNDFMWcxk4rrA","type":"Entry","createdAt":"2021-08-12T18:17:09.481Z","updatedAt":"2021-08-12T18:17:09.481Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"pip install 'ray[default]'","body":"pip install 'ray[default]'"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs begin our Ray journey by creating a Ray task. This can be done by decorating a normal Python function with @ray.remote. This creates a task which can be scheduled across your laptop's CPU cores (or Ray cluster).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Consider the two functions below which generate Fibonacci sequences (integer sequence characterized by the fact that every number after the first two is the sum of the two preceding ones). The first is a normal python function and the second is a Ray task.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"22VwJrYkF1TYE9N1FS5eXL","type":"Entry","createdAt":"2021-08-12T18:18:18.201Z","updatedAt":"2021-08-12T18:18:18.201Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Fibonacci local and remote","body":"import os\nimport time\nimport ray\n\n# Normal Python\ndef fibonacci_local(sequence_size):\n    fibonacci = []\n    for i in range(0, sequence_size):\n        if i \u003c 2:\n            fibonacci.append(i)\n            continue\n        fibonacci.append(fibonacci[i-1]+fibonacci[i-2])\n    return sequence_size\n\n# Ray task\n@ray.remote\ndef fibonacci_distributed(sequence_size):\n    fibonacci = []\n    for i in range(0, sequence_size):\n        if i \u003c 2:\n            fibonacci.append(i)\n            continue\n        fibonacci.append(fibonacci[i-1]+fibonacci[i-2])\n    return sequence_size\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are a couple of things to note regarding these two functions. First, they are identical except for the @ray.remote decorator on the fibonacci_distributed function.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The second thing to note is the small return value. They are not returning the Fibonacci sequences themselves, but the sequence size, which is an integer.Â  This is important, because it might lessen the value of a distributed function by designing it so that it requires or returns a lot of data (parameters). Engineers often refer to this as the input/output (IO) of a distributed function.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing Local vs Remote Performance","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The functions in this section will allow us to compare how long it takes to generate multiple long Fibonacci sequences both locally and in parallel. It is important to note that both functions below utilize os.cpu_count() which returns the number of CPUs in the system.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"54SXSBTuKUayBitHV0SrKc","type":"Entry","createdAt":"2021-08-12T18:20:03.593Z","updatedAt":"2021-08-12T21:18:17.799Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"os.cpu_count()","body":"os.cpu_count()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"paIFMkTXJ1wDurn4EtxUM","type":"Asset","createdAt":"2021-08-12T21:18:55.296Z","updatedAt":"2021-08-12T21:31:05.140Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"os cpucount","description":"The machine used in this tutorial has eight CPUs which means that each function below will generate 8 Fibonacci sequences.","file":{"url":"//images.ctfassets.net/xjan103pcp94/paIFMkTXJ1wDurn4EtxUM/da4d62db6720bafae6b79732267fa8c1/os.cpu_countBigger.png","details":{"size":10542,"image":{"width":652,"height":134}},"fileName":"os.cpu_countBigger.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"43HStbGxOqxYXSH1ZiYCUB","type":"Entry","createdAt":"2021-08-12T18:29:12.449Z","updatedAt":"2021-08-12T18:29:12.449Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"normal python vs run_remote Ray","body":"\n# Normal Python\ndef run_local(sequence_size):\n    start_time = time.time()\n    results = [fibonacci_local(sequence_size) for _ in range(os.cpu_count())]\n    duration = time.time() - start_time\n    print('Sequence size: {}, Local execution time: {}'.format(sequence_size, duration))\n\n# Ray\ndef run_remote(sequence_size):\n    # Starting Ray\n    ray.init()\n    start_time = time.time()\n    results = ray.get([fibonacci_distributed.remote(sequence_size) for _ in range(os.cpu_count())])\n    duration = time.time() - start_time\n    print('Sequence size: {}, Remote execution time: {}'.format(sequence_size, duration))  \n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Before getting into how the code for run_local and run_remote work, let's run both of these functions to see how long it takes to generate multiple 100000 number Fibonacci sequences both locally and remotely.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4cMVjiUB2AE87FBezGtfwP","type":"Entry","createdAt":"2021-08-12T18:29:55.125Z","updatedAt":"2021-08-12T18:29:55.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"run local vs run remote","body":"run_local(100000)\nrun_remote(100000)"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5R29PpjjLWmpy47q0ZcaSm","type":"Asset","createdAt":"2021-08-12T18:31:20.666Z","updatedAt":"2021-08-12T18:31:20.666Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"first distributed run_local run_remote","file":{"url":"//images.ctfassets.net/xjan103pcp94/5R29PpjjLWmpy47q0ZcaSm/e8e31d311963e266377017bde7bba81e/localremoteOutput.png","details":{"size":56866,"image":{"width":1160,"height":340}},"fileName":"localremoteOutput.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The run_remote function parallelized the computation across multiple cpus which resulted in a smaller processing time (1.76s vs 4.20s).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The Ray APIÂ ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"In order to better understand why run_remote was faster, let's briefly go over the code and along the way explain how the Ray API works.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xL53Bs7d4tDw0tRAlwP7y","type":"Asset","createdAt":"2021-08-12T18:32:53.495Z","updatedAt":"2021-08-12T18:32:53.495Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"run_remote yellow","file":{"url":"//images.ctfassets.net/xjan103pcp94/2xL53Bs7d4tDw0tRAlwP7y/e480af08c8a919a16c5d4f7eb8736277/run_remote_init_yellow.png","details":{"size":112667,"image":{"width":1600,"height":373}},"fileName":"run_remote_init_yellow.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The ray.init() command starts all of the relevant Ray processes. By default, Ray creates one worker process per CPU core. If you would want to run Ray on a cluster, you would need to pass in a cluster address with something like ray.init(address= 'InsertAddressHere').","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2YKe8I5CcCFWz7AMP3hCeg","type":"Asset","createdAt":"2021-08-12T18:38:26.784Z","updatedAt":"2021-08-12T18:38:26.784Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"run_remote remote fibonacci_distributed.remote","file":{"url":"//images.ctfassets.net/xjan103pcp94/2YKe8I5CcCFWz7AMP3hCeg/a8176d41153ea3950da968de71f20be2/ray_remote_remote.png","details":{"size":127368,"image":{"width":2695,"height":629}},"fileName":"ray_remote_remote.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"fibonacci_distributed.remote(100000)","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"WVgyApqF1JHhHmW12j9qQ","type":"Asset","createdAt":"2021-08-12T20:59:12.103Z","updatedAt":"2021-08-12T20:59:12.103Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"fibonacci_distributed.remote(100000)","file":{"url":"//images.ctfassets.net/xjan103pcp94/WVgyApqF1JHhHmW12j9qQ/0871bb81486d25f2acea6965a505ba88/fibonacci_distributed_remote.png","details":{"size":22343,"image":{"width":796,"height":114}},"fileName":"fibonacci_distributed_remote.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Calling fibonacci_distributed.remote(sequence_size) immediately returns a future and not the return value of the function. The actual function execution will take place in the background. Since it returns immediately, each function call can be executed in parallel. This makes generating those multiple 100000 long fibonacci sequences take less time.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4CgUHJZN6Tn0nBwgwykJae","type":"Asset","createdAt":"2021-08-12T21:00:13.491Z","updatedAt":"2021-08-12T21:00:13.491Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray.get","file":{"url":"//images.ctfassets.net/xjan103pcp94/4CgUHJZN6Tn0nBwgwykJae/a4485d5f26c191ede3a503577f51f352/ray_remote_get.png","details":{"size":125763,"image":{"width":2695,"height":629}},"fileName":"ray_remote_get.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3MfuX62xtaw4TBEe7KBE8U","type":"Asset","createdAt":"2021-08-12T21:01:41.965Z","updatedAt":"2021-08-12T21:01:41.965Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray get results","file":{"url":"//images.ctfassets.net/xjan103pcp94/3MfuX62xtaw4TBEe7KBE8U/6023be4619b7c76b869a761ee38de18c/ray_get_results.png","details":{"size":23349,"image":{"width":1204,"height":126}},"fileName":"ray get results.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"ray.get retrieves the resulting value from the task when it completes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, it is important to note that when the process calling ray.init() terminates, the Ray runtime will also terminate. Note that if you try and run ray.init() more than once you may get a RuntimeError (Maybe you called ray.init twice by accident?). This can be solved by using ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.shutdown()","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"f6zrwMtxDInCHI7rtsCtz","type":"Entry","createdAt":"2021-08-12T21:02:32.237Z","updatedAt":"2021-08-12T21:02:32.237Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray.shutdown 1st distributed application","body":"# To explicitly stop or restart Ray, use the shutdown API\nray.shutdown()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Dashboard","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray comes with a dashboard that is available at http://127.0.0.1:8265 after you call the ray.init function.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Among ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-dashboard.html#ray-dashboard"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"other things","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", the dashboard lets you:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Understand Ray memory utilization and debug memory errors.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"See per-actor resource usage, executed tasks, logs, and more.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"View cluster metrics.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Kill actors and profile your Ray jobs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"See errors and exceptions at a glance.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"View logs across many machines in a single pane.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"See ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" jobs and trial information.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The dashboard below shows the resource utilization on a per-node and per-worker basis after running run_remote(200000). Notice how the dashboard shows the function fibonacci_distributed thatâs running in each worker. Itâs a good idea to observe your distributed functions while they are running. That way, if you see one worker doing all the work, then you may be using the ray.get function incorrectly. Also, if you see your total CPU utilization getting close to 100 percent, you may be doing too much.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7slD3bTCoh7Wsd0LIScROV","type":"Asset","createdAt":"2021-08-12T21:03:54.003Z","updatedAt":"2021-08-12T21:03:54.003Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray dashboard 8 core","file":{"url":"//images.ctfassets.net/xjan103pcp94/7slD3bTCoh7Wsd0LIScROV/29127717361206b65fab68981b56e59d/8CoreMichael.png","details":{"size":369929,"image":{"width":2106,"height":1326}},"fileName":"8CoreMichael.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Trade-offs in distributed computing","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"This tutorial used Fibonacci sequences because they provide several options for tweaking computing and IO. You can alter the amount of computing that each function call requires by increasing and decreasing the sequence size. The greater the sequence size, the more computing you need to generate the sequence, whereas the smaller the sequence size, the less computing you need. If the computation you distribute is too small, the overhead of Ray would dominate the total processing time, and you wouldnât get any value out of distributing our functions.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"IO is also essential when distributing functions. If you modified these functions to return the sequences they calculate, the IO would increase as the sequence size increased. At some point, the time needed to transmit the data would dominate the total time required to complete the multiple calls to the distributed function. This is important if you are distributing your functions over a cluster. This would require the use of a network, and network calls are more costly than the interprocess communication used in this tutorial.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Therefore, it is recommended that you try to experiment with both the distributed Fibonacci function and the local Fibonacci function. Try to determine the minimum sequence size needed to benefit from a remote function. Once you figure out the computing, play with the IO to see what happens to overall performance. Distributed architectures, regardless of the tool you use, work best when they donât have to move a lot of data around.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Fortunately, a major benefit of Ray is the ability to maintain entire objects remotely. This helps mitigate the IO problem. Letâs look at that next.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Remote Objects as Actors","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Just as Ray translates Python functions to the distributed setting as tasks, Ray translates Python classes to the distributed setting as actors. Ray provides actors to allow you to parallelize an instance of a class. Code wise, all you need to add to a Python class is the @ray.remote decorator to make it an actor. When you make an instance of that class, Ray creates a new actor which is a process that runs in the cluster and holds a copy of the object.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Since they are remote objects, they can hold data, and their methods can manipulate that data. This helps cut down on interprocess communication. Consider using an actor if you find yourself writing too many tasks that return data, which in turn are sent to other tasks.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs now look at the actor below.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"TwIHe3DkT9AQ6nocBiax0","type":"Entry","createdAt":"2021-08-12T21:05:46.558Z","updatedAt":"2021-08-12T21:05:46.558Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Ray Actor","body":"from collections import namedtuple\nimport csv\nimport tarfile\nimport time\n\nimport ray\n\n@ray.remote\nclass GSODActor():\n\n    def __init__(self, year, high_temp):\n        self.high_temp = float(high_temp)\n        self.high_temp_count = None\n        self.rows = []\n        self.stations = None\n        self.year = year\n\n    def get_row_count(self):\n        return len(self.rows)\n\n    def get_high_temp_count(self):\n        if self.high_temp_count is None:\n            filtered = [l for l in self.rows if float(l.TEMP) \u003e= self.high_temp]\n            self.high_temp_count = len(filtered)\n        return self.high_temp_count\n\n    def get_station_count(self):\n        return len(self.stations)\n\n    def get_stations(self):\n        return self.stations\n\n    def get_high_temp_count(self, stations):\n        filtered_rows = [l for l in self.rows if float(l.TEMP) \u003e= self.high_temp and l.STATION in stations]\n        return len(filtered_rows)\n\n    def load_data(self):\n        file_name = self.year + '.tar.gz'\n        row = namedtuple('Row', ('STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'NAME', 'TEMP', 'TEMP_ATTRIBUTES', 'DEWP',\n                                 'DEWP_ATTRIBUTES', 'SLP', 'SLP_ATTRIBUTES', 'STP', 'STP_ATTRIBUTES', 'VISIB', 'VISIB_ATTRIBUTES',\n                                 'WDSP', 'WDSP_ATTRIBUTES', 'MXSPD', \n                                 'GUST', 'MAX', 'MAX_ATTRIBUTES', 'MIN', 'MIN_ATTRIBUTES', 'PRCP',\n                                 'PRCP_ATTRIBUTES', 'SNDP', 'FRSHTT'))\n\n        tar = tarfile.open(file_name, 'r:gz')\n        for member in tar.getmembers():\n            member_handle = tar.extractfile(member)\n            byte_data = member_handle.read()\n            decoded_string = byte_data.decode()\n            lines = decoded_string.splitlines()\n            reader = csv.reader(lines, delimiter=',')\n\n            # Get all the rows in the member. Skip the header.\n            _ = next(reader)\n            file_rows = [row(*l) for l in reader]\n            self.rows += file_rows\n\n        self.stations = {l.STATION for l in self.rows}\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The code above can be used to load and manipulate data from a public dataset known as the Global Surface Summary of the Day (GSOD). The dataset is managed by the National Oceanic and Atmospheric Administration (NOAA) and it is freely available on their ","nodeType":"text"},{"data":{"uri":"https://www.ncei.noaa.gov/data/global-summary-of-the-day/archive/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"site","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". NOAA currently maintains data from over 9,000 stations worldwide and the GSOD dataset contains daily summary information from these stations. There is one gzip file for each year from 1929 to 2020. For this tutorial, you only need to download the files for ","nodeType":"text"},{"data":{"uri":"https://www.ncei.noaa.gov/data/global-summary-of-the-day/archive/1980.tar.gz"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"1980","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://www.ncei.noaa.gov/data/global-summary-of-the-day/archive/2020.tar.gz"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"2020","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The goal of this actor experiment is to compute how many readings from 1980 and 2020 were 100 degrees or greater and determine if 2020 had more extreme temperatures than 1980. In order to implement a fair comparison, only stations that existed in both 1980 and 2020 should be considered. So, the logic of this experiment looks like this:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Load 1980 data.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Load 2020 data.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Get a list of stations that existed in 1980.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Get a list of stations that existed in 2020.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Determine the intersection of stations.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Get the number of readings that were 100 degrees or greater from the intersection of stations during 1980.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Get the number of readings that were 100 degrees or greater from the intersection of stations during 2020.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Print the results.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The problem is that this logic is completely sequential; one thing only happens after another. With Ray, a lot of this logic can be done in parallel.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The table below shows a more parallelizable logic.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"68pZlG1nraPtehugiEkHS0","type":"Asset","createdAt":"2021-08-12T21:07:50.037Z","updatedAt":"2021-08-12T21:07:50.037Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Actor Logic","file":{"url":"//images.ctfassets.net/xjan103pcp94/68pZlG1nraPtehugiEkHS0/c6cca4f793189d8cac6a1705b667f83e/RayActorLogic.png","details":{"size":41120,"image":{"width":850,"height":278}},"fileName":"RayActorLogic.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Writing out the logic in this fashion is an excellent way of making sure you are executing everything that you can in a parallelizable way. The code below implements this logic. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3KKnRHGruKzanOKTyoGWe5","type":"Entry","createdAt":"2021-08-12T21:08:53.055Z","updatedAt":"2021-08-12T21:08:53.055Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"compare_years actor","body":"# Code assumes you have the 1980.tar.gz and 2020.tar.gz files in your current working directory.\ndef compare_years(year1, year2, high_temp):\n\n    # if you know that you need fewer than the default number of workers,\n    # you can modify the num_cpus parameter\n    ray.init(num_cpus=2)\n\n    # Create actor processes\n    gsod_y1 = GSODActor.remote(year1, high_temp)\n    gsod_y2 = GSODActor.remote(year2, high_temp)\n\n    ray.get([gsod_y1.load_data.remote(), gsod_y2.load_data.remote()])\n\n    y1_stations, y2_stations = ray.get([gsod_y1.get_stations.remote(),\n               \t                    gsod_y2.get_stations.remote()])\n\n    intersection = set.intersection(y1_stations, y2_stations)\n\n    y1_count, y2_count = ray.get([gsod_y1.get_high_temp_count.remote(intersection),\n                                  gsod_y2.get_high_temp_count.remote(intersection)])\n\n    print('Number of stations in common: {}'.format(len(intersection)))\n    print('{} - High temp count for common stations: {}'.format(year1, y1_count))\n    print('{} - High temp count for common stations: {}'.format(year2, y2_count))\n\n#Running the code below will output which year had more extreme temperatures\ncompare_years('1980', '2020', 100)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2ByyLQUfrMmGJY26r8i7zf","type":"Asset","createdAt":"2021-08-12T21:09:46.703Z","updatedAt":"2021-08-12T21:09:46.703Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"compare years","file":{"url":"//images.ctfassets.net/xjan103pcp94/2ByyLQUfrMmGJY26r8i7zf/4acbc0305931d25c6e33533afcfa3ebe/compare_years.png","details":{"size":78224,"image":{"width":1460,"height":296}},"fileName":"compare_years.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are a couple important things to mention about the code above. First, putting the @ray.remote decorator at the class level enabled all class methods to be called remotely. Second, the code above utilizes two actor processes (gsod_y1 and gsod_y2) which can execute methods in parallel (though each actor can only execute one method at a time). This is what enabled the loading and processing of the 1980 and 2020 data at the same time.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a fast, simple distributed execution framework that makes it easy to scale your applications and to leverage state of the art machine learning libraries.Â  This tutorial showed how using Ray makes it easy to take your existing Python code that runs sequentially and transform it into a distributed application with minimal code changes. While the experiments here were all performed on the same machine, ","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/how-to-scale-python-on-every-major-cloud-provider-5e5df3e88274"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray also makes it easy to scale your Python code on every major cloud provider","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". If youâre interested in learning more about Ray, check out the ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray project on GitHub","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", follow ","nodeType":"text"},{"data":{"uri":"https://twitter.com/raydistributed"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"@raydistributed on twitter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and sign up for the ","nodeType":"text"},{"data":{"uri":"https://anyscale.us5.list-manage.com/subscribe?u=524b25758d03ad7ec4f64105f\u0026id=d94e960a03"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray newsletter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3w1vm8nJgJ1YKcBvy8eHXU","type":"Asset","createdAt":"2021-08-12T21:35:52.571Z","updatedAt":"2021-08-12T21:35:52.571Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Dashboard 8 Core","file":{"url":"//images.ctfassets.net/xjan103pcp94/3w1vm8nJgJ1YKcBvy8eHXU/eb733efbaf26b0c1c31378f9eb349e22/8CPUMichael.png","details":{"size":373794,"image":{"width":2274,"height":1322}},"fileName":"8CPUMichael.png","contentType":"image/png"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Zk7jixFrLqHf3IJLzrO2B","type":"Entry","createdAt":"2022-05-03T18:47:31.422Z","updatedAt":"2022-05-03T18:47:31.422Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Ray Summit: Aug 23-24 in SF!","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5LTWfPAScfUtzXj2KOUALz","type":"Entry","createdAt":"2022-04-12T15:29:42.674Z","updatedAt":"2022-10-04T22:06:07.976Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":17,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"summit"}},"locale":"en-US"},"fields":{"title":"Ray Summit 2022","slug":"ray-summit-2022","description":"Centered on Ray, the open-source distributed computing framework, Ray Summit brings together ML practitioners, data scientists, and more to discuss the growing demand for distributed computing.","sections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xIJZXdAqhOEizUCUuQaiV","type":"Entry","createdAt":"2022-03-31T16:04:49.746Z","updatedAt":"2022-09-12T20:58:36.991Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":20,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-hero","header":"The Place for _Everything Ray_","body":"Missed us in San Francisco for Ray Summit 2022? Don't fret, you can watch your favorite sessions on demand! ","ctaText":"View Sessions","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"9F9S7NLaIdPL4ThYwVlFW","type":"Asset","createdAt":"2022-09-06T14:49:38.936Z","updatedAt":"2022-09-06T14:53:02.350Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Presented by Anyscale","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/9F9S7NLaIdPL4ThYwVlFW/8ecd09b334942298f0b8c2d5567ec032/presented-by-anyscale.svg","details":{"size":22813,"image":{"width":289,"height":32}},"fileName":"presented-by-anyscale.svg","contentType":"image/svg+xml"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4A0d4TUfp0TCqUA60CXopR","type":"Asset","createdAt":"2022-09-06T14:53:48.720Z","updatedAt":"2022-09-06T14:53:48.720Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4A0d4TUfp0TCqUA60CXopR/2e4240c268d074d840f7a00d069a768a/rs-collage_2x.jpg","details":{"size":568547,"image":{"width":2400,"height":1376}},"fileName":"rs-collage@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6FGUP0cXunTbgs2UemeAU","type":"Asset","createdAt":"2022-09-06T14:54:09.975Z","updatedAt":"2022-09-06T14:54:09.975Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event, stacked vertically","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6FGUP0cXunTbgs2UemeAU/16ed175024bc9cc0a2fae3cb13538dff/rs-collage-mobile_2x.jpg","details":{"size":243282,"image":{"width":750,"height":1420}},"fileName":"rs-collage-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4RKa7qVhWoIaoNSa1ZH2SK"}}],"type":"2022 Summit Hero","styling":["transparent-header"]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6GL7ZzuSLadWdVFKHj7yIH","type":"Entry","createdAt":"2022-04-20T18:00:12.964Z","updatedAt":"2022-08-08T22:50:29.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-keynote-speakers","header":"Keynote Speakers","ctaText":"See all speakers","ctaLink":"/ray-summit-2022/speakers","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6VhewtcICU9SCXjwd5qhHJ"}}],"type":"2022 Summit Speakers","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JTi5fsyo0Q3YrF3SAQglg","type":"Entry","createdAt":"2022-08-08T22:49:55.163Z","updatedAt":"2022-08-09T07:03:38.615Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-ray-summit-selected-speakers","header":"Highlighted Talks","ctaText":"Full Agenda","ctaLink":"/ray-summit-2022/agenda","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"50eS4cKXBNE4GHezdZ8Rws"}},{"sys":{"type":"Link","linkType":"Entry","id":"5iioKfOgEqVzgQ1DMmA1Wy"}},{"sys":{"type":"Link","linkType":"Entry","id":"YCBUZOXJGB0qcJF8TKjfI"}}],"type":"2022 Summit Agenda Preview"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5cgHuoJ713DewhVY6iUYrG","type":"Entry","createdAt":"2022-05-03T21:57:45.147Z","updatedAt":"2022-05-09T13:41:03.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-trainings","header":"Exclusive Ray Training Sessions","body":"Ray Summit is a great opportunity to level up your Ray skills through hands-on training sessions with the builders and maintainers of Ray. Weâre offering several pre-conference training sessions on August 22. Space is limited â [reserve your spot now](https://www.myeventi.events/raysummit22/attendee/)!","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3sYQTUmD5X5ncvGRFyIRR6","type":"Asset","createdAt":"2022-05-03T04:40:25.040Z","updatedAt":"2022-05-03T04:40:25.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image desktop","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3sYQTUmD5X5ncvGRFyIRR6/a77c675b07d63e73dd79ada9d182498a/training-image_2x.jpg","details":{"size":313509,"image":{"width":1136,"height":1480}},"fileName":"training-image@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7rsC56a4YCgENLyxsHyeIb","type":"Asset","createdAt":"2022-05-03T04:40:54.089Z","updatedAt":"2022-05-03T04:40:54.089Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image mobile","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7rsC56a4YCgENLyxsHyeIb/e2592c9b4424048b3c916fbbbc313065/training-image-mobile_2x.jpg","details":{"size":153801,"image":{"width":830,"height":874}},"fileName":"training-image-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4Hb3J9SrCFTDXqIhIdSIwa"}},{"sys":{"type":"Link","linkType":"Entry","id":"hZ4Tl1wvj7CwF00slwpLg"}},{"sys":{"type":"Link","linkType":"Entry","id":"YJ5KWyjg4NGf29NrfHcJX"}}],"type":"2022 Summit Trainings"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mRJQtuDbIhAuEbwLT2V1K","type":"Entry","createdAt":"2022-03-31T21:42:56.117Z","updatedAt":"2022-04-11T23:46:07.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-reasons-to-attend","header":"Why Attend?","body":"As the annual user conference for the [open source Ray community](https://www.ray.io/community), Ray Summit is a unique opportunity to enhance your Ray skills, build your professional network, and learn what's on the Ray roadmap.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"5f6gxk4a4ujE2y6FZ8ATQ5"}},{"sys":{"type":"Link","linkType":"Entry","id":"20QwyiOmkHCEOkMvp0Jhtk"}},{"sys":{"type":"Link","linkType":"Entry","id":"6QxivsuhhLoVSloRQs9ZlQ"}}],"type":"2022 Summit Content Tiles"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"15k0SGqB2ZKOUfAsM3JnyV","type":"Entry","createdAt":"2022-03-31T16:05:55.739Z","updatedAt":"2022-08-05T00:25:20.225Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":16,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-about","header":"Celebrate the Ray community","body":"Get ready for our epic community bash at the Art of Tinkering exhibit at the San Francisco Exploratorium on August 23 from 5:30 to 9:30 p.m. Come explore, play, and celebrate with the Ray Community. This is an exclusive opportunity to experience this amazing interactive exhibition before it closes in September.","ctaText":"Check out the agenda","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UcZT4h1P7uFDz6cn6JH8l","type":"Asset","createdAt":"2022-06-02T20:37:22.299Z","updatedAt":"2022-06-02T20:37:22.299Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Man taking a photo on his phone","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2UcZT4h1P7uFDz6cn6JH8l/5040a9909f18ab015c27b220077f916e/taking-picture.jpg","details":{"size":107846,"image":{"width":980,"height":772}},"fileName":"taking-picture.jpg","contentType":"image/jpeg"}}}],"type":"2022 Summit Split Image","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4P8Jn0q7nBU05iD6DmcQCC","type":"Entry","createdAt":"2022-04-01T19:15:31.735Z","updatedAt":"2022-04-11T16:36:59.573Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-social-tiles","header":"Join the Conversation","body":"Ready to get involved in the Ray community before the conference? Ask a question in the forums. Open a pull request. Or share why youâre excited with the hashtag __#RaySummit__ on Twitter.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6OMTlXkJ1OHXNkEjyPrXTH"}},{"sys":{"type":"Link","linkType":"Entry","id":"7EE28ee1ATQtodoLloT6mg"}},{"sys":{"type":"Link","linkType":"Entry","id":"1CRAzFxndL37wiNg4x8WKV"}},{"sys":{"type":"Link","linkType":"Entry","id":"2qrec5rR8kabZxCHaPS7NH"}}],"type":"2022 Summit Social Tiles"}}],"headerLogo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70DIdxwT8XY96aw7vMUHOu","type":"Asset","createdAt":"2022-06-01T15:21:46.804Z","updatedAt":"2022-06-01T15:21:46.804Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo Dark Blue","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/70DIdxwT8XY96aw7vMUHOu/5ca0a8b7533c8df70bc0e032b889862a/rs-logo-nav.svg","details":{"size":9954,"image":{"width":108,"height":48}},"fileName":"rs-logo-nav.svg","contentType":"image/svg+xml"}}},"headerLogoWhite":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3qyLXKGgGfWhv4oPqYl0SE","type":"Asset","createdAt":"2022-04-20T21:50:58.774Z","updatedAt":"2022-04-20T21:50:58.774Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3qyLXKGgGfWhv4oPqYl0SE/c4dee9e7a344c39db11fefafbf255b5b/rs-logo-horizontal.svg","details":{"size":9975,"image":{"width":108,"height":48}},"fileName":"rs-logo-horizontal.svg","contentType":"image/svg+xml"}}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KUtRQbdqJo1RrWMrVyfEm","type":"Entry","createdAt":"2022-03-31T16:21:13.402Z","updatedAt":"2022-04-08T17:25:33.228Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Home","link":"/ray-summit-2022"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4MvQcRSkHLMqIJoO9O2cQe","type":"Entry","createdAt":"2022-03-31T16:17:27.336Z","updatedAt":"2022-09-12T16:59:51.221Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"On-Demand","link":"/ray-summit-2022/agenda","subLinks":[{"sys":{"type":"Link","linkType":"Entry","id":"3N9kRDz36tieTI4YGVWKBJ"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46B5diwEDDr3kB2XDoGfDP","type":"Entry","createdAt":"2022-05-11T17:13:42.174Z","updatedAt":"2022-05-11T17:13:42.174Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Speakers","link":"/ray-summit-2022/speakers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Dt5fy6A7rBpXdG7HeKMGd","type":"Entry","createdAt":"2022-03-31T16:17:43.592Z","updatedAt":"2022-03-31T16:17:43.592Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Sponsors","link":"/ray-summit-2022/sponsors"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SvRE9pnyq3BXUPGKRWx6W","type":"Entry","createdAt":"2022-06-03T19:29:31.009Z","updatedAt":"2022-06-03T19:29:31.009Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Who Attends","link":"/ray-summit-2022/who-attends"}}],"mainSponsors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"xMQIOU9n7gVS4mkX2Nlag","type":"Entry","createdAt":"2021-01-31T16:33:06.495Z","updatedAt":"2021-05-11T13:28:31.312Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Ray","url":"https://ray.io","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jLroqm98RCMLGASthyuPv","type":"Asset","createdAt":"2021-05-11T13:28:26.930Z","updatedAt":"2021-05-11T13:28:26.930Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/jLroqm98RCMLGASthyuPv/f76cc04827316de5d3f8b529f6a3f02c/Ray_BM.svg","details":{"size":4742,"image":{"width":93,"height":36}},"fileName":"Ray_BM.svg","contentType":"image/svg+xml"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11syIHhtX8JajSlXHSHE1O","type":"Entry","createdAt":"2021-01-31T16:31:24.509Z","updatedAt":"2021-05-11T13:28:00.309Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Anyscale","url":"https://www.anyscale.com","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UkDmTzmZHIOG7yxFP4sVL","type":"Asset","createdAt":"2021-05-11T13:27:55.219Z","updatedAt":"2021-05-11T13:27:55.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Anyscale","file":{"url":"//images.ctfassets.net/xjan103pcp94/5UkDmTzmZHIOG7yxFP4sVL/db10aebda9a9aad10903470781a6f2e6/ANSC_BM.svg","details":{"size":7431,"image":{"width":158,"height":36}},"fileName":"ANSC_BM.svg","contentType":"image/svg+xml"}}}}}],"footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cfwECWrOqJ5Fegr3YpReS","type":"Entry","createdAt":"2022-04-01T21:22:12.001Z","updatedAt":"2022-04-13T17:35:29.511Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-footer","body":"Â©Anyscale, Inc 2022\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/privacy-policy\"\u003ePrivacy Policy\u003c/a\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/code-of-conduct\"\u003eCode of Conduct\u003c/a\u003e"}},"recommendations":[]}},"ctaText":"Register now","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5nTlh9josVs5zpZqU3vTbn","type":"Asset","createdAt":"2022-05-03T18:47:25.125Z","updatedAt":"2022-05-03T18:47:25.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-ray-summit-2022","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5nTlh9josVs5zpZqU3vTbn/e66043f62ed4a4b53d7e182e50289ec3/blog-recommended-content-ray-summit-2022.png","details":{"size":17234,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-ray-summit-2022.png","contentType":"image/png"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"59lrHxy21yr2NLyTOxK70I","type":"Entry","createdAt":"2022-03-24T21:46:46.981Z","updatedAt":"2022-03-24T21:46:46.981Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Parallelizing Python code","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6dwKBw7V5sw7cl5lfEmDlo","type":"Entry","createdAt":"2021-09-02T16:39:30.889Z","updatedAt":"2022-06-22T16:55:57.303Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":10,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Parallelizing Python Code ","slug":"parallelizing-python-code","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7xsDT3qwgziVxkIpEDqo5Q","type":"Entry","createdAt":"2021-09-01T21:13:33.309Z","updatedAt":"2021-09-01T21:13:33.309Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Dawid Borycki","slug":"dawid-borycki","link":"https://www.linkedin.com/in/dawidborycki/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-09-02","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This article reviews some common options for parallelizing Python code including process-based parallelism, specialized libraries, ipython parallel, and Ray.\n","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Python is great for tasks like training machine learning models, performing numerical simulations, and quickly developing proof-of-concept solutions without setting up development tools and installing several dependencies. When performing these tasks, you also want to use your underlying hardware as much as possible for quick results. Parallelizing Python code enables this. However, using the standard CPython implementation means you cannot fully use the underlying hardware because of the global interpreter lock (GIL) that prevents running the bytecode from multiple threads simultaneously.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This article reviews some common options for parallelizing Python code including:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.python.org/3/library/multiprocessing.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Process-based parallelism","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Specialized libraries","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://ipython.readthedocs.io/en/stable/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"IPython Parallel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"For each technique, this article lists some advantages and disadvantages and shows a code sample to help you understand what itâs like to use.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How to Parallelize Python Code","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are several common ways to parallelize Python code. You can launch several application instances or a script to perform jobs in parallel. This approach is great when you donât need to exchange data between parallel jobs. Otherwise, sharing data between processes significantly reduces performance when aggregating data.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Starting multiple threads within the same process allows you to share data between jobs more efficiently. In this case, thread-based parallelization can offload some work to the background. However, the standard CPython implementationâs global interpreter lock (GIL) prevents running the bytecode in multiple threads simultaneously.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The sample function below simulates complex calculations (meant to mimic activation functions)Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ZhjxMjb34NNejPqZr3m6x","type":"Entry","createdAt":"2021-09-01T21:17:58.733Z","updatedAt":"2021-09-01T21:17:58.733Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"iterations_count = round(1e7)","body":"iterations_count = round(1e7)\ndef complex_operation(input_index):\n   print(\"Complex operation. Input index: {:2d}\".format(input_index))\n   [math.exp(i) * math.sinh(i) for i in [1] * iterations_count]\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"complex_operation","nodeType":"text"},{"data":{},"marks":[],"value":" executes several times to better estimate the processing time. It divides the long-running operation into a batch of smaller ones. It does this by dividing the input values into several subsets and then processing the inputs from those subsets in parallel.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hereâs the code that runs ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"complex_operation","nodeType":"text"},{"data":{},"marks":[],"value":" several times (input range of ten) and measures the execution time with the timebudget package:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2jhikURwzrxs4RqmCKpPEr","type":"Entry","createdAt":"2021-09-01T22:04:46.592Z","updatedAt":"2021-09-01T22:04:46.592Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"@timebudget","body":"@timebudget\ndef run_complex_operations(operation, input):\n   for i in input:\n      operation(i) \n\ninput = range(10)\nrun_complex_operations(complex_operation, input) \n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"After executing ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/mGalarnyk/8c491fbdfe6ce3e498a7f62f03fa9ca4"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"this script","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", you will get an output similar to the one below:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1lIyKj3DsJzMsJBW6YuBlD","type":"Asset","createdAt":"2021-09-01T22:06:02.993Z","updatedAt":"2021-09-01T22:14:52.851Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"parallelizing blog 1","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1lIyKj3DsJzMsJBW6YuBlD/e4e011eeb7c1db8d6c90d00b07690204/parallyzing_blog_1.png","details":{"size":77487,"image":{"width":512,"height":236}},"fileName":"parallyzing blog 1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"As you can see, it took about 39 seconds to execute this code on the laptop used in this tutorial. Letâs see how to improve this result.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Process-Based Parallelism","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The first approach is to use process-based parallelism. With this approach, it is possible to start several processes at the same time (concurrently). This way, they can concurrently perform calculations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Starting from Python 3, the","nodeType":"text"},{"data":{"uri":"https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing"},"content":[{"data":{},"marks":[],"value":" multiprocessing package","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is preinstalled and gives us a convenient syntax for launching concurrent processes. It provides the Pool object, which automatically divides input into subsets and distributes them among many processes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://gist.github.com/mGalarnyk/b5455b0454815b04363ef9994f22fbf3"},"content":[{"data":{},"marks":[],"value":"Here is an example","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" of how to use a Pool object to launch ten processes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1abfJSxSAm73kPyLcr2YYC","type":"Entry","createdAt":"2021-09-01T22:06:59.165Z","updatedAt":"2021-09-01T22:12:02.079Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"import math","body":"import math\nimport numpy as np\nfrom timebudget import timebudget\nfrom multiprocessing import Pool\n\niterations_count = round(1e7)\n\ndef complex_operation(input_index):\n    print(\"Complex operation. Input index: {:2d}\\n\".format(input_index))\n    [math.exp(i) * math.sinh(i) for i in [1] * iterations_count]\n\n@timebudget\ndef run_complex_operations(operation, input, pool):\n    pool.map(operation, input)\n\nprocesses_count = 10\n\nif __name__ == '__main__':\n    processes_pool = Pool(processes_count)\n    run_complex_operations(complex_operation, range(10), processes_pool)   \n","language":"python","caption":"[Code sample](https://gist.github.com/mGalarnyk/b5455b0454815b04363ef9994f22fbf3)"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Each process concurrently performs the complex operation. So, the code could theoretically reduce the total execution time by up to ten times. However, the output from the code below only shows about a fourfold improvement (39 seconds in the previous section vs 9.4 in this section).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6B7ewbHMg9JmRmf397uhEK","type":"Asset","createdAt":"2021-09-01T22:08:30.198Z","updatedAt":"2021-09-01T22:15:10.841Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"parallelizing blog 2","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6B7ewbHMg9JmRmf397uhEK/0ed9effb3ca1ac369f82dc2057e44a1e/parallyzing_blog_2.png","details":{"size":63680,"image":{"width":512,"height":364}},"fileName":"parallyzing blog 2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are a couple reasons why the improvement is not tenfold. First, the maximum number of processes that can run concurrently depends on the the number of CPUs in the system. You can find out how many CPUs your system has by using the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"os.cpu_count()","nodeType":"text"},{"data":{},"marks":[],"value":" method.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3a52SVCuKhfB5cfURStgO5","type":"Entry","createdAt":"2021-09-01T22:08:56.719Z","updatedAt":"2021-09-09T22:30:56.730Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"import os (parallelizing python)","body":"import os\nprint('Number of CPUs in the system: {}'.format(os.cpu_count()))\n"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3hFb0PY8xY6qi8hrzNlVY5","type":"Asset","createdAt":"2021-09-01T22:09:39.529Z","updatedAt":"2021-09-09T22:31:46.280Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"locale":"en-US"},"fields":{"title":"parallelizing blog 3","description":"The machine used in this tutorial has eight CPUs","file":{"url":"//images.ctfassets.net/xjan103pcp94/3hFb0PY8xY6qi8hrzNlVY5/26af309e69d23edb05774db80dd83f77/importOS.png","details":{"size":17081,"image":{"width":776,"height":60}},"fileName":"importOS.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The next reason why the improvement is not more is that the computations in this tutorial are relatively small. Finally, it is important to note that there is usually some overhead when parallelizing computation as processes that want to communicate must utilize ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Inter-process_communication"},"content":[{"data":{},"marks":[],"value":"interprocess communication mechanisms","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". This means that for very small tasks parallelizing computation is often slower than serial computation (normal Python). If you are interested in learning more about multiprocessing, Selva Prabhakaran has an ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray"},"content":[{"data":{},"marks":[],"value":"excellent blog","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" which inspired this section of the tutorial. If you would like to learn about some more of the trade-offs in parallel/distributed computing, ","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/writing-your-first-distributed-python-application-with-ray-4248ebc07f41"},"content":[{"data":{},"marks":[],"value":"check out this tutorial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4ggc9Rzv8uvnLwQM6PO56F","type":"Asset","createdAt":"2021-09-01T22:14:08.712Z","updatedAt":"2021-09-01T22:14:08.712Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"parallelizing blog 4","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4ggc9Rzv8uvnLwQM6PO56F/05d2176bf19135f4ca6414c1e6847cf5/parallyzing_blog_4.png","details":{"size":41104,"image":{"width":1136,"height":344}},"fileName":"parallyzing blog 4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Specialized Libraries","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://stackoverflow.com/questions/36479159/why-are-numpy-calculations-not-affected-by-the-global-interpreter-lock"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Many calculations for specialized libraries like NumPy are unaffected by the GIL","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and can use threads and other techniques to work in parallel. This section of the tutorial goes over the benefits of combining NumPy and multiprocessing","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To demonstrate the differences between the naÃ¯ve implementation and the NumPy-based implementation, an additional function needs to be implemented:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3IEvF1mTH3N4nknYheGorV","type":"Entry","createdAt":"2021-09-01T22:16:14.653Z","updatedAt":"2021-09-01T22:16:14.653Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"def complex_operation_numpy(input_index):","body":"def complex_operation_numpy(input_index):\n      print(\"Complex operation (numpy). Input index: {:2d}\".format(input_index))\n\n      data = np.ones(iterations_count)\n      np.exp(data) * np.sinh(data)\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The code now uses the NumPy exp and sinh functions to perform calculations on the input sequence. Then, the code executes complex_operation and complex_operation_numpy ten times using the processes pool to compare their performance:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"qOivysVk6k2lrlzdOARHt","type":"Entry","createdAt":"2021-09-01T22:16:54.024Z","updatedAt":"2021-09-01T22:16:54.024Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"processes_count = 10","body":"processes_count = 10\ninput = range(10)\n\nif __name__ == '__main__':\n    processes_pool = Pool(processes_count)\n    print(âWithout NumPyâ)\n    run_complex_operations(complex_operation, input, processes_pool)\n    print(âNumPyâ)\n    run_complex_operations(complex_operation_numpy, input, processes_pool)\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The output below shows the performance with and without NumPy for ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/mGalarnyk/703c53bb98aa94d66bb6c49d48ce5c09"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"this script","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3VOltZ8c6mAkocPZIjc7LR","type":"Asset","createdAt":"2021-09-01T22:17:45.820Z","updatedAt":"2021-09-01T22:17:45.820Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"parallelizing blog 5","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3VOltZ8c6mAkocPZIjc7LR/ec5caa75c801b673706848fa2b661f18/parallelizing_blog_5.png","details":{"size":92946,"image":{"width":512,"height":341}},"fileName":"parallelizing blog 5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"NumPy offers a rapid boost in performance. Here, NumPy reduced the computation time to about 10 percent of the original time (859ms vs 9.515sec). One reason why it is faster is because most processing in NumPy is vectorized. With vectorization, the underlying code is effectively âparallelizedâ because the operation can calculate multiple array elements at once, rather than looping through them one at a time. If you are interested in learning more about this, Jake Vanderplas gave an excellent talk on the subject ","nodeType":"text"},{"data":{"uri":"https://youtu.be/EEUXKG97YRw?t=613"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"13k36Q3LmF673vVrbUPqZa","type":"Asset","createdAt":"2021-09-01T22:19:18.055Z","updatedAt":"2021-09-01T22:19:18.055Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"parallelizing blog 6","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/13k36Q3LmF673vVrbUPqZa/28396aaa33aa8618afabc0eca99a163b/parallelizing_blog_6.png","details":{"size":58112,"image":{"width":1138,"height":372}},"fileName":"parallelizing blog 6.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"IPython Parallel","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The IPython shell supports interactive parallel and distributed computing across multiple IPython instances. IPython Parallel was developed (almost) together with IPython.Â  When IPython was renamed to Jupyter, they split out IPython Parallel into its own package. ","nodeType":"text"},{"data":{"uri":"https://ipython.org/ipython-doc/3/parallel/parallel_intro.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"IPython Parallel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" has a number of advantages, but perhaps the biggest advantage is that it enables parallel applications to be developed, executed, and monitored interactively. When using IPython Parallel for parallel computing, you typically start with the ipcluster command.Â Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"64uuCAseEQroqm32TkSDNX","type":"Entry","createdAt":"2021-09-01T22:35:55.681Z","updatedAt":"2021-09-01T22:35:55.681Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ipcluster start -n 10","body":"ipcluster start -n 10","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The last parameter controls the number of engines (nodes) to launch. The command above becomes available after ","nodeType":"text"},{"data":{"uri":"https://ipyparallel.readthedocs.io/en/latest/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"installing the ipyparallel Python package","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Below is a sample output:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ebalmkxu1ukvpH4poPoFr","type":"Asset","createdAt":"2021-09-01T22:39:31.719Z","updatedAt":"2021-09-01T22:39:31.719Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"parallelizing blog 7","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6ebalmkxu1ukvpH4poPoFr/f2aa39aa743664d2e931b3dbf860b6f0/parallelizing_blog_7.png","details":{"size":67784,"image":{"width":512,"height":319}},"fileName":"parallelizing blog 7.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The next step is to provide Python code that should connect to ipcluster and start parallel jobs. Fortunately, IPython provides a convenient API for doing this. The code looks like process-based parallelism based on the Pool object:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"aYoLda5Xxm7gqSbr8KNYc","type":"Entry","createdAt":"2021-09-01T22:40:21.451Z","updatedAt":"2021-09-01T22:40:53.474Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"import math import numpy as np","body":"import math\nimport numpy as np\nfrom timebudget import timebudget\nimport ipyparallel as ipp\n\niterations_count = round(1e7)\n\ndef complex_operation(input_index):\n    print(\"Complex operation. Input index: {:2d}\".format(input_index))\n\n    [math.exp(i) * math.sinh(i) for i in [1] * iterations_count]\n\ndef complex_operation_numpy(input_index):\n    print(\"Complex operation (numpy). Input index: {:2d}\".format(input_index))\n\n    data = np.ones(iterations_count)\n    np.exp(data) * np.sinh(data)\n\n@timebudget\ndef run_complex_operations(operation, input, pool):\n    pool.map(operation, input)\n\nclient_ids = ipp.Client()\npool = client_ids[:]\n\ninput = range(10)\nprint('Without NumPy')\nrun_complex_operations(complex_operation, input, pool)\nprint('NumPy')\nrun_complex_operations(complex_operation_numpy, input, pool)\n","language":"python","caption":"[Code sample](https://gist.github.com/mGalarnyk/6dab23cc6485f145d2b148fc64d34b3c)"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The code above executed in a new tab in the terminal produces the output shown below:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3oOlwnkO2edLArTuWAimXk","type":"Asset","createdAt":"2021-09-01T22:41:48.786Z","updatedAt":"2021-09-01T22:41:48.786Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"parallelizing blog 8","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3oOlwnkO2edLArTuWAimXk/318561ba4a59504e34f9a7b54850ea16/parallelizing_blog_8.png","details":{"size":37920,"image":{"width":512,"height":313}},"fileName":"parallelizing blog 8.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The execution times with and without NumPy for IPython Parallel are 13.88 ms and 9.98 ms, respectively. Note, there are no logs included in the standard output, however they can be assessed with additional commands.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"25yTzWsBo114khG50lNYoY","type":"Asset","createdAt":"2021-09-01T22:47:23.369Z","updatedAt":"2021-09-01T22:47:23.369Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"parallelizing blog 9","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/25yTzWsBo114khG50lNYoY/0fc1f36d15924685e4a8cdd4dce1b2e9/parallelizing_blog_9.png","details":{"size":49289,"image":{"width":1136,"height":326}},"fileName":"parallelizing blog 9.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Like IPython Parallel, ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" can be used for parallel ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"and","nodeType":"text"},{"data":{},"marks":[],"value":" distributed computing. Ray is a fast, simple distributed execution framework that makes it easy to scale your applications and to leverage state of the art machine learning libraries. Using Ray, you can take Python code that runs sequentially and transform it into a distributed application with minimal code changes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"JRSqxrziyanjUq8ba6Rz6","type":"Asset","createdAt":"2021-09-01T22:49:57.493Z","updatedAt":"2021-09-01T22:51:28.882Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"parallelizing blog 10","description":"While this tutorial briefly goes over how Ray makes it easy to parallelize plain Python code, it is important to note that Ray and its ecosystem also make it easy to parallelize existing libraries like [scikit-learn](https://medium.com/distributed-computing-with-ray/how-to-speed-up-scikit-learn-model-training-aaf17e2d1e1), [XGBoost](https://www.anyscale.com/blog/distributed-xgboost-training-with-ray), [LightGBM](https://www.anyscale.com/blog/introducing-distributed-lightgbm-training-with-ray), [PyTorch](https://medium.com/pytorch/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-fd83c98fdead), and much more. \n","file":{"url":"//images.ctfassets.net/xjan103pcp94/JRSqxrziyanjUq8ba6Rz6/d24278ef959def650efd27d118da370c/parallelizing_blog_10.png","details":{"size":60895,"image":{"width":512,"height":195}},"fileName":"parallelizing blog 10.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To use Ray, ray.init() is needed to start all of the relevant Ray processes. By default, Ray creates one worker process per CPU core. If you would want to run Ray on a cluster, you would need to pass in a cluster address with something like ray.init(address='insertAddressHere').","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1j7jrZ01ulNOiMfLrz0UN7","type":"Entry","createdAt":"2021-09-01T22:52:20.163Z","updatedAt":"2021-09-01T22:52:20.163Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray.init()","body":"ray.init()\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The next step is to create a Ray task. This can be done by decorating a normal Python function with the @ray.remote decorator. This creates a task which can be scheduled across your laptopâs CPU cores (or Ray cluster). Hereâs an example for the previously created complex_operation_numpy:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6LuOb8rKdgjTmziDCfJy1g","type":"Entry","createdAt":"2021-09-01T22:52:46.541Z","updatedAt":"2021-09-02T19:20:34.793Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"@ray.remote","body":"@ray.remote\ndef complex_operation_numpy(input_index):\n   print(\"Complex operation (numpy). Input index: {:2d}\".format(input_index))\n   data = np.ones(iterations_count)\n   np.exp(data) * np.sinh(data)\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the last step, execute these functions within the ray runtime, like so:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1UnnCgrctCnKFm44YKOQdB","type":"Entry","createdAt":"2021-09-01T22:53:21.661Z","updatedAt":"2021-09-02T19:22:26.195Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"@timebudget","body":"@timebudget\ndef run_complex_operations(operation, input):\n   ray.get([operation.remote(i) for i in input])\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"After executing ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/mGalarnyk/30c8672620c8655a37940be935899a57"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"this script","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", you will get an output similar to the one below:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1W223ROJ7pJBUXHzhgXDjG","type":"Asset","createdAt":"2021-09-01T22:53:57.373Z","updatedAt":"2021-09-01T22:53:57.373Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"parallelizing blog 11","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1W223ROJ7pJBUXHzhgXDjG/df1d586e64955c7dfa540a8d4fc2780e/parallelizing_blog_11.png","details":{"size":115794,"image":{"width":512,"height":333}},"fileName":"parallelizing blog 11.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The execution times with and without NumPy for Ray are 3.382sec and 419.98ms, respectively. It is important to remember that the performance benefits of Ray will be more pronounced when executing long-running tasks like the graph below shows.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42cIJnjlhuFKgQSjE1ieAP","type":"Asset","createdAt":"2021-09-01T22:54:28.139Z","updatedAt":"2021-09-01T22:55:14.329Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"parallelizing blog 12","description":"Ray has more pronounced benefits when running bigger jobs [(image source)](https://towardsdatascience.com/10x-faster-parallel-python-without-python-multiprocessing-e5017c93cce1)\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/42cIJnjlhuFKgQSjE1ieAP/6b46d3912acd4a912bbb155ae326be9b/parallelizing_blog_12.jpeg","details":{"size":20292,"image":{"width":512,"height":256}},"fileName":"parallelizing blog 12.jpeg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you would like to learn about Rayâs syntax, there is an introductory tutorial on it ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"25l9VaeFvbiyo58XfLjxHD","type":"Asset","createdAt":"2021-09-01T22:55:55.041Z","updatedAt":"2021-09-01T22:55:55.041Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"parallelizing blog 13","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/25l9VaeFvbiyo58XfLjxHD/e5c33468431e2b5029def040d5354bac/parallelizing_blog_13.png","details":{"size":104718,"image":{"width":1136,"height":610}},"fileName":"parallelizing blog 13.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Alternative Python Implementations","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"One final consideration is that you can apply multithreading using other Python implementations. Examples include IronPython for .NET and Jython for Java. In such cases, you could use the low-level threading support from the underlying frameworks. This approach is beneficial if you already have experience with the multi-processing capabilities of .NET or Java.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"This article reviewed common approaches for parallelizing Python through code samples and by highlighting some of their advantages and disadvantages. We performed tests using benchmarks on simple numerical data. It is important to keep in mind that parallelized code often introduces some overhead and that the benefits of parallelization are more pronounced with bigger jobs rather than the short computations in this tutorial.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Keep in mind that the parallelization can be more powerful for other applications. Especially when dealing with typical AI-based tasks in which you must perform repetitive fine-tuning of your models. In such cases, ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" offers the best support due to its rich ecosystem, autoscaling, fault tolerance, and capability of using remote machines.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6JUJVFL5DeeHcjyZICDWor","type":"Asset","createdAt":"2021-09-02T16:11:05.027Z","updatedAt":"2021-09-02T16:11:05.027Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"With and without NumPy Ray","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6JUJVFL5DeeHcjyZICDWor/625fbfc21738362e5d468da8036a5506/withWithoutNumPy.png","details":{"size":115794,"image":{"width":512,"height":333}},"fileName":"withWithoutNumPy.png","contentType":"image/png"}}},"recommendations":[],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true}},"thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6U9bmkQ9tjwDTXlRi08Sgl","type":"Asset","createdAt":"2022-03-24T21:46:44.263Z","updatedAt":"2022-03-24T21:46:44.263Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-code-light","file":{"url":"//images.ctfassets.net/xjan103pcp94/6U9bmkQ9tjwDTXlRi08Sgl/f850a87e8c09a32a17a5a3d38eb98e68/blog-recommended-content-code-light.jpg","details":{"size":41278,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-code-light.jpg","contentType":"image/jpeg"}}},"recommendations":null}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4SxDXA7Cn7iI77RGNM9o93","type":"Entry","createdAt":"2021-07-22T17:15:58.520Z","updatedAt":"2022-06-22T17:01:46.468Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Best Reinforcement Learning Talks from Ray Summit 2021","slug":"best-reinforcement-learning-talks-from-ray-summit-2021","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-07-22","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},"intro":"An overview of some of the best reinforcement learning talks presented at the second Ray Summit","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Summit 2021 had numerous impressive technical sessions about using Ray for scalable Python, machine learning (including deep learning), reinforcement learning, and data processing. All 57 talks, including 12 keynotes, 45 sessions, and 2 tutorials, are available for free on-demand on the ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit platform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and will be published on the ","nodeType":"text"},{"data":{"uri":"https://www.youtube.com/channel/UC7L1tZw52rtgmIB4fr_f40w"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Anyscale YouTube channel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" August 2nd. This post will highlight a few of the most watched reinforcement learning talks from Ray Summit 2021 that showcase some impressive uses of RLlib. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"24x Speedup for Reinforcement Learning with RLlib + RayÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1JJIYrAFt743BcxbxoYnxZ","type":"Asset","createdAt":"2021-07-22T16:58:39.984Z","updatedAt":"2021-07-22T16:58:39.984Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"24x RLlib Scaling","description":" RLlib + Ray Clusters cut experiment time down to ~20 minutes from ~7-10 hours with Stable Baselines.","file":{"url":"//images.ctfassets.net/xjan103pcp94/1JJIYrAFt743BcxbxoYnxZ/c96e1e64d8f323fae0ea06f3592fdc69/24x_RLLibscaling.jpg","details":{"size":172830,"image":{"width":1920,"height":1080}},"fileName":"24x_RLLibscaling.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Training a reinforcement learning agent is compute intensive. Under classical deep learning assumptions, bigger and better GPUs reduce training time. In practice, reinforcement learning can require millions of samples from a relatively slow CPU-only environment, leading to a bottleneck in training that GPUs do not solve. ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/events/2021/06/22/a-24x-speedup-for-reinforcement-learning-with-rllib-+-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Raoul Khouriâs talk","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" showed that training agents with RLlib removes this bottleneck because its Ray integration allows scaling to many CPUs across a cluster of commodity machines. This greatly cuts training wall-time down by orders of magnitude. Check out the talk â","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/events/2021/06/22/a-24x-speedup-for-reinforcement-learning-with-rllib-+-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"24x Speedup for Reinforcement Learning with RLlib + Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"â to learn more. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Orchestrating robotics operations with SageMaker + RLlib","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4Tk5ZslNuFsjIpH2oWuWCv","type":"Asset","createdAt":"2021-07-22T17:04:08.524Z","updatedAt":"2021-07-22T17:04:08.524Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Sahika Genc","description":"RLlib and SageMaker RL make it faster to experiment and manage robotics RL workflows and create end-to-end solutions without having to rebuild each time.","file":{"url":"//images.ctfassets.net/xjan103pcp94/4Tk5ZslNuFsjIpH2oWuWCv/30a764bf942a8451bc10def800bc64a1/SahikaGenc.jpeg","details":{"size":59920,"image":{"width":900,"height":517}},"fileName":"SahikaGenc.jpeg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Robots require the integration of technologies such as image recognition, sensing, artificial intelligence, machine learning (ML), and reinforcement learning (RL) in ways that are new to the field of robotics. Orchestrating robotics operations to train, simulate, and deploy reinforcement learning applications is difficult and time-consuming. Sahika Genc's talk, â","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/events/2021/06/23/introducing-amazon-sagemaker-kubeflow-reinforcement-learning-pipelines-for"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Introducing Amazon SageMaker Kubeflow Reinforcement Learning Pipelines for Robotics","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"â, examines use cases from Woodside Energy and General Electric Aviation, which demonstrate how RLlib and SageMaker RL make it faster to experiment, manage robotics RL workflows, and create end-to-end solutions without having to rebuild each time. This talk is recommended for anyone interested in robotics.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Offline Reinforcement Learning with RLlib (Microsoft)","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RAeSEPG2Fu0CC9qj8nax0","type":"Asset","createdAt":"2021-07-22T17:05:50.200Z","updatedAt":"2021-07-22T17:05:50.200Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Offline Reinforcement Learning with RLlib","description":"The need of an environment is a constraint that prevents the application of reinforcement learning in fields where having a simulator is difficult or unfeasible, such as healthcare.","file":{"url":"//images.ctfassets.net/xjan103pcp94/3RAeSEPG2Fu0CC9qj8nax0/a5e57be7ced6bd1a6f51c79a8cf47c62/offlineRL.jpg","details":{"size":242653,"image":{"width":1920,"height":1080}},"fileName":"offlineRL.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Reinforcement learning is a fast growing field that is starting to make an impact across different engineering areas. However, reinforcement learning is typically framed as an online learning approach where an environment (simulated or real) is required during the learning process. The need of an environment is typically a constraint that prevents the application of reinforcement learning in fields where having a simulator is difficult or unfeasible, such as healthcare.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Check out Edi Palenciaâs highly practical talk â","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/events/2021/06/22/offline-rl-with-rllib"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Offline Reinforcement Learning with RLlib","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"â that covers how to apply reinforcement learning with RLlib to AI/ML problems where the only available resource is a dataset, i.e., a recording of interactions of an agent in an environment.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Reinforcement Learning to Optimize IAP Offer Recommendations in Mobile Games","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4mjBHwWikmzJjw3D1yjRxY","type":"Asset","createdAt":"2021-07-22T17:48:22.481Z","updatedAt":"2021-07-22T18:03:09.737Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"Wildlife Recommender","description":"Learn how Wildlife Studios provides personalized offer recommendations to each player. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/4mjBHwWikmzJjw3D1yjRxY/da37ea4f8b6bda5b7a8c9a48ef5d0820/WildlifeRecommender.png","details":{"size":247935,"image":{"width":1072,"height":600}},"fileName":"WildlifeRecommender.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A large part of mobile game revenue comes from In-App Purchases (IAP), and offers play a relevant role there. Offers are defined as sales opportunities that present a set of virtual items (like gems, for example) with a discount when compared to regular purchases in the game store. Wildlife Studios has a key goal which is to define, for any given player at any given time, what is the best offer that they can show to maximize long-term profits. Check out Wildlife Studies' talk \"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/events/2021/06/22/using-reinforcement-learning-to-optimize-iap-offer-recommendations-in-mobile-games"},"content":[{"data":{},"marks":[],"value":"Using Reinforcement Learning to Optimize IAP Offer Recommendations in Mobile Games","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"\" to learn how they use Reinforcement Learning (RL) algorithms and Ray to tackle this problem, from formulating the problem and setting up their clusters, to the RL agents' deployment in production. This talk was very popular for both reinforcement learning and game enthusiasts!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Neural MMO: Building a Massively Multiagent Research Platform with Ray and RLlib","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3m9gJvDmbpqCxL8Ad3UyAo","type":"Asset","createdAt":"2021-07-22T17:09:07.653Z","updatedAt":"2021-07-22T17:09:07.653Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Neural MMO","description":"Neural MMO is an environment modeled from Massively Multiplayer Online games â a genre supporting hundreds to thousands of concurrent players.","file":{"url":"//images.ctfassets.net/xjan103pcp94/3m9gJvDmbpqCxL8Ad3UyAo/611eba9628e112426e85cdc92c54efd6/Copy_of_NMMO_RaySummit_2021.jpg","details":{"size":535253,"image":{"width":1920,"height":1080}},"fileName":"Copy of NMMO_RaySummit_2021.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Reinforcement learning has solved Go, DoTA, Starcraft â some of the hardest, most strategy intensive games for humans. Nonetheless, these games are missing fundamental aspects of real-world intelligence: large agent populations, ad-hoc collaboration vs. competition, and extremely long time horizons, among others. ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/events/2021/06/22/neural-mmo-building-a-massively-multiagent-research-platform-with-ray-and-rllib"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Joseph Suarezâs talk","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" started by introducing Neural MMO, which is an environment modeled off of Massively Multiplayer Online games. The talk then proceeded to discuss how Ray + RLlib enables some key enabling features of the project. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"This article only highlighted a few of our most watched reinforcement learning talks from Ray Summit 2021. Ray Summit also had many impressive machine learning talks which a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/best-machine-learning-talks-from-ray-summit-2021"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"previous post covered","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". If you would like to learn about how to get started with RLlib as well as its best practices, check out ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/anyscale/raysummit2021/content/Videos/Svaz4RtzmzQ6xWfpQ"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RLlib tutorial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" from Sven Mika, lead maintainer of RLlib. If you would like to keep up to date with all things Ray, consider ","nodeType":"text"},{"data":{"uri":"https://twitter.com/raydistributed"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"following @raydistributed on twitter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://anyscale.us5.list-manage.com/subscribe?u=524b25758d03ad7ec4f64105f\u0026id=d94e960a03"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"sign up for the newsletter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and be sure to attend the next Ray Summit.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3m9gJvDmbpqCxL8Ad3UyAo","type":"Asset","createdAt":"2021-07-22T17:09:07.653Z","updatedAt":"2021-07-22T17:09:07.653Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Neural MMO","description":"Neural MMO is an environment modeled from Massively Multiplayer Online games â a genre supporting hundreds to thousands of concurrent players.","file":{"url":"//images.ctfassets.net/xjan103pcp94/3m9gJvDmbpqCxL8Ad3UyAo/611eba9628e112426e85cdc92c54efd6/Copy_of_NMMO_RaySummit_2021.jpg","details":{"size":535253,"image":{"width":1920,"height":1080}},"fileName":"Copy of NMMO_RaySummit_2021.jpg","contentType":"image/jpeg"}}},"mainImageFit":"contain","tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}}],"recommendations":[]}}],"activeTag":null,"activeType":null,"author":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael","recommendations":[]}},"page":1,"totalPages":1,"allTypes":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KeH0JzrIEpuheazhMAhKf","type":"Entry","createdAt":"2022-06-15T01:31:23.950Z","updatedAt":"2022-06-15T01:31:23.950Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"User Story","slug":"user-story","tintColor":"#6D59C6"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42F9iHs5a05rRrOuqEQwvB","type":"Entry","createdAt":"2022-06-15T01:30:47.322Z","updatedAt":"2022-06-15T01:30:47.322Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Culture","slug":"culture","tintColor":"#39C8E5"}}],"allTags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KwkbI6zRqcE9KD5iKuP8W","type":"Entry","createdAt":"2021-12-05T04:51:33.974Z","updatedAt":"2021-12-05T04:51:33.974Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"RayDP","identifier":"ray_dp"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1OiVmfyPaDqRNavVG5Yp1l","type":"Entry","createdAt":"2021-12-05T04:50:12.541Z","updatedAt":"2021-12-05T04:50:12.541Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Retail","identifier":"retail"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"161LHBXwPkxUmhjt1YbfJH","type":"Entry","createdAt":"2021-12-05T04:49:42.528Z","updatedAt":"2021-12-05T04:49:42.528Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Gaming","identifier":"gaming"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"BDTwwkSW9VtmSkJojGbx8","type":"Entry","createdAt":"2021-12-05T04:49:30.169Z","updatedAt":"2021-12-05T04:49:30.169Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Government","identifier":"government"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4fzKvVOn2R8U6TTi2bcb2R","type":"Entry","createdAt":"2021-12-05T04:49:10.881Z","updatedAt":"2021-12-05T04:49:10.881Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"HLS","identifier":"hls"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"GGg4W2UlqfVP2iPMsc8J1","type":"Entry","createdAt":"2021-12-05T04:44:22.472Z","updatedAt":"2021-12-05T04:44:22.472Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Financial","identifier":"financial"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i6fp6Gfdy5wFeDDOaKcfh","type":"Entry","createdAt":"2021-12-03T22:33:42.248Z","updatedAt":"2021-12-03T22:33:42.248Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Anyscale","identifier":"anyscale"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6chYLcIc6yEB2EtLv2vngw","type":"Entry","createdAt":"2021-11-23T01:06:51.725Z","updatedAt":"2021-11-30T22:20:19.315Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Healthcare","identifier":"healthcare"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BzSmyQ2ojATOq6qOpD4PX","type":"Entry","createdAt":"2021-10-02T19:52:20.790Z","updatedAt":"2021-10-02T19:52:20.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Technology","identifier":"technology"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}}]},"__N_SSP":true},"page":"/blog","query":{"author":"michael-galarnyk"},"buildId":"-oT8_WeW9m_lAxr26cJvY","isFallback":false,"gssp":true,"customServer":true,"scriptLoader":[]}</script><script type="text/javascript" id="hs-script-loader" async="" defer="" src="https://js.hs-scripts.com/20523749.js"></script><script type="text/javascript">
  _linkedin_partner_id = "4210313";
  window._linkedin_data_partner_ids = window._linkedin_data_partner_ids || [];
  window._linkedin_data_partner_ids.push(_linkedin_partner_id);
</script><script type="text/javascript">
  (function(l) { if (!l){window.lintrk = function(a,b){window.lintrk.q.push([a,b])};
  window.lintrk.q=[]} var s = document.getElementsByTagName("script")[0];
  var b = document.createElement("script");
  b.type = "text/javascript";b.async = true;
  b.src = "https://snap.licdn.com/li.lms-analytics/insight.min.js";
  s.parentNode.insertBefore(b, s);})(window.lintrk);
</script><noscript><img height="1" width="1" style="display:none" alt="" src="https://px.ads.linkedin.com/collect/?pid=4210313&amp;fmt=gif"/></noscript></body></html>