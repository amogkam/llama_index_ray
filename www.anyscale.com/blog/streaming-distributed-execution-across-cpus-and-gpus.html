<!DOCTYPE html><html lang="en-US"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Streaming distributed execution across CPUs and GPUs</title><meta name="description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta property="og:title" content="Streaming distributed execution across CPUs and GPUs"/><meta property="og:description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta property="og:site_name" content="Anyscale"/><meta property="og:image" content="https://images.ctfassets.net/xjan103pcp94/5qTVayH4YnKM7pGL8CiSwc/99806986301b595308c6c89a8bad6249/images_and_diagrams__13_.png"/><meta property="fb:app_id" content=""/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta name="twitter:site" content=""/><meta name="twitter:title" content="Streaming distributed execution across CPUs and GPUs"/><meta name="twitter:image" content="https://images.ctfassets.net/xjan103pcp94/5qTVayH4YnKM7pGL8CiSwc/99806986301b595308c6c89a8bad6249/images_and_diagrams__13_.png"/><meta name="next-head-count" content="14"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><link rel="icon" href="../static/favicon.ico"/><script>
(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-P8H6KQG');
</script><script src="https://tag.clearbitscripts.com/v1/pk_7a96be585f731437b35c7bad25c432b7/tags.js"></script><script src="https://www.googleoptimize.com/optimize.js?id=OPT-NPGB9W3"></script><script>
  !function(f,b,e,v,n,t,s)
  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
  n.queue=[];t=b.createElement(e);t.async=!0;
  t.src=v;s=b.getElementsByTagName(e)[0];
  s.parentNode.insertBefore(t,s)}(window,document,'script',
  'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '168099398554987');
  fbq('track', 'PageView');
</script><link rel="stylesheet" href="../static/font-awesome-4.7.0/css/font-awesome.min.css"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="true"/><script src="https://fast.wistia.com/assets/external/E-v1.js" async=""></script><meta name="ahrefs-site-verification" content="4be3c5d78cdad8823d038256ac06d4336ae7a84632dac0ed57975703fc14fe6b"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="../_next/static/css/dbba207740458c19.css" as="style"/><link rel="stylesheet" href="../_next/static/css/dbba207740458c19.css" data-n-g=""/><link rel="preload" href="../_next/static/css/8a274f3d4edb6eb9.css" as="style"/><link rel="stylesheet" href="../_next/static/css/8a274f3d4edb6eb9.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="../_next/static/chunks/webpack-94adde92c5e1c75b.js" defer=""></script><script src="../_next/static/chunks/framework-95c3ca6de27174dd.js" defer=""></script><script src="../_next/static/chunks/main-774d7304f1973b2a.js" defer=""></script><script src="../_next/static/chunks/pages/_app-fd8cb5a9775605f5.js" defer=""></script><script src="../_next/static/chunks/0f1ac474-1cd8c801d37caef0.js" defer=""></script><script src="../_next/static/chunks/1812-a871a62a8e8aa88c.js" defer=""></script><script src="../_next/static/chunks/9065-e24d5adee6cbba35.js" defer=""></script><script src="../_next/static/chunks/9614-b709f46f6d53dd35.js" defer=""></script><script src="../_next/static/chunks/8423-007cfdc371b27898.js" defer=""></script><script src="../_next/static/chunks/7846-ec0724342ae15ba3.js" defer=""></script><script src="../_next/static/chunks/pages/blog/[id]-4165e8a9d7b23d0c.js" defer=""></script><script src="../_next/static/-oT8_WeW9m_lAxr26cJvY/_buildManifest.js" defer=""></script><script src="../_next/static/-oT8_WeW9m_lAxr26cJvY/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Rubik&display=swap">@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4i1Uw.woff) format('woff')}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFWkUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFU0UzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFVUUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFWUUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFV0UzdYPFkZVO.woff) format('woff');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href="https://fonts.googleapis.com/css2?family=Outfit:wght@400;500;600;700&display=swap">@font-face{font-family:'Outfit';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4TC1C4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4QK1C4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4e6yC4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4deyC4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,400;0,700;1,400;1,700&display=swap">@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTeEw.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINscg.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKtdU.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7MIU.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGqZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuE6ZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuFKZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGKZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGaZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuF6ZJW9XjDg.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINce_fuJGl18QRY.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINccvfuJGl18QRY.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcdffuJGl18QRY.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINceffuJGl18QRY.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcePfuJGl18QRY.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcdvfuJGl18Q.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdzeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdXeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdLeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhd7eFaxOedfTDw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhd_eFaxOedfTDw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdHeFaxOedc.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIxsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIVsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIJsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AI5sdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AI9sdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIFsdP3pBms.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body class=""><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-P8H6KQG" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next" data-reactroot=""><svg xmlns="http://www.w3.org/2000/svg" class="sr-only"><symbol id="svg-arrow-right-gradient"><path d="M1.5 7.5h18" stroke="url(#paint0_linear_3927_132230)" stroke-width="1.5" stroke-linecap="round"></path><path d="m16.6562 3.65625 3.8442 3.84412-3.8442 3.84413" stroke="url(#paint1_linear_3927_132230)" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><defs><linearGradient id="paint0_linear_3927_132230" x1="1.06972" y1="8.58929" x2="12.2319" y2="-1.53819" gradientUnits="userSpaceOnUse"><stop stop-color="#8B20BE"></stop><stop offset="1" stop-color="#3579FF" stop-opacity=".97"></stop></linearGradient><linearGradient id="paint1_linear_3927_132230" x1="12.377" y1="7.75171" x2="16.9307" y2="11.8684" gradientUnits="userSpaceOnUse"><stop stop-color="#8B20BE"></stop><stop offset="1" stop-color="#3579FF" stop-opacity=".97"></stop></linearGradient></defs></symbol></svg><div class="Banner_container__zEmFV"><a href="https://raysummit.anyscale.com/?utm_source=anyscale&amp;utm_medium=website&amp;utm_campaign=ray_summit_2023&amp;utm_content=homepage_ticker" rel="noreferrer nofollow" target="_blank"><span class="Banner_banner-text__wr6Xi"><span class="Banner_main-text__cpJ6H">Are you passionate about Ray, scalable AI and distributed computing? Join us at Ray Summit!</span><span class="Banner_cta-button__0JCtt"> Find out more! <!-- --> <svg width="23" height="14" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1.5 6.75a.75.75 0 0 0 0 1.5v-1.5Zm18 1.5a.75.75 0 0 0 0-1.5v1.5Zm-18 0h18v-1.5h-18v1.5Z" fill="#fff"></path><path d="M17.187 3.126a.75.75 0 0 0-1.061 1.06l1.06-1.06ZM20.5 7.5l.53.53a.75.75 0 0 0 0-1.06l-.53.53Zm-4.374 3.314a.75.75 0 1 0 1.06 1.06l-1.06-1.06Zm0-6.627L19.97 8.03l1.06-1.061-3.843-3.844-1.061 1.06ZM19.97 6.97l-3.844 3.844 1.06 1.06 3.845-3.843L19.97 6.97Z" fill="#fff"></path></svg></span></span></a><button class="Banner_close-button__9ksfY" type="button" aria-label="Close Banner"></button></div><header class="inner-shadow sticky top-0 z-[1000] h-24 transform py-5 transition-transform duration-500 bg-primary translate-y-0"><div class="container mx-auto flex items-center justify-between"><a href="../index.html" class="text-white"><svg width="141" height="32" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><g clip-path="url(#anyscale-logo_svg__a)" fill="currentColor"><path d="M10.653 17.46H3.865C1.697 17.46 0 19.16 0 21.332v6.797c0 2.17 1.697 3.87 3.865 3.87h6.788c2.169 0 3.866-1.7 3.866-3.87V21.33c0-2.171-1.792-3.87-3.866-3.87Zm1.509 10.573c0 .85-.66 1.51-1.509 1.51H3.865a1.49 1.49 0 0 1-1.508-1.51v-6.796c0-.85.66-1.51 1.508-1.51h6.788c.849 0 1.509.66 1.509 1.51v6.796Z"></path><path d="M28.095 11.988h-8.108V3.87C19.892 1.7 18.196 0 16.027 0H3.865C1.697 0 0 1.7 0 3.87v6.608c0 2.171 1.697 3.87 3.865 3.87h13.67v13.687c0 2.171 1.698 3.87 3.866 3.87h6.6c2.168 0 3.865-1.699 3.865-3.87V15.858c0-2.076-1.697-3.775-3.771-3.87Zm-24.23 0a1.49 1.49 0 0 1-1.508-1.51V3.87c0-.85.66-1.51 1.508-1.51h12.162c.849 0 1.509.66 1.509 1.51v8.118H3.866Zm25.738 16.047c0 .85-.66 1.51-1.508 1.51h-6.6a1.49 1.49 0 0 1-1.508-1.51V14.348h8.108c.848 0 1.508.66 1.508 1.51v12.177ZM79.194 17.462l-3.017-8.118h-3.3l4.714 11.327-2.546 5.853h3.112l7.165-17.18H82.21l-3.016 8.118ZM93.335 14.819c-.66-.284-1.414-.472-2.075-.661-.66-.189-1.225-.378-1.79-.66-.378-.19-.566-.473-.566-.85 0-.378.188-.661.472-.85.847-.472 1.79-.378 2.639.094.377.284.566.661.566 1.133h2.828c0-1.133-.566-2.171-1.414-2.832-.943-.755-2.169-1.133-3.3-1.038-.849 0-1.697.094-2.45.472-.66.283-1.227.66-1.604 1.227-.377.472-.565 1.133-.565 1.794 0 .66.188 1.321.565 1.887.377.472.849.85 1.414 1.039.66.283 1.414.472 2.168.66.66.19 1.226.378 1.886.661.377.19.566.472.566.85 0 .377-.189.66-.471.85-.377.283-.85.377-1.32.377-.472 0-1.038-.094-1.415-.472-.377-.283-.565-.66-.66-1.133h-2.922c0 .755.282 1.416.754 1.982.472.567 1.131 1.133 1.791 1.416.849.378 1.698.567 2.546.567.849 0 1.697-.095 2.45-.472.661-.283 1.227-.66 1.604-1.227.377-.567.565-1.133.565-1.794 0-.66-.188-1.321-.66-1.888-.565-.66-1.037-.944-1.602-1.132ZM102.575 11.702c.566 0 1.036.095 1.508.472.377.283.66.755.849 1.227h3.205a5.366 5.366 0 0 0-1.886-3.02 5.348 5.348 0 0 0-3.582-1.039c-1.037 0-2.074.284-3.017.755a5.486 5.486 0 0 0-2.074 2.077 7.427 7.427 0 0 0 0 6.325c.472.85 1.131 1.604 2.074 2.076.943.472 1.98.756 3.017.756 1.32 0 2.546-.378 3.582-1.133a5.366 5.366 0 0 0 1.886-3.02h-3.111c-.283 1.037-1.32 1.698-2.357 1.604-.754 0-1.509-.283-1.98-.944a5.119 5.119 0 0 1 0-5.192 1.914 1.914 0 0 1 1.886-.944ZM118.412 10.1a.405.405 0 0 1-.377.378c-.094 0-.188 0-.188-.095a3.382 3.382 0 0 0-.944-.66c-.659-.378-1.507-.567-2.262-.567-.943 0-1.885.283-2.734.755a4.459 4.459 0 0 0-1.885 2.077c-.472.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.376.85 1.037 1.604 1.885 2.17.849.473 1.697.756 2.64.756.754 0 1.603-.189 2.263-.567.659-.283 1.131-.755 1.602-1.321v1.699h2.923V9.345h-2.923v.755Zm-.377 6.891c-.283.472-.66.944-1.132 1.227-.47.283-1.037.378-1.507.378-.567 0-1.037-.189-1.509-.378-.472-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.094-1.228.472-1.794.848-1.51 2.64-1.982 4.147-1.227h.095c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM55.907 10.1a.406.406 0 0 1-.377.378c-.095 0-.19 0-.19-.095a3.384 3.384 0 0 0-.942-.66c-.66-.378-1.508-.567-2.263-.567-.942 0-1.885.283-2.733.755a4.463 4.463 0 0 0-1.886 2.077c-.471.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.377.85 1.037 1.604 1.886 2.17.848.473 1.697.756 2.64.756.753 0 1.602-.189 2.262-.567.66-.283 1.131-.755 1.603-1.321v1.699h2.922V9.345h-2.922v.755Zm-.472 6.891c-.283.472-.66.944-1.131 1.227-.471.283-1.037.378-1.509.378-.565 0-1.037-.189-1.508-.378-.471-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.095-1.228.472-1.794.848-1.51 2.64-1.982 4.148-1.227h.094c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM126.708 5.477h-2.923v15.575h2.923V5.477ZM140.191 14.914c0-1.038-.189-2.077-.755-3.02-.471-.85-1.131-1.605-2.074-1.983a6.78 6.78 0 0 0-6.033 0 5.485 5.485 0 0 0-2.075 2.077c-.565 1.038-.754 2.077-.754 3.21 0 1.132.189 2.17.754 3.114a5.485 5.485 0 0 0 2.075 2.077c.942.472 1.979.755 3.016.755a6.56 6.56 0 0 0 3.489-1.038c.943-.661 1.602-1.7 1.885-2.738h-3.016c-.377.944-1.321 1.51-2.358 1.416-.659 0-1.414-.283-1.885-.755-.566-.472-.848-1.227-.848-1.982h8.484c.095-.378.095-.755.095-1.133Zm-8.579-.85c.093-.755.377-1.415.942-1.887.566-.472 1.226-.661 1.886-.661.754 0 1.414.189 1.979.66.566.473.849 1.133.849 1.794l-5.656.095ZM67.692 9.159a4.7 4.7 0 0 0-1.98.472c-.377.188-.754.377-1.037.66a.286.286 0 0 1-.377 0c-.094-.094-.094-.094-.094-.188v-.756H61.28v11.705h2.923v-6.418c-.094-.755.188-1.51.754-2.171.471-.472 1.226-.756 1.886-.756.66 0 1.414.284 1.885.755.472.567.754 1.322.66 2.172v6.418h2.923v-6.89c.094-1.322-.283-2.644-1.226-3.682-.848-.85-2.168-1.416-3.394-1.321Z"></path></g><defs><clipPath id="anyscale-logo_svg__a"><path fill="currentColor" d="M0 0h140.19v32H0z"></path></clipPath></defs></svg></a><div class="lg:relative lg:flex lg:flex-row hidden"><div class="max-h-[calc(100vh-192px)] items-center overflow-y-auto py-[1px] shadow-[0_-1px_0_#DAE3F2] lg:flex lg:overflow-visible lg:p-0 lg:shadow-none"><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Products</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="../platform.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Anyscale Platform</div></a><a href="../ray-open-source.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Open Source</div></a></div></div></div></div></div></div><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Solutions</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="container mx-auto mb-4 lg:mb-0 mt-2 lg:mt-0 lg:mr-12"><div class="header-card h-full rounded-[5px] p-6 lg:w-[296px]"><h5 id="scalable-ai-and-python">Scalable AI and Python</h5>
<p>Leverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more.</p></div></div><div class="grid lg:flex grid-cols-1 md:grid-cols-2 gap-x-10 lg:gap-x-12 container mx-auto"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><p class="text-base leading-normal font-normal text-darkGrey-300 px-6 lg:px-4 mb-4">Scalable AI and Python</p><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2"><a href="../data-ingestion.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Data Ingestion</div></a><a href="../reinforcement-learning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Reinforcement Learning</div></a><a href="../ray-air.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray AIR</div></a><a href="../model-serving.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Model Serving</div></a><a href="../hyperparameter-tuning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Hyperparameter Tuning</div></a></div></div></div><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><p class="text-base leading-normal font-normal text-darkGrey-300 px-6 lg:px-4 mb-4">Use Cases</p><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2"><a href="../large-language-models.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Large Language Models</div></a><a href="../demand-forecasting.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Demand Forecasting/Pricing</div></a><a href="../industrial-automation.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Industrial Automation</div></a><a href="../machine-learning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Scalable ML Platforms</div></a><a href="../natural-language-processing.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">NLP</div></a><a href="../recommendation-system.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Recommendation System</div></a></div></div></div></div></div></div></div><a class="block font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4 shadow-[0_1px_0_#DAE3F2] lg:shadow-none" href="../ray-ecosystem.html"><div class="w-full container mx-auto">Ecosystem</div></a><a class="block font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4 shadow-[0_1px_0_#DAE3F2] lg:shadow-none" href="../user-stories.html"><div class="w-full container mx-auto">Success Stories</div></a><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Learn</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="how-ray-solves-common-production-challenges-for-generative-ai.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Blog</div></a><a href="https://raysummit.anyscale.com/?utm_source=anyscale&amp;utm_medium=website&amp;utm_campaign=ray_summit_2023&amp;utm_content=home_nav-pulldown"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Summit 2023</div></a><a href="../event-category/rl-summit.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Videos &amp; Webinars</div></a><a href="https://github.com/ray-project/ray-educational-materials"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Training</div></a><a href="https://docs.anyscale.com"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Anyscale Docs</div></a></div></div></div></div></div></div><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Company</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="../about.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">About us</div></a><a href="../press.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">News</div></a><a href="../careers.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Careers</div></a><a href="../community.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Community</div></a></div></div></div></div></div></div></div><div class="absolute bottom-0 left-0 right-0 bg-white py-5 shadow-[0_-1px_0_#DAE3F2] lg:relative lg:ml-5 lg:bg-transparent lg:p-0 lg:shadow-none"><div class="container mx-auto flex justify-end"><a class="btn bg-gradient min-w-[320px] justify-center bg-blue-900 text-center lg:min-w-0" href="../beta.html">Try It Now</a></div></div></div><div class="cursor-pointer py-4 pl-6 lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M20.05 11H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95ZM20.05 16H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95ZM20.05 6H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95Z" fill="#fff"></path></svg></div></div></header><div class="Layout_page-sections__2__gb overflow-x-hidden"><div class="BlogPost_root__ly_um"><div class="Breadcrumbs_root__53EXt"><a class="Breadcrumbs_link__HBYkg" href="../index.html">Home</a><a class="Breadcrumbs_link__HBYkg" href="how-ray-solves-common-production-challenges-for-generative-ai.html">Blog</a><span class="Breadcrumbs_link__HBYkg">Blog Detail</span></div><div class="BlogPost_inner__70rgb"><div class="BlogPost_main__husly"><div class="container"><div class="ArticleHero_inner__lg98I"><h1 class="ArticleHero_title__JB2va">Streaming distributed execution across CPUs and GPUs</h1><div class="ArticleHero_details__DZgPt"><div class="container"><span class="ArticleDetails_authors__aqDWy"><span>By </span><a href="../blog@author=eric-liang.html">Eric Liang</a><span>, </span><a href="../blog@author=stephanie-wang.html">Stephanie Wang</a><span> and </span><a href="../blog@author=cheng-su.html">Cheng Su</a><span>   </span></span><span class="ArticleDetails_article-published-tags__9VxRY"><span class="article-published">|   <!-- -->May 11, 2023</span></span></div></div><div class="ArticleHero_image-intro__mj4hS"></div></div></div><div class="ArticleBody_container__QoiWj ArticleBody_page_article__R6nNl"><div class="ArticleBody_inner__ml2H8"><p><i>In a </i><a href="offline-batch-inference-comparing-ray-apache-spark-and-sagemaker.html"><u><i>previous blog,</i></u></a><i> we showed how Ray Data can provide speedups over SageMaker and frameworks like Apache Spark for large-scale batch inference workloads. This blog post delves further into how Ray Data streaming works and how to use it for your own ML pipelines.</i></p><p>Some of the most demanding machine learning (ML) use cases we have encountered involve pipelines that span both CPU and GPU devices in distributed environments. These situations arise in various workloads, including:</p><ul><li><p>Batch inference, which involves a CPU-intensive preprocessing stage (e.g., video decoding or image resizing) before utilizing a GPU-intensive model to make predictions.</p></li><li><p>Distributed training, where similar CPU-heavy transformations are required to prepare or augment the dataset prior to GPU training.</p></li></ul><p>Interestingly, in many of these workloads, the preprocessing steps often prove to be the bottleneck. This can happen when preprocessing requires <b>parallelism across multiple nodes</b> and <b>significant memory to buffer results</b> between the CPU and GPU.</p><p>For instance, consider the decoding of compressed images in memory. A typical JPEG decompression ratio is 10:1, which implies that the output memory size may be ten times that of the input, resulting in substantial memory pressure in addition to CPU load. The challenges become more complex with other data modalities, such as video; for example, H264 can decompress at a 2000:1 ratio, producing 200GB of frame outputs for a 100MB file input. This means that practitioners trying to offload CPU-heavy preprocessing onto multiple CPUs or machines have to deal with intermediate results that are far larger than their already sizable source datasets:</p><div class="ArticleBody_image__rd3Dj" style="width:1493px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/7ntuAY4zsUXtrtIxSofnyg/1fdb9631162d6b66777d205d1d3a7e1c/Screenshot_from_2023-05-10_11-00-43.png" alt="streaming_figure_1"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">Figure 1. Decoding of compressed images with large images spilled to disk</span></div><p>Executed naively, these compute and memory-intensive workloads fail to fully utilize expensive GPU hardware. For example, we could buffer 200GB of frame outputs on disk, but that could add minutes of overheads. Even if this is overlapped with GPU computation, we can end up with long pauses on the GPU. This motivates fully streamed execution in the cluster setting, which avoids such delays by streaming intermediate data through cluster memory:</p><div class="ArticleBody_image__rd3Dj" style="width:1199px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/40VpN8OKxEM7wHohV802gE/c07086bfe677dc5e3109496f5a4ec083/Screenshot_from_2023-05-10_11-01-00.png" alt="streaming_figure_2"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">Figure 2. Decoding intermediate data streamed through memory</span></div><p>In this blog post, we guide you through our development of a versatile streaming backend for Ray Data, and show examples of how to implement the demanding use cases mentioned earlier. We discuss the performance benefits of implementing pipelined execution across CPUs and GPUs compared to bulk processing, and discuss its broader applicability to various workloads, including distributed training.</p><h2>Pipelined execution in a cluster</h2><p>To understand the necessity and advantages of pipelined (or streaming) execution across CPU and GPU devices, let&#x27;s first examine how a <a href="https://en.wikipedia.org/wiki/Bulk_synchronous_parallel"><u>bulk synchronous parallel</u></a> (BSP) framework might handle batch inference. Because of its simplicity and generality, BSP is a common way frameworks (e.g., MapReduce, Apache Spark) parallelize distributed computations.</p><p>Note that pipelining within a single machine is already commonly used in data loading libraries such as <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><u>Torch DataLoader</u></a> or <a href="https://petastorm.readthedocs.io/en/latest/index.html"><u>Petastorm.</u></a> Here, we discuss pipelining computations across an entire cluster of machines.</p><p>A typical batch inference job consists of the following operations:</p><div class="ArticleBody_image__rd3Dj" style="width:1526px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/6mkS4HlXNovPW6Dl387jYe/1a707558e51ac00b776ae6f5d808b74e/Screenshot_from_2023-05-10_12-23-58.png" alt="streaming_figure_3"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">Figure 3. A typical batch inference job sequential operations</span></div><p>To implement such an inference job, you&#x27;d write the code using Ray Data (or another similar framework):</p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">3
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">4
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">5
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">6
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">7
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">8
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">9
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">10
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">11
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">12
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">13
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">14
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">15
</span></code><span style="display:block"><span style="color:#c678dd">import</span><span> ray
</span></span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#5c6370"># Define model and preprocessor.</span><span>
</span></span><span style="display:block">model = ...
</span><span style="display:block">preprocess_fn = ...
</span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#5c6370"># Define the inference function.</span><span>
</span></span><span style="display:block"><span></span><span class="hljs-function" style="color:#c678dd">def</span><span class="hljs-function"> </span><span class="hljs-function" style="color:#61aeee">model_fn</span><span class="hljs-function">(</span><span class="hljs-function hljs-params">batch: </span><span class="hljs-function hljs-params" style="color:#d19a66">Dict</span><span class="hljs-function hljs-params">[</span><span class="hljs-function hljs-params" style="color:#e6c07b">str</span><span class="hljs-function hljs-params">, np.ndarray]</span><span class="hljs-function">):</span><span>
</span></span><span style="display:block"><span>    </span><span style="color:#c678dd">return</span><span> {</span><span style="color:#98c379">&quot;results&quot;</span><span>: model(batch)}
</span></span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#5c6370"># The following snippet implements the pipeline.</span><span>
</span></span><span style="display:block"><span>ray.data.read_parquet(...) \           </span><span style="color:#5c6370"># 1. Load</span><span>
</span></span><span style="display:block"><span>    .map_batches(preprocess_fn) \      </span><span style="color:#5c6370"># 2. Preprocess</span><span>
</span></span><span style="display:block"><span>    .map_batches(model_fn) \           </span><span style="color:#5c6370"># 3. Inference</span><span>
</span></span><span style="display:block"><span>    .write_parquet(...)                </span><span style="color:#5c6370"># 4. Save</span><span>
</span></span><span style="display:block">
</span></code></pre></div><p>In a BSP framework, each of these operations can be modeled as a 1:1 transformation, which can be fused together into a single stage. Each task within the stage will execute the mentioned operations locally in sequence:</p><div class="ArticleBody_image__rd3Dj" style="width:1511px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/4oQ40RlSK1AF7Y4ZnVGV8J/a0413e90a55ec69d7d0ba49554a7f2a0/Screenshot_from_2023-05-10_11-07-39.png" alt="streaming_figure_4"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">Figure 4. BSP framework operations modeled as a 1:1 transformation</span></div><p>This execution strategy is <b>memory optimal</b>, as there is no intermediate data and execution is inherently pipelined (since there is only a single stage). However, challenges emerge in a heterogeneous setting, where certain pipeline steps may prefer to run on CPUs or GPUs independently. In Ray Data, such inference pipelines are expressed with transformations that launch actors scheduled onto GPUs <code>(num_gpus=1 and compute=ActorPoolStrategy)</code>:</p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">3
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">4
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">5
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">6
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">7
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">8
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">9
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">10
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">11
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">12
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">13
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">14
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">15
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">16
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">17
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">18
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">19
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">20
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">21
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">22
</span></code><span style="display:block"><span style="color:#c678dd">import</span><span> ray
</span></span><span style="display:block"><span></span><span style="color:#c678dd">from</span><span> ray.data </span><span style="color:#c678dd">import</span><span> ActorPoolStrategy
</span></span><span style="display:block">
</span><span style="display:block">Model = ...
</span><span style="display:block">preprocess_fn = ...
</span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#5c6370"># Define the model as a stateful class with cached setup.</span><span>
</span></span><span style="display:block"><span></span><span class="hljs-class" style="color:#c678dd">class</span><span class="hljs-class"> </span><span class="hljs-class" style="color:#61aeee">MyModelCallableCls</span><span class="hljs-class">:</span><span>
</span></span><span style="display:block"><span>    </span><span class="hljs-function" style="color:#c678dd">def</span><span class="hljs-function"> </span><span class="hljs-function" style="color:#61aeee">__init__</span><span class="hljs-function">(</span><span class="hljs-function hljs-params">self</span><span class="hljs-function">):</span><span>
</span></span><span style="display:block">        self.model = Model(...)
</span><span style="display:block">
</span><span style="display:block"><span>    </span><span class="hljs-function" style="color:#c678dd">def</span><span class="hljs-function"> </span><span class="hljs-function" style="color:#61aeee">__call__</span><span class="hljs-function">(</span><span class="hljs-function hljs-params">self, batch: </span><span class="hljs-function hljs-params" style="color:#d19a66">Dict</span><span class="hljs-function hljs-params">[</span><span class="hljs-function hljs-params" style="color:#e6c07b">str</span><span class="hljs-function hljs-params">, np.ndarray]</span><span class="hljs-function">) -&gt;
</span></span><span style="display:block"><span class="hljs-function">        </span><span class="hljs-function" style="color:#c678dd">return</span><span class="hljs-function"> {&quot;results&quot;:</span><span> self.model(batch)}
</span></span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#5c6370"># Modify the pipeline to use GPU actors.</span><span>
</span></span><span style="display:block">ray.data.read_parquet(...) \
</span><span style="display:block">    .map_batches(preprocess_fn) \
</span><span style="display:block">    .map_batches(
</span><span style="display:block">          MyModelCallableCls,
</span><span style="display:block"><span>          num_gpus=</span><span style="color:#d19a66">1</span><span>,
</span></span><span style="display:block">          compute=ActorPoolStrategy(size=N)) \
</span><span style="display:block">    .write_parquet(...)
</span></code></pre></div><p>In the above code snippet, the load and preprocessing steps run on CPU (Stage 1), inference runs on GPUs (Stage 2), and then the result saving runs on CPU again (Stage 3). This configuration leads to spilling of data to remote storage when intermediate results (e.g., decoded video frames) exceed cluster memory sizes. Hence, we see that <b>BSP is not memory optimal for heterogeneous workloads</b>:</p><div class="ArticleBody_image__rd3Dj" style="width:1750px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/66FIu8nzl2kZRB1Ht5xZFL/295ab9176fab5d04ee47ac53c5e1b418/Screenshot_from_2023-05-10_11-02-18.png" alt="streaming_figure_5"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">Figure 5. BSP is not memory optimal for heterogeneous workloads</span></div><p>These unnecessary overheads can be avoided with end-to-end pipelining (i.e., streaming) across the cluster, which we present in the next section.</p><h2>Ray Data streaming</h2><p>In Ray 2.4, we&#x27;ve changed the default execution strategy of Ray Data to streaming from bulk synchronous. These streaming Datasets are fully backwards compatible with the existing API, i.e., they can be transformed lazily with map operations, support shuffle operations, and also caching / materialization in memory:</p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">3
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">4
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">5
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">6
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">7
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">8
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">9
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">10
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">11
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">12
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">13
</span></code><span style="display:block"><span style="color:#5c6370"># Create a dataset over parquet files</span><span>
</span></span><span style="display:block">ds: ray.data.Dataset = ray.data.read_parquet(...)
</span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#5c6370"># Transform the dataset</span><span>
</span></span><span style="display:block">ds = ds.map_batches(my_preprocess_fn)
</span><span style="display:block">ds = ds.map_batches(my_model_fn)
</span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#5c6370"># Iterate over dataset batches in streaming fashion</span><span>
</span></span><span style="display:block"><span></span><span style="color:#c678dd">for</span><span> batch </span><span style="color:#c678dd">in</span><span> ds.iter_batches():
</span></span><span style="display:block"><span>   </span><span style="color:#e6c07b">print</span><span>(batch)
</span></span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#5c6370"># Materialize all contents of the dataset in memory</span><span>
</span></span><span style="display:block">ds = ds.materialize()
</span></code></pre></div><p>In other words, the Ray Data API now leverages streaming execution for improved performance on large datasets, with the same simple transformation API as in previous Ray versions.</p><h2>Video processing example</h2><p>To more fully understand the capabilities of this API, let&#x27;s walk through what this looks like in an example video processing pipeline. In the pipeline, we&#x27;ll first decode video frames, annotate each frame with an ML model, apply a classification model to the annotated frames, and then save the results to JSON:</p><div class="ArticleBody_image__rd3Dj" style="width:1478px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/s1kmF3kQraHomGuDsvOQt/9385b71522afe76eebc6342f54387ffa/Screenshot_from_2023-05-10_12-24-36.png" alt="streaming_figure_6"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">Figure 6. A pipeline decoding video frames, annotating frames, applying classification, and saving results to JSON.
</span></div><p>Let&#x27;s start by declaring the components of our pipeline. For brevity, we&#x27;ll just provide stub classes here, but you can find the <a href="https://gist.github.com/ericl/ac086f0a9bdf41b6101dca665045dfec"><u>full skeleton here</u></a> (and the <a href="https://docs.ray.io/en/master/data/batch_inference.html"><u>batch inference docs</u></a>) that can be adapted to your own workloads. Data between these steps is kept in dicts of numpy arrays:</p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">3
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">4
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">5
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">6
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">7
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">8
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">9
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">10
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">11
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">12
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">13
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">14
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">15
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">16
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">17
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">18
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">19
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">20
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">21
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">22
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">23
</span></code><span style="display:block"><span style="color:#5c6370"># Declare a function that decodes frames.</span><span>
</span></span><span style="display:block"><span></span><span class="hljs-function" style="color:#c678dd">def</span><span class="hljs-function"> </span><span class="hljs-function" style="color:#61aeee">decode_frames</span><span class="hljs-function">(</span><span class="hljs-function hljs-params">batch: </span><span class="hljs-function hljs-params" style="color:#d19a66">Dict</span><span class="hljs-function hljs-params">[</span><span class="hljs-function hljs-params" style="color:#e6c07b">str</span><span class="hljs-function hljs-params">, np.ndarray]</span><span class="hljs-function">) -&gt; </span><span class="hljs-function" style="color:#d19a66">Dict</span><span class="hljs-function">:</span><span>
</span></span><span style="display:block"><span>    video_data = batch[</span><span style="color:#98c379">&quot;bytes&quot;</span><span>]
</span></span><span style="display:block">    ...
</span><span style="display:block"><span>    </span><span style="color:#c678dd">return</span><span> {</span><span style="color:#98c379">&quot;frames&quot;</span><span>: decoded_frames}
</span></span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#5c6370"># Declare a model that annotates decoded frame data.</span><span>
</span></span><span style="display:block"><span></span><span class="hljs-class" style="color:#c678dd">class</span><span class="hljs-class"> </span><span class="hljs-class" style="color:#61aeee">FrameAnnotator</span><span class="hljs-class">:</span><span>
</span></span><span style="display:block"><span>    </span><span class="hljs-function" style="color:#c678dd">def</span><span class="hljs-function"> </span><span class="hljs-function" style="color:#61aeee">__init__</span><span class="hljs-function">(</span><span class="hljs-function hljs-params">self</span><span class="hljs-function">):</span><span>
</span></span><span style="display:block">        self.model = ...
</span><span style="display:block">
</span><span style="display:block"><span>    </span><span class="hljs-function" style="color:#c678dd">def</span><span class="hljs-function"> </span><span class="hljs-function" style="color:#61aeee">__call__</span><span class="hljs-function">(</span><span class="hljs-function hljs-params">self, batch: </span><span class="hljs-function hljs-params" style="color:#d19a66">Dict</span><span class="hljs-function hljs-params">[</span><span class="hljs-function hljs-params" style="color:#e6c07b">str</span><span class="hljs-function hljs-params">, np.ndarray]</span><span class="hljs-function">) -&gt; </span><span class="hljs-function" style="color:#d19a66">Dict</span><span class="hljs-function">:</span><span>
</span></span><span style="display:block"><span>        frames = batch[</span><span style="color:#98c379">&quot;frames&quot;</span><span>]
</span></span><span style="display:block"><span>        </span><span style="color:#c678dd">return</span><span> {</span><span style="color:#98c379">&quot;annotated_frames&quot;</span><span>: self.model(frames)}
</span></span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#5c6370"># Declare a model that classifies annotated frame data.</span><span>
</span></span><span style="display:block"><span></span><span class="hljs-class" style="color:#c678dd">class</span><span class="hljs-class"> </span><span class="hljs-class" style="color:#61aeee">FrameClassifier</span><span class="hljs-class">:</span><span>
</span></span><span style="display:block"><span>    </span><span class="hljs-function" style="color:#c678dd">def</span><span class="hljs-function"> </span><span class="hljs-function" style="color:#61aeee">__init__</span><span class="hljs-function">(</span><span class="hljs-function hljs-params">self</span><span class="hljs-function">):</span><span>
</span></span><span style="display:block">        self.model = ...
</span><span style="display:block">
</span><span style="display:block"><span>    </span><span class="hljs-function" style="color:#c678dd">def</span><span class="hljs-function"> </span><span class="hljs-function" style="color:#61aeee">__call__</span><span class="hljs-function">(</span><span class="hljs-function hljs-params">self, batch: </span><span class="hljs-function hljs-params" style="color:#d19a66">Dict</span><span class="hljs-function hljs-params">[</span><span class="hljs-function hljs-params" style="color:#e6c07b">str</span><span class="hljs-function hljs-params">, np.ndarray]</span><span class="hljs-function">) -&gt; </span><span class="hljs-function" style="color:#d19a66">Dict</span><span class="hljs-function">:</span><span>
</span></span><span style="display:block"><span>        frames = batch[</span><span style="color:#98c379">&quot;annotated_frames&quot;</span><span>]
</span></span><span style="display:block"><span>        </span><span style="color:#c678dd">return</span><span> {</span><span style="color:#98c379">&quot;results&quot;</span><span>: self.model(frames)}</span></span></code></pre></div><p>Next, we&#x27;ll use Ray Data to connect these steps together. This takes just a few lines of code, and we can customize the resource requirements per step to configure CPUs and GPUs:</p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">3
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">4
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">5
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">6
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">7
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">8
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">9
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">10
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">11
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">12
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">13
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">14
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">15
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">16
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">17
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">18
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">19
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">20
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">21
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">22
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">23
</span></code><span style="display:block"><span style="color:#5c6370"># Create a dataset from video binary data.</span><span>
</span></span><span style="display:block">ds = ray.data.read_binary(...)
</span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#5c6370"># Apply the decode step. We can customize the resources per</span><span>
</span></span><span style="display:block"><span></span><span style="color:#5c6370"># task. Here each decode task requests 4 CPUs from Ray.</span><span>
</span></span><span style="display:block"><span>ds = ds.map_batches(decode_frames, num_cpus=</span><span style="color:#d19a66">4</span><span>)
</span></span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#5c6370"># Apply the annotation step, using an actor pool of size 5.</span><span>
</span></span><span style="display:block"><span></span><span style="color:#5c6370"># Each actor runs on a CPU.</span><span>
</span></span><span style="display:block">ds = ds.map_batches(
</span><span style="display:block">    FrameAnnotator,
</span><span style="display:block"><span>    compute=ActorPoolStrategy(size=</span><span style="color:#d19a66">5</span><span>))
</span></span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#5c6370"># Apply the classification step, using a pool of 2 GPU actors,</span><span>
</span></span><span style="display:block"><span></span><span style="color:#5c6370"># and a fixed data batch size of 64 for the actor calls.</span><span>
</span></span><span style="display:block">ds = ds.map_batches(
</span><span style="display:block">    FrameClassifier,
</span><span style="display:block"><span>    num_gpus=</span><span style="color:#d19a66">1</span><span>,
</span></span><span style="display:block"><span>    batch_size=</span><span style="color:#d19a66">64</span><span>,
</span></span><span style="display:block"><span>    compute=ActorPoolStrategy(size=</span><span style="color:#d19a66">2</span><span>))
</span></span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#5c6370"># Trigger execution and write outputs to json.</span><span>
</span></span><span style="display:block"><span>ds.write_json(</span><span style="color:#98c379">&quot;/tmp/output&quot;</span><span>)
</span></span><span style="display:block">
</span></code></pre></div><p>The configured streaming topology from above looks like this logically. Since each step has a unique resource requirement / task vs actor strategy, here every step becomes its own stage (Ray Data fuses steps with equivalent resource requirements):</p><div class="ArticleBody_image__rd3Dj" style="width:1445px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/2XaeSiV04QKLILBLAlD0t5/d75fceb54323c7587312028b40a4b37c/Screenshot_from_2023-05-10_11-02-51.png" alt="streaming_figure_7"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">Figure 7: Streaming execution across CPUs and GPUs</span></div><p>We run the above script (<a href="https://gist.github.com/ericl/ac086f0a9bdf41b6101dca665045dfec"><u>GitHub gist</u></a>) in an Anyscale workspace with 2 GPU nodes and 5 CPU nodes (you can use the <a href="https://docs.ray.io/en/master/cluster/getting-started.html"><u>Ray cluster launcher</u></a> to also launch a cluster for this):</p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-shell"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">3
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">4
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">5
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">6
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">7
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">8
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">9
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">10
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">11
</span></code><span style="display:block"><span style="color:#61aeee">$</span><span class="bash"> python workload.py</span><span>
</span></span><span style="display:block">
</span><span style="display:block">2023-05-02 15:10:01,105 INFO streaming_executor.py:91 -- Executing DAG
</span><span style="display:block">InputDataBuffer[Input] -&gt; TaskPoolMapOperator[MapBatches(decode_frames)] -&gt; 
</span><span style="display:block">ActorPoolMapOperator[MapBatches(FrameAnnotator)] -&gt;
</span><span style="display:block">ActorPoolMapOperator[MapBatches(FrameClassifier)] -&gt; 
</span><span style="display:block">TaskPoolMapOperator[Write]
</span><span style="display:block">2023-05-02 15:10:01,128 INFO actor_pool_map_operator.py:114 -- MapBatches(FrameAnnotator): Waiting for 5 pool actors to start...
</span><span style="display:block">2023-05-02 15:10:02,691 INFO actor_pool_map_operator.py:114 -- MapBatches(FrameClassifier): Waiting for 2 pool actors to start...
</span><span style="display:block">
</span><span style="display:block">Running: 25.0/112.0 CPU, 2.0/2.0 GPU, 33.19 GiB/32.27 GiB object_store_memory:  28%|███▍        | 285/1000 [01:40&lt;03:12,  3.71it/s]
</span></code></pre></div><p>While the workload is running, we can view the observability graphs in the <a href="https://docs.ray.io/en/master/ray-core/ray-dashboard.html"><u>Ray Dashboard</u></a>. We can see from the actor metrics that our 7 worker actors were active for the entire run (RUNNING_TASK), and that Ray Data kept them busy with MapBatches(FrameClassifier) as well as MapBatches(FrameAnnotator) tasks. Because of the streaming execution strategy, tasks for all stages run concurrently:</p><div class="ArticleBody_image__rd3Dj" style="width:1216px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/5MDgPW8470uVoy6MFBqowq/39f6300387d895145313c0caee8149cf/image3.png" alt="streaming_figure_8"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">Figure 8. View active tasks and actors for this job in the Ray Dashboard</span></div><p>We can also inspect hardware utilization in the Ray dashboard. We&#x27;ll focus on the network activity. We can see two nodes receiving ~500MiB/s (these are the two GPU nodes hosting the FrameClassifier actors), and a number of other nodes driving significant traffic from 100-200MiB/s, likely sending data to the GPU nodes or between the decode and FrameAnnotator steps:</p><div class="ArticleBody_image__rd3Dj" style="width:1635px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/6QEOgHncDDrSl3SKl9YzWF/bae6e33fce2749f2ade8afd73681e16b/Screenshot_from_2023-05-10_12-42-27.png" alt="streaming_figure_9"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">Figure 9. Ray Dashboard showing network activity</span></div><p>The overall pipeline finishes in ~5 minutes, successfully processing ~350GB of video frame data total on a heterogeneous cluster of CPUs and GPUs. We see that Ray Data was able to pipeline the entire execution across the cluster to make the best use of memory.</p><h2>Discussion</h2><h3>Optimizations</h3><p>In order to seamlessly execute such streaming topologies, Ray Data provides a number of optimizations under the hood. This includes:</p><p><b>Memory stability</b>: Ray Data relies on the underlying Ray scheduler to schedule tasks and actors, but still needs to manage back-pressure across the streaming topology to bound memory usage and avoid object store spilling. It does this by only scheduling new tasks if it would keep the streaming execution under <a href="https://docs.ray.io/en/master/data/data-internals.html#streaming-execution"><u>configured resources limits</u></a>. Intuitively, enforcing a cap on intermediate result memory usage is needed to avoid degrading to bulk execution.</p><p><b>Data locality</b>: While Ray will already place tasks on nodes where their input arguments are local, Ray Data&#x27;s streaming backend extends this to optimize the scheduling of actor tasks. For example, in the above example, a lot of network traffic is avoided between the `decode_frames` and `FrameAnnotator` steps by routing decoded frames to actors that are on the same node.</p><p><br/><b>Fault tolerance</b>: Ray Data leverages Ray&#x27;s built-in <a href="https://docs.ray.io/en/master/ray-core/fault-tolerance.html"><u>fault tolerance</u></a> in order to handle object loss in large jobs. When objects are lost, they are recomputed based on their task lineage tracked by Ray. If actors were needed to produce these objects, Ray restarts these actors prior to re-submitting the task.</p><h3>Scalability</h3><p>To give an idea of the upper scalability envelope of Ray Data streaming, we ran a number of synthetic benchmarks that stressed Ray&#x27;s object manager and Ray Data&#x27;s <a href="https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/execution/streaming_executor.py"><u>streaming executor</u></a> processing a 20TiB array dataset. Under the hood, Ray Data is using <code>ray.remote() / ray.wait() </code>to orchestrate the streaming topology, which means it is similarly scalable as Ray itself:</p><div class="ArticleBody_image__rd3Dj" style="width:1636px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/2vKUP0VZYl0yPNspQmfU05/0566c92beb5ea743d9ecc3404a551705/image9.png" alt="streaming_figure_10"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">Figure 10. Synthetic benchmarks stressing Ray's object manager and Ray Data's streaming executor processing a 20TiB array dataset</span></div><p>These benchmarks were run on a cluster of 500 machines, and test pipelines with between one to four stages similar to the video streaming example. We see that for the simplest pipeline on this cluster, Ray Data can process input data at a rate exceeding 1TiB/s. For more complex multi-stage pipelines, Ray Data can still sustain about 100-200GiB/s of end-to-end throughput on a large cluster.</p><h3>Distributed Training</h3><p>A significant portion of <a href="https://docs.ray.io/en/master/train/train.html"><u>Ray Train</u></a> users today are also using Ray Data as a distributed data backend for ML training. Ray Data streaming allows these users to efficiently work with datasets that are larger than cluster memory for training, leveraging both CPU and GPU nodes to speed up their jobs. We are working on enhancing Ray Train&#x27;s API to natively work with Ray Data streaming.</p><h2>What&#x27;s next?</h2><p>Today, early users are taking advantage of the Ray Data streaming backend to create efficient large-scale inference pipelines over unstructured data, including video and audio data. To learn more about how to build your own pipeline, check out the <a href="https://docs.ray.io/en/master/data/batch_inference.html"><u>batch inference tutorial</u></a> in our docs. We&#x27;ll also be following up with a blog on more unstructured application examples. To learn more generally about streaming in Ray Data, check out the <a href="https://docs.ray.io/en/master/data/data.html"><u>library docs</u></a>, and let us know what <a href="https://github.com/ray-project/ray/issues"><u>improvements</u></a> you&#x27;d like to see to Ray Data.</p><p>Also, we will be presenting at the <a href="https://raysummit.anyscale.com/">Ray Summit</a>. <a href="https://raysummit.anyscale.com/"><u>Register now</u></a> — early bird registration is open until<b> June 30, 2023.</b></p></div></div></div><div class="BlogPost_aside__BK_Wk"><div class="root"><h4 class="ArticleExtras_label__JQEEO">Sharing</h4><div class="ArticleExtras_sharing__LSXs1 ArticleExtras_section__26jYL"><a target="_blank" rel="noreferrer" href="streaming-distributed-execution-across-cpus-and-gpus.html" aria-label="Share on Facebook"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M24 12c0-6.628-5.372-12-12-12S0 5.372 0 12c0 5.99 4.388 10.955 10.125 11.855v-8.386H7.078V12h3.047V9.356c0-3.007 1.79-4.668 4.533-4.668 1.312 0 2.686.234 2.686.234v2.953H15.83c-1.49 0-1.955.926-1.955 1.875V12h3.328l-.532 3.469h-2.796v8.386C19.613 22.955 24 17.99 24 12Z" fill="#999"></path></svg><div class="ArticleExtras_active__cv689"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M24 12c0-6.628-5.372-12-12-12S0 5.372 0 12c0 5.99 4.388 10.955 10.125 11.855v-8.386H7.078V12h3.047V9.356c0-3.007 1.79-4.668 4.533-4.668 1.312 0 2.686.234 2.686.234v2.953H15.83c-1.49 0-1.955.926-1.955 1.875V12h3.328l-.532 3.469h-2.796v8.386C19.613 22.955 24 17.99 24 12Z" fill="#234999"></path></svg></div></a><a target="_blank" rel="noreferrer" href="streaming-distributed-execution-across-cpus-and-gpus.html" aria-label="Share on Twitter"><svg width="24" height="20" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M24 2.436c-.955.358-1.791.597-2.866.716 1.075-.597 1.791-1.552 2.15-2.746-.956.597-2.03.955-3.105 1.194-1.79-2.03-4.895-2.15-6.925-.239-1.314 1.194-1.91 2.985-1.433 4.776C7.88 5.9 4.299 3.988 1.79 1.003a4.305 4.305 0 0 0-.716 2.388c0 1.672.835 3.105 2.149 4.06-.717 0-1.433-.12-2.15-.597v.12c0 2.268 1.672 4.298 3.94 4.775-.477.12-.835.24-1.313.24-.358 0-.597 0-.955-.12.597 2.03 2.508 3.343 4.538 3.343-1.672 1.313-3.821 2.15-6.09 2.15-.358 0-.836 0-1.194-.12 2.269 1.433 4.896 2.268 7.522 2.268 7.642.12 13.851-6.089 13.97-13.73v-.837c1.075-.716 1.911-1.552 2.508-2.507Z" fill="#999"></path></svg><div class="ArticleExtras_active__cv689"><svg width="24" height="20" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M24 2.436c-.955.358-1.791.597-2.866.716 1.075-.597 1.791-1.552 2.15-2.746-.956.597-2.03.955-3.105 1.194-1.79-2.03-4.895-2.15-6.925-.239-1.314 1.194-1.91 2.985-1.433 4.776C7.88 5.9 4.299 3.988 1.79 1.003a4.305 4.305 0 0 0-.716 2.388c0 1.672.835 3.105 2.149 4.06-.717 0-1.433-.12-2.15-.597v.12c0 2.268 1.672 4.298 3.94 4.775-.477.12-.835.24-1.313.24-.358 0-.597 0-.955-.12.597 2.03 2.508 3.343 4.538 3.343-1.672 1.313-3.821 2.15-6.09 2.15-.358 0-.836 0-1.194-.12 2.269 1.433 4.896 2.268 7.522 2.268 7.642.12 13.851-6.089 13.97-13.73v-.837c1.075-.716 1.911-1.552 2.508-2.507Z" fill="#234999"></path></svg></div></a><a target="_blank" rel="noreferrer" href="streaming-distributed-execution-across-cpus-and-gpus.html" aria-label="Share on Linkedin"><svg width="24" height="25" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M22.191 0H1.809C.844 0 0 .724 0 1.688v20.624c0 1.085.844 1.809 1.809 1.809h20.382c.965 0 1.809-.724 1.809-1.689V1.688C24 .724 23.156 0 22.191 0ZM7.236 20.14H3.618V9.287h3.618v10.855ZM5.427 7.84c-.965 0-1.809-.845-1.809-1.93 0-1.086.844-1.93 1.93-1.93 1.085-.12 1.93.603 2.05 1.688.12 1.086-.724 2.05-1.688 2.171h-.483Zm14.955 12.3h-3.618v-5.788c0-1.447-.483-2.412-1.81-2.412-.843 0-1.567.482-1.808 1.326-.12.242-.12.603-.12.845v6.03H9.406V9.286h3.618v1.568c.724-1.206 1.93-1.809 3.256-1.809 2.413 0 4.222 1.568 4.222 4.945v6.15h-.121Z" fill="#999"></path></svg><div class="ArticleExtras_active__cv689"><svg width="24" height="25" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M22.191 0H1.809C.844 0 0 .724 0 1.688v20.624c0 1.085.844 1.809 1.809 1.809h20.382c.965 0 1.809-.724 1.809-1.689V1.688C24 .724 23.156 0 22.191 0ZM7.236 20.14H3.618V9.287h3.618v10.855ZM5.427 7.84c-.965 0-1.809-.845-1.809-1.93 0-1.086.844-1.93 1.93-1.93 1.085-.12 1.93.603 2.05 1.688.12 1.086-.724 2.05-1.688 2.171h-.483Zm14.955 12.3h-3.618v-5.788c0-1.447-.483-2.412-1.81-2.412-.843 0-1.567.482-1.808 1.326-.12.242-.12.603-.12.845v6.03H9.406V9.286h3.618v1.568c.724-1.206 1.93-1.809 3.256-1.809 2.413 0 4.222 1.568 4.222 4.945v6.15h-.121Z" fill="#234999"></path></svg></div></a></div><div class="tags-wrapper ArticleExtras_section__26jYL"><h4 class="ArticleExtras_label__JQEEO">Tags</h4><span class="ArticleExtras_tags__Q_xZs"><span class="tag">Ray Datasets</span><span>, </span><span class="tag">Ray Core</span></span></div><div class="ArticleExtras_form-wrapper__rTr1C"><h4 class="ArticleExtras_label__JQEEO">Sign up for product updates</h4><div class="HubspotEmailForm_root__Rwc_O HubspotEmailForm_condensed___nNtN"><div class="HubspotEmailForm_form___3bhH"><div id="hubspotForm"></div></div></div></div></div><div class="root"><h4 class="RecommendedContent_header__3Cuyf">Recommended content</h4><a class="RecommendedContent_item__e9DDs" href="introducing-the-anyscale-databricks-connector.html"><div class="RecommendedContent_thumbnail__bD3bd"><div class="Image_wrapper__beWl0"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/71IpYQvsxplh7okIL7RJVx/b193e65fbc2d6500e3ccb2162d323080/0_-_Updated_Databricks_Anyscale_Image.png" alt="0 -Anyscale Databricks headline image"/></div></div></div><div class="RecommendedContent_item-content__rXTae"><h4>Introducing the Anyscale Databricks Connector</h4><span>Read more<svg width="15" height="12" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1 6h12.222m0 0-5-5m5 5-5 5" stroke="#234999" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></div></a><a class="RecommendedContent_item__e9DDs" href="ray-2-5-features-training-and-serving-for-llms-multi-gpu-training-in-rllib.html"><div class="RecommendedContent_thumbnail__bD3bd"><div class="Image_wrapper__beWl0"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/2ty9UoIKnnNnrvsudSmAvH/d42b6c80596baf3b1cd13103bdc97a55/image1.png" alt="Ray_2.5_main_image"/></div></div></div><div class="RecommendedContent_item-content__rXTae"><h4>Ray 2.5 features training and serving for LLMs, Multi-GPU training in RLlib, and enhanced Ray Data support</h4><span>Read more<svg width="15" height="12" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1 6h12.222m0 0-5-5m5 5-5 5" stroke="#234999" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></div></a><a class="RecommendedContent_item__e9DDs" href="announcing-aviary-open-source-multi-llm-serving-solution.html"><div class="RecommendedContent_thumbnail__bD3bd"><div class="Image_wrapper__beWl0"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/1c2aG5pfLWU6YenPBZIHvI/742f647b979b4eb4b682e37fc35c71a7/aviary-light-mode.png" alt="Aviary Light Mode"/></div></div></div><div class="RecommendedContent_item-content__rXTae"><h4>Announcing Aviary: Open Source Multi-LLM Serving</h4><span>Read more<svg width="15" height="12" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1 6h12.222m0 0-5-5m5 5-5 5" stroke="#234999" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></div></a></div></div></div></div></div><footer class="relative bg-primary pt-30 pb-30 md:pt-36 md:pb-[180px] lg:pt-20"><div class="container mx-auto flex flex-col space-y-12 md:space-y-16 lg:flex-row lg:justify-between lg:space-y-0"><div class="flex flex-col gap-16"><div class="flex flex-col gap-6"><div class="h-10 text-white"><svg width="141" height="32" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="origin-top-left scale-125"><g clip-path="url(#anyscale-logo_svg__a)" fill="currentColor"><path d="M10.653 17.46H3.865C1.697 17.46 0 19.16 0 21.332v6.797c0 2.17 1.697 3.87 3.865 3.87h6.788c2.169 0 3.866-1.7 3.866-3.87V21.33c0-2.171-1.792-3.87-3.866-3.87Zm1.509 10.573c0 .85-.66 1.51-1.509 1.51H3.865a1.49 1.49 0 0 1-1.508-1.51v-6.796c0-.85.66-1.51 1.508-1.51h6.788c.849 0 1.509.66 1.509 1.51v6.796Z"></path><path d="M28.095 11.988h-8.108V3.87C19.892 1.7 18.196 0 16.027 0H3.865C1.697 0 0 1.7 0 3.87v6.608c0 2.171 1.697 3.87 3.865 3.87h13.67v13.687c0 2.171 1.698 3.87 3.866 3.87h6.6c2.168 0 3.865-1.699 3.865-3.87V15.858c0-2.076-1.697-3.775-3.771-3.87Zm-24.23 0a1.49 1.49 0 0 1-1.508-1.51V3.87c0-.85.66-1.51 1.508-1.51h12.162c.849 0 1.509.66 1.509 1.51v8.118H3.866Zm25.738 16.047c0 .85-.66 1.51-1.508 1.51h-6.6a1.49 1.49 0 0 1-1.508-1.51V14.348h8.108c.848 0 1.508.66 1.508 1.51v12.177ZM79.194 17.462l-3.017-8.118h-3.3l4.714 11.327-2.546 5.853h3.112l7.165-17.18H82.21l-3.016 8.118ZM93.335 14.819c-.66-.284-1.414-.472-2.075-.661-.66-.189-1.225-.378-1.79-.66-.378-.19-.566-.473-.566-.85 0-.378.188-.661.472-.85.847-.472 1.79-.378 2.639.094.377.284.566.661.566 1.133h2.828c0-1.133-.566-2.171-1.414-2.832-.943-.755-2.169-1.133-3.3-1.038-.849 0-1.697.094-2.45.472-.66.283-1.227.66-1.604 1.227-.377.472-.565 1.133-.565 1.794 0 .66.188 1.321.565 1.887.377.472.849.85 1.414 1.039.66.283 1.414.472 2.168.66.66.19 1.226.378 1.886.661.377.19.566.472.566.85 0 .377-.189.66-.471.85-.377.283-.85.377-1.32.377-.472 0-1.038-.094-1.415-.472-.377-.283-.565-.66-.66-1.133h-2.922c0 .755.282 1.416.754 1.982.472.567 1.131 1.133 1.791 1.416.849.378 1.698.567 2.546.567.849 0 1.697-.095 2.45-.472.661-.283 1.227-.66 1.604-1.227.377-.567.565-1.133.565-1.794 0-.66-.188-1.321-.66-1.888-.565-.66-1.037-.944-1.602-1.132ZM102.575 11.702c.566 0 1.036.095 1.508.472.377.283.66.755.849 1.227h3.205a5.366 5.366 0 0 0-1.886-3.02 5.348 5.348 0 0 0-3.582-1.039c-1.037 0-2.074.284-3.017.755a5.486 5.486 0 0 0-2.074 2.077 7.427 7.427 0 0 0 0 6.325c.472.85 1.131 1.604 2.074 2.076.943.472 1.98.756 3.017.756 1.32 0 2.546-.378 3.582-1.133a5.366 5.366 0 0 0 1.886-3.02h-3.111c-.283 1.037-1.32 1.698-2.357 1.604-.754 0-1.509-.283-1.98-.944a5.119 5.119 0 0 1 0-5.192 1.914 1.914 0 0 1 1.886-.944ZM118.412 10.1a.405.405 0 0 1-.377.378c-.094 0-.188 0-.188-.095a3.382 3.382 0 0 0-.944-.66c-.659-.378-1.507-.567-2.262-.567-.943 0-1.885.283-2.734.755a4.459 4.459 0 0 0-1.885 2.077c-.472.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.376.85 1.037 1.604 1.885 2.17.849.473 1.697.756 2.64.756.754 0 1.603-.189 2.263-.567.659-.283 1.131-.755 1.602-1.321v1.699h2.923V9.345h-2.923v.755Zm-.377 6.891c-.283.472-.66.944-1.132 1.227-.47.283-1.037.378-1.507.378-.567 0-1.037-.189-1.509-.378-.472-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.094-1.228.472-1.794.848-1.51 2.64-1.982 4.147-1.227h.095c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM55.907 10.1a.406.406 0 0 1-.377.378c-.095 0-.19 0-.19-.095a3.384 3.384 0 0 0-.942-.66c-.66-.378-1.508-.567-2.263-.567-.942 0-1.885.283-2.733.755a4.463 4.463 0 0 0-1.886 2.077c-.471.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.377.85 1.037 1.604 1.886 2.17.848.473 1.697.756 2.64.756.753 0 1.602-.189 2.262-.567.66-.283 1.131-.755 1.603-1.321v1.699h2.922V9.345h-2.922v.755Zm-.472 6.891c-.283.472-.66.944-1.131 1.227-.471.283-1.037.378-1.509.378-.565 0-1.037-.189-1.508-.378-.471-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.095-1.228.472-1.794.848-1.51 2.64-1.982 4.148-1.227h.094c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM126.708 5.477h-2.923v15.575h2.923V5.477ZM140.191 14.914c0-1.038-.189-2.077-.755-3.02-.471-.85-1.131-1.605-2.074-1.983a6.78 6.78 0 0 0-6.033 0 5.485 5.485 0 0 0-2.075 2.077c-.565 1.038-.754 2.077-.754 3.21 0 1.132.189 2.17.754 3.114a5.485 5.485 0 0 0 2.075 2.077c.942.472 1.979.755 3.016.755a6.56 6.56 0 0 0 3.489-1.038c.943-.661 1.602-1.7 1.885-2.738h-3.016c-.377.944-1.321 1.51-2.358 1.416-.659 0-1.414-.283-1.885-.755-.566-.472-.848-1.227-.848-1.982h8.484c.095-.378.095-.755.095-1.133Zm-8.579-.85c.093-.755.377-1.415.942-1.887.566-.472 1.226-.661 1.886-.661.754 0 1.414.189 1.979.66.566.473.849 1.133.849 1.794l-5.656.095ZM67.692 9.159a4.7 4.7 0 0 0-1.98.472c-.377.188-.754.377-1.037.66a.286.286 0 0 1-.377 0c-.094-.094-.094-.094-.094-.188v-.756H61.28v11.705h2.923v-6.418c-.094-.755.188-1.51.754-2.171.471-.472 1.226-.756 1.886-.756.66 0 1.414.284 1.885.755.472.567.754 1.322.66 2.172v6.418h2.923v-6.89c.094-1.322-.283-2.644-1.226-3.682-.848-.85-2.168-1.416-3.394-1.321Z"></path></g><defs><clipPath id="anyscale-logo_svg__a"><path fill="currentColor" d="M0 0h140.19v32H0z"></path></clipPath></defs></svg></div><div class="font-light leading-normal text-lightBlue-300 opacity-70 hidden lg:flex"><span>© Anyscale, Inc 2022</span></div></div><div class="z-10 hidden flex-col gap-10 lg:flex"><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Anyscale</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://www.linkedin.com/company/joinanyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M15.784 14.168a1.667 1.667 0 1 1-3.334-.002 1.667 1.667 0 0 1 3.334.002Zm.05 2.9H12.5V27.5h3.334V17.067Zm5.266 0h-3.317V27.5h3.284v-5.475c0-3.05 3.975-3.334 3.975 0V27.5h3.291v-6.609c0-5.141-5.883-4.95-7.266-2.424l.033-1.4Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://www.facebook.com/AnyscaleCompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M21.667 21.247h2.083l.834-3.333h-2.917v-1.667c0-.858 0-1.666 1.666-1.666h1.25v-2.8a23.49 23.49 0 0 0-2.38-.117c-2.263 0-3.87 1.38-3.87 3.917v2.333h-2.5v3.333h2.5v7.084h3.334v-7.084Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://twitter.com/anyscalecompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/anyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Ray</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://twitter.com/raydistributed"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/ray-project/ray"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div></div></div><div class="grid grid-cols-2 gap-x-8 gap-y-12 md:grid-cols-3 md:gap-y-10 md:gap-x-16"><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Products</div><div><ul>
<li><a href="../platform.html">Anyscale Compute Platform</a></li>
<li><a href="../ray-open-source.html">Ray Open Source</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Events</div><div><ul>
<li><a href="../event-category/rl-summit.html">Webinars</a></li>
<li><a href="../event-category/rl-summit.html">Meetups</a></li>
<li><a href="../event-category/rl-summit.html">Summits</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Company</div><div><ul>
<li><a href="../about.html">About Us</a></li>
<li><a href="../press.html">News</a></li>
<li><a href="../careers.html">Careers</a></li>
<li><a href="../community.html">Community</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Learn</div><div><ul>
<li><a href="how-ray-solves-common-production-challenges-for-generative-ai.html">Blog</a></li>
<li><a href="../event-category/rl-summit.html">Demos &amp; Webinars</a></li>
<li><a href="../event-category/rl-summit.html">Anyscale Academy</a></li>
<li><a href="https://docs.anyscale.com">Anyscale Docs</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Scalable AI and Python</div><div><ul>
<li><a href="../data-ingestion.html">Data Ingestion</a></li>
<li><a href="../reinforcement-learning.html">Reinforcement Learning</a></li>
<li><a href="../ray-air.html">Ray AIR</a></li>
<li><a href="../model-serving.html">Model Serving</a></li>
<li><a href="../hyperparameter-tuning.html">Hyperparameter Tuning</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Use Cases</div><div><ul>
<li><a href="../demand-forecasting.html">Demand Forecasting/Pricing</a></li>
<li><a href="../industrial-automation.html">Industrial Automation</a></li>
<li><a href="../machine-learning.html">Scalable ML Platforms</a></li>
<li><a href="../natural-language-processing.html">NLP</a></li>
<li><a href="../recommendation-system.html">Recommendation System</a></li>
</ul>
</div></div></div><div class="grid-col-1 grid gap-12 md:grid-cols-3 md:gap-16 lg:hidden"><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Anyscale</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://www.linkedin.com/company/joinanyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M15.784 14.168a1.667 1.667 0 1 1-3.334-.002 1.667 1.667 0 0 1 3.334.002Zm.05 2.9H12.5V27.5h3.334V17.067Zm5.266 0h-3.317V27.5h3.284v-5.475c0-3.05 3.975-3.334 3.975 0V27.5h3.291v-6.609c0-5.141-5.883-4.95-7.266-2.424l.033-1.4Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://www.facebook.com/AnyscaleCompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M21.667 21.247h2.083l.834-3.333h-2.917v-1.667c0-.858 0-1.666 1.666-1.666h1.25v-2.8a23.49 23.49 0 0 0-2.38-.117c-2.263 0-3.87 1.38-3.87 3.917v2.333h-2.5v3.333h2.5v7.084h3.334v-7.084Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://twitter.com/anyscalecompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/anyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Ray</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://twitter.com/raydistributed"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/ray-project/ray"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="md:self-end md:text-right"><div class="font-light leading-normal text-lightBlue-300 opacity-70"><span>© Anyscale, Inc 2022</span></div></div></div></div><div class="pointer-events-none absolute bottom-0 left-0"><svg width="612" height="297" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="hidden lg:block"><g opacity="0.5" clip-path="url(#dots_svg__a)"><mask id="dots_svg__c" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="0" y="0" width="612" height="297"><path transform="rotate(90 611.5 .5)" fill="url(#dots_svg__b)" stroke="#F2F3F7" d="M611.5.5h296v611h-296z"></path></mask><g mask="url(#dots_svg__c)" stroke="#A2B7D4"><g opacity="0.2" stroke-linecap="round"><path d="M598 350v-688M582 350v-688M566 350v-688M550 350v-688M534 350v-688M518 350v-688M502 350v-688M486 350v-688M470 350v-688M454 350v-688M438 350v-688M422 350v-688M406 350v-688M390 350v-688M374 350v-688M358 350v-688M342 350v-688M326 350v-688M310 350v-688M294 350v-688M278 350v-688M262 350v-688M246 350v-688M230 350v-688M214 350v-688M198 350v-688M182 350v-688M166 350v-688M150 350v-688M134 350v-688M118 350v-688M102 350v-688M86 350v-688M70 350v-688M54 350v-688M38 350v-688M22 350v-688M6 350v-688"></path></g><g opacity="0.2" stroke-linecap="round"><path d="M1014 14H0M1014 30H0M1014 46H0M1014 62H0M1014 78H0M1014 94H0M1014 110H0M1014 126H0M1014 142H0M1014 158H0M1014 174H0M1014 190H4M1014 206H4M1014 222H4M1014 238H4M1014 254H4M1014 270H4M1014 286H4"></path></g><path fill="#4C6996" d="M615.5 12.5v3h-3v-3zM615.5 28.5v3h-3v-3zM615.5 44.5v3h-3v-3zM615.5 60.5v3h-3v-3zM615.5 76.5v3h-3v-3zM615.5 92.5v3h-3v-3zM615.5 108.5v3h-3v-3zM615.5 124.5v3h-3v-3zM615.5 140.5v3h-3v-3zM615.5 156.5v3h-3v-3zM615.5 172.5v3h-3v-3zM615.5 188.5v3h-3v-3zM615.5 204.5v3h-3v-3zM615.5 220.5v3h-3v-3zM615.5 236.5v3h-3v-3zM615.5 252.5v3h-3v-3zM615.5 268.5v3h-3v-3zM615.5 284.5v3h-3v-3zM599.5 12.5v3h-3v-3zM599.5 28.5v3h-3v-3zM599.5 44.5v3h-3v-3zM599.5 60.5v3h-3v-3zM599.5 76.5v3h-3v-3zM599.5 92.5v3h-3v-3zM599.5 108.5v3h-3v-3zM599.5 124.5v3h-3v-3zM599.5 140.5v3h-3v-3zM599.5 156.5v3h-3v-3zM599.5 172.5v3h-3v-3zM599.5 188.5v3h-3v-3zM599.5 204.5v3h-3v-3zM599.5 220.5v3h-3v-3zM599.5 236.5v3h-3v-3zM599.5 252.5v3h-3v-3zM599.5 268.5v3h-3v-3zM599.5 284.5v3h-3v-3zM583.5 12.5v3h-3v-3zM583.5 28.5v3h-3v-3zM583.5 44.5v3h-3v-3zM583.5 60.5v3h-3v-3zM583.5 76.5v3h-3v-3zM583.5 92.5v3h-3v-3zM583.5 108.5v3h-3v-3zM583.5 124.5v3h-3v-3zM583.5 140.5v3h-3v-3zM583.5 156.5v3h-3v-3zM583.5 172.5v3h-3v-3zM583.5 188.5v3h-3v-3zM583.5 204.5v3h-3v-3zM583.5 220.5v3h-3v-3zM583.5 236.5v3h-3v-3zM583.5 252.5v3h-3v-3zM583.5 268.5v3h-3v-3zM583.5 284.5v3h-3v-3zM567.5 12.5v3h-3v-3zM567.5 28.5v3h-3v-3zM567.5 44.5v3h-3v-3zM567.5 60.5v3h-3v-3zM567.5 76.5v3h-3v-3zM567.5 92.5v3h-3v-3zM567.5 108.5v3h-3v-3zM567.5 124.5v3h-3v-3zM567.5 140.5v3h-3v-3zM567.5 156.5v3h-3v-3zM567.5 172.5v3h-3v-3zM567.5 188.5v3h-3v-3zM567.5 204.5v3h-3v-3zM567.5 220.5v3h-3v-3zM567.5 236.5v3h-3v-3zM567.5 252.5v3h-3v-3zM567.5 268.5v3h-3v-3zM567.5 284.5v3h-3v-3zM551.5 12.5v3h-3v-3zM551.5 28.5v3h-3v-3zM551.5 44.5v3h-3v-3zM551.5 60.5v3h-3v-3zM551.5 76.5v3h-3v-3zM551.5 92.5v3h-3v-3zM551.5 108.5v3h-3v-3zM551.5 124.5v3h-3v-3zM551.5 140.5v3h-3v-3zM551.5 156.5v3h-3v-3zM551.5 172.5v3h-3v-3zM551.5 188.5v3h-3v-3zM551.5 204.5v3h-3v-3zM551.5 220.5v3h-3v-3zM551.5 236.5v3h-3v-3zM551.5 252.5v3h-3v-3zM551.5 268.5v3h-3v-3zM551.5 284.5v3h-3v-3zM535.5 12.5v3h-3v-3zM535.5 28.5v3h-3v-3zM535.5 44.5v3h-3v-3zM535.5 60.5v3h-3v-3zM535.5 76.5v3h-3v-3zM535.5 92.5v3h-3v-3zM535.5 108.5v3h-3v-3zM535.5 124.5v3h-3v-3zM535.5 140.5v3h-3v-3zM535.5 156.5v3h-3v-3zM535.5 172.5v3h-3v-3zM535.5 188.5v3h-3v-3zM535.5 204.5v3h-3v-3zM535.5 220.5v3h-3v-3zM535.5 236.5v3h-3v-3zM535.5 252.5v3h-3v-3zM535.5 268.5v3h-3v-3zM535.5 284.5v3h-3v-3zM519.5 12.5v3h-3v-3zM519.5 28.5v3h-3v-3zM519.5 44.5v3h-3v-3zM519.5 60.5v3h-3v-3zM519.5 76.5v3h-3v-3zM519.5 92.5v3h-3v-3zM519.5 108.5v3h-3v-3zM519.5 124.5v3h-3v-3zM519.5 140.5v3h-3v-3zM519.5 156.5v3h-3v-3zM519.5 172.5v3h-3v-3zM519.5 188.5v3h-3v-3zM519.5 204.5v3h-3v-3zM519.5 220.5v3h-3v-3zM519.5 236.5v3h-3v-3zM519.5 252.5v3h-3v-3zM519.5 268.5v3h-3v-3zM519.5 284.5v3h-3v-3zM503.5 12.5v3h-3v-3zM503.5 28.5v3h-3v-3zM503.5 44.5v3h-3v-3zM503.5 60.5v3h-3v-3zM503.5 76.5v3h-3v-3zM503.5 92.5v3h-3v-3zM503.5 108.5v3h-3v-3zM503.5 124.5v3h-3v-3zM503.5 140.5v3h-3v-3zM503.5 156.5v3h-3v-3zM503.5 172.5v3h-3v-3zM503.5 188.5v3h-3v-3zM503.5 204.5v3h-3v-3zM503.5 220.5v3h-3v-3zM503.5 236.5v3h-3v-3zM503.5 252.5v3h-3v-3zM503.5 268.5v3h-3v-3zM503.5 284.5v3h-3v-3zM487.5 12.5v3h-3v-3zM487.5 28.5v3h-3v-3zM487.5 44.5v3h-3v-3zM487.5 60.5v3h-3v-3zM487.5 76.5v3h-3v-3zM487.5 92.5v3h-3v-3zM487.5 108.5v3h-3v-3zM487.5 124.5v3h-3v-3zM487.5 140.5v3h-3v-3zM487.5 156.5v3h-3v-3zM487.5 172.5v3h-3v-3zM487.5 188.5v3h-3v-3zM487.5 204.5v3h-3v-3zM487.5 220.5v3h-3v-3zM487.5 236.5v3h-3v-3zM487.5 252.5v3h-3v-3zM487.5 268.5v3h-3v-3zM487.5 284.5v3h-3v-3zM471.5 12.5v3h-3v-3zM471.5 28.5v3h-3v-3zM471.5 44.5v3h-3v-3zM471.5 60.5v3h-3v-3zM471.5 76.5v3h-3v-3zM471.5 92.5v3h-3v-3zM471.5 108.5v3h-3v-3zM471.5 124.5v3h-3v-3zM471.5 140.5v3h-3v-3zM471.5 156.5v3h-3v-3zM471.5 172.5v3h-3v-3zM471.5 188.5v3h-3v-3zM471.5 204.5v3h-3v-3zM471.5 220.5v3h-3v-3zM471.5 236.5v3h-3v-3zM471.5 252.5v3h-3v-3zM471.5 268.5v3h-3v-3zM471.5 284.5v3h-3v-3zM455.5 12.5v3h-3v-3zM455.5 28.5v3h-3v-3zM455.5 44.5v3h-3v-3zM455.5 60.5v3h-3v-3zM455.5 76.5v3h-3v-3zM455.5 92.5v3h-3v-3zM455.5 108.5v3h-3v-3zM455.5 124.5v3h-3v-3zM455.5 140.5v3h-3v-3zM455.5 156.5v3h-3v-3zM455.5 172.5v3h-3v-3zM455.5 188.5v3h-3v-3zM455.5 204.5v3h-3v-3zM455.5 220.5v3h-3v-3zM455.5 236.5v3h-3v-3zM455.5 252.5v3h-3v-3zM455.5 268.5v3h-3v-3zM455.5 284.5v3h-3v-3zM439.5 12.5v3h-3v-3zM439.5 28.5v3h-3v-3zM439.5 44.5v3h-3v-3zM439.5 60.5v3h-3v-3zM439.5 76.5v3h-3v-3zM439.5 92.5v3h-3v-3zM439.5 108.5v3h-3v-3zM439.5 124.5v3h-3v-3zM439.5 140.5v3h-3v-3zM439.5 156.5v3h-3v-3zM439.5 172.5v3h-3v-3zM439.5 188.5v3h-3v-3zM439.5 204.5v3h-3v-3zM439.5 220.5v3h-3v-3zM439.5 236.5v3h-3v-3zM439.5 252.5v3h-3v-3zM439.5 268.5v3h-3v-3zM439.5 284.5v3h-3v-3zM423.5 12.5v3h-3v-3zM423.5 28.5v3h-3v-3zM423.5 44.5v3h-3v-3zM423.5 60.5v3h-3v-3zM423.5 76.5v3h-3v-3zM423.5 92.5v3h-3v-3zM423.5 108.5v3h-3v-3zM423.5 124.5v3h-3v-3zM423.5 140.5v3h-3v-3zM423.5 156.5v3h-3v-3zM423.5 172.5v3h-3v-3zM423.5 188.5v3h-3v-3zM423.5 204.5v3h-3v-3zM423.5 220.5v3h-3v-3zM423.5 236.5v3h-3v-3zM423.5 252.5v3h-3v-3zM423.5 268.5v3h-3v-3zM423.5 284.5v3h-3v-3zM407.5 12.5v3h-3v-3zM407.5 28.5v3h-3v-3zM407.5 44.5v3h-3v-3zM407.5 60.5v3h-3v-3zM407.5 76.5v3h-3v-3zM407.5 92.5v3h-3v-3zM407.5 108.5v3h-3v-3zM407.5 124.5v3h-3v-3zM407.5 140.5v3h-3v-3zM407.5 156.5v3h-3v-3zM407.5 172.5v3h-3v-3zM407.5 188.5v3h-3v-3zM407.5 204.5v3h-3v-3zM407.5 220.5v3h-3v-3zM407.5 236.5v3h-3v-3zM407.5 252.5v3h-3v-3zM407.5 268.5v3h-3v-3zM407.5 284.5v3h-3v-3zM391.5 12.5v3h-3v-3zM391.5 28.5v3h-3v-3zM391.5 44.5v3h-3v-3zM391.5 60.5v3h-3v-3zM391.5 76.5v3h-3v-3zM391.5 92.5v3h-3v-3zM391.5 108.5v3h-3v-3zM391.5 124.5v3h-3v-3zM391.5 140.5v3h-3v-3zM391.5 156.5v3h-3v-3zM391.5 172.5v3h-3v-3zM391.5 188.5v3h-3v-3zM391.5 204.5v3h-3v-3zM391.5 220.5v3h-3v-3zM391.5 236.5v3h-3v-3zM391.5 252.5v3h-3v-3zM391.5 268.5v3h-3v-3zM391.5 284.5v3h-3v-3zM375.5 12.5v3h-3v-3zM375.5 28.5v3h-3v-3zM375.5 44.5v3h-3v-3zM375.5 60.5v3h-3v-3zM375.5 76.5v3h-3v-3zM375.5 92.5v3h-3v-3zM375.5 108.5v3h-3v-3zM375.5 124.5v3h-3v-3zM375.5 140.5v3h-3v-3zM375.5 156.5v3h-3v-3zM375.5 172.5v3h-3v-3zM375.5 188.5v3h-3v-3zM375.5 204.5v3h-3v-3zM375.5 220.5v3h-3v-3z"></path><rect x="375.5" y="236.5" width="3" height="3" rx="1.5" transform="rotate(90 375.5 236.5)" fill="#4C6996"></rect><path fill="#4C6996" d="M375.5 252.5v3h-3v-3zM375.5 268.5v3h-3v-3zM375.5 284.5v3h-3v-3zM359.5 12.5v3h-3v-3zM359.5 28.5v3h-3v-3zM359.5 44.5v3h-3v-3zM359.5 60.5v3h-3v-3zM359.5 76.5v3h-3v-3zM359.5 92.5v3h-3v-3zM359.5 108.5v3h-3v-3zM359.5 124.5v3h-3v-3zM359.5 140.5v3h-3v-3zM359.5 156.5v3h-3v-3zM359.5 172.5v3h-3v-3zM359.5 188.5v3h-3v-3zM359.5 204.5v3h-3v-3zM359.5 220.5v3h-3v-3zM359.5 236.5v3h-3v-3zM359.5 252.5v3h-3v-3zM359.5 268.5v3h-3v-3zM359.5 284.5v3h-3v-3zM343.5 12.5v3h-3v-3zM343.5 28.5v3h-3v-3zM343.5 44.5v3h-3v-3zM343.5 60.5v3h-3v-3zM343.5 76.5v3h-3v-3zM343.5 92.5v3h-3v-3zM343.5 108.5v3h-3v-3zM343.5 124.5v3h-3v-3zM343.5 140.5v3h-3v-3zM343.5 156.5v3h-3v-3zM343.5 172.5v3h-3v-3zM343.5 188.5v3h-3v-3zM343.5 204.5v3h-3v-3zM343.5 220.5v3h-3v-3zM343.5 236.5v3h-3v-3zM343.5 252.5v3h-3v-3zM343.5 268.5v3h-3v-3zM343.5 284.5v3h-3v-3zM327.5 12.5v3h-3v-3zM327.5 28.5v3h-3v-3zM327.5 44.5v3h-3v-3zM327.5 60.5v3h-3v-3zM327.5 76.5v3h-3v-3zM327.5 92.5v3h-3v-3zM327.5 108.5v3h-3v-3zM327.5 124.5v3h-3v-3zM327.5 140.5v3h-3v-3zM327.5 156.5v3h-3v-3zM327.5 172.5v3h-3v-3zM327.5 188.5v3h-3v-3zM327.5 204.5v3h-3v-3zM327.5 220.5v3h-3v-3zM327.5 236.5v3h-3v-3zM327.5 252.5v3h-3v-3zM327.5 268.5v3h-3v-3zM327.5 284.5v3h-3v-3zM311.5 12.5v3h-3v-3zM311.5 28.5v3h-3v-3zM311.5 44.5v3h-3v-3zM311.5 60.5v3h-3v-3zM311.5 76.5v3h-3v-3zM311.5 92.5v3h-3v-3zM311.5 108.5v3h-3v-3zM311.5 124.5v3h-3v-3zM311.5 140.5v3h-3v-3zM311.5 156.5v3h-3v-3zM311.5 172.5v3h-3v-3zM311.5 188.5v3h-3v-3zM311.5 204.5v3h-3v-3zM311.5 220.5v3h-3v-3zM311.5 236.5v3h-3v-3zM311.5 252.5v3h-3v-3zM311.5 268.5v3h-3v-3zM311.5 284.5v3h-3v-3zM295.5 12.5v3h-3v-3zM295.5 28.5v3h-3v-3zM295.5 44.5v3h-3v-3zM295.5 60.5v3h-3v-3zM295.5 76.5v3h-3v-3zM295.5 92.5v3h-3v-3zM295.5 108.5v3h-3v-3zM295.5 124.5v3h-3v-3zM295.5 140.5v3h-3v-3zM295.5 156.5v3h-3v-3zM295.5 172.5v3h-3v-3zM295.5 188.5v3h-3v-3zM295.5 204.5v3h-3v-3zM295.5 220.5v3h-3v-3zM295.5 236.5v3h-3v-3zM295.5 252.5v3h-3v-3zM295.5 268.5v3h-3v-3zM295.5 284.5v3h-3v-3zM279.5 12.5v3h-3v-3zM279.5 28.5v3h-3v-3zM279.5 44.5v3h-3v-3zM279.5 60.5v3h-3v-3zM279.5 76.5v3h-3v-3zM279.5 92.5v3h-3v-3zM279.5 108.5v3h-3v-3zM279.5 124.5v3h-3v-3zM279.5 140.5v3h-3v-3zM279.5 156.5v3h-3v-3zM279.5 172.5v3h-3v-3zM279.5 188.5v3h-3v-3zM279.5 204.5v3h-3v-3zM279.5 220.5v3h-3v-3zM279.5 236.5v3h-3v-3zM279.5 252.5v3h-3v-3zM279.5 268.5v3h-3v-3zM279.5 284.5v3h-3v-3zM263.5 12.5v3h-3v-3zM263.5 28.5v3h-3v-3zM263.5 44.5v3h-3v-3zM263.5 60.5v3h-3v-3zM263.5 76.5v3h-3v-3zM263.5 92.5v3h-3v-3zM263.5 108.5v3h-3v-3zM263.5 124.5v3h-3v-3zM263.5 140.5v3h-3v-3zM263.5 156.5v3h-3v-3zM263.5 172.5v3h-3v-3zM263.5 188.5v3h-3v-3zM263.5 204.5v3h-3v-3zM263.5 220.5v3h-3v-3zM263.5 236.5v3h-3v-3zM263.5 252.5v3h-3v-3zM263.5 268.5v3h-3v-3zM263.5 284.5v3h-3v-3zM247.5 12.5v3h-3v-3zM247.5 28.5v3h-3v-3zM247.5 44.5v3h-3v-3zM247.5 60.5v3h-3v-3zM247.5 76.5v3h-3v-3zM247.5 92.5v3h-3v-3zM247.5 108.5v3h-3v-3zM247.5 124.5v3h-3v-3zM247.5 140.5v3h-3v-3zM247.5 156.5v3h-3v-3zM247.5 172.5v3h-3v-3zM247.5 188.5v3h-3v-3zM247.5 204.5v3h-3v-3zM247.5 220.5v3h-3v-3zM247.5 236.5v3h-3v-3zM247.5 252.5v3h-3v-3zM247.5 268.5v3h-3v-3zM247.5 284.5v3h-3v-3zM231.5 12.5v3h-3v-3zM231.5 28.5v3h-3v-3zM231.5 44.5v3h-3v-3zM231.5 60.5v3h-3v-3zM231.5 76.5v3h-3v-3zM231.5 92.5v3h-3v-3zM231.5 108.5v3h-3v-3zM231.5 124.5v3h-3v-3zM231.5 140.5v3h-3v-3zM231.5 156.5v3h-3v-3zM231.5 172.5v3h-3v-3zM231.5 188.5v3h-3v-3zM231.5 204.5v3h-3v-3zM231.5 220.5v3h-3v-3zM231.5 236.5v3h-3v-3zM231.5 252.5v3h-3v-3zM231.5 268.5v3h-3v-3zM231.5 284.5v3h-3v-3zM215.5 12.5v3h-3v-3zM215.5 28.5v3h-3v-3zM215.5 44.5v3h-3v-3zM215.5 60.5v3h-3v-3zM215.5 76.5v3h-3v-3zM215.5 92.5v3h-3v-3zM215.5 108.5v3h-3v-3zM215.5 124.5v3h-3v-3zM215.5 140.5v3h-3v-3zM215.5 156.5v3h-3v-3zM215.5 172.5v3h-3v-3zM215.5 188.5v3h-3v-3zM215.5 204.5v3h-3v-3zM215.5 220.5v3h-3v-3zM215.5 236.5v3h-3v-3zM215.5 252.5v3h-3v-3zM215.5 268.5v3h-3v-3zM215.5 284.5v3h-3v-3zM199.5 12.5v3h-3v-3zM199.5 28.5v3h-3v-3zM199.5 44.5v3h-3v-3zM199.5 60.5v3h-3v-3zM199.5 76.5v3h-3v-3zM199.5 92.5v3h-3v-3zM199.5 108.5v3h-3v-3zM199.5 124.5v3h-3v-3zM199.5 140.5v3h-3v-3zM199.5 156.5v3h-3v-3zM199.5 172.5v3h-3v-3zM199.5 188.5v3h-3v-3zM199.5 204.5v3h-3v-3zM199.5 220.5v3h-3v-3zM199.5 236.5v3h-3v-3zM199.5 252.5v3h-3v-3zM199.5 268.5v3h-3v-3zM199.5 284.5v3h-3v-3zM183.5 12.5v3h-3v-3zM183.5 28.5v3h-3v-3zM183.5 44.5v3h-3v-3zM183.5 60.5v3h-3v-3zM183.5 76.5v3h-3v-3zM183.5 92.5v3h-3v-3zM183.5 108.5v3h-3v-3zM183.5 124.5v3h-3v-3zM183.5 140.5v3h-3v-3zM183.5 156.5v3h-3v-3zM183.5 172.5v3h-3v-3zM183.5 188.5v3h-3v-3zM183.5 204.5v3h-3v-3zM183.5 220.5v3h-3v-3zM183.5 236.5v3h-3v-3zM183.5 252.5v3h-3v-3zM183.5 268.5v3h-3v-3zM183.5 284.5v3h-3v-3zM167.5 12.5v3h-3v-3zM167.5 28.5v3h-3v-3zM167.5 44.5v3h-3v-3zM167.5 60.5v3h-3v-3zM167.5 76.5v3h-3v-3zM167.5 92.5v3h-3v-3zM167.5 108.5v3h-3v-3zM167.5 124.5v3h-3v-3zM167.5 140.5v3h-3v-3zM167.5 156.5v3h-3v-3zM167.5 172.5v3h-3v-3zM167.5 188.5v3h-3v-3zM167.5 204.5v3h-3v-3zM167.5 220.5v3h-3v-3zM167.5 236.5v3h-3v-3zM167.5 252.5v3h-3v-3zM167.5 268.5v3h-3v-3zM167.5 284.5v3h-3v-3zM151.5 12.5v3h-3v-3zM151.5 28.5v3h-3v-3zM151.5 44.5v3h-3v-3zM151.5 60.5v3h-3v-3zM151.5 76.5v3h-3v-3zM151.5 92.5v3h-3v-3zM151.5 108.5v3h-3v-3zM151.5 124.5v3h-3v-3zM151.5 140.5v3h-3v-3zM151.5 156.5v3h-3v-3zM151.5 172.5v3h-3v-3zM151.5 188.5v3h-3v-3zM151.5 204.5v3h-3v-3zM151.5 220.5v3h-3v-3zM151.5 236.5v3h-3v-3zM151.5 252.5v3h-3v-3zM151.5 268.5v3h-3v-3zM151.5 284.5v3h-3v-3zM135.5 12.5v3h-3v-3zM135.5 28.5v3h-3v-3zM135.5 44.5v3h-3v-3zM135.5 60.5v3h-3v-3zM135.5 76.5v3h-3v-3zM135.5 92.5v3h-3v-3zM135.5 108.5v3h-3v-3zM135.5 124.5v3h-3v-3zM135.5 140.5v3h-3v-3zM135.5 156.5v3h-3v-3zM135.5 172.5v3h-3v-3zM135.5 188.5v3h-3v-3zM135.5 204.5v3h-3v-3zM135.5 220.5v3h-3v-3zM135.5 236.5v3h-3v-3zM135.5 252.5v3h-3v-3zM135.5 268.5v3h-3v-3zM135.5 284.5v3h-3v-3zM119.5 12.5v3h-3v-3zM119.5 28.5v3h-3v-3zM119.5 44.5v3h-3v-3zM119.5 60.5v3h-3v-3zM119.5 76.5v3h-3v-3zM119.5 92.5v3h-3v-3zM119.5 108.5v3h-3v-3zM119.5 124.5v3h-3v-3zM119.5 140.5v3h-3v-3zM119.5 156.5v3h-3v-3zM119.5 172.5v3h-3v-3zM119.5 188.5v3h-3v-3zM119.5 204.5v3h-3v-3zM119.5 220.5v3h-3v-3zM119.5 236.5v3h-3v-3zM119.5 252.5v3h-3v-3zM119.5 268.5v3h-3v-3zM119.5 284.5v3h-3v-3zM103.5 12.5v3h-3v-3zM103.5 28.5v3h-3v-3zM103.5 44.5v3h-3v-3zM103.5 60.5v3h-3v-3zM103.5 76.5v3h-3v-3zM103.5 92.5v3h-3v-3zM103.5 108.5v3h-3v-3zM103.5 124.5v3h-3v-3zM103.5 140.5v3h-3v-3zM103.5 156.5v3h-3v-3zM103.5 172.5v3h-3v-3zM103.5 188.5v3h-3v-3zM103.5 204.5v3h-3v-3zM103.5 220.5v3h-3v-3zM103.5 236.5v3h-3v-3zM103.5 252.5v3h-3v-3zM103.5 268.5v3h-3v-3zM103.5 284.5v3h-3v-3zM87.5 12.5v3h-3v-3zM87.5 28.5v3h-3v-3zM87.5 44.5v3h-3v-3zM87.5 60.5v3h-3v-3zM87.5 76.5v3h-3v-3zM87.5 92.5v3h-3v-3zM87.5 108.5v3h-3v-3zM87.5 124.5v3h-3v-3zM87.5 140.5v3h-3v-3zM87.5 156.5v3h-3v-3zM87.5 172.5v3h-3v-3zM87.5 188.5v3h-3v-3zM87.5 204.5v3h-3v-3zM87.5 220.5v3h-3v-3zM87.5 236.5v3h-3v-3zM87.5 252.5v3h-3v-3zM87.5 268.5v3h-3v-3zM87.5 284.5v3h-3v-3zM71.5 12.5v3h-3v-3zM71.5 28.5v3h-3v-3zM71.5 44.5v3h-3v-3zM71.5 60.5v3h-3v-3zM71.5 76.5v3h-3v-3zM71.5 92.5v3h-3v-3zM71.5 108.5v3h-3v-3zM71.5 124.5v3h-3v-3zM71.5 140.5v3h-3v-3zM71.5 156.5v3h-3v-3zM71.5 172.5v3h-3v-3zM71.5 188.5v3h-3v-3zM71.5 204.5v3h-3v-3zM71.5 220.5v3h-3v-3zM71.5 236.5v3h-3v-3zM71.5 252.5v3h-3v-3zM71.5 268.5v3h-3v-3zM71.5 284.5v3h-3v-3zM55.5 12.5v3h-3v-3zM55.5 28.5v3h-3v-3zM55.5 44.5v3h-3v-3zM55.5 60.5v3h-3v-3zM55.5 76.5v3h-3v-3zM55.5 92.5v3h-3v-3zM55.5 108.5v3h-3v-3zM55.5 124.5v3h-3v-3zM55.5 140.5v3h-3v-3zM55.5 156.5v3h-3v-3zM55.5 172.5v3h-3v-3zM55.5 188.5v3h-3v-3zM55.5 204.5v3h-3v-3zM55.5 220.5v3h-3v-3zM55.5 236.5v3h-3v-3zM55.5 252.5v3h-3v-3zM55.5 268.5v3h-3v-3zM55.5 284.5v3h-3v-3zM39.5 12.5v3h-3v-3zM39.5 28.5v3h-3v-3zM39.5 44.5v3h-3v-3zM39.5 60.5v3h-3v-3zM39.5 76.5v3h-3v-3zM39.5 92.5v3h-3v-3zM39.5 108.5v3h-3v-3zM39.5 124.5v3h-3v-3zM39.5 140.5v3h-3v-3zM39.5 156.5v3h-3v-3zM39.5 172.5v3h-3v-3zM39.5 188.5v3h-3v-3zM39.5 204.5v3h-3v-3zM39.5 220.5v3h-3v-3zM39.5 236.5v3h-3v-3zM39.5 252.5v3h-3v-3zM39.5 268.5v3h-3v-3zM39.5 284.5v3h-3v-3zM23.5 12.5v3h-3v-3zM23.5 28.5v3h-3v-3zM23.5 44.5v3h-3v-3zM23.5 60.5v3h-3v-3zM23.5 76.5v3h-3v-3zM23.5 92.5v3h-3v-3zM23.5 108.5v3h-3v-3zM23.5 124.5v3h-3v-3zM23.5 140.5v3h-3v-3zM23.5 156.5v3h-3v-3zM23.5 172.5v3h-3v-3zM23.5 188.5v3h-3v-3zM23.5 204.5v3h-3v-3zM23.5 220.5v3h-3v-3zM23.5 236.5v3h-3v-3zM23.5 252.5v3h-3v-3zM23.5 268.5v3h-3v-3zM23.5 284.5v3h-3v-3zM7.5 12.5v3h-3v-3zM7.5 28.5v3h-3v-3zM7.5 44.5v3h-3v-3zM7.5 60.5v3h-3v-3zM7.5 76.5v3h-3v-3zM7.5 92.5v3h-3v-3zM7.5 108.5v3h-3v-3zM7.5 124.5v3h-3v-3zM7.5 140.5v3h-3v-3zM7.5 156.5v3h-3v-3zM7.5 172.5v3h-3v-3zM7.5 188.5v3h-3v-3zM7.5 204.5v3h-3v-3zM7.5 220.5v3h-3v-3zM7.5 236.5v3h-3v-3zM7.5 252.5v3h-3v-3zM7.5 268.5v3h-3v-3zM7.5 284.5v3h-3v-3z"></path></g></g><defs><radialGradient id="dots_svg__b" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="rotate(-136.519 627.256 92.637) scale(929.292 677.674)"><stop stop-color="#D9D9D9"></stop><stop offset="0.574" stop-color="#D9D9D9" stop-opacity="0"></stop></radialGradient><clipPath id="dots_svg__a"><path fill="#fff" transform="rotate(90 306 306)" d="M0 0h297v612H0z"></path></clipPath></defs></svg></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"site":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ke4xTi5OD9gnn2YtRXyCV","type":"Entry","createdAt":"2022-10-14T08:31:23.161Z","updatedAt":"2023-06-06T20:01:24.167Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"site"}},"locale":"en-US"},"fields":{"title":"Anyscale","description":"From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab.","identifier":"site-master-new","footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"16eb9I0yhaIALNOPR8WKTY","type":"Entry","createdAt":"2020-09-27T16:03:59.581Z","updatedAt":"2022-10-27T01:20:31.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer","body":"© Anyscale, Inc 2022","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"C7AXPFHtleWPOZ9BuT73K","type":"Asset","createdAt":"2020-09-28T14:40:50.925Z","updatedAt":"2020-09-28T14:40:50.925Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"anyscale_logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/C7AXPFHtleWPOZ9BuT73K/927f5130df1357db041d7a7908ab572e/Graphic_only__transparent_background_.png","details":{"size":1415,"image":{"width":331,"height":331}},"fileName":"Graphic only (transparent background).png","contentType":"image/png"}}}],"subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1cDdKq2eUfklc6w52dU6dg","type":"Entry","createdAt":"2020-09-27T16:05:42.307Z","updatedAt":"2022-10-27T01:20:26.714Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-1","header":"Products","body":"- [Anyscale Compute Platform](/platform)\n- [Ray Open Source](/ray-open-source)\n"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5tpXhcAJwSdhX8Wqw9HYTt","type":"Entry","createdAt":"2020-09-27T16:06:37.409Z","updatedAt":"2022-10-27T01:20:21.887Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":10,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-2","header":"Events","body":"- [Webinars](/events)\n- [Meetups](/events)\n- [Summits](/events)\n"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3c3LEomScM93hIg3i2zItU","type":"Entry","createdAt":"2020-09-27T16:07:42.856Z","updatedAt":"2022-10-27T01:20:18.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-3","header":"Company","body":"- [About Us](/about)\n- [News](/press)\n- [Careers](/careers)\n- [Community](/community)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6du4SSXPTxDZKoIVNHVVnT","type":"Entry","createdAt":"2022-10-27T01:20:13.320Z","updatedAt":"2022-10-27T01:20:13.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-4","header":"Learn","body":"- [Blog](/blog)\n- [Demos \u0026 Webinars](/events)\n- [Anyscale Academy](/events)\n- [Anyscale Docs](https://docs.anyscale.com)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1hovCXhGcWlgJmrqNzJ79k","type":"Entry","createdAt":"2022-10-27T01:20:09.895Z","updatedAt":"2022-10-27T01:20:09.895Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-5","header":"Scalable AI and Python","body":"- [Data Ingestion](/data-ingestion)\n- [Reinforcement Learning](/reinforcement-learning)\n- [Ray AIR](/ray-air)\n- [Model Serving](/model-serving)\n- [Hyperparameter Tuning](/hyperparameter-tuning)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"YFy5fUqwOD5ShdMaYEvzB","type":"Entry","createdAt":"2022-10-27T01:20:06.631Z","updatedAt":"2022-10-27T01:20:06.631Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-6","header":"Use Cases","body":"- [Demand Forecasting/Pricing](/demand-forecasting)\n- [Industrial Automation](/industrial-automation)\n- [Scalable ML Platforms](/machine-learning)\n- [NLP](/natural-language-processing)\n- [Recommendation System](/recommendation-system)"}}]}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"KBAZ5OavkdZfNTVjdJkp3","type":"Entry","createdAt":"2022-10-14T10:47:29.682Z","updatedAt":"2022-10-14T10:47:29.682Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Products","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KNN1Kmzo7jxSlxX1k74XQ","type":"Entry","createdAt":"2022-10-07T13:14:07.892Z","updatedAt":"2022-10-07T13:14:07.892Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"52Mmru1brS6ZkmItPeIW86","type":"Entry","createdAt":"2022-10-07T13:13:22.198Z","updatedAt":"2022-11-03T01:33:41.860Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Anyscale Platform","link":"/platform"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6cN6ZwdiFNiPcqIhd7EiQY","type":"Entry","createdAt":"2022-10-07T13:13:49.673Z","updatedAt":"2022-10-07T13:13:49.673Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Open Source","link":"/ray-open-source"}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5R1OTMfuLnPqont0Fjn9Xb","type":"Entry","createdAt":"2022-10-07T13:20:19.578Z","updatedAt":"2022-10-14T10:43:21.550Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Solutions","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1mp3IF3ZJE5JAViu3L9E95","type":"Entry","createdAt":"2022-10-07T13:16:00.346Z","updatedAt":"2022-10-07T13:21:01.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Scalable AI and Python","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4VUyKorAkgQP2V295h8eW3","type":"Entry","createdAt":"2022-10-07T13:15:35.949Z","updatedAt":"2022-10-07T13:15:35.949Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Data Ingestion","link":"/data-ingestion"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"Ouf5WgAzYmLovCZFNWqYm","type":"Entry","createdAt":"2022-10-07T13:17:09.728Z","updatedAt":"2022-10-07T13:17:09.728Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Reinforcement Learning","link":"/reinforcement-learning"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5kVPKqUbXBbKmrkvsImcIi","type":"Entry","createdAt":"2022-10-07T13:17:50.307Z","updatedAt":"2022-10-07T13:17:50.307Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray AIR","link":"/ray-air"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5uUidEvWw1oqjWrRcskEW","type":"Entry","createdAt":"2022-10-07T13:18:21.246Z","updatedAt":"2022-10-07T13:18:21.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Model Serving","link":"/model-serving"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Rbw0CrBN1KecPafCPvF4H","type":"Entry","createdAt":"2022-10-07T13:18:45.601Z","updatedAt":"2022-10-07T13:18:45.601Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Hyperparameter Tuning","link":"/hyperparameter-tuning"}}],"card":"##### Scalable AI and Python\n\nLeverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more."}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2RgWCuIiZe2TI6Nx5Ns7sB","type":"Entry","createdAt":"2022-10-07T13:25:03.101Z","updatedAt":"2023-05-31T12:47:25.274Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Use Cases","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3XMbpU1mPdsU7gktltlAzX","type":"Entry","createdAt":"2023-05-31T12:43:54.011Z","updatedAt":"2023-05-31T12:43:54.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Large Language Models","link":"/large-language-models"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6MAYw6HTSmH5JHcOoYjCzC","type":"Entry","createdAt":"2022-10-07T13:22:29.165Z","updatedAt":"2022-10-07T13:22:29.165Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Demand Forecasting/Pricing","link":"/demand-forecasting"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"30BlKhv6h4neaZwjA2jlxO","type":"Entry","createdAt":"2022-10-07T13:23:09.471Z","updatedAt":"2022-10-07T13:23:09.471Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Industrial Automation","link":"/industrial-automation"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2gGwUVxDuBeUso1uQH2gEW","type":"Entry","createdAt":"2022-10-07T13:23:32.145Z","updatedAt":"2022-10-07T13:23:32.145Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Scalable ML Platforms","link":"/machine-learning"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1cbwbmIvP5ZkV9iNEraQGP","type":"Entry","createdAt":"2022-10-07T13:23:59.732Z","updatedAt":"2022-10-07T13:23:59.732Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"NLP","link":"/natural-language-processing"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2A7FMbckpN5bgW1wp5ZuSh","type":"Entry","createdAt":"2022-10-07T13:24:41.861Z","updatedAt":"2022-10-07T13:24:41.861Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Recommendation System","link":"/recommendation-system"}}],"card":"##### Use Cases\n\nLeverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more."}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QNdrsmqz82oiBJvFTQ26n","type":"Entry","createdAt":"2023-04-13T11:37:45.354Z","updatedAt":"2023-04-13T11:37:45.354Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ecosystem","link":"/ray-ecosystem"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i0nHjF3pmjzGAdEE4V5MG","type":"Entry","createdAt":"2021-12-05T02:08:18.719Z","updatedAt":"2022-03-03T02:01:08.894Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Success Stories","link":"/user-stories"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5mFZvzY56aQCY4xvlBDi9W","type":"Entry","createdAt":"2022-10-14T10:46:35.218Z","updatedAt":"2022-10-14T10:46:35.218Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Learn","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"32tQWjMRAI5qci97W3cuie","type":"Entry","createdAt":"2022-10-07T13:28:46.057Z","updatedAt":"2023-03-22T20:31:55.243Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JQbOCOkxHfH2ZPGNNQVbt","type":"Entry","createdAt":"2021-12-05T14:57:28.664Z","updatedAt":"2021-12-05T14:57:28.664Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Blog","link":"/blog"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4qEkJgYkXZmiDq3lsxg0xW","type":"Entry","createdAt":"2023-03-22T20:31:21.666Z","updatedAt":"2023-03-22T20:31:21.666Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Summit 2023","link":"https://raysummit.anyscale.com/?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=home_nav-pulldown"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48sdeWXptwt3UgEZzeTt3k","type":"Entry","createdAt":"2022-10-07T13:26:28.331Z","updatedAt":"2022-10-07T13:26:28.331Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Videos \u0026 Webinars","link":"/events"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6hMfRHFDZUXcJH1tIq0xhY","type":"Entry","createdAt":"2021-10-21T20:59:36.918Z","updatedAt":"2023-03-06T20:24:26.811Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Training","link":"https://github.com/ray-project/ray-educational-materials"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cVWy7hmqbAzJCW6d7cLUx","type":"Entry","createdAt":"2022-10-07T13:28:33.537Z","updatedAt":"2022-10-07T13:28:33.537Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Anyscale Docs","link":"https://docs.anyscale.com"}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Qjs7zNfhrinHTNnzMpVIO","type":"Entry","createdAt":"2022-10-14T10:44:42.403Z","updatedAt":"2022-10-14T10:44:42.403Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Company","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ZFDrG8YGgU0oyNgBOId7X","type":"Entry","createdAt":"2022-10-07T13:32:38.746Z","updatedAt":"2022-10-07T13:32:38.746Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2RD65wmpn4gmL5Bvxu5VOD","type":"Entry","createdAt":"2020-09-08T17:53:32.413Z","updatedAt":"2021-06-20T15:04:11.482Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"About us","link":"/about"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71EHE1t3QrjnQb8hD9O8Ww","type":"Entry","createdAt":"2022-10-07T13:32:07.593Z","updatedAt":"2022-10-07T13:32:07.593Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"News","link":"/press"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3TNUPaz7ViQ6hbiZciBOQV","type":"Entry","createdAt":"2020-09-28T23:19:39.838Z","updatedAt":"2022-05-12T19:18:18.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Careers","link":"/careers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ubc5oYY5moS7HexIp7AjO","type":"Entry","createdAt":"2022-07-22T20:40:53.600Z","updatedAt":"2022-07-22T20:40:53.600Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Community","link":"/community"}}]}}]}}],"bannerText":"Are you passionate about Ray, scalable AI and distributed computing? Join us at Ray Summit!","bannerLink":"https://raysummit.anyscale.com/?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=homepage_ticker","bannerCta":" Find out more! "}},"title":"Streaming distributed execution across CPUs and GPUs","seoTitle":"Streaming distributed execution across CPUs and GPUs","slug":"streaming-distributed-execution-across-cpus-and-gpus","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2aJRcdEj9iAktAnqn0wucE","type":"Entry","createdAt":"2021-02-16T07:30:38.421Z","updatedAt":"2021-02-16T07:30:38.421Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Stephanie Wang","slug":"stephanie-wang","link":"https://github.com/stephanie-wang"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2edothjNBpyQEu2aX55KRl","type":"Entry","createdAt":"2023-05-10T15:30:16.699Z","updatedAt":"2023-05-10T15:30:16.699Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Cheng Su","slug":"cheng-su"}}],"publishedDate":"2023-05-11","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"In a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/offline-batch-inference-comparing-ray-apache-spark-and-sagemaker"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"previous blog,","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" we showed how Ray Data can provide speedups over SageMaker and frameworks like Apache Spark for large-scale batch inference workloads. This blog post delves further into how Ray Data streaming works and how to use it for your own ML pipelines.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Some of the most demanding machine learning (ML) use cases we have encountered involve pipelines that span both CPU and GPU devices in distributed environments. These situations arise in various workloads, including:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Batch inference, which involves a CPU-intensive preprocessing stage (e.g., video decoding or image resizing) before utilizing a GPU-intensive model to make predictions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed training, where similar CPU-heavy transformations are required to prepare or augment the dataset prior to GPU training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Interestingly, in many of these workloads, the preprocessing steps often prove to be the bottleneck. This can happen when preprocessing requires ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"parallelism across multiple nodes","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"significant memory to buffer results","nodeType":"text"},{"data":{},"marks":[],"value":" between the CPU and GPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For instance, consider the decoding of compressed images in memory. A typical JPEG decompression ratio is 10:1, which implies that the output memory size may be ten times that of the input, resulting in substantial memory pressure in addition to CPU load. The challenges become more complex with other data modalities, such as video; for example, H264 can decompress at a 2000:1 ratio, producing 200GB of frame outputs for a 100MB file input. This means that practitioners trying to offload CPU-heavy preprocessing onto multiple CPUs or machines have to deal with intermediate results that are far larger than their already sizable source datasets:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7ntuAY4zsUXtrtIxSofnyg","type":"Asset","createdAt":"2023-05-10T15:33:44.406Z","updatedAt":"2023-05-10T18:55:03.562Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"streaming_figure_1","description":"Figure 1. Decoding of compressed images with large images spilled to disk","file":{"url":"//images.ctfassets.net/xjan103pcp94/7ntuAY4zsUXtrtIxSofnyg/1fdb9631162d6b66777d205d1d3a7e1c/Screenshot_from_2023-05-10_11-00-43.png","details":{"size":64895,"image":{"width":1493,"height":436}},"fileName":"Screenshot from 2023-05-10 11-00-43.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Executed naively, these compute and memory-intensive workloads fail to fully utilize expensive GPU hardware. For example, we could buffer 200GB of frame outputs on disk, but that could add minutes of overheads. Even if this is overlapped with GPU computation, we can end up with long pauses on the GPU. This motivates fully streamed execution in the cluster setting, which avoids such delays by streaming intermediate data through cluster memory:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"40VpN8OKxEM7wHohV802gE","type":"Asset","createdAt":"2023-05-10T15:36:35.326Z","updatedAt":"2023-05-10T18:56:06.767Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_2","description":"Figure 2. Decoding intermediate data streamed through memory","file":{"url":"//images.ctfassets.net/xjan103pcp94/40VpN8OKxEM7wHohV802gE/c07086bfe677dc5e3109496f5a4ec083/Screenshot_from_2023-05-10_11-01-00.png","details":{"size":35974,"image":{"width":1199,"height":268}},"fileName":"Screenshot from 2023-05-10 11-01-00.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we guide you through our development of a versatile streaming backend for Ray Data, and show examples of how to implement the demanding use cases mentioned earlier. We discuss the performance benefits of implementing pipelined execution across CPUs and GPUs compared to bulk processing, and discuss its broader applicability to various workloads, including distributed training.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pipelined execution in a cluster","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To understand the necessity and advantages of pipelined (or streaming) execution across CPU and GPU devices, let's first examine how a ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Bulk_synchronous_parallel"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"bulk synchronous parallel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (BSP) framework might handle batch inference. Because of its simplicity and generality, BSP is a common way frameworks (e.g., MapReduce, Apache Spark) parallelize distributed computations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Note that pipelining within a single machine is already commonly used in data loading libraries such as ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Torch DataLoader","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://petastorm.readthedocs.io/en/latest/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Petastorm.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" Here, we discuss pipelining computations across an entire cluster of machines.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical batch inference job consists of the following operations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6mkS4HlXNovPW6Dl387jYe","type":"Asset","createdAt":"2023-05-10T15:40:10.724Z","updatedAt":"2023-05-10T19:28:57.091Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"locale":"en-US"},"fields":{"title":"streaming_figure_3","description":"Figure 3. A typical batch inference job sequential operations","file":{"url":"//images.ctfassets.net/xjan103pcp94/6mkS4HlXNovPW6Dl387jYe/1a707558e51ac00b776ae6f5d808b74e/Screenshot_from_2023-05-10_12-23-58.png","details":{"size":33844,"image":{"width":1526,"height":244}},"fileName":"Screenshot from 2023-05-10 12-23-58.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To implement such an inference job, you'd write the code using Ray Data (or another similar framework):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1kYVOacE6epR4czS5YoOHX","type":"Entry","createdAt":"2023-05-10T15:41:24.239Z","updatedAt":"2023-05-10T15:41:24.239Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_1","body":"import ray\n\n# Define model and preprocessor.\nmodel = ...\npreprocess_fn = ...\n\n# Define the inference function.\ndef model_fn(batch: Dict[str, np.ndarray]):\n    return {\"results\": model(batch)}\n\n# The following snippet implements the pipeline.\nray.data.read_parquet(...) \\           # 1. Load\n    .map_batches(preprocess_fn) \\      # 2. Preprocess\n    .map_batches(model_fn) \\           # 3. Inference\n    .write_parquet(...)                # 4. Save\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a BSP framework, each of these operations can be modeled as a 1:1 transformation, which can be fused together into a single stage. Each task within the stage will execute the mentioned operations locally in sequence:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4oQ40RlSK1AF7Y4ZnVGV8J","type":"Asset","createdAt":"2023-05-10T15:44:10.631Z","updatedAt":"2023-05-10T18:59:16.654Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_4","description":"Figure 4. BSP framework operations modeled as a 1:1 transformation","file":{"url":"//images.ctfassets.net/xjan103pcp94/4oQ40RlSK1AF7Y4ZnVGV8J/a0413e90a55ec69d7d0ba49554a7f2a0/Screenshot_from_2023-05-10_11-07-39.png","details":{"size":103326,"image":{"width":1511,"height":583}},"fileName":"Screenshot from 2023-05-10 11-07-39.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This execution strategy is ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"memory optimal","nodeType":"text"},{"data":{},"marks":[],"value":", as there is no intermediate data and execution is inherently pipelined (since there is only a single stage). However, challenges emerge in a heterogeneous setting, where certain pipeline steps may prefer to run on CPUs or GPUs independently. In Ray Data, such inference pipelines are expressed with transformations that launch actors scheduled onto GPUs ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"(num_gpus=1 and compute=ActorPoolStrategy)","nodeType":"text"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3nLf8c4wMq9KInsI4ca7HB","type":"Entry","createdAt":"2023-05-10T15:46:05.506Z","updatedAt":"2023-05-10T16:00:09.475Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_2","body":"import ray\nfrom ray.data import ActorPoolStrategy\n\nModel = ...\npreprocess_fn = ...\n\n# Define the model as a stateful class with cached setup.\nclass MyModelCallableCls:\n    def __init__(self):\n        self.model = Model(...)\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e\n        return {\"results\": self.model(batch)}\n\n# Modify the pipeline to use GPU actors.\nray.data.read_parquet(...) \\\n    .map_batches(preprocess_fn) \\\n    .map_batches(\n          MyModelCallableCls,\n          num_gpus=1,\n          compute=ActorPoolStrategy(size=N)) \\\n    .write_parquet(...)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the above code snippet, the load and preprocessing steps run on CPU (Stage 1), inference runs on GPUs (Stage 2), and then the result saving runs on CPU again (Stage 3). This configuration leads to spilling of data to remote storage when intermediate results (e.g., decoded video frames) exceed cluster memory sizes. Hence, we see that ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"BSP is not memory optimal for heterogeneous workloads","nodeType":"text"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"66FIu8nzl2kZRB1Ht5xZFL","type":"Asset","createdAt":"2023-05-10T15:49:26.155Z","updatedAt":"2023-05-10T19:00:24.815Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_5","description":"Figure 5. BSP is not memory optimal for heterogeneous workloads","file":{"url":"//images.ctfassets.net/xjan103pcp94/66FIu8nzl2kZRB1Ht5xZFL/295ab9176fab5d04ee47ac53c5e1b418/Screenshot_from_2023-05-10_11-02-18.png","details":{"size":162611,"image":{"width":1750,"height":730}},"fileName":"Screenshot from 2023-05-10 11-02-18.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"These unnecessary overheads can be avoided with end-to-end pipelining (i.e., streaming) across the cluster, which we present in the next section.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data streaming","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In Ray 2.4, we've changed the default execution strategy of Ray Data to streaming from bulk synchronous. These streaming Datasets are fully backwards compatible with the existing API, i.e., they can be transformed lazily with map operations, support shuffle operations, and also caching / materialization in memory:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5CSJWRWgN6pQwxcFMBhMSr","type":"Entry","createdAt":"2023-05-10T15:50:53.654Z","updatedAt":"2023-05-10T15:59:44.870Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_3","body":"# Create a dataset over parquet files\nds: ray.data.Dataset = ray.data.read_parquet(...)\n\n# Transform the dataset\nds = ds.map_batches(my_preprocess_fn)\nds = ds.map_batches(my_model_fn)\n\n# Iterate over dataset batches in streaming fashion\nfor batch in ds.iter_batches():\n   print(batch)\n\n# Materialize all contents of the dataset in memory\nds = ds.materialize()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In other words, the Ray Data API now leverages streaming execution for improved performance on large datasets, with the same simple transformation API as in previous Ray versions.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Video processing example","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To more fully understand the capabilities of this API, let's walk through what this looks like in an example video processing pipeline. In the pipeline, we'll first decode video frames, annotate each frame with an ML model, apply a classification model to the annotated frames, and then save the results to JSON:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"s1kmF3kQraHomGuDsvOQt","type":"Asset","createdAt":"2023-05-10T15:55:25.682Z","updatedAt":"2023-05-10T19:30:26.377Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"streaming_figure_6","description":"Figure 6. A pipeline decoding video frames, annotating frames, applying classification, and saving results to JSON.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/s1kmF3kQraHomGuDsvOQt/9385b71522afe76eebc6342f54387ffa/Screenshot_from_2023-05-10_12-24-36.png","details":{"size":38452,"image":{"width":1478,"height":230}},"fileName":"Screenshot from 2023-05-10 12-24-36.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's start by declaring the components of our pipeline. For brevity, we'll just provide stub classes here, but you can find the ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/ericl/ac086f0a9bdf41b6101dca665045dfec"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"full skeleton here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (and the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference docs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") that can be adapted to your own workloads. Data between these steps is kept in dicts of numpy arrays:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3DoR7DQ01JPe4k0MjPdubR","type":"Entry","createdAt":"2023-05-10T15:57:40.210Z","updatedAt":"2023-05-10T15:59:24.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_5","body":"# Declare a function that decodes frames.\ndef decode_frames(batch: Dict[str, np.ndarray]) -\u003e Dict:\n    video_data = batch[\"bytes\"]\n    ...\n    return {\"frames\": decoded_frames}\n\n# Declare a model that annotates decoded frame data.\nclass FrameAnnotator:\n    def __init__(self):\n        self.model = ...\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e Dict:\n        frames = batch[\"frames\"]\n        return {\"annotated_frames\": self.model(frames)}\n\n# Declare a model that classifies annotated frame data.\nclass FrameClassifier:\n    def __init__(self):\n        self.model = ...\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e Dict:\n        frames = batch[\"annotated_frames\"]\n        return {\"results\": self.model(frames)}","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next, we'll use Ray Data to connect these steps together. This takes just a few lines of code, and we can customize the resource requirements per step to configure CPUs and GPUs:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1U2wSU4PEADDsQpVQqJJAa","type":"Entry","createdAt":"2023-05-10T15:59:06.340Z","updatedAt":"2023-05-10T15:59:06.340Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_6","body":"# Create a dataset from video binary data.\nds = ray.data.read_binary(...)\n\n# Apply the decode step. We can customize the resources per\n# task. Here each decode task requests 4 CPUs from Ray.\nds = ds.map_batches(decode_frames, num_cpus=4)\n\n# Apply the annotation step, using an actor pool of size 5.\n# Each actor runs on a CPU.\nds = ds.map_batches(\n    FrameAnnotator,\n    compute=ActorPoolStrategy(size=5))\n\n# Apply the classification step, using a pool of 2 GPU actors,\n# and a fixed data batch size of 64 for the actor calls.\nds = ds.map_batches(\n    FrameClassifier,\n    num_gpus=1,\n    batch_size=64,\n    compute=ActorPoolStrategy(size=2))\n\n# Trigger execution and write outputs to json.\nds.write_json(\"/tmp/output\")\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The configured streaming topology from above looks like this logically. Since each step has a unique resource requirement / task vs actor strategy, here every step becomes its own stage (Ray Data fuses steps with equivalent resource requirements):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2XaeSiV04QKLILBLAlD0t5","type":"Asset","createdAt":"2023-05-11T15:44:03.608Z","updatedAt":"2023-05-11T15:44:03.608Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"streaming_figure_7","description":"Figure 7: Streaming execution across CPUs and GPUs","file":{"url":"//images.ctfassets.net/xjan103pcp94/2XaeSiV04QKLILBLAlD0t5/d75fceb54323c7587312028b40a4b37c/Screenshot_from_2023-05-10_11-02-51.png","details":{"size":68124,"image":{"width":1445,"height":299}},"fileName":"Screenshot from 2023-05-10 11-02-51.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We run the above script (","nodeType":"text"},{"data":{"uri":"https://gist.github.com/ericl/ac086f0a9bdf41b6101dca665045dfec"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GitHub gist","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") in an Anyscale workspace with 2 GPU nodes and 5 CPU nodes (you can use the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/getting-started.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray cluster launcher","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to also launch a cluster for this):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3hsRFIiW2RGkFM5Igr2jVT","type":"Entry","createdAt":"2023-05-10T16:04:25.585Z","updatedAt":"2023-05-10T16:06:19.918Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_7","body":"$ python workload.py\n\n2023-05-02 15:10:01,105 INFO streaming_executor.py:91 -- Executing DAG\nInputDataBuffer[Input] -\u003e TaskPoolMapOperator[MapBatches(decode_frames)] -\u003e \nActorPoolMapOperator[MapBatches(FrameAnnotator)] -\u003e\nActorPoolMapOperator[MapBatches(FrameClassifier)] -\u003e \nTaskPoolMapOperator[Write]\n2023-05-02 15:10:01,128 INFO actor_pool_map_operator.py:114 -- MapBatches(FrameAnnotator): Waiting for 5 pool actors to start...\n2023-05-02 15:10:02,691 INFO actor_pool_map_operator.py:114 -- MapBatches(FrameClassifier): Waiting for 2 pool actors to start...\n\nRunning: 25.0/112.0 CPU, 2.0/2.0 GPU, 33.19 GiB/32.27 GiB object_store_memory:  28%|███▍        | 285/1000 [01:40\u003c03:12,  3.71it/s]","language":"Shell"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"While the workload is running, we can view the observability graphs in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-core/ray-dashboard.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Dashboard","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". We can see from the actor metrics that our 7 worker actors were active for the entire run (RUNNING_TASK), and that Ray Data kept them busy with MapBatches(FrameClassifier) as well as MapBatches(FrameAnnotator) tasks. Because of the streaming execution strategy, tasks for all stages run concurrently:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5MDgPW8470uVoy6MFBqowq","type":"Asset","createdAt":"2023-05-10T16:09:18.740Z","updatedAt":"2023-05-10T16:09:27.512Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_8","description":"Figure 8. View active tasks and actors for this job in the Ray Dashboard","file":{"url":"//images.ctfassets.net/xjan103pcp94/5MDgPW8470uVoy6MFBqowq/39f6300387d895145313c0caee8149cf/image3.png","details":{"size":77415,"image":{"width":1216,"height":516}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can also inspect hardware utilization in the Ray dashboard. We'll focus on the network activity. We can see two nodes receiving ~500MiB/s (these are the two GPU nodes hosting the FrameClassifier actors), and a number of other nodes driving significant traffic from 100-200MiB/s, likely sending data to the GPU nodes or between the decode and FrameAnnotator steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6QEOgHncDDrSl3SKl9YzWF","type":"Asset","createdAt":"2023-05-10T16:12:30.030Z","updatedAt":"2023-05-10T19:54:24.559Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"streaming_figure_9","description":"Figure 9. Ray Dashboard showing network activity","file":{"url":"//images.ctfassets.net/xjan103pcp94/6QEOgHncDDrSl3SKl9YzWF/bae6e33fce2749f2ade8afd73681e16b/Screenshot_from_2023-05-10_12-42-27.png","details":{"size":107840,"image":{"width":1635,"height":707}},"fileName":"Screenshot from 2023-05-10 12-42-27.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The overall pipeline finishes in ~5 minutes, successfully processing ~350GB of video frame data total on a heterogeneous cluster of CPUs and GPUs. We see that Ray Data was able to pipeline the entire execution across the cluster to make the best use of memory.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Discussion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Optimizations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"In order to seamlessly execute such streaming topologies, Ray Data provides a number of optimizations under the hood. This includes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Memory stability","nodeType":"text"},{"data":{},"marks":[],"value":": Ray Data relies on the underlying Ray scheduler to schedule tasks and actors, but still needs to manage back-pressure across the streaming topology to bound memory usage and avoid object store spilling. It does this by only scheduling new tasks if it would keep the streaming execution under ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/data-internals.html#streaming-execution"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"configured resources limits","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Intuitively, enforcing a cap on intermediate result memory usage is needed to avoid degrading to bulk execution.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data locality","nodeType":"text"},{"data":{},"marks":[],"value":": While Ray will already place tasks on nodes where their input arguments are local, Ray Data's streaming backend extends this to optimize the scheduling of actor tasks. For example, in the above example, a lot of network traffic is avoided between the `decode_frames` and `FrameAnnotator` steps by routing decoded frames to actors that are on the same node.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Fault tolerance","nodeType":"text"},{"data":{},"marks":[],"value":": Ray Data leverages Ray's built-in ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-core/fault-tolerance.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"fault tolerance","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in order to handle object loss in large jobs. When objects are lost, they are recomputed based on their task lineage tracked by Ray. If actors were needed to produce these objects, Ray restarts these actors prior to re-submitting the task.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scalability","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"To give an idea of the upper scalability envelope of Ray Data streaming, we ran a number of synthetic benchmarks that stressed Ray's object manager and Ray Data's ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/execution/streaming_executor.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"streaming executor","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" processing a 20TiB array dataset. Under the hood, Ray Data is using ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.remote() / ray.wait() ","nodeType":"text"},{"data":{},"marks":[],"value":"to orchestrate the streaming topology, which means it is similarly scalable as Ray itself:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2vKUP0VZYl0yPNspQmfU05","type":"Asset","createdAt":"2023-05-10T16:17:37.701Z","updatedAt":"2023-05-10T16:17:37.701Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"streaming_figure_10","description":"Figure 10. Synthetic benchmarks stressing Ray's object manager and Ray Data's streaming executor processing a 20TiB array dataset","file":{"url":"//images.ctfassets.net/xjan103pcp94/2vKUP0VZYl0yPNspQmfU05/0566c92beb5ea743d9ecc3404a551705/image9.png","details":{"size":44857,"image":{"width":1636,"height":640}},"fileName":"image9.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"These benchmarks were run on a cluster of 500 machines, and test pipelines with between one to four stages similar to the video streaming example. We see that for the simplest pipeline on this cluster, Ray Data can process input data at a rate exceeding 1TiB/s. For more complex multi-stage pipelines, Ray Data can still sustain about 100-200GiB/s of end-to-end throughput on a large cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed Training","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"A significant portion of ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" users today are also using Ray Data as a distributed data backend for ML training. Ray Data streaming allows these users to efficiently work with datasets that are larger than cluster memory for training, leveraging both CPU and GPU nodes to speed up their jobs. We are working on enhancing Ray Train's API to natively work with Ray Data streaming.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What's next?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Today, early users are taking advantage of the Ray Data streaming backend to create efficient large-scale inference pipelines over unstructured data, including video and audio data. To learn more about how to build your own pipeline, check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference tutorial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in our docs. We'll also be following up with a blog on more unstructured application examples. To learn more generally about streaming in Ray Data, check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/data.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"library docs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and let us know what ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/issues"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"improvements","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" you'd like to see to Ray Data.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Also, we will be presenting at the ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Register now","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" — early bird registration is open until","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" June 30, 2023.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5qTVayH4YnKM7pGL8CiSwc","type":"Asset","createdAt":"2023-05-11T15:46:46.691Z","updatedAt":"2023-05-11T15:46:46.691Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_streaming_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5qTVayH4YnKM7pGL8CiSwc/99806986301b595308c6c89a8bad6249/images_and_diagrams__13_.png","details":{"size":83586,"image":{"width":960,"height":540}},"fileName":"images and diagrams (13).png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"recommendations":[{"fields":{"content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7xkHJWpQ3mRxrgAQw3oYFk","type":"Entry","createdAt":"2023-06-15T13:47:45.558Z","updatedAt":"2023-06-16T13:24:48.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Introducing the Anyscale Databricks Connector","seoTitle":"Integrating Anyscale Ray with Databricks","slug":"introducing-the-anyscale-databricks-connector","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3e6JquwKuIz6QvCWO0BAz","type":"Entry","createdAt":"2023-06-15T13:16:27.453Z","updatedAt":"2023-06-15T13:16:27.453Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Greene","slug":"eric-greene","link":"https://www.linkedin.com/in/greene/"}}],"publishedDate":"2023-06-15","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Introducing the Anyscale Databricks connector. Simple Data Transfer and Efficient AI Workflows with Anyscale and Databricks.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The Anyscale Databricks Connector is a new Ray Datasets capability that facilitates easy data transfer between Databricks clusters and Anyscale hosted Ray clusters. The connector makes it easy for Data Scientists using Anyscale to leverage their Databricks data lake during machine learning and discovery.  It also enables a simpler way for machine learning engineers to create end-to-end workloads by allowing the entire ML pipeline to be executed within a single Python script. By taking advantage of the highly-scalable nature of Ray and Ray Datasets, machine learning workloads such as training, tuning and batch serving jobs can be executed more quickly and at lower cost, all while taking advantage of the latest advancements in AI through other Ray integrations with Hugging Face, XGBoost, LightGBM and many AI frameworks and libraries.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Key Benefits","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simple access to data","nodeType":"text"},{"data":{},"marks":[],"value":" - Access Databricks data through SQL queries within Anyscale Workspaces Visual Code or JupyterHub environments, empowering data science discovery and development.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Improved data security and governance","nodeType":"text"},{"data":{},"marks":[],"value":" - Ensure data security and governance by directly copying data from Databricks data lakes to Ray clusters, eliminating the need for intermediate steps and maintaining separate controls over sensitive data. All data is encrypted in transit and at rest.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Highly-scalable data exchange","nodeType":"text"},{"data":{},"marks":[],"value":" - Take advantage of the parallel read and write capabilities of Ray Datasets, enabling the exchange of terabytes of data in minutes. This capability significantly speeds up data-intensive operations and enhances scalability while reducing job run times and overall costs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simplified workload development","nodeType":"text"},{"data":{},"marks":[],"value":" - Simplify machine learning workflows by consolidating all the necessary logic into a single script. This script can query features, train and tune models, and score and materialize results back into the data lake, streamlining the entire process.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Unlock the latest AI Innovations","nodeType":"text"},{"data":{},"marks":[],"value":" - Leverage the power of Databricks for querying and joining data, while benefiting from the scalability and simplicity of Ray AIR for machine learning and AI development. Ray AIR provides integration to the latest AI innovations such as pretrained Hugging Face language models.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Simple, Secure and Scalable Data Access","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing the Anyscale Databricks connector, large datasets can be queried from a Databricks SQL warehouse, and quickly transferred into a Ray Dataset distributed across the Ray cluster. The Databricks connector reads query results in chunks of data, in parallel across the Ray cluster. The size of the dataset and the speed at which it can be transferred scales based on the size of your Ray cluster and the number of simultaneous requests the underlying Databricks SQL warehouse supports.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6taTlZK9ffeivO5RCUJiQ4","type":"Asset","createdAt":"2023-06-15T13:27:43.116Z","updatedAt":"2023-06-15T13:27:43.116Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"1 - Anyscale Databricks Architecture","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6taTlZK9ffeivO5RCUJiQ4/c5fc0bfef768f0fb6e8785bf71ded1e2/1_-_Anyscale_Databricks_Arch.jpg","details":{"size":95989,"image":{"width":761,"height":478}},"fileName":"1 - Anyscale Databricks Arch.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Anyscale Databricks connectors, parallel reads and writes and overall speed will scale with the size of the Ray cluster. The benchmarks for reading and writing demonstrate how the cluster scales out to support larger data sets.  ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2KimYEe845rk6fb2oYaCmz","type":"Asset","createdAt":"2023-06-15T13:35:36.597Z","updatedAt":"2023-06-15T13:35:36.597Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2 - Anyscale Databricks Table Example","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2KimYEe845rk6fb2oYaCmz/8302017d6535b5d676d3dfb50dc12cbd/2_-_Anyscale_Databricks_Table_Example.jpg","details":{"size":218942,"image":{"width":2178,"height":1002}},"fileName":"2 - Anyscale Databricks Table Example.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Simplified ML Development","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing the Databricks connector from within an Anyscale Workspaces, Data Scientists and Machine Learning Engineers have a unified experience while developing workloads. The data query, feature engineering, training, tuning and inference can all be executed within a single Python script that scales across a Ray cluster. Ray integrates with the latest AI and Machine learning libraries, enabling the most advanced ML workloads that work with 3rd party libs  like Hugging Face, XGBoost, LightGBM, and most PyTorch and TensorFlow model architectures.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7c7IX8TpJKbaXdnLIwoD88","type":"Asset","createdAt":"2023-06-15T13:39:22.298Z","updatedAt":"2023-06-15T13:39:22.298Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"3 - Anyscale Databricks UI","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7c7IX8TpJKbaXdnLIwoD88/f17fb8c8884ce197c9ad35a8ce3f6b4e/3_-_Anyscale_Databricks_Product_UI.jpg","details":{"size":242920,"image":{"width":2226,"height":1230}},"fileName":"3 - Anyscale Databricks Product UI.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2muX02EO70X2MXN8wBRyUJ","type":"Asset","createdAt":"2023-06-15T13:41:08.580Z","updatedAt":"2023-06-15T13:41:08.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"4 - Anyscale Databricks Product UI 2","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2muX02EO70X2MXN8wBRyUJ/cc5301c192c66e632cb3141a9e5bde55/4_-_Anyscale_Databricks_Product_UI_2.jpg","details":{"size":383181,"image":{"width":1860,"height":1282}},"fileName":"4 - Anyscale Databricks Product UI 2.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlock new use cases with AI and ML ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nAnyscale and Ray integrate with most open source AI and ML libraries, enabling the latest innovation in AI to be applied to Databricks data. Ray makes working with Hugging face, XGBoost, LightGBM, TensorFlow and PyTorch and SciKit Learn a unified experience, with the added benefits of scaling with distributed training, tuning and serving.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical ML workload to train a LightGBM can be implemented in 20 lines of code.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6rUeSQ88fvbTd68jkhrii0","type":"Asset","createdAt":"2023-06-15T13:42:58.984Z","updatedAt":"2023-06-15T13:42:58.984Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"5 - Anyscale Databricks deployment","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6rUeSQ88fvbTd68jkhrii0/43de8dc64eaf8792ad73ac6faf8b4551/5_-_Anyscale_Databricks_deployment.jpg","details":{"size":272363,"image":{"width":1470,"height":1402}},"fileName":"5 - Anyscale Databricks deployment.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Want to see for yourself? ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/signup"},"content":[{"data":{},"marks":[],"value":"Request a Trial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" today!\n","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71IpYQvsxplh7okIL7RJVx","type":"Asset","createdAt":"2023-06-15T13:50:54.646Z","updatedAt":"2023-06-16T13:19:09.178Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"locale":"en-US"},"fields":{"title":"0 -Anyscale Databricks headline image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/71IpYQvsxplh7okIL7RJVx/b193e65fbc2d6500e3ccb2162d323080/0_-_Updated_Databricks_Anyscale_Image.png","details":{"size":90723,"image":{"width":1500,"height":1000}},"fileName":"0 - Updated Databricks Anyscale Image.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false,"recommendations":[]}}}},{"fields":{"content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4fUXNiWIPmVK9q6xW4YWV0","type":"Entry","createdAt":"2023-06-13T14:22:00.442Z","updatedAt":"2023-06-13T14:22:32.590Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Ray 2.5 features training and serving for LLMs, Multi-GPU training in RLlib, and enhanced Ray Data support","seoTitle":"Ray 2.5 features training and serving for LLMs, Multi-GPU training in RLlib, and enhanced Ray Data support","slug":"ray-2-5-features-training-and-serving-for-llms-multi-gpu-training-in-rllib","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}}],"publishedDate":"2023-06-13","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.5.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2. 5 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"release features focus on a number of enhancements and improvements across the Ray ecosystem. In this blog, we expound on a few key features, including:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Support for training LLMs with Ray Train","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ability to serve LLMs with Ray Serve","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Multi-GPU learner stack in RLlib for cost efficiency and scalable RL-agent training ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Performant and improved approach to batch inference at scale","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2ty9UoIKnnNnrvsudSmAvH","type":"Asset","createdAt":"2023-06-12T22:29:43.226Z","updatedAt":"2023-06-12T22:29:43.226Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray_2.5_main_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2ty9UoIKnnNnrvsudSmAvH/d42b6c80596baf3b1cd13103bdc97a55/image1.png","details":{"size":158300,"image":{"width":960,"height":540}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Improved support for LLMs in Ray Train","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"This release comes with a couple key features for improving LLM support in ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/train.html"},"content":[{"data":{},"marks":[],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed Checkpointing for distributed models: ","nodeType":"text"},{"data":{},"marks":[],"value":"With the recent influx of LLMs, we’ve noticed that there has been a lack of support across different frameworks for managing large model checkpoints. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"One common workaround is to gather the entire model checkpoint onto a single worker, before uploading it to some cloud storage. This introduces two problems (see Figure 1): ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"An extra step of communication bottleneck by the bandwidth of a single node.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Can lead to out of memory (OOM) issues for sufficiently large models during gathering of model states.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this release, we’re introducing a new experimental feature for supporting large model checkpoints that resolves these problems.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6HXGTDK9EABlzG19g3DmIX","type":"Asset","createdAt":"2023-06-12T22:34:17.334Z","updatedAt":"2023-06-12T22:34:17.334Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_2.5_release_fig_1","description":"Figure 1. Single node uploading the full checkpoint after gathering from all workers","file":{"url":"//images.ctfassets.net/xjan103pcp94/6HXGTDK9EABlzG19g3DmIX/9ec4291d7df025c2220da44789dcaeb3/image2.png","details":{"size":185446,"image":{"width":1999,"height":1017}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In model parallel training workloads, different partitions of a model are held by different workers, in contrast to data parallel training workloads, where the same model is replicated across different workers. To support proper checkpointing of distributed models, Ray Train can now be configured to save different partitions of the model held by each worker and upload its respective partitions directly to cloud storage. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6WdvLOMIcwsoDsro3P9362","type":"Asset","createdAt":"2023-06-12T22:36:11.421Z","updatedAt":"2023-06-12T22:36:11.421Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_2.5_figure_2","description":"Figure 2. Individual worker nodes uploading their respective checkpoints\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/6WdvLOMIcwsoDsro3P9362/eefd86d37bfb5779946e56f9dbbba6bb/image3.png","details":{"size":225462,"image":{"width":1999,"height":1042}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To use this feature, enable cloud storage, then include ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"_checkpoint_keep_all_ranks","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"_checkpoint_upload_from_workers","nodeType":"text"},{"data":{},"marks":[],"value":" as part of ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/ray-air/api/doc/ray.air.RunConfig.html#ray-air-runconfig"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RunConfig","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". This feature will work for the following trainer APIs:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.TorchTrainer.html"},"content":[{"data":{},"marks":[],"value":"TorchTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/api/doc/ray.train.huggingface.AccelerateTrainer.html#ray.train.huggingface.AccelerateTrainer"},"content":[{"data":{},"marks":[],"value":"AccelerateTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (with deepspeed and FSDP integrations)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/api/doc/ray.train.huggingface.TransformersTrainer.html#ray.train.huggingface.TransformersTrainer"},"content":[{"data":{},"marks":[],"value":"TransformersTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (with deepspeed and FSDP integrations)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Note","nodeType":"text"},{"data":{},"marks":[],"value":": This feature should only be turned on if your training loop is configured to save the sharded model state per worker. For example, when using ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"TransformersTrainer","nodeType":"text"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"AccelerateTrainer","nodeType":"text"},{"data":{},"marks":[],"value":" with deepspeed, the ","nodeType":"text"},{"data":{"uri":"https://deepspeed.readthedocs.io/en/latest/zero3.html#deepspeed.runtime.zero.config.DeepSpeedZeroConfig.gather_16bit_weights_on_model_save"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"gather_16bit_weights_on_model_save deepspeed configuration","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" should be set to False. See the example below for a skeleton of what your training script should look like:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UnXTMYsWcyYWDzEfohDzo","type":"Entry","createdAt":"2023-06-12T22:40:59.358Z","updatedAt":"2023-06-12T22:40:59.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_2.5_release_code_snippet_1","body":"def trainer_init_per_worker(train_dataset, eval_dataset=None, **config):\n    deepspeed = {\n        ...,\n        \"zero_optimization\": {\n            # Configure deepspeed to save checkpoint shards.\n            \"gather_16bit_weights_on_model_save\": False,\n            ...\n        }\n    }\n    training_args = TrainingArguments(\n        ...,\n        deepspeed=deepspeed,\n    )\n    trainer = Trainer(..., args=training_args)\n    return trainer\n\ntrainer = TransformersTrainer(\n    trainer_init_per_worker=trainer_init_per_worker,\n    scaling_config=ScalingConfig(num_workers=4),\n    run_config=RunConfig(\n        # Requirement: Use cloud storage\n        # Your checkpoints will be found within \"s3://your-s3-bucket/example\"\n        storage_path=\"s3://your-s3-bucket\",\n        name=\"example\",\n        checkpoint_config=CheckpointConfig(\n            _checkpoint_keep_all_ranks=True,\n            _checkpoint_upload_from_workers=True,\n        ),\n    )\n    datasets=...\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"For other supported trainers, we plan to write full-fledged examples showing their distributed checkpoint configuration in the documentation shortly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"LightningTrainer FSDP support: ","nodeType":"text"},{"data":{},"marks":[],"value":"In ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/announcing-ray-2-4-0-infrastructure-for-llm-training-tuning-inference-and"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2.4,","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" we released alpha support for the LightningTrainer. After user feedback, we’ve introduced support for ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/dolly_lightning_fsdp_finetuning.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FSDP","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"LightningTrainer","nodeType":"text"},{"data":{},"marks":[],"value":", and an example can be found ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/dolly_lightning_fsdp_finetuning.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"HuggingFace Trainer renaming: ","nodeType":"text"},{"data":{},"marks":[],"value":"In this release, for naming consistency and logical modularity, we are also renaming the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"HuggingFaceTrainer","nodeType":"text"},{"data":{},"marks":[],"value":" to ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"TransformersTrainer","nodeType":"text"},{"data":{},"marks":[],"value":", and we are also moving the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"AccelerateTrainer","nodeType":"text"},{"data":{},"marks":[],"value":" into the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"HuggingFace","nodeType":"text"},{"data":{},"marks":[],"value":" package, so that we can have a more intuitive organization of these integrations. For example,","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DMVT7rhes70fG1fDO9XT3","type":"Entry","createdAt":"2023-06-12T22:42:51.134Z","updatedAt":"2023-06-12T22:42:51.134Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_2.5_release_code_snippet_2","body":"from ray.train.huggingface import AccelerateTrainer, TransformersTrainer","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve for serving LLMs","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We have added two experimental features that augment the use of Ray Serve for online batch inference for streaming responses and model multiplexing for load balancing and serving multiple models across multiple replicas.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Streaming Response","nodeType":"text"},{"data":{},"marks":[],"value":": Some applications, in particular text generation in large language models (LLMs) or video processing, require return of incremental results to the caller. For instance, in the case of LLMs or large neural networks, a full forward pass could take multiple seconds, so providing incremental results offers a better user experience.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can achieve returning a ","nodeType":"text"},{"data":{"uri":"https://www.starlette.io/responses/#streamingresponse"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"StreamingResponse","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" from your HTTP request by wrapping a Python generator in your HTTP handler. Supported in basic HTTP ingress deployments in ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/serve/http-guide.html#serve-fastapi-http"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FastAPI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", the code snippet below shows how to.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7z6Hpu3CoSVOQudOmSNPU2","type":"Entry","createdAt":"2023-06-12T22:47:47.404Z","updatedAt":"2023-06-12T22:47:47.404Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_2.5_release_code_snippet_3","body":"import time\nfrom typing import Generator\n\nimport requests\nfrom starlette.responses import StreamingResponse\nfrom starlette.requests import Request\n\nfrom ray import serve\n@serve.deployment\nclass StreamingResponder:\n    def generate_numbers(self, max: int) -\u003e Generator[str, None, None]:\n        for i in range(max):\n            yield str(i)\n            time.sleep(0.1)\n\n    def __call__(self, request: Request) -\u003e StreamingResponse:\n        max = request.query_params.get(\"max\", \"25\")\n        gen = self.generate_numbers(int(max))\n        return StreamingResponse(gen, status_code=200, media_type=\"text/plain\")\n\nserve.run(StreamingResponder.bind())\n\nr = requests.get(\"http://localhost:8000?max=10\", stream=True)\nstart = time.time()\nr.raise_for_status()\nfor chunk in r.iter_content(chunk_size=None, decode_unicode=True):\n    print(f\"Got result {round(time.time()-start, 1)}s after start: '{chunk}'\")","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This short snippet yields the following streaming response:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38k4c6oQO3N8jvh1k3VScN","type":"Entry","createdAt":"2023-06-12T22:48:47.801Z","updatedAt":"2023-06-12T22:48:47.801Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_2.5_release_code_snippet_4","body":"…\nGot result 0.0s after start: '0'\nGot result 0.1s after start: '1'\nGot result 0.2s after start: '2'\nGot result 0.3s after start: '3'\nGot result 0.4s after start: '4'\nGot result 0.5s after start: '5'\nGot result 0.6s after start: '6'\nGot result 0.7s after start: '7'\nGot result 0.8s after start: '8'\nGot result 0.9s after start: '9'\n(ServeReplica:default_StreamingResponder pid=41052) INFO 2023-05-25 10:49:52,230 default_StreamingResponder default_StreamingResponder#qlZFCa yomKnJifNJ / default replica.py:634 - __CALL__ OK 1017.6ms\n","language":"shell"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Model Multiplexing: ","nodeType":"text"},{"data":{},"marks":[],"value":"A common use case we observe among ML practitioners is deploying multiple models that have dissimilar model shapes. For example, a different network architecture is trained for a particular SKU, user_id, or geo-location but takes similar inputs and produces a respective output. The multiple models are deployed across a pool of replicas among which requests are load balanced. When a request arrives, depending on the request header that contains model id such as SKU, user_id, or zip_code, the request is routed to the right and respective model replica.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For brevity we refer you to an example in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/serve/model-multiplexing.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" of how to write a multiplexed deployment for the above mentioned use case.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Multi-GPU stack for cost efficient, scalable, Multi-GPU RL agents training","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The training of reinforcement learning (RL) agents is hindered by the sampling process, which acts as the main bottleneck. While sampling can be distributed across multiple compute nodes as RolloutWorkers and simulators, the training phase is restricted to a single node. Consequently, the number of GPUs available for training is limited to a single GPU. This again creates another bottleneck on the batch size that can be effectively trained due to the memory constraints of a single GPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1erBs3bcBldQrmMcIGp0rK","type":"Asset","createdAt":"2023-06-12T22:52:56.639Z","updatedAt":"2023-06-12T22:52:56.639Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_2.5_release_figure_3","description":"Figure 3. Challenges and solutions for RLlib data collection and training ","file":{"url":"//images.ctfassets.net/xjan103pcp94/1erBs3bcBldQrmMcIGp0rK/327dd22c43747c4566c57762d2ba06ee/image4.png","details":{"size":106554,"image":{"width":1204,"height":900}},"fileName":"image4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In RLlib we introduce a multi-node, multi-gpu training stack that addresses both the challenges  and bottlenecks shown in Figure 3. With this new stack we can combine different types of GPUs to reduce costs by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"1.7x.","nodeType":"text"},{"data":{},"marks":[],"value":" ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In an upcoming blog, we detail implementation and experimentation showing RLlib's Proximal Policy Optimization (PPO) implementation on the ","nodeType":"text"},{"data":{"uri":"https://gymnasium.farama.org/environments/atari/breakout/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"ALE/Breakout-V5","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" environment on the new multi GPU training stack, using an increasing number of GPUs and larger batch sizes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performant and improved batch inference","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"One common and imperative workload that requires efficiency and optimized usage of hardware accelerators–both CPUs and GPUs–is batch inference. In the 2.4 Ray release, we introduced ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/streaming-distributed-execution-across-cpus-and-gpus"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data streaming execution","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" mode, which allows saturation of CPUs and GPUs for workloads such as offline ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/offline-batch-inference-comparing-ray-apache-spark-and-sagemaker"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Further improving Ray Data in this release, Ray Data provides additional enhancements. For instance, a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"strict mode ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"is enabled by default. This means that schemas are required for all Datasets, and standalone Python objects are no longer supported. Together with benefits from simplification, this also aligns the Ray Data API closer to industry-standard distributed data APIs like Apache Spark and emerging standards for machine learning datasets like HuggingFace.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Also, the default batch format is fixed to ","nodeType":"text"},{"data":{"uri":"https://numpy.org/"},"content":[{"data":{},"marks":[],"value":"NumPy","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", giving better performance for ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", along with the support of concurrent actors for ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ActorPool ","nodeType":"text"},{"data":{},"marks":[],"value":"helps too.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2SGa5CwRAvIGWOCpBR8Ha6","type":"Entry","createdAt":"2023-06-12T22:58:25.253Z","updatedAt":"2023-06-12T22:59:30.124Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_2.5_release_code_snippet_5","body":"from typing import Dict\nimport numpy as np\n\nimport ray\n\n# Step 1: Create a Ray Dataset from in-memory Numpy arrays.\n# You can also create a Ray Dataset from many other sources and file\n# formats.\nds = ray.data.from_numpy(np.asarray([\"Complete this\", \"for me\"]))\n\n# Step 2: Define a Predictor class for inference.\n# Use a class to initialize the model just once in `__init__`\n# and re-use it for inference across multiple batches.\nclass HuggingFacePredictor:\n    def __init__(self):\n        from transformers import pipeline\n        # Initialize a pre-trained GPT2 Huggingface pipeline.\n        self.model = pipeline(\"text-generation\", model=\"gpt2\")\n\n    # Logic for inference on 1 batch of data.\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e Dict[str, list]:\n        # Get the predictions from the input batch.\n        predictions = self.model(list(batch[\"data\"]), max_length=20, num_return_sequences=1)\n        # `predictions` is a list of length-one lists. For example:\n        # [[{'generated_text': 'output_1'}], ..., [{'generated_text': 'output_2'}]]\n        # Modify the output to get it into the following format instead:\n        # ['output_1', 'output_2']\n        batch[\"output\"] = [sequences[0][\"generated_text\"] for sequences in predictions]\n        return batch\n\n# Use 2 parallel actors for inference. Each actor predicts on a\n# different partition of data.\nscale = ray.data.ActorPoolStrategy(size=2)\n# Step 3: Map the Predictor over the Dataset to get predictions.\npredictions = ds.map_batches(HuggingFacePredictor, compute=scale)\n# Step 4: Show one prediction output.\npredictions.show(limit=1)\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"With each release of Ray, we strive toward ease of use, performance, and stability. This release marched towards that end by:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"extending Ray Train functionality to support distributed checkpointing for large language models","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"enhancing user experience in Ray Serve by returning HTTP streaming response to HTTP input requests  ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"extending Ray Serve functionality for multi-model serving by multiplexing among replicas of dissimilar shaped model architectures but similar input data types","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"solving bottlenecks and challenges in RLlib agent training by introducing a new multi-gpu, multi-node training stack for RLlib","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"improving easy use of Ray Data for batch inference ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"We want to thank all contributors for their valuable contributions to this new release of ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.5.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2.5","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Your enduring support continues to foster the wider use of Ray adoption.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Have a go at the latest release with pip install “ray[default]” and let us know of your feedback. We’re always delighted to share new Ray releases with you and equally interested to hear your feedback – feel free to reach out to us on ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Github","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discuss","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Join our ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/community"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Community","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the ","nodeType":"text"},{"data":{"uri":"https://forms.gle/9TSdDYUgxYs8SA9e8"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray #LLM slack channel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have our ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit 2023 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"early-bird registration open until 6/30. Secure your spot, save some money, savor the community camaraderie at the summit.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What’s Next?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stay tuned for additional Ray 2.5 related blogs on RLlib, meanwhile take a peek at the following blogs:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/announcing-aviary-open-source-multi-llm-serving-solution"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Announcing Aviary: Open Source Multi-LLM Serving","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/streaming-distributed-execution-across-cpus-and-gpus"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Streaming distributed execution across CPUs and GPUs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/offline-batch-inference-comparing-ray-apache-spark-and-sagemaker"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2ty9UoIKnnNnrvsudSmAvH","type":"Asset","createdAt":"2023-06-12T22:29:43.226Z","updatedAt":"2023-06-12T22:29:43.226Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray_2.5_main_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2ty9UoIKnnNnrvsudSmAvH/d42b6c80596baf3b1cd13103bdc97a55/image1.png","details":{"size":158300,"image":{"width":960,"height":540}},"fileName":"image1.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}}],"recommendations":[]}}}},{"fields":{"content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2GdxAQ2xXKoNlzKc7DXvLS","type":"Entry","createdAt":"2023-05-31T19:04:09.560Z","updatedAt":"2023-06-01T03:29:01.792Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Announcing Aviary: Open Source Multi-LLM Serving","slug":"announcing-aviary-open-source-multi-llm-serving-solution","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-05-31","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Exec Summary","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": Open source LLMs are getting better every day, so today we’re open sourcing Aviary to make testing, evaluating and deploying Open Source LLMs easier – we found it harder than we thought it should be so we used Ray Serve to fix it. ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Try it out with Aviary Explorer ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://aviary.anyscale.com/"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Github Repo ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/aviary"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Video demo and explanation ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.youtube.com/watch?v=WmqPfQOXJ-4"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Slack support ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Forum support ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/c/llms-generative-ai/27"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Want a managed version of Aviary? Sign up ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://bit.ly/run-aviary"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". \n\nWe’re excited to announce the release of Aviary: a new open source project that simplifies and enables easy self-hosted serving of multiple LLM models efficiently. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We’re big fans of open source LLMs here at Anyscale. The rate of improvement of open source LLMs has been ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.semianalysis.com/p/google-we-have-no-moat-and-neither"},"content":[{"nodeType":"text","value":"nothing short of phenomenal","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". This has given the AI community many options beyond the big “closed” players like OpenAI, Anthropic, Cohere and more. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Why are companies exploring self hosted open source LLMs? There are a few reasons: ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Cost: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"LLMs are very expensive to operate. A single query can cost tens of cents. Using open source can be considerably cheaper. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Latency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"By colocating LLMs and business logic, sometimes even on the same machine, latency – one of the biggest issues with deploying LLMs – can be kept low.  ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Transparency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Organizations want to understand exactly what is happening in their models. Open source allows them to understand what’s happening inside them.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Deployment flexibility: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Open Source models can be deployed on premise, in the cloud using the user’s cloud resources, or as part of a SaaS offering.  ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Data Control: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Data governance issues are much easier to guarantee if the data does not leave your control. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Customization:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" Customizing/fine-tuning on proprietary data to enable domain specific answers at high quality.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We were writing some tools for ourselves to really understand the answers to the questions above, as well as the advantages of closed models, mainly focused around quality. A funny thing happened on the way, however: we discovered that while the open source models have improved, the infrastructure for serving LLMs has not kept up. We started building our own library for loading the models, autoscaling them efficiently, etc on top of Ray Serve, dealing with variations between the models on things like stop tokens etc. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It was then that we realized, actually the rest of the LLM community might benefit from this library as well. So, today we are releasing Aviary as open source. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary helps leverage the advantages of open source models by building on the solid foundation of ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io"},"content":[{"nodeType":"text","value":"Ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", the popular framework for scalable AI. In particular, it takes advantage of ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"nodeType":"text","value":"Ray Serve","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", the highly flexible serving framework that is part of Ray. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary does this by: ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Providing an extensive suite of pre-configured open source LLMs, with reasonable defaults that work out of the box. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Integrating acceleration approaches like DeepSpeed with the packaged LLMs. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Simplifying the deployment of multiple LLMs within a single unified framework. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Simplifying the addition of new LLMs to within minutes in most cases","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Offering unique autoscaling support, including scale-to-zero (a first in open source). ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We’ve also included a demo Gradio frontend that shows off what’s possible with these capabilities, as well as some command line tools to address the evaluation questions that originally motivated this project. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"You can see these features in the video below: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jx26kCBQB1GRKEulIDxoi","type":"Entry","createdAt":"2023-05-31T02:10:20.974Z","updatedAt":"2023-05-31T02:10:20.974Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Aviary Announcement Video","videoUrl":"https://www.youtube.com/watch?v=WmqPfQOXJ-4"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"While existing solutions have some of these features individually and we are grateful to build on their foundations (such as Hugging Face’s text-generation-inference), none of the existing solutions brings these capabilities together in a way that is convenient for users. We are also planning to continue to expand the feature set of Aviary, adding support for features like streaming, continuous batching and others.  ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary is also open to community contributions – especially for adding new LLMs. We’ll then redeploy these in production so the rest of the world can use those LLMs too. We will also be supporting Aviary through Slack, Discourse, GitHub issues and an LLM-based RayBot. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"At the same time we understand that not everyone wants to take on the additional challenges of maintaining their own aviary of models. For that reason, we will also be offering a hosted version of Aviary that builds on top of our Anyscale managed Ray platform and offers additional features around deployment including:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Using spot instances with on-demand fallback for large potential savings","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Zero downtime upgrades with no dropped requests","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Faster deployment and autoscaling","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Improved observability tools. ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We are also announcing that Aviary will be available at no additional charge for existing Anyscale customers via our workspaces solution, and we’re actively onboarding new Aviary customers now. If you’d like to deploy Aviary, please reach out to us ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://bit.ly/run-aviary"},"content":[{"nodeType":"text","value":"here","marks":[],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1c2aG5pfLWU6YenPBZIHvI","type":"Asset","createdAt":"2023-05-31T19:00:13.845Z","updatedAt":"2023-05-31T19:00:13.845Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Aviary Light Mode","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1c2aG5pfLWU6YenPBZIHvI/742f647b979b4eb4b682e37fc35c71a7/aviary-light-mode.png","details":{"size":712180,"image":{"width":3030,"height":1750}},"fileName":"aviary-light-mode.png","contentType":"image/png"}}},"mainImageFit":"contain","hideIntro":true,"showMainImage":true,"recommendations":[]}}}}],"bannerText":null,"bannerLink":null},"__N_SSG":true},"page":"/blog/[id]","query":{"id":"streaming-distributed-execution-across-cpus-and-gpus"},"buildId":"-oT8_WeW9m_lAxr26cJvY","isFallback":false,"gsp":true,"customServer":true,"scriptLoader":[]}</script><script type="text/javascript" id="hs-script-loader" async="" defer="" src="https://js.hs-scripts.com/20523749.js"></script><script type="text/javascript">
  _linkedin_partner_id = "4210313";
  window._linkedin_data_partner_ids = window._linkedin_data_partner_ids || [];
  window._linkedin_data_partner_ids.push(_linkedin_partner_id);
</script><script type="text/javascript">
  (function(l) { if (!l){window.lintrk = function(a,b){window.lintrk.q.push([a,b])};
  window.lintrk.q=[]} var s = document.getElementsByTagName("script")[0];
  var b = document.createElement("script");
  b.type = "text/javascript";b.async = true;
  b.src = "https://snap.licdn.com/li.lms-analytics/insight.min.js";
  s.parentNode.insertBefore(b, s);})(window.lintrk);
</script><noscript><img height="1" width="1" style="display:none" alt="" src="https://px.ads.linkedin.com/collect/?pid=4210313&amp;fmt=gif"/></noscript></body></html>