<!DOCTYPE html><html lang="en-US"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Why Third Generation ML Platforms are More Performant | Anyscale</title><meta name="description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta property="og:title" content="Why Third Generation ML Platforms are More Performant | Anyscale"/><meta property="og:description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta property="og:site_name" content="Anyscale"/><meta property="og:image" content="https://images.ctfassets.net/xjan103pcp94/5fTEska4fx1SFm7wlBB7JF/cb6fc1348b8a28bc0b91ef9dfb080adb/mlplatformCropped.png"/><meta property="fb:app_id" content=""/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta name="twitter:site" content=""/><meta name="twitter:title" content="Why Third Generation ML Platforms are More Performant | Anyscale"/><meta name="twitter:image" content="https://images.ctfassets.net/xjan103pcp94/5fTEska4fx1SFm7wlBB7JF/cb6fc1348b8a28bc0b91ef9dfb080adb/mlplatformCropped.png"/><meta name="next-head-count" content="14"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><link rel="icon" href="../static/favicon.ico"/><script>
(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-P8H6KQG');
</script><script src="https://tag.clearbitscripts.com/v1/pk_7a96be585f731437b35c7bad25c432b7/tags.js"></script><script src="https://www.googleoptimize.com/optimize.js?id=OPT-NPGB9W3"></script><script>
  !function(f,b,e,v,n,t,s)
  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
  n.queue=[];t=b.createElement(e);t.async=!0;
  t.src=v;s=b.getElementsByTagName(e)[0];
  s.parentNode.insertBefore(t,s)}(window,document,'script',
  'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '168099398554987');
  fbq('track', 'PageView');
</script><link rel="stylesheet" href="../static/font-awesome-4.7.0/css/font-awesome.min.css"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="true"/><script src="https://fast.wistia.com/assets/external/E-v1.js" async=""></script><meta name="ahrefs-site-verification" content="4be3c5d78cdad8823d038256ac06d4336ae7a84632dac0ed57975703fc14fe6b"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="../_next/static/css/dbba207740458c19.css" as="style"/><link rel="stylesheet" href="../_next/static/css/dbba207740458c19.css" data-n-g=""/><link rel="preload" href="../_next/static/css/8a274f3d4edb6eb9.css" as="style"/><link rel="stylesheet" href="../_next/static/css/8a274f3d4edb6eb9.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="../_next/static/chunks/webpack-94adde92c5e1c75b.js" defer=""></script><script src="../_next/static/chunks/framework-95c3ca6de27174dd.js" defer=""></script><script src="../_next/static/chunks/main-774d7304f1973b2a.js" defer=""></script><script src="../_next/static/chunks/pages/_app-fd8cb5a9775605f5.js" defer=""></script><script src="../_next/static/chunks/0f1ac474-1cd8c801d37caef0.js" defer=""></script><script src="../_next/static/chunks/1812-a871a62a8e8aa88c.js" defer=""></script><script src="../_next/static/chunks/9065-e24d5adee6cbba35.js" defer=""></script><script src="../_next/static/chunks/9614-b709f46f6d53dd35.js" defer=""></script><script src="../_next/static/chunks/8423-007cfdc371b27898.js" defer=""></script><script src="../_next/static/chunks/7846-ec0724342ae15ba3.js" defer=""></script><script src="../_next/static/chunks/pages/blog/[id]-4165e8a9d7b23d0c.js" defer=""></script><script src="../_next/static/-oT8_WeW9m_lAxr26cJvY/_buildManifest.js" defer=""></script><script src="../_next/static/-oT8_WeW9m_lAxr26cJvY/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Rubik&display=swap">@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4i1Uw.woff) format('woff')}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFWkUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFU0UzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFVUUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFWUUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFV0UzdYPFkZVO.woff) format('woff');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href="https://fonts.googleapis.com/css2?family=Outfit:wght@400;500;600;700&display=swap">@font-face{font-family:'Outfit';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4TC1C4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4QK1C4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4e6yC4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4deyC4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,400;0,700;1,400;1,700&display=swap">@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTeEw.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINscg.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKtdU.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7MIU.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGqZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuE6ZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuFKZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGKZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGaZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuF6ZJW9XjDg.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINce_fuJGl18QRY.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINccvfuJGl18QRY.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcdffuJGl18QRY.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINceffuJGl18QRY.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcePfuJGl18QRY.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcdvfuJGl18Q.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdzeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdXeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdLeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhd7eFaxOedfTDw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhd_eFaxOedfTDw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdHeFaxOedc.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIxsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIVsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIJsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AI5sdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AI9sdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIFsdP3pBms.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body class=""><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-P8H6KQG" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next" data-reactroot=""><svg xmlns="http://www.w3.org/2000/svg" class="sr-only"><symbol id="svg-arrow-right-gradient"><path d="M1.5 7.5h18" stroke="url(#paint0_linear_3927_132230)" stroke-width="1.5" stroke-linecap="round"></path><path d="m16.6562 3.65625 3.8442 3.84412-3.8442 3.84413" stroke="url(#paint1_linear_3927_132230)" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><defs><linearGradient id="paint0_linear_3927_132230" x1="1.06972" y1="8.58929" x2="12.2319" y2="-1.53819" gradientUnits="userSpaceOnUse"><stop stop-color="#8B20BE"></stop><stop offset="1" stop-color="#3579FF" stop-opacity=".97"></stop></linearGradient><linearGradient id="paint1_linear_3927_132230" x1="12.377" y1="7.75171" x2="16.9307" y2="11.8684" gradientUnits="userSpaceOnUse"><stop stop-color="#8B20BE"></stop><stop offset="1" stop-color="#3579FF" stop-opacity=".97"></stop></linearGradient></defs></symbol></svg><div class="Banner_container__zEmFV"><a href="https://raysummit.anyscale.com/?utm_source=anyscale&amp;utm_medium=website&amp;utm_campaign=ray_summit_2023&amp;utm_content=homepage_ticker" rel="noreferrer nofollow" target="_blank"><span class="Banner_banner-text__wr6Xi"><span class="Banner_main-text__cpJ6H">Are you passionate about Ray, scalable AI and distributed computing? Join us at Ray Summit!</span><span class="Banner_cta-button__0JCtt"> Find out more! <!-- --> <svg width="23" height="14" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1.5 6.75a.75.75 0 0 0 0 1.5v-1.5Zm18 1.5a.75.75 0 0 0 0-1.5v1.5Zm-18 0h18v-1.5h-18v1.5Z" fill="#fff"></path><path d="M17.187 3.126a.75.75 0 0 0-1.061 1.06l1.06-1.06ZM20.5 7.5l.53.53a.75.75 0 0 0 0-1.06l-.53.53Zm-4.374 3.314a.75.75 0 1 0 1.06 1.06l-1.06-1.06Zm0-6.627L19.97 8.03l1.06-1.061-3.843-3.844-1.061 1.06ZM19.97 6.97l-3.844 3.844 1.06 1.06 3.845-3.843L19.97 6.97Z" fill="#fff"></path></svg></span></span></a><button class="Banner_close-button__9ksfY" type="button" aria-label="Close Banner"></button></div><header class="inner-shadow sticky top-0 z-[1000] h-24 transform py-5 transition-transform duration-500 bg-primary translate-y-0"><div class="container mx-auto flex items-center justify-between"><a href="../index.html" class="text-white"><svg width="141" height="32" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><g clip-path="url(#anyscale-logo_svg__a)" fill="currentColor"><path d="M10.653 17.46H3.865C1.697 17.46 0 19.16 0 21.332v6.797c0 2.17 1.697 3.87 3.865 3.87h6.788c2.169 0 3.866-1.7 3.866-3.87V21.33c0-2.171-1.792-3.87-3.866-3.87Zm1.509 10.573c0 .85-.66 1.51-1.509 1.51H3.865a1.49 1.49 0 0 1-1.508-1.51v-6.796c0-.85.66-1.51 1.508-1.51h6.788c.849 0 1.509.66 1.509 1.51v6.796Z"></path><path d="M28.095 11.988h-8.108V3.87C19.892 1.7 18.196 0 16.027 0H3.865C1.697 0 0 1.7 0 3.87v6.608c0 2.171 1.697 3.87 3.865 3.87h13.67v13.687c0 2.171 1.698 3.87 3.866 3.87h6.6c2.168 0 3.865-1.699 3.865-3.87V15.858c0-2.076-1.697-3.775-3.771-3.87Zm-24.23 0a1.49 1.49 0 0 1-1.508-1.51V3.87c0-.85.66-1.51 1.508-1.51h12.162c.849 0 1.509.66 1.509 1.51v8.118H3.866Zm25.738 16.047c0 .85-.66 1.51-1.508 1.51h-6.6a1.49 1.49 0 0 1-1.508-1.51V14.348h8.108c.848 0 1.508.66 1.508 1.51v12.177ZM79.194 17.462l-3.017-8.118h-3.3l4.714 11.327-2.546 5.853h3.112l7.165-17.18H82.21l-3.016 8.118ZM93.335 14.819c-.66-.284-1.414-.472-2.075-.661-.66-.189-1.225-.378-1.79-.66-.378-.19-.566-.473-.566-.85 0-.378.188-.661.472-.85.847-.472 1.79-.378 2.639.094.377.284.566.661.566 1.133h2.828c0-1.133-.566-2.171-1.414-2.832-.943-.755-2.169-1.133-3.3-1.038-.849 0-1.697.094-2.45.472-.66.283-1.227.66-1.604 1.227-.377.472-.565 1.133-.565 1.794 0 .66.188 1.321.565 1.887.377.472.849.85 1.414 1.039.66.283 1.414.472 2.168.66.66.19 1.226.378 1.886.661.377.19.566.472.566.85 0 .377-.189.66-.471.85-.377.283-.85.377-1.32.377-.472 0-1.038-.094-1.415-.472-.377-.283-.565-.66-.66-1.133h-2.922c0 .755.282 1.416.754 1.982.472.567 1.131 1.133 1.791 1.416.849.378 1.698.567 2.546.567.849 0 1.697-.095 2.45-.472.661-.283 1.227-.66 1.604-1.227.377-.567.565-1.133.565-1.794 0-.66-.188-1.321-.66-1.888-.565-.66-1.037-.944-1.602-1.132ZM102.575 11.702c.566 0 1.036.095 1.508.472.377.283.66.755.849 1.227h3.205a5.366 5.366 0 0 0-1.886-3.02 5.348 5.348 0 0 0-3.582-1.039c-1.037 0-2.074.284-3.017.755a5.486 5.486 0 0 0-2.074 2.077 7.427 7.427 0 0 0 0 6.325c.472.85 1.131 1.604 2.074 2.076.943.472 1.98.756 3.017.756 1.32 0 2.546-.378 3.582-1.133a5.366 5.366 0 0 0 1.886-3.02h-3.111c-.283 1.037-1.32 1.698-2.357 1.604-.754 0-1.509-.283-1.98-.944a5.119 5.119 0 0 1 0-5.192 1.914 1.914 0 0 1 1.886-.944ZM118.412 10.1a.405.405 0 0 1-.377.378c-.094 0-.188 0-.188-.095a3.382 3.382 0 0 0-.944-.66c-.659-.378-1.507-.567-2.262-.567-.943 0-1.885.283-2.734.755a4.459 4.459 0 0 0-1.885 2.077c-.472.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.376.85 1.037 1.604 1.885 2.17.849.473 1.697.756 2.64.756.754 0 1.603-.189 2.263-.567.659-.283 1.131-.755 1.602-1.321v1.699h2.923V9.345h-2.923v.755Zm-.377 6.891c-.283.472-.66.944-1.132 1.227-.47.283-1.037.378-1.507.378-.567 0-1.037-.189-1.509-.378-.472-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.094-1.228.472-1.794.848-1.51 2.64-1.982 4.147-1.227h.095c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM55.907 10.1a.406.406 0 0 1-.377.378c-.095 0-.19 0-.19-.095a3.384 3.384 0 0 0-.942-.66c-.66-.378-1.508-.567-2.263-.567-.942 0-1.885.283-2.733.755a4.463 4.463 0 0 0-1.886 2.077c-.471.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.377.85 1.037 1.604 1.886 2.17.848.473 1.697.756 2.64.756.753 0 1.602-.189 2.262-.567.66-.283 1.131-.755 1.603-1.321v1.699h2.922V9.345h-2.922v.755Zm-.472 6.891c-.283.472-.66.944-1.131 1.227-.471.283-1.037.378-1.509.378-.565 0-1.037-.189-1.508-.378-.471-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.095-1.228.472-1.794.848-1.51 2.64-1.982 4.148-1.227h.094c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM126.708 5.477h-2.923v15.575h2.923V5.477ZM140.191 14.914c0-1.038-.189-2.077-.755-3.02-.471-.85-1.131-1.605-2.074-1.983a6.78 6.78 0 0 0-6.033 0 5.485 5.485 0 0 0-2.075 2.077c-.565 1.038-.754 2.077-.754 3.21 0 1.132.189 2.17.754 3.114a5.485 5.485 0 0 0 2.075 2.077c.942.472 1.979.755 3.016.755a6.56 6.56 0 0 0 3.489-1.038c.943-.661 1.602-1.7 1.885-2.738h-3.016c-.377.944-1.321 1.51-2.358 1.416-.659 0-1.414-.283-1.885-.755-.566-.472-.848-1.227-.848-1.982h8.484c.095-.378.095-.755.095-1.133Zm-8.579-.85c.093-.755.377-1.415.942-1.887.566-.472 1.226-.661 1.886-.661.754 0 1.414.189 1.979.66.566.473.849 1.133.849 1.794l-5.656.095ZM67.692 9.159a4.7 4.7 0 0 0-1.98.472c-.377.188-.754.377-1.037.66a.286.286 0 0 1-.377 0c-.094-.094-.094-.094-.094-.188v-.756H61.28v11.705h2.923v-6.418c-.094-.755.188-1.51.754-2.171.471-.472 1.226-.756 1.886-.756.66 0 1.414.284 1.885.755.472.567.754 1.322.66 2.172v6.418h2.923v-6.89c.094-1.322-.283-2.644-1.226-3.682-.848-.85-2.168-1.416-3.394-1.321Z"></path></g><defs><clipPath id="anyscale-logo_svg__a"><path fill="currentColor" d="M0 0h140.19v32H0z"></path></clipPath></defs></svg></a><div class="lg:relative lg:flex lg:flex-row hidden"><div class="max-h-[calc(100vh-192px)] items-center overflow-y-auto py-[1px] shadow-[0_-1px_0_#DAE3F2] lg:flex lg:overflow-visible lg:p-0 lg:shadow-none"><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Products</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="../platform.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Anyscale Platform</div></a><a href="../ray-open-source.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Open Source</div></a></div></div></div></div></div></div><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Solutions</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="container mx-auto mb-4 lg:mb-0 mt-2 lg:mt-0 lg:mr-12"><div class="header-card h-full rounded-[5px] p-6 lg:w-[296px]"><h5 id="scalable-ai-and-python">Scalable AI and Python</h5>
<p>Leverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more.</p></div></div><div class="grid lg:flex grid-cols-1 md:grid-cols-2 gap-x-10 lg:gap-x-12 container mx-auto"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><p class="text-base leading-normal font-normal text-darkGrey-300 px-6 lg:px-4 mb-4">Scalable AI and Python</p><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2"><a href="../data-ingestion.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Data Ingestion</div></a><a href="../reinforcement-learning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Reinforcement Learning</div></a><a href="../ray-air.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray AIR</div></a><a href="../model-serving.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Model Serving</div></a><a href="../hyperparameter-tuning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Hyperparameter Tuning</div></a></div></div></div><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><p class="text-base leading-normal font-normal text-darkGrey-300 px-6 lg:px-4 mb-4">Use Cases</p><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2"><a href="../large-language-models.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Large Language Models</div></a><a href="../demand-forecasting.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Demand Forecasting/Pricing</div></a><a href="../industrial-automation.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Industrial Automation</div></a><a href="../machine-learning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Scalable ML Platforms</div></a><a href="../natural-language-processing.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">NLP</div></a><a href="../recommendation-system.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Recommendation System</div></a></div></div></div></div></div></div></div><a class="block font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4 shadow-[0_1px_0_#DAE3F2] lg:shadow-none" href="../ray-ecosystem.html"><div class="w-full container mx-auto">Ecosystem</div></a><a class="block font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4 shadow-[0_1px_0_#DAE3F2] lg:shadow-none" href="../user-stories.html"><div class="w-full container mx-auto">Success Stories</div></a><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Learn</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="how-ray-solves-common-production-challenges-for-generative-ai.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Blog</div></a><a href="https://raysummit.anyscale.com/?utm_source=anyscale&amp;utm_medium=website&amp;utm_campaign=ray_summit_2023&amp;utm_content=home_nav-pulldown"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Summit 2023</div></a><a href="../event-category/rl-summit.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Videos &amp; Webinars</div></a><a href="https://github.com/ray-project/ray-educational-materials"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Training</div></a><a href="https://docs.anyscale.com"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Anyscale Docs</div></a></div></div></div></div></div></div><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Company</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="../about.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">About us</div></a><a href="../press.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">News</div></a><a href="../careers.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Careers</div></a><a href="../community.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Community</div></a></div></div></div></div></div></div></div><div class="absolute bottom-0 left-0 right-0 bg-white py-5 shadow-[0_-1px_0_#DAE3F2] lg:relative lg:ml-5 lg:bg-transparent lg:p-0 lg:shadow-none"><div class="container mx-auto flex justify-end"><a class="btn bg-gradient min-w-[320px] justify-center bg-blue-900 text-center lg:min-w-0" href="../beta.html">Try It Now</a></div></div></div><div class="cursor-pointer py-4 pl-6 lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M20.05 11H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95ZM20.05 16H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95ZM20.05 6H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95Z" fill="#fff"></path></svg></div></div></header><div class="Layout_page-sections__2__gb overflow-x-hidden"><div class="BlogPost_root__ly_um"><div class="Breadcrumbs_root__53EXt"><a class="Breadcrumbs_link__HBYkg" href="../index.html">Home</a><a class="Breadcrumbs_link__HBYkg" href="how-ray-solves-common-production-challenges-for-generative-ai.html">Blog</a><span class="Breadcrumbs_link__HBYkg">Blog Detail</span></div><div class="BlogPost_inner__70rgb"><div class="BlogPost_main__husly"><div class="container"><div class="ArticleHero_inner__lg98I"><h1 class="ArticleHero_title__JB2va">Why Third Generation ML Platforms are More Performant</h1><div class="ArticleHero_details__DZgPt"><div class="container"><span class="ArticleDetails_authors__aqDWy"><span>By </span><a href="../blog@author=eric-liang.html">Eric Liang</a><span>, </span><a href="../blog@author=chen-shen.html">Chen Shen</a><span> and </span><a href="../blog@author=waleed-kadous.html">Waleed Kadous</a><span>   </span></span><span class="ArticleDetails_article-published-tags__9VxRY"><span class="article-published">|   <!-- -->October 6, 2021</span></span></div></div></div></div><div class="ArticleBody_container__QoiWj ArticleBody_page_article__R6nNl"><div class="ArticleBody_inner__ml2H8"><p>In a <a href="the-third-generation-of-production-ml-architectures.html"><u>previous blog post</u></a>, we defined a &quot;3rd generation ML platform&quot; as one that offered full programmability for ML workflows. Key to a 3rd generation platform is the<b> </b>concept of a <b>programmable compute layer</b>. In this blog, we report on emerging patterns of distributed compute we see in advanced ML platform workloads. We show how <a href="https://www.ray.io/"><u>Ray</u></a>, a leading programmable compute layer, improves performance by 3-9x in relevant production workloads.</p><div class="ArticleBody_image__rd3Dj" style="width:2156px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/6ecYQnlH7n2Tcv8Ovf1IEZ/79da29e55ec74c834913c2072c6ebf8d/Fig1.png" alt="2ndVS3rdGeneration"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">A &quot;2nd generation&quot; (left) vs &quot;3rd generation&quot; (right) ML platform. Light-blue boxes represent clusters, and light-purple represents libraries. The 3rd generation platform eliminates cluster compute silos and improves performance and programmability.</span></div><h2>Performance Overheads in Second Generation Platforms</h2><p>Distributed ML workflows are typically composed from a few types of compute patterns: collective (i.e., a set of processes communicating with each other like in <a href="https://github.com/horovod/horovod"><u>distributed SGD</u></a>), chaining (i.e., a <a href="https://docs.metaflow.org/metaflow/basics#linear"><u>sequential workflow</u></a> of tasks run one after the other), and nesting (i.e., tasks that kick off other tasks, commonly seen in <a href="https://docs.ray.io/en/latest/tune/index.html"><u>hyperparameter tuning</u></a>):</p><div class="ArticleBody_image__rd3Dj" style="width:2180px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/7KYcuMZ98l5DdtDi10vHxp/594784d15b520df3924340af95bc0b86/CollectiveChainingNesting.png" alt="collectingChainingNesting"/></div></div></div><p>In <a href="the-third-generation-of-production-ml-architectures.html"><u>first generation platforms</u></a>, ML workflows were implemented as custom-built systems optimized for a specific workflow. In second generation platforms, flexibility was achieved by relying on <a href="https://neptune.ai/blog/best-workflow-and-pipeline-orchestration-tools"><u>workflow orchestrators</u></a> to provide chaining and nesting, gluing together separate systems that internally implement high-optimized collective operations.<i> </i>The following figure illustrates a 2nd generation distributed ML platform. Each step is run as a separate cluster by a workflow orchestrator (e.g., <a href="https://engineering.fb.com/2016/05/09/core-data/introducing-fblearner-flow-facebook-s-ai-backbone/"><u>FBLearner Flow</u></a>, <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html"><u>SageMaker Steps</u></a>, <a href="https://metaflow.org/"><u>Metaflow</u></a>, <a href="https://www.kubeflow.org/"><u>KubeFlow</u></a>):</p><div class="ArticleBody_image__rd3Dj" style="width:1676px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/60TSWgITq3zG5xHqP8z3Xi/badf3cbb8c64666b880e7b28d407e6bf/WorkflowOrchestrator.png" alt="WorkflowOrchestrator"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">A &quot;2nd generation&quot; distributed ML platform. A workflow orchestrator is needed to connect components implemented as separate distributed systems, limiting the programmability--- and performance--- of the platform.</span></div><p>However, this 2nd generation architecture imposes performance overheads and limits programmability. This is due to:</p><ol><li><p><b>Scheduling overheads. </b>These platforms rely on separately scheduled VMs or containers per step to distribute the workload. Each step may launch its own distributed framework (e.g., Spark or Distributed PyTorch). This leads to several seconds to minutes of scheduling overhead per step, and prevents optimizations like pipelining.</p></li><li><p><b>Data movement overheads</b>. The overhead between steps can be reduced substantially if data is kept in memory between steps when possible and not materialized to storage.</p></li><li><p><b>Programmability overheads</b>. Expressing fine-grained nesting or pipelining can require substantial changes to distributed systems code that are rarely accessible to end-users of existing ML platforms.</p></li></ol><h2>Chaining and Nesting can be Performance Bottlenecks</h2><p>In many cases, chaining and nesting are <i>not</i> performance bottlenecks. This is because the data transferred is small, or scheduling overhead is small compared to execution time. However, there are a growing number of scenarios where bottlenecks do arise. Here we overview several use cases that benefit from optimized chaining and nesting.</p><div><table>
<thead>
<tr>
<th>Use case</th>
<th>2nd generation platforms</th>
<th>3rd generation platforms</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Data Ingest for Model Training (Chaining):</strong> Chaining data processing and ML training is simple when the data can fit onto a single machine. It becomes challenging at <a href="https://arxiv.org/abs/2108.09373">large scale</a>, when you want to <a href="https://speakerdeck.com/anyscale/per-epoch-shuffling-data-loader-mix-it-up-as-you-train-clark-zinzow-anyscale">shuffle data globally</a> during training, or pipeline data preparation with training to <a href="https://docs.ray.io/en/latest/data/dataset-pipeline.html">reduce latency</a>. Although materializing data to cluster storage provides certain benefits, frequent I/O to cluster storage introduces a lot of overhead.</td>
<td><img src="https://images.contentful.com/xjan103pcp94/45y4oFh98EwSKMRaxzHcJ/93fee7c4c8c217df3a41173f52c6a733/01_twineditSimpleChain.png" alt="01_twineditSimpleChain"></td>
<td><img src="https://images.contentful.com/xjan103pcp94/whcbju48HM91FOIIyMHrC/2e2b019b20786f16f84b6479dbeed31b/01_twineditPipelined.png" alt="01_twineditPipelined"></td>
</tr>
<tr>
<td><strong>Serving Pipelines (Chaining):</strong> Similarly, passing data efficiently is critical to the performance of disaggregated <a href="serving-ml-models-in-production-common-patterns.html">model serving pipelines</a>. In a disaggregated pipeline, large models are split into multiple parts that can be deployed and scaled separately (e.g., fully-connected networks vs memory-intensive embedding lookups). Predictions can be composed of multiple models in a DAG structure (e.g., <a href="https://robovision.ai/">RoboVision</a> uses a 5-model stack for vehicle detection pipeline).</td>
<td><img src="https://images.contentful.com/xjan103pcp94/4UyLFxAEkYrZUIP0AWDXbO/cf95c047d9d4df80066ff766b12ee006/02_2nd_4x2_22font_SimpleServing.png" alt="02_2nd_4x2_22font_SimpleServing"></td>
<td><img src="https://images.contentful.com/xjan103pcp94/30rncKJXFtnXBK12vcHKyD/7915253df3ff9c68c2faea56e4a45342/02_3rd_4x2_22font_DisaggregatedModelServingPipeline.png" alt="02_3rd_4x2_22font_DisaggregatedModelServingPipeline"></td>
</tr>
<tr>
<td><strong>Batch Scoring (Nesting):</strong> Scoring a model on a large dataset can be quite slow without <a href="https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/ai/batch-scoring-python">distributed computation</a>, which naturally generates a nested workload during the course of training. Nesting can be difficult to express, let alone efficiently, in traditional workflow orchestrators.</td>
<td><img src="https://images.contentful.com/xjan103pcp94/5uWbLSWU26EZp5Q9kkEMMh/41c443ee64006e8ee8d88eb20cd65f70/03_2nd_4x2_22font_SequentuilScoring.png" alt="03_2nd_4x2_22font_SequentuilScoring"></td>
<td><img src="https://images.contentful.com/xjan103pcp94/1IxBk5YiSbdFPRUYneqBKy/8c39b2d0586bfa4af2ab84063a191433/Screen_Shot_2021-10-05_at_6.55.50_PM.png" alt="03_3rd_4x2_22font_DistributedScoring"></td>
</tr>
<tr>
<td><strong>Hyperparameter Tuning (Nesting):</strong> Nesting also occurs naturally during hyperparameter tuning, which seeks to explore many variants of existing workloads. Nesting is needed for <a href="https://docs.ray.io/en/latest/tune/index.html">distributed trials</a>, and efficiency is important for supporting lightweight trials and <a href="https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-scheduler-pbt">population based approaches</a>.</td>
<td><img src="https://images.contentful.com/xjan103pcp94/6BHEgjIjRWRIymCYfQ1xKc/234d498153325597f75936d9e95cbb96/04_2nd_4x2_22font_SimpleNesting.png" alt="04_2nd_4x2_22font_SimpleNesting"></td>
<td><img src="https://images.contentful.com/xjan103pcp94/2oh4v2FUq0C5RS2FlwBSah/5708b8ccf119b334af0b4584921633a3/04_3rd_4x2_22font_DistributedtrialsPBP.png" alt="04_3rd_4x2_22font_DistributedtrialsPBP"></td>
</tr>
<tr>
<td><strong>Workflow DAGs (Both Chaining and Nesting):</strong> Finally, both chaining and nesting occur naturally in higher level workflow DAGs implemented by workflow orchestrators. Here two bottlenecks can occur: (1) if large amounts of data is passed or shared between workflow steps, it is desirable to pass data at memory-speed rather than reading and writing to cluster storage, and (2) if steps are small, scheduling overhead can dominate workflow run time.</td>
<td><img src="https://images.contentful.com/xjan103pcp94/5pHdKYmzsuBM6WAAa2WAur/b45d46823841e0d089d2d80628bb7265/05_2nd_4x2_22font_DAG.png" alt="05_2nd_4x2_22font_DAG"></td>
<td><img src="https://images.contentful.com/xjan103pcp94/DZmefXHDWCnvBWm0m05wG/aa46b9911cf20c6f4281a8e3763592a9/05_3rd_4x2_22font_inMemoryLow-overhead.png" alt="05_3rd_4x2_22font_inMemoryLow-overhead"></td>
</tr>
</tbody>
</table>
</div><h2>Third Generation Platforms Accelerate Chaining and Nesting</h2><p>A <a href="the-third-generation-of-production-ml-architectures.html"><u>third generation ML platform</u></a> eliminates the above performance bottlenecks. Users and builders of third generation platforms are able to:</p><ol><li><p>Implement chaining and nesting of distributed steps with minimal scheduling and data movement overheads.</p></li><li><p>Programmatically author ML workflows, weaving together steps like ingest, transform, and training without needing to wrangle separate distributed systems.</p></li></ol><p>This is possible with the use of a <i>programmable compute layer </i>such as <a href="https://www.ray.io/"><u>Ray</u></a>, which can serve as a replacement to (or <a href="https://www.astronomer.io/blog/airflow-ray-data-science-story"><u>accelerator for</u></a>) workflow orchestrators. In a 3rd gen platform, distributed logic such as <a href="https://medium.com/distributed-computing-with-ray/data-processing-support-in-ray-ae8da34dce7e"><u>data processing</u></a> is implemented as <a href="https://docs.ray.io/en/latest/ray-libraries.html"><u>libraries</u></a> within the compute layer:</p><div class="ArticleBody_image__rd3Dj" style="width:720px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/5b9o8llgwycHgDtV9c1r8m/ab3f90039ba3ac7cf2577c1658aad85c/3rdGenDistributed.png" alt="3rdGenDistributed"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">A &quot;3rd generation&quot; distributed ML platform. The programmable compute layer allows distributed steps to be tightly woven together in code, eliminating the overheads of separate clusters.</span></div><h2>Benchmarks</h2><p>In this section we highlight some of the performance gains users have seen by leveraging Ray&#x27;s support for efficient chaining and nesting of computations:</p><h3>Uber: Shuffled ML Ingest Pipeline</h3><p>In a previous blog post, Uber introduced how they were leveraging Ray for <a href="https://eng.uber.com/horovod-ray/"><u>elastic scheduling and training with Horovod</u></a>. In a follow-up project, another team is using Ray&#x27;s <a href="https://docs.ray.io/en/master/data/dataset-pipeline.html"><u>Dataset Pipeline</u></a> to implement shuffled ML ingest, where data is globally shuffled across cluster CPU workers per iteration of GPU training. Here we analyze the importance of pipelining and in-memory data exchange. We ran a training workload reading 500GB of data on a cluster of 70 CPU nodes and 16 GPU nodes, and find ingest throughput is 3x higher with the pipelining and in-memory data exchange enabled by Ray:</p><div class="ArticleBody_image__rd3Dj" style="width:1938px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/soBLd3ZStUVqfRH8BCxtR/bafe8e1c993e605a88ae521cdfc99584/Shuffled_Ingest_Throughput.png" alt="Shuffled Ingest"/></div></div></div><h3>Wildlife Studios: Chained Model Pipelines</h3><p>Mobile gaming giant <a href="https://wildlifestudios.com/"><u>Wildlife Studios’</u></a> legacy system for serving revenue-generating in-game offers was not scaling to meet their latency and cost requirements. After switching to Ray Serve, their Dynamic Offers team was able to <a href="wildlife-studios-serves-in-game-offers-3x-faster-at-1-10th-the-cost-with-ray.html"><u>serve offers three times faster</u></a>. The Ray Serve architecture provided support for parallel inference on multiple models in a pipeline, decreasing latency and minimizing idle machines:</p><div class="ArticleBody_image__rd3Dj" style="width:1804px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/1qt32KDRT1K8MzgWLfQ2iq/eaae25d1ae4d466294c568fbe380a0e6/P95_Latency_in_Production_Cluster.png" alt="p95"/></div></div></div><h3>Anastasia.ai: Nested Model Evaluation</h3><p><a href="https://anastasia.ai/"><u>Anastasia</u></a> provides a powerful platform that enables organizations to operate AI capacities at scale with a fraction of the resources and effort traditionally required. They were able to accelerate a demand prediction problem using <a href="https://docs.ray.io/en/latest/tune/index.html"><u>Ray Tune</u></a>, and got up to <a href="how-anastasia-implements-ray-and-anyscale-to-speed-up-ml-processes-9x.html"><u>9x performance gains</u></a> due to fine-grained re-use of resources compared to a coarse grained orchestrator:</p><div class="ArticleBody_image__rd3Dj" style="width:2020px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/4ASH8c3Ia98IGL4o1u1FSS/5f9db5896146bc065699d6caacb5739e/Relative_Time_to_Evaluate_on_384-core_Cluster.png" alt="RelativeTimeEvaluate384CoreCluster"/></div></div></div><h2>Conclusion</h2><p>In this blog, we showed how a programmable compute layer such as Ray can provide 3-9x performance improvements for production ML workloads, eliminating bottlenecks found in 2nd generation production architectures. This is in addition to the productivity and operational benefits of having a programmable architecture.</p><p>As a final note, the ideas presented here apply generally to distributed programming as well as to ML workloads. If you&#x27;re interested in the performance and programmability of ML applications and distributed computing, check out the <a href="https://www.ray.io/"><u>Ray project</u></a> and consider <a href="https://jobs.lever.co/anyscale"><u>joining us</u></a>.</p></div></div></div><div class="BlogPost_aside__BK_Wk"><div class="root"><h4 class="ArticleExtras_label__JQEEO">Sharing</h4><div class="ArticleExtras_sharing__LSXs1 ArticleExtras_section__26jYL"><a target="_blank" rel="noreferrer" href="why-third-generation-ml-platforms-are-more-performant.html" aria-label="Share on Facebook"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M24 12c0-6.628-5.372-12-12-12S0 5.372 0 12c0 5.99 4.388 10.955 10.125 11.855v-8.386H7.078V12h3.047V9.356c0-3.007 1.79-4.668 4.533-4.668 1.312 0 2.686.234 2.686.234v2.953H15.83c-1.49 0-1.955.926-1.955 1.875V12h3.328l-.532 3.469h-2.796v8.386C19.613 22.955 24 17.99 24 12Z" fill="#999"></path></svg><div class="ArticleExtras_active__cv689"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M24 12c0-6.628-5.372-12-12-12S0 5.372 0 12c0 5.99 4.388 10.955 10.125 11.855v-8.386H7.078V12h3.047V9.356c0-3.007 1.79-4.668 4.533-4.668 1.312 0 2.686.234 2.686.234v2.953H15.83c-1.49 0-1.955.926-1.955 1.875V12h3.328l-.532 3.469h-2.796v8.386C19.613 22.955 24 17.99 24 12Z" fill="#234999"></path></svg></div></a><a target="_blank" rel="noreferrer" href="why-third-generation-ml-platforms-are-more-performant.html" aria-label="Share on Twitter"><svg width="24" height="20" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M24 2.436c-.955.358-1.791.597-2.866.716 1.075-.597 1.791-1.552 2.15-2.746-.956.597-2.03.955-3.105 1.194-1.79-2.03-4.895-2.15-6.925-.239-1.314 1.194-1.91 2.985-1.433 4.776C7.88 5.9 4.299 3.988 1.79 1.003a4.305 4.305 0 0 0-.716 2.388c0 1.672.835 3.105 2.149 4.06-.717 0-1.433-.12-2.15-.597v.12c0 2.268 1.672 4.298 3.94 4.775-.477.12-.835.24-1.313.24-.358 0-.597 0-.955-.12.597 2.03 2.508 3.343 4.538 3.343-1.672 1.313-3.821 2.15-6.09 2.15-.358 0-.836 0-1.194-.12 2.269 1.433 4.896 2.268 7.522 2.268 7.642.12 13.851-6.089 13.97-13.73v-.837c1.075-.716 1.911-1.552 2.508-2.507Z" fill="#999"></path></svg><div class="ArticleExtras_active__cv689"><svg width="24" height="20" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M24 2.436c-.955.358-1.791.597-2.866.716 1.075-.597 1.791-1.552 2.15-2.746-.956.597-2.03.955-3.105 1.194-1.79-2.03-4.895-2.15-6.925-.239-1.314 1.194-1.91 2.985-1.433 4.776C7.88 5.9 4.299 3.988 1.79 1.003a4.305 4.305 0 0 0-.716 2.388c0 1.672.835 3.105 2.149 4.06-.717 0-1.433-.12-2.15-.597v.12c0 2.268 1.672 4.298 3.94 4.775-.477.12-.835.24-1.313.24-.358 0-.597 0-.955-.12.597 2.03 2.508 3.343 4.538 3.343-1.672 1.313-3.821 2.15-6.09 2.15-.358 0-.836 0-1.194-.12 2.269 1.433 4.896 2.268 7.522 2.268 7.642.12 13.851-6.089 13.97-13.73v-.837c1.075-.716 1.911-1.552 2.508-2.507Z" fill="#234999"></path></svg></div></a><a target="_blank" rel="noreferrer" href="why-third-generation-ml-platforms-are-more-performant.html" aria-label="Share on Linkedin"><svg width="24" height="25" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M22.191 0H1.809C.844 0 0 .724 0 1.688v20.624c0 1.085.844 1.809 1.809 1.809h20.382c.965 0 1.809-.724 1.809-1.689V1.688C24 .724 23.156 0 22.191 0ZM7.236 20.14H3.618V9.287h3.618v10.855ZM5.427 7.84c-.965 0-1.809-.845-1.809-1.93 0-1.086.844-1.93 1.93-1.93 1.085-.12 1.93.603 2.05 1.688.12 1.086-.724 2.05-1.688 2.171h-.483Zm14.955 12.3h-3.618v-5.788c0-1.447-.483-2.412-1.81-2.412-.843 0-1.567.482-1.808 1.326-.12.242-.12.603-.12.845v6.03H9.406V9.286h3.618v1.568c.724-1.206 1.93-1.809 3.256-1.809 2.413 0 4.222 1.568 4.222 4.945v6.15h-.121Z" fill="#999"></path></svg><div class="ArticleExtras_active__cv689"><svg width="24" height="25" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M22.191 0H1.809C.844 0 0 .724 0 1.688v20.624c0 1.085.844 1.809 1.809 1.809h20.382c.965 0 1.809-.724 1.809-1.689V1.688C24 .724 23.156 0 22.191 0ZM7.236 20.14H3.618V9.287h3.618v10.855ZM5.427 7.84c-.965 0-1.809-.845-1.809-1.93 0-1.086.844-1.93 1.93-1.93 1.085-.12 1.93.603 2.05 1.688.12 1.086-.724 2.05-1.688 2.171h-.483Zm14.955 12.3h-3.618v-5.788c0-1.447-.483-2.412-1.81-2.412-.843 0-1.567.482-1.808 1.326-.12.242-.12.603-.12.845v6.03H9.406V9.286h3.618v1.568c.724-1.206 1.93-1.809 3.256-1.809 2.413 0 4.222 1.568 4.222 4.945v6.15h-.121Z" fill="#234999"></path></svg></div></a></div><div class="tags-wrapper ArticleExtras_section__26jYL"><h4 class="ArticleExtras_label__JQEEO">Tags</h4><span class="ArticleExtras_tags__Q_xZs"><span class="tag">Ray Tune</span><span>, </span><span class="tag">Ray Core</span><span>, </span><span class="tag">Ray Serve</span></span></div><div class="ArticleExtras_form-wrapper__rTr1C"><h4 class="ArticleExtras_label__JQEEO">Sign up for product updates</h4><div class="HubspotEmailForm_root__Rwc_O HubspotEmailForm_condensed___nNtN"><div class="HubspotEmailForm_form___3bhH"><div id="hubspotForm"></div></div></div></div></div><div class="root"><h4 class="RecommendedContent_header__3Cuyf">Recommended content</h4><a class="RecommendedContent_item__e9DDs" href="the-third-generation-of-production-ml-architectures.html"><div class="RecommendedContent_thumbnail__bD3bd"><div class="Image_wrapper__beWl0"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/2mI1ydQdbxC8VN9jq5SE9O/b7509b9f4f23ddf47620aa465a919914/blog-recommended-content-rl-robot-bubble-dark.jpg" alt="blog-recommended-content-rl-robot-bubble-dark"/></div></div></div><div class="RecommendedContent_item-content__rXTae"><h4>Why Third Generation ML Platforms are More Performant</h4><span>Read more<svg width="15" height="12" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1 6h12.222m0 0-5-5m5 5-5 5" stroke="#234999" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></div></a><a class="RecommendedContent_item__e9DDs" href="deep-dive-data-ingest-in-a-third-generation-ml-architecture.html"><div class="RecommendedContent_thumbnail__bD3bd"><div class="Image_wrapper__beWl0"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/qyo3m0dc8CvMNCfhxDy5e/409ed6f79aea5822b10aaa365318a005/blog-recommended-content-code-dark.jpg" alt="blog-recommended-content-code-dark"/></div></div></div><div class="RecommendedContent_item-content__rXTae"><h4>Deep Dive: Data ingest in a third-generation ML architecture</h4><span>Read more<svg width="15" height="12" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1 6h12.222m0 0-5-5m5 5-5 5" stroke="#234999" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></div></a><a class="RecommendedContent_item__e9DDs" href="introducing-the-anyscale-databricks-connector.html"><div class="RecommendedContent_thumbnail__bD3bd"><div class="Image_wrapper__beWl0"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/71IpYQvsxplh7okIL7RJVx/b193e65fbc2d6500e3ccb2162d323080/0_-_Updated_Databricks_Anyscale_Image.png" alt="0 -Anyscale Databricks headline image"/></div></div></div><div class="RecommendedContent_item-content__rXTae"><h4>Introducing the Anyscale Databricks Connector</h4><span>Read more<svg width="15" height="12" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1 6h12.222m0 0-5-5m5 5-5 5" stroke="#234999" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></div></a></div></div></div></div></div><footer class="relative bg-primary pt-30 pb-30 md:pt-36 md:pb-[180px] lg:pt-20"><div class="container mx-auto flex flex-col space-y-12 md:space-y-16 lg:flex-row lg:justify-between lg:space-y-0"><div class="flex flex-col gap-16"><div class="flex flex-col gap-6"><div class="h-10 text-white"><svg width="141" height="32" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="origin-top-left scale-125"><g clip-path="url(#anyscale-logo_svg__a)" fill="currentColor"><path d="M10.653 17.46H3.865C1.697 17.46 0 19.16 0 21.332v6.797c0 2.17 1.697 3.87 3.865 3.87h6.788c2.169 0 3.866-1.7 3.866-3.87V21.33c0-2.171-1.792-3.87-3.866-3.87Zm1.509 10.573c0 .85-.66 1.51-1.509 1.51H3.865a1.49 1.49 0 0 1-1.508-1.51v-6.796c0-.85.66-1.51 1.508-1.51h6.788c.849 0 1.509.66 1.509 1.51v6.796Z"></path><path d="M28.095 11.988h-8.108V3.87C19.892 1.7 18.196 0 16.027 0H3.865C1.697 0 0 1.7 0 3.87v6.608c0 2.171 1.697 3.87 3.865 3.87h13.67v13.687c0 2.171 1.698 3.87 3.866 3.87h6.6c2.168 0 3.865-1.699 3.865-3.87V15.858c0-2.076-1.697-3.775-3.771-3.87Zm-24.23 0a1.49 1.49 0 0 1-1.508-1.51V3.87c0-.85.66-1.51 1.508-1.51h12.162c.849 0 1.509.66 1.509 1.51v8.118H3.866Zm25.738 16.047c0 .85-.66 1.51-1.508 1.51h-6.6a1.49 1.49 0 0 1-1.508-1.51V14.348h8.108c.848 0 1.508.66 1.508 1.51v12.177ZM79.194 17.462l-3.017-8.118h-3.3l4.714 11.327-2.546 5.853h3.112l7.165-17.18H82.21l-3.016 8.118ZM93.335 14.819c-.66-.284-1.414-.472-2.075-.661-.66-.189-1.225-.378-1.79-.66-.378-.19-.566-.473-.566-.85 0-.378.188-.661.472-.85.847-.472 1.79-.378 2.639.094.377.284.566.661.566 1.133h2.828c0-1.133-.566-2.171-1.414-2.832-.943-.755-2.169-1.133-3.3-1.038-.849 0-1.697.094-2.45.472-.66.283-1.227.66-1.604 1.227-.377.472-.565 1.133-.565 1.794 0 .66.188 1.321.565 1.887.377.472.849.85 1.414 1.039.66.283 1.414.472 2.168.66.66.19 1.226.378 1.886.661.377.19.566.472.566.85 0 .377-.189.66-.471.85-.377.283-.85.377-1.32.377-.472 0-1.038-.094-1.415-.472-.377-.283-.565-.66-.66-1.133h-2.922c0 .755.282 1.416.754 1.982.472.567 1.131 1.133 1.791 1.416.849.378 1.698.567 2.546.567.849 0 1.697-.095 2.45-.472.661-.283 1.227-.66 1.604-1.227.377-.567.565-1.133.565-1.794 0-.66-.188-1.321-.66-1.888-.565-.66-1.037-.944-1.602-1.132ZM102.575 11.702c.566 0 1.036.095 1.508.472.377.283.66.755.849 1.227h3.205a5.366 5.366 0 0 0-1.886-3.02 5.348 5.348 0 0 0-3.582-1.039c-1.037 0-2.074.284-3.017.755a5.486 5.486 0 0 0-2.074 2.077 7.427 7.427 0 0 0 0 6.325c.472.85 1.131 1.604 2.074 2.076.943.472 1.98.756 3.017.756 1.32 0 2.546-.378 3.582-1.133a5.366 5.366 0 0 0 1.886-3.02h-3.111c-.283 1.037-1.32 1.698-2.357 1.604-.754 0-1.509-.283-1.98-.944a5.119 5.119 0 0 1 0-5.192 1.914 1.914 0 0 1 1.886-.944ZM118.412 10.1a.405.405 0 0 1-.377.378c-.094 0-.188 0-.188-.095a3.382 3.382 0 0 0-.944-.66c-.659-.378-1.507-.567-2.262-.567-.943 0-1.885.283-2.734.755a4.459 4.459 0 0 0-1.885 2.077c-.472.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.376.85 1.037 1.604 1.885 2.17.849.473 1.697.756 2.64.756.754 0 1.603-.189 2.263-.567.659-.283 1.131-.755 1.602-1.321v1.699h2.923V9.345h-2.923v.755Zm-.377 6.891c-.283.472-.66.944-1.132 1.227-.47.283-1.037.378-1.507.378-.567 0-1.037-.189-1.509-.378-.472-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.094-1.228.472-1.794.848-1.51 2.64-1.982 4.147-1.227h.095c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM55.907 10.1a.406.406 0 0 1-.377.378c-.095 0-.19 0-.19-.095a3.384 3.384 0 0 0-.942-.66c-.66-.378-1.508-.567-2.263-.567-.942 0-1.885.283-2.733.755a4.463 4.463 0 0 0-1.886 2.077c-.471.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.377.85 1.037 1.604 1.886 2.17.848.473 1.697.756 2.64.756.753 0 1.602-.189 2.262-.567.66-.283 1.131-.755 1.603-1.321v1.699h2.922V9.345h-2.922v.755Zm-.472 6.891c-.283.472-.66.944-1.131 1.227-.471.283-1.037.378-1.509.378-.565 0-1.037-.189-1.508-.378-.471-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.095-1.228.472-1.794.848-1.51 2.64-1.982 4.148-1.227h.094c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM126.708 5.477h-2.923v15.575h2.923V5.477ZM140.191 14.914c0-1.038-.189-2.077-.755-3.02-.471-.85-1.131-1.605-2.074-1.983a6.78 6.78 0 0 0-6.033 0 5.485 5.485 0 0 0-2.075 2.077c-.565 1.038-.754 2.077-.754 3.21 0 1.132.189 2.17.754 3.114a5.485 5.485 0 0 0 2.075 2.077c.942.472 1.979.755 3.016.755a6.56 6.56 0 0 0 3.489-1.038c.943-.661 1.602-1.7 1.885-2.738h-3.016c-.377.944-1.321 1.51-2.358 1.416-.659 0-1.414-.283-1.885-.755-.566-.472-.848-1.227-.848-1.982h8.484c.095-.378.095-.755.095-1.133Zm-8.579-.85c.093-.755.377-1.415.942-1.887.566-.472 1.226-.661 1.886-.661.754 0 1.414.189 1.979.66.566.473.849 1.133.849 1.794l-5.656.095ZM67.692 9.159a4.7 4.7 0 0 0-1.98.472c-.377.188-.754.377-1.037.66a.286.286 0 0 1-.377 0c-.094-.094-.094-.094-.094-.188v-.756H61.28v11.705h2.923v-6.418c-.094-.755.188-1.51.754-2.171.471-.472 1.226-.756 1.886-.756.66 0 1.414.284 1.885.755.472.567.754 1.322.66 2.172v6.418h2.923v-6.89c.094-1.322-.283-2.644-1.226-3.682-.848-.85-2.168-1.416-3.394-1.321Z"></path></g><defs><clipPath id="anyscale-logo_svg__a"><path fill="currentColor" d="M0 0h140.19v32H0z"></path></clipPath></defs></svg></div><div class="font-light leading-normal text-lightBlue-300 opacity-70 hidden lg:flex"><span>© Anyscale, Inc 2022</span></div></div><div class="z-10 hidden flex-col gap-10 lg:flex"><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Anyscale</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://www.linkedin.com/company/joinanyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M15.784 14.168a1.667 1.667 0 1 1-3.334-.002 1.667 1.667 0 0 1 3.334.002Zm.05 2.9H12.5V27.5h3.334V17.067Zm5.266 0h-3.317V27.5h3.284v-5.475c0-3.05 3.975-3.334 3.975 0V27.5h3.291v-6.609c0-5.141-5.883-4.95-7.266-2.424l.033-1.4Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://www.facebook.com/AnyscaleCompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M21.667 21.247h2.083l.834-3.333h-2.917v-1.667c0-.858 0-1.666 1.666-1.666h1.25v-2.8a23.49 23.49 0 0 0-2.38-.117c-2.263 0-3.87 1.38-3.87 3.917v2.333h-2.5v3.333h2.5v7.084h3.334v-7.084Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://twitter.com/anyscalecompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/anyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Ray</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://twitter.com/raydistributed"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/ray-project/ray"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div></div></div><div class="grid grid-cols-2 gap-x-8 gap-y-12 md:grid-cols-3 md:gap-y-10 md:gap-x-16"><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Products</div><div><ul>
<li><a href="../platform.html">Anyscale Compute Platform</a></li>
<li><a href="../ray-open-source.html">Ray Open Source</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Events</div><div><ul>
<li><a href="../event-category/rl-summit.html">Webinars</a></li>
<li><a href="../event-category/rl-summit.html">Meetups</a></li>
<li><a href="../event-category/rl-summit.html">Summits</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Company</div><div><ul>
<li><a href="../about.html">About Us</a></li>
<li><a href="../press.html">News</a></li>
<li><a href="../careers.html">Careers</a></li>
<li><a href="../community.html">Community</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Learn</div><div><ul>
<li><a href="how-ray-solves-common-production-challenges-for-generative-ai.html">Blog</a></li>
<li><a href="../event-category/rl-summit.html">Demos &amp; Webinars</a></li>
<li><a href="../event-category/rl-summit.html">Anyscale Academy</a></li>
<li><a href="https://docs.anyscale.com">Anyscale Docs</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Scalable AI and Python</div><div><ul>
<li><a href="../data-ingestion.html">Data Ingestion</a></li>
<li><a href="../reinforcement-learning.html">Reinforcement Learning</a></li>
<li><a href="../ray-air.html">Ray AIR</a></li>
<li><a href="../model-serving.html">Model Serving</a></li>
<li><a href="../hyperparameter-tuning.html">Hyperparameter Tuning</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Use Cases</div><div><ul>
<li><a href="../demand-forecasting.html">Demand Forecasting/Pricing</a></li>
<li><a href="../industrial-automation.html">Industrial Automation</a></li>
<li><a href="../machine-learning.html">Scalable ML Platforms</a></li>
<li><a href="../natural-language-processing.html">NLP</a></li>
<li><a href="../recommendation-system.html">Recommendation System</a></li>
</ul>
</div></div></div><div class="grid-col-1 grid gap-12 md:grid-cols-3 md:gap-16 lg:hidden"><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Anyscale</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://www.linkedin.com/company/joinanyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M15.784 14.168a1.667 1.667 0 1 1-3.334-.002 1.667 1.667 0 0 1 3.334.002Zm.05 2.9H12.5V27.5h3.334V17.067Zm5.266 0h-3.317V27.5h3.284v-5.475c0-3.05 3.975-3.334 3.975 0V27.5h3.291v-6.609c0-5.141-5.883-4.95-7.266-2.424l.033-1.4Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://www.facebook.com/AnyscaleCompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M21.667 21.247h2.083l.834-3.333h-2.917v-1.667c0-.858 0-1.666 1.666-1.666h1.25v-2.8a23.49 23.49 0 0 0-2.38-.117c-2.263 0-3.87 1.38-3.87 3.917v2.333h-2.5v3.333h2.5v7.084h3.334v-7.084Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://twitter.com/anyscalecompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/anyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Ray</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://twitter.com/raydistributed"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/ray-project/ray"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="md:self-end md:text-right"><div class="font-light leading-normal text-lightBlue-300 opacity-70"><span>© Anyscale, Inc 2022</span></div></div></div></div><div class="pointer-events-none absolute bottom-0 left-0"><svg width="612" height="297" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="hidden lg:block"><g opacity="0.5" clip-path="url(#dots_svg__a)"><mask id="dots_svg__c" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="0" y="0" width="612" height="297"><path transform="rotate(90 611.5 .5)" fill="url(#dots_svg__b)" stroke="#F2F3F7" d="M611.5.5h296v611h-296z"></path></mask><g mask="url(#dots_svg__c)" stroke="#A2B7D4"><g opacity="0.2" stroke-linecap="round"><path d="M598 350v-688M582 350v-688M566 350v-688M550 350v-688M534 350v-688M518 350v-688M502 350v-688M486 350v-688M470 350v-688M454 350v-688M438 350v-688M422 350v-688M406 350v-688M390 350v-688M374 350v-688M358 350v-688M342 350v-688M326 350v-688M310 350v-688M294 350v-688M278 350v-688M262 350v-688M246 350v-688M230 350v-688M214 350v-688M198 350v-688M182 350v-688M166 350v-688M150 350v-688M134 350v-688M118 350v-688M102 350v-688M86 350v-688M70 350v-688M54 350v-688M38 350v-688M22 350v-688M6 350v-688"></path></g><g opacity="0.2" stroke-linecap="round"><path d="M1014 14H0M1014 30H0M1014 46H0M1014 62H0M1014 78H0M1014 94H0M1014 110H0M1014 126H0M1014 142H0M1014 158H0M1014 174H0M1014 190H4M1014 206H4M1014 222H4M1014 238H4M1014 254H4M1014 270H4M1014 286H4"></path></g><path fill="#4C6996" d="M615.5 12.5v3h-3v-3zM615.5 28.5v3h-3v-3zM615.5 44.5v3h-3v-3zM615.5 60.5v3h-3v-3zM615.5 76.5v3h-3v-3zM615.5 92.5v3h-3v-3zM615.5 108.5v3h-3v-3zM615.5 124.5v3h-3v-3zM615.5 140.5v3h-3v-3zM615.5 156.5v3h-3v-3zM615.5 172.5v3h-3v-3zM615.5 188.5v3h-3v-3zM615.5 204.5v3h-3v-3zM615.5 220.5v3h-3v-3zM615.5 236.5v3h-3v-3zM615.5 252.5v3h-3v-3zM615.5 268.5v3h-3v-3zM615.5 284.5v3h-3v-3zM599.5 12.5v3h-3v-3zM599.5 28.5v3h-3v-3zM599.5 44.5v3h-3v-3zM599.5 60.5v3h-3v-3zM599.5 76.5v3h-3v-3zM599.5 92.5v3h-3v-3zM599.5 108.5v3h-3v-3zM599.5 124.5v3h-3v-3zM599.5 140.5v3h-3v-3zM599.5 156.5v3h-3v-3zM599.5 172.5v3h-3v-3zM599.5 188.5v3h-3v-3zM599.5 204.5v3h-3v-3zM599.5 220.5v3h-3v-3zM599.5 236.5v3h-3v-3zM599.5 252.5v3h-3v-3zM599.5 268.5v3h-3v-3zM599.5 284.5v3h-3v-3zM583.5 12.5v3h-3v-3zM583.5 28.5v3h-3v-3zM583.5 44.5v3h-3v-3zM583.5 60.5v3h-3v-3zM583.5 76.5v3h-3v-3zM583.5 92.5v3h-3v-3zM583.5 108.5v3h-3v-3zM583.5 124.5v3h-3v-3zM583.5 140.5v3h-3v-3zM583.5 156.5v3h-3v-3zM583.5 172.5v3h-3v-3zM583.5 188.5v3h-3v-3zM583.5 204.5v3h-3v-3zM583.5 220.5v3h-3v-3zM583.5 236.5v3h-3v-3zM583.5 252.5v3h-3v-3zM583.5 268.5v3h-3v-3zM583.5 284.5v3h-3v-3zM567.5 12.5v3h-3v-3zM567.5 28.5v3h-3v-3zM567.5 44.5v3h-3v-3zM567.5 60.5v3h-3v-3zM567.5 76.5v3h-3v-3zM567.5 92.5v3h-3v-3zM567.5 108.5v3h-3v-3zM567.5 124.5v3h-3v-3zM567.5 140.5v3h-3v-3zM567.5 156.5v3h-3v-3zM567.5 172.5v3h-3v-3zM567.5 188.5v3h-3v-3zM567.5 204.5v3h-3v-3zM567.5 220.5v3h-3v-3zM567.5 236.5v3h-3v-3zM567.5 252.5v3h-3v-3zM567.5 268.5v3h-3v-3zM567.5 284.5v3h-3v-3zM551.5 12.5v3h-3v-3zM551.5 28.5v3h-3v-3zM551.5 44.5v3h-3v-3zM551.5 60.5v3h-3v-3zM551.5 76.5v3h-3v-3zM551.5 92.5v3h-3v-3zM551.5 108.5v3h-3v-3zM551.5 124.5v3h-3v-3zM551.5 140.5v3h-3v-3zM551.5 156.5v3h-3v-3zM551.5 172.5v3h-3v-3zM551.5 188.5v3h-3v-3zM551.5 204.5v3h-3v-3zM551.5 220.5v3h-3v-3zM551.5 236.5v3h-3v-3zM551.5 252.5v3h-3v-3zM551.5 268.5v3h-3v-3zM551.5 284.5v3h-3v-3zM535.5 12.5v3h-3v-3zM535.5 28.5v3h-3v-3zM535.5 44.5v3h-3v-3zM535.5 60.5v3h-3v-3zM535.5 76.5v3h-3v-3zM535.5 92.5v3h-3v-3zM535.5 108.5v3h-3v-3zM535.5 124.5v3h-3v-3zM535.5 140.5v3h-3v-3zM535.5 156.5v3h-3v-3zM535.5 172.5v3h-3v-3zM535.5 188.5v3h-3v-3zM535.5 204.5v3h-3v-3zM535.5 220.5v3h-3v-3zM535.5 236.5v3h-3v-3zM535.5 252.5v3h-3v-3zM535.5 268.5v3h-3v-3zM535.5 284.5v3h-3v-3zM519.5 12.5v3h-3v-3zM519.5 28.5v3h-3v-3zM519.5 44.5v3h-3v-3zM519.5 60.5v3h-3v-3zM519.5 76.5v3h-3v-3zM519.5 92.5v3h-3v-3zM519.5 108.5v3h-3v-3zM519.5 124.5v3h-3v-3zM519.5 140.5v3h-3v-3zM519.5 156.5v3h-3v-3zM519.5 172.5v3h-3v-3zM519.5 188.5v3h-3v-3zM519.5 204.5v3h-3v-3zM519.5 220.5v3h-3v-3zM519.5 236.5v3h-3v-3zM519.5 252.5v3h-3v-3zM519.5 268.5v3h-3v-3zM519.5 284.5v3h-3v-3zM503.5 12.5v3h-3v-3zM503.5 28.5v3h-3v-3zM503.5 44.5v3h-3v-3zM503.5 60.5v3h-3v-3zM503.5 76.5v3h-3v-3zM503.5 92.5v3h-3v-3zM503.5 108.5v3h-3v-3zM503.5 124.5v3h-3v-3zM503.5 140.5v3h-3v-3zM503.5 156.5v3h-3v-3zM503.5 172.5v3h-3v-3zM503.5 188.5v3h-3v-3zM503.5 204.5v3h-3v-3zM503.5 220.5v3h-3v-3zM503.5 236.5v3h-3v-3zM503.5 252.5v3h-3v-3zM503.5 268.5v3h-3v-3zM503.5 284.5v3h-3v-3zM487.5 12.5v3h-3v-3zM487.5 28.5v3h-3v-3zM487.5 44.5v3h-3v-3zM487.5 60.5v3h-3v-3zM487.5 76.5v3h-3v-3zM487.5 92.5v3h-3v-3zM487.5 108.5v3h-3v-3zM487.5 124.5v3h-3v-3zM487.5 140.5v3h-3v-3zM487.5 156.5v3h-3v-3zM487.5 172.5v3h-3v-3zM487.5 188.5v3h-3v-3zM487.5 204.5v3h-3v-3zM487.5 220.5v3h-3v-3zM487.5 236.5v3h-3v-3zM487.5 252.5v3h-3v-3zM487.5 268.5v3h-3v-3zM487.5 284.5v3h-3v-3zM471.5 12.5v3h-3v-3zM471.5 28.5v3h-3v-3zM471.5 44.5v3h-3v-3zM471.5 60.5v3h-3v-3zM471.5 76.5v3h-3v-3zM471.5 92.5v3h-3v-3zM471.5 108.5v3h-3v-3zM471.5 124.5v3h-3v-3zM471.5 140.5v3h-3v-3zM471.5 156.5v3h-3v-3zM471.5 172.5v3h-3v-3zM471.5 188.5v3h-3v-3zM471.5 204.5v3h-3v-3zM471.5 220.5v3h-3v-3zM471.5 236.5v3h-3v-3zM471.5 252.5v3h-3v-3zM471.5 268.5v3h-3v-3zM471.5 284.5v3h-3v-3zM455.5 12.5v3h-3v-3zM455.5 28.5v3h-3v-3zM455.5 44.5v3h-3v-3zM455.5 60.5v3h-3v-3zM455.5 76.5v3h-3v-3zM455.5 92.5v3h-3v-3zM455.5 108.5v3h-3v-3zM455.5 124.5v3h-3v-3zM455.5 140.5v3h-3v-3zM455.5 156.5v3h-3v-3zM455.5 172.5v3h-3v-3zM455.5 188.5v3h-3v-3zM455.5 204.5v3h-3v-3zM455.5 220.5v3h-3v-3zM455.5 236.5v3h-3v-3zM455.5 252.5v3h-3v-3zM455.5 268.5v3h-3v-3zM455.5 284.5v3h-3v-3zM439.5 12.5v3h-3v-3zM439.5 28.5v3h-3v-3zM439.5 44.5v3h-3v-3zM439.5 60.5v3h-3v-3zM439.5 76.5v3h-3v-3zM439.5 92.5v3h-3v-3zM439.5 108.5v3h-3v-3zM439.5 124.5v3h-3v-3zM439.5 140.5v3h-3v-3zM439.5 156.5v3h-3v-3zM439.5 172.5v3h-3v-3zM439.5 188.5v3h-3v-3zM439.5 204.5v3h-3v-3zM439.5 220.5v3h-3v-3zM439.5 236.5v3h-3v-3zM439.5 252.5v3h-3v-3zM439.5 268.5v3h-3v-3zM439.5 284.5v3h-3v-3zM423.5 12.5v3h-3v-3zM423.5 28.5v3h-3v-3zM423.5 44.5v3h-3v-3zM423.5 60.5v3h-3v-3zM423.5 76.5v3h-3v-3zM423.5 92.5v3h-3v-3zM423.5 108.5v3h-3v-3zM423.5 124.5v3h-3v-3zM423.5 140.5v3h-3v-3zM423.5 156.5v3h-3v-3zM423.5 172.5v3h-3v-3zM423.5 188.5v3h-3v-3zM423.5 204.5v3h-3v-3zM423.5 220.5v3h-3v-3zM423.5 236.5v3h-3v-3zM423.5 252.5v3h-3v-3zM423.5 268.5v3h-3v-3zM423.5 284.5v3h-3v-3zM407.5 12.5v3h-3v-3zM407.5 28.5v3h-3v-3zM407.5 44.5v3h-3v-3zM407.5 60.5v3h-3v-3zM407.5 76.5v3h-3v-3zM407.5 92.5v3h-3v-3zM407.5 108.5v3h-3v-3zM407.5 124.5v3h-3v-3zM407.5 140.5v3h-3v-3zM407.5 156.5v3h-3v-3zM407.5 172.5v3h-3v-3zM407.5 188.5v3h-3v-3zM407.5 204.5v3h-3v-3zM407.5 220.5v3h-3v-3zM407.5 236.5v3h-3v-3zM407.5 252.5v3h-3v-3zM407.5 268.5v3h-3v-3zM407.5 284.5v3h-3v-3zM391.5 12.5v3h-3v-3zM391.5 28.5v3h-3v-3zM391.5 44.5v3h-3v-3zM391.5 60.5v3h-3v-3zM391.5 76.5v3h-3v-3zM391.5 92.5v3h-3v-3zM391.5 108.5v3h-3v-3zM391.5 124.5v3h-3v-3zM391.5 140.5v3h-3v-3zM391.5 156.5v3h-3v-3zM391.5 172.5v3h-3v-3zM391.5 188.5v3h-3v-3zM391.5 204.5v3h-3v-3zM391.5 220.5v3h-3v-3zM391.5 236.5v3h-3v-3zM391.5 252.5v3h-3v-3zM391.5 268.5v3h-3v-3zM391.5 284.5v3h-3v-3zM375.5 12.5v3h-3v-3zM375.5 28.5v3h-3v-3zM375.5 44.5v3h-3v-3zM375.5 60.5v3h-3v-3zM375.5 76.5v3h-3v-3zM375.5 92.5v3h-3v-3zM375.5 108.5v3h-3v-3zM375.5 124.5v3h-3v-3zM375.5 140.5v3h-3v-3zM375.5 156.5v3h-3v-3zM375.5 172.5v3h-3v-3zM375.5 188.5v3h-3v-3zM375.5 204.5v3h-3v-3zM375.5 220.5v3h-3v-3z"></path><rect x="375.5" y="236.5" width="3" height="3" rx="1.5" transform="rotate(90 375.5 236.5)" fill="#4C6996"></rect><path fill="#4C6996" d="M375.5 252.5v3h-3v-3zM375.5 268.5v3h-3v-3zM375.5 284.5v3h-3v-3zM359.5 12.5v3h-3v-3zM359.5 28.5v3h-3v-3zM359.5 44.5v3h-3v-3zM359.5 60.5v3h-3v-3zM359.5 76.5v3h-3v-3zM359.5 92.5v3h-3v-3zM359.5 108.5v3h-3v-3zM359.5 124.5v3h-3v-3zM359.5 140.5v3h-3v-3zM359.5 156.5v3h-3v-3zM359.5 172.5v3h-3v-3zM359.5 188.5v3h-3v-3zM359.5 204.5v3h-3v-3zM359.5 220.5v3h-3v-3zM359.5 236.5v3h-3v-3zM359.5 252.5v3h-3v-3zM359.5 268.5v3h-3v-3zM359.5 284.5v3h-3v-3zM343.5 12.5v3h-3v-3zM343.5 28.5v3h-3v-3zM343.5 44.5v3h-3v-3zM343.5 60.5v3h-3v-3zM343.5 76.5v3h-3v-3zM343.5 92.5v3h-3v-3zM343.5 108.5v3h-3v-3zM343.5 124.5v3h-3v-3zM343.5 140.5v3h-3v-3zM343.5 156.5v3h-3v-3zM343.5 172.5v3h-3v-3zM343.5 188.5v3h-3v-3zM343.5 204.5v3h-3v-3zM343.5 220.5v3h-3v-3zM343.5 236.5v3h-3v-3zM343.5 252.5v3h-3v-3zM343.5 268.5v3h-3v-3zM343.5 284.5v3h-3v-3zM327.5 12.5v3h-3v-3zM327.5 28.5v3h-3v-3zM327.5 44.5v3h-3v-3zM327.5 60.5v3h-3v-3zM327.5 76.5v3h-3v-3zM327.5 92.5v3h-3v-3zM327.5 108.5v3h-3v-3zM327.5 124.5v3h-3v-3zM327.5 140.5v3h-3v-3zM327.5 156.5v3h-3v-3zM327.5 172.5v3h-3v-3zM327.5 188.5v3h-3v-3zM327.5 204.5v3h-3v-3zM327.5 220.5v3h-3v-3zM327.5 236.5v3h-3v-3zM327.5 252.5v3h-3v-3zM327.5 268.5v3h-3v-3zM327.5 284.5v3h-3v-3zM311.5 12.5v3h-3v-3zM311.5 28.5v3h-3v-3zM311.5 44.5v3h-3v-3zM311.5 60.5v3h-3v-3zM311.5 76.5v3h-3v-3zM311.5 92.5v3h-3v-3zM311.5 108.5v3h-3v-3zM311.5 124.5v3h-3v-3zM311.5 140.5v3h-3v-3zM311.5 156.5v3h-3v-3zM311.5 172.5v3h-3v-3zM311.5 188.5v3h-3v-3zM311.5 204.5v3h-3v-3zM311.5 220.5v3h-3v-3zM311.5 236.5v3h-3v-3zM311.5 252.5v3h-3v-3zM311.5 268.5v3h-3v-3zM311.5 284.5v3h-3v-3zM295.5 12.5v3h-3v-3zM295.5 28.5v3h-3v-3zM295.5 44.5v3h-3v-3zM295.5 60.5v3h-3v-3zM295.5 76.5v3h-3v-3zM295.5 92.5v3h-3v-3zM295.5 108.5v3h-3v-3zM295.5 124.5v3h-3v-3zM295.5 140.5v3h-3v-3zM295.5 156.5v3h-3v-3zM295.5 172.5v3h-3v-3zM295.5 188.5v3h-3v-3zM295.5 204.5v3h-3v-3zM295.5 220.5v3h-3v-3zM295.5 236.5v3h-3v-3zM295.5 252.5v3h-3v-3zM295.5 268.5v3h-3v-3zM295.5 284.5v3h-3v-3zM279.5 12.5v3h-3v-3zM279.5 28.5v3h-3v-3zM279.5 44.5v3h-3v-3zM279.5 60.5v3h-3v-3zM279.5 76.5v3h-3v-3zM279.5 92.5v3h-3v-3zM279.5 108.5v3h-3v-3zM279.5 124.5v3h-3v-3zM279.5 140.5v3h-3v-3zM279.5 156.5v3h-3v-3zM279.5 172.5v3h-3v-3zM279.5 188.5v3h-3v-3zM279.5 204.5v3h-3v-3zM279.5 220.5v3h-3v-3zM279.5 236.5v3h-3v-3zM279.5 252.5v3h-3v-3zM279.5 268.5v3h-3v-3zM279.5 284.5v3h-3v-3zM263.5 12.5v3h-3v-3zM263.5 28.5v3h-3v-3zM263.5 44.5v3h-3v-3zM263.5 60.5v3h-3v-3zM263.5 76.5v3h-3v-3zM263.5 92.5v3h-3v-3zM263.5 108.5v3h-3v-3zM263.5 124.5v3h-3v-3zM263.5 140.5v3h-3v-3zM263.5 156.5v3h-3v-3zM263.5 172.5v3h-3v-3zM263.5 188.5v3h-3v-3zM263.5 204.5v3h-3v-3zM263.5 220.5v3h-3v-3zM263.5 236.5v3h-3v-3zM263.5 252.5v3h-3v-3zM263.5 268.5v3h-3v-3zM263.5 284.5v3h-3v-3zM247.5 12.5v3h-3v-3zM247.5 28.5v3h-3v-3zM247.5 44.5v3h-3v-3zM247.5 60.5v3h-3v-3zM247.5 76.5v3h-3v-3zM247.5 92.5v3h-3v-3zM247.5 108.5v3h-3v-3zM247.5 124.5v3h-3v-3zM247.5 140.5v3h-3v-3zM247.5 156.5v3h-3v-3zM247.5 172.5v3h-3v-3zM247.5 188.5v3h-3v-3zM247.5 204.5v3h-3v-3zM247.5 220.5v3h-3v-3zM247.5 236.5v3h-3v-3zM247.5 252.5v3h-3v-3zM247.5 268.5v3h-3v-3zM247.5 284.5v3h-3v-3zM231.5 12.5v3h-3v-3zM231.5 28.5v3h-3v-3zM231.5 44.5v3h-3v-3zM231.5 60.5v3h-3v-3zM231.5 76.5v3h-3v-3zM231.5 92.5v3h-3v-3zM231.5 108.5v3h-3v-3zM231.5 124.5v3h-3v-3zM231.5 140.5v3h-3v-3zM231.5 156.5v3h-3v-3zM231.5 172.5v3h-3v-3zM231.5 188.5v3h-3v-3zM231.5 204.5v3h-3v-3zM231.5 220.5v3h-3v-3zM231.5 236.5v3h-3v-3zM231.5 252.5v3h-3v-3zM231.5 268.5v3h-3v-3zM231.5 284.5v3h-3v-3zM215.5 12.5v3h-3v-3zM215.5 28.5v3h-3v-3zM215.5 44.5v3h-3v-3zM215.5 60.5v3h-3v-3zM215.5 76.5v3h-3v-3zM215.5 92.5v3h-3v-3zM215.5 108.5v3h-3v-3zM215.5 124.5v3h-3v-3zM215.5 140.5v3h-3v-3zM215.5 156.5v3h-3v-3zM215.5 172.5v3h-3v-3zM215.5 188.5v3h-3v-3zM215.5 204.5v3h-3v-3zM215.5 220.5v3h-3v-3zM215.5 236.5v3h-3v-3zM215.5 252.5v3h-3v-3zM215.5 268.5v3h-3v-3zM215.5 284.5v3h-3v-3zM199.5 12.5v3h-3v-3zM199.5 28.5v3h-3v-3zM199.5 44.5v3h-3v-3zM199.5 60.5v3h-3v-3zM199.5 76.5v3h-3v-3zM199.5 92.5v3h-3v-3zM199.5 108.5v3h-3v-3zM199.5 124.5v3h-3v-3zM199.5 140.5v3h-3v-3zM199.5 156.5v3h-3v-3zM199.5 172.5v3h-3v-3zM199.5 188.5v3h-3v-3zM199.5 204.5v3h-3v-3zM199.5 220.5v3h-3v-3zM199.5 236.5v3h-3v-3zM199.5 252.5v3h-3v-3zM199.5 268.5v3h-3v-3zM199.5 284.5v3h-3v-3zM183.5 12.5v3h-3v-3zM183.5 28.5v3h-3v-3zM183.5 44.5v3h-3v-3zM183.5 60.5v3h-3v-3zM183.5 76.5v3h-3v-3zM183.5 92.5v3h-3v-3zM183.5 108.5v3h-3v-3zM183.5 124.5v3h-3v-3zM183.5 140.5v3h-3v-3zM183.5 156.5v3h-3v-3zM183.5 172.5v3h-3v-3zM183.5 188.5v3h-3v-3zM183.5 204.5v3h-3v-3zM183.5 220.5v3h-3v-3zM183.5 236.5v3h-3v-3zM183.5 252.5v3h-3v-3zM183.5 268.5v3h-3v-3zM183.5 284.5v3h-3v-3zM167.5 12.5v3h-3v-3zM167.5 28.5v3h-3v-3zM167.5 44.5v3h-3v-3zM167.5 60.5v3h-3v-3zM167.5 76.5v3h-3v-3zM167.5 92.5v3h-3v-3zM167.5 108.5v3h-3v-3zM167.5 124.5v3h-3v-3zM167.5 140.5v3h-3v-3zM167.5 156.5v3h-3v-3zM167.5 172.5v3h-3v-3zM167.5 188.5v3h-3v-3zM167.5 204.5v3h-3v-3zM167.5 220.5v3h-3v-3zM167.5 236.5v3h-3v-3zM167.5 252.5v3h-3v-3zM167.5 268.5v3h-3v-3zM167.5 284.5v3h-3v-3zM151.5 12.5v3h-3v-3zM151.5 28.5v3h-3v-3zM151.5 44.5v3h-3v-3zM151.5 60.5v3h-3v-3zM151.5 76.5v3h-3v-3zM151.5 92.5v3h-3v-3zM151.5 108.5v3h-3v-3zM151.5 124.5v3h-3v-3zM151.5 140.5v3h-3v-3zM151.5 156.5v3h-3v-3zM151.5 172.5v3h-3v-3zM151.5 188.5v3h-3v-3zM151.5 204.5v3h-3v-3zM151.5 220.5v3h-3v-3zM151.5 236.5v3h-3v-3zM151.5 252.5v3h-3v-3zM151.5 268.5v3h-3v-3zM151.5 284.5v3h-3v-3zM135.5 12.5v3h-3v-3zM135.5 28.5v3h-3v-3zM135.5 44.5v3h-3v-3zM135.5 60.5v3h-3v-3zM135.5 76.5v3h-3v-3zM135.5 92.5v3h-3v-3zM135.5 108.5v3h-3v-3zM135.5 124.5v3h-3v-3zM135.5 140.5v3h-3v-3zM135.5 156.5v3h-3v-3zM135.5 172.5v3h-3v-3zM135.5 188.5v3h-3v-3zM135.5 204.5v3h-3v-3zM135.5 220.5v3h-3v-3zM135.5 236.5v3h-3v-3zM135.5 252.5v3h-3v-3zM135.5 268.5v3h-3v-3zM135.5 284.5v3h-3v-3zM119.5 12.5v3h-3v-3zM119.5 28.5v3h-3v-3zM119.5 44.5v3h-3v-3zM119.5 60.5v3h-3v-3zM119.5 76.5v3h-3v-3zM119.5 92.5v3h-3v-3zM119.5 108.5v3h-3v-3zM119.5 124.5v3h-3v-3zM119.5 140.5v3h-3v-3zM119.5 156.5v3h-3v-3zM119.5 172.5v3h-3v-3zM119.5 188.5v3h-3v-3zM119.5 204.5v3h-3v-3zM119.5 220.5v3h-3v-3zM119.5 236.5v3h-3v-3zM119.5 252.5v3h-3v-3zM119.5 268.5v3h-3v-3zM119.5 284.5v3h-3v-3zM103.5 12.5v3h-3v-3zM103.5 28.5v3h-3v-3zM103.5 44.5v3h-3v-3zM103.5 60.5v3h-3v-3zM103.5 76.5v3h-3v-3zM103.5 92.5v3h-3v-3zM103.5 108.5v3h-3v-3zM103.5 124.5v3h-3v-3zM103.5 140.5v3h-3v-3zM103.5 156.5v3h-3v-3zM103.5 172.5v3h-3v-3zM103.5 188.5v3h-3v-3zM103.5 204.5v3h-3v-3zM103.5 220.5v3h-3v-3zM103.5 236.5v3h-3v-3zM103.5 252.5v3h-3v-3zM103.5 268.5v3h-3v-3zM103.5 284.5v3h-3v-3zM87.5 12.5v3h-3v-3zM87.5 28.5v3h-3v-3zM87.5 44.5v3h-3v-3zM87.5 60.5v3h-3v-3zM87.5 76.5v3h-3v-3zM87.5 92.5v3h-3v-3zM87.5 108.5v3h-3v-3zM87.5 124.5v3h-3v-3zM87.5 140.5v3h-3v-3zM87.5 156.5v3h-3v-3zM87.5 172.5v3h-3v-3zM87.5 188.5v3h-3v-3zM87.5 204.5v3h-3v-3zM87.5 220.5v3h-3v-3zM87.5 236.5v3h-3v-3zM87.5 252.5v3h-3v-3zM87.5 268.5v3h-3v-3zM87.5 284.5v3h-3v-3zM71.5 12.5v3h-3v-3zM71.5 28.5v3h-3v-3zM71.5 44.5v3h-3v-3zM71.5 60.5v3h-3v-3zM71.5 76.5v3h-3v-3zM71.5 92.5v3h-3v-3zM71.5 108.5v3h-3v-3zM71.5 124.5v3h-3v-3zM71.5 140.5v3h-3v-3zM71.5 156.5v3h-3v-3zM71.5 172.5v3h-3v-3zM71.5 188.5v3h-3v-3zM71.5 204.5v3h-3v-3zM71.5 220.5v3h-3v-3zM71.5 236.5v3h-3v-3zM71.5 252.5v3h-3v-3zM71.5 268.5v3h-3v-3zM71.5 284.5v3h-3v-3zM55.5 12.5v3h-3v-3zM55.5 28.5v3h-3v-3zM55.5 44.5v3h-3v-3zM55.5 60.5v3h-3v-3zM55.5 76.5v3h-3v-3zM55.5 92.5v3h-3v-3zM55.5 108.5v3h-3v-3zM55.5 124.5v3h-3v-3zM55.5 140.5v3h-3v-3zM55.5 156.5v3h-3v-3zM55.5 172.5v3h-3v-3zM55.5 188.5v3h-3v-3zM55.5 204.5v3h-3v-3zM55.5 220.5v3h-3v-3zM55.5 236.5v3h-3v-3zM55.5 252.5v3h-3v-3zM55.5 268.5v3h-3v-3zM55.5 284.5v3h-3v-3zM39.5 12.5v3h-3v-3zM39.5 28.5v3h-3v-3zM39.5 44.5v3h-3v-3zM39.5 60.5v3h-3v-3zM39.5 76.5v3h-3v-3zM39.5 92.5v3h-3v-3zM39.5 108.5v3h-3v-3zM39.5 124.5v3h-3v-3zM39.5 140.5v3h-3v-3zM39.5 156.5v3h-3v-3zM39.5 172.5v3h-3v-3zM39.5 188.5v3h-3v-3zM39.5 204.5v3h-3v-3zM39.5 220.5v3h-3v-3zM39.5 236.5v3h-3v-3zM39.5 252.5v3h-3v-3zM39.5 268.5v3h-3v-3zM39.5 284.5v3h-3v-3zM23.5 12.5v3h-3v-3zM23.5 28.5v3h-3v-3zM23.5 44.5v3h-3v-3zM23.5 60.5v3h-3v-3zM23.5 76.5v3h-3v-3zM23.5 92.5v3h-3v-3zM23.5 108.5v3h-3v-3zM23.5 124.5v3h-3v-3zM23.5 140.5v3h-3v-3zM23.5 156.5v3h-3v-3zM23.5 172.5v3h-3v-3zM23.5 188.5v3h-3v-3zM23.5 204.5v3h-3v-3zM23.5 220.5v3h-3v-3zM23.5 236.5v3h-3v-3zM23.5 252.5v3h-3v-3zM23.5 268.5v3h-3v-3zM23.5 284.5v3h-3v-3zM7.5 12.5v3h-3v-3zM7.5 28.5v3h-3v-3zM7.5 44.5v3h-3v-3zM7.5 60.5v3h-3v-3zM7.5 76.5v3h-3v-3zM7.5 92.5v3h-3v-3zM7.5 108.5v3h-3v-3zM7.5 124.5v3h-3v-3zM7.5 140.5v3h-3v-3zM7.5 156.5v3h-3v-3zM7.5 172.5v3h-3v-3zM7.5 188.5v3h-3v-3zM7.5 204.5v3h-3v-3zM7.5 220.5v3h-3v-3zM7.5 236.5v3h-3v-3zM7.5 252.5v3h-3v-3zM7.5 268.5v3h-3v-3zM7.5 284.5v3h-3v-3z"></path></g></g><defs><radialGradient id="dots_svg__b" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="rotate(-136.519 627.256 92.637) scale(929.292 677.674)"><stop stop-color="#D9D9D9"></stop><stop offset="0.574" stop-color="#D9D9D9" stop-opacity="0"></stop></radialGradient><clipPath id="dots_svg__a"><path fill="#fff" transform="rotate(90 306 306)" d="M0 0h297v612H0z"></path></clipPath></defs></svg></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"site":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ke4xTi5OD9gnn2YtRXyCV","type":"Entry","createdAt":"2022-10-14T08:31:23.161Z","updatedAt":"2023-06-06T20:01:24.167Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"site"}},"locale":"en-US"},"fields":{"title":"Anyscale","description":"From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab.","identifier":"site-master-new","footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"16eb9I0yhaIALNOPR8WKTY","type":"Entry","createdAt":"2020-09-27T16:03:59.581Z","updatedAt":"2022-10-27T01:20:31.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer","body":"© Anyscale, Inc 2022","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"C7AXPFHtleWPOZ9BuT73K","type":"Asset","createdAt":"2020-09-28T14:40:50.925Z","updatedAt":"2020-09-28T14:40:50.925Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"anyscale_logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/C7AXPFHtleWPOZ9BuT73K/927f5130df1357db041d7a7908ab572e/Graphic_only__transparent_background_.png","details":{"size":1415,"image":{"width":331,"height":331}},"fileName":"Graphic only (transparent background).png","contentType":"image/png"}}}],"subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1cDdKq2eUfklc6w52dU6dg","type":"Entry","createdAt":"2020-09-27T16:05:42.307Z","updatedAt":"2022-10-27T01:20:26.714Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-1","header":"Products","body":"- [Anyscale Compute Platform](/platform)\n- [Ray Open Source](/ray-open-source)\n"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5tpXhcAJwSdhX8Wqw9HYTt","type":"Entry","createdAt":"2020-09-27T16:06:37.409Z","updatedAt":"2022-10-27T01:20:21.887Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":10,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-2","header":"Events","body":"- [Webinars](/events)\n- [Meetups](/events)\n- [Summits](/events)\n"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3c3LEomScM93hIg3i2zItU","type":"Entry","createdAt":"2020-09-27T16:07:42.856Z","updatedAt":"2022-10-27T01:20:18.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-3","header":"Company","body":"- [About Us](/about)\n- [News](/press)\n- [Careers](/careers)\n- [Community](/community)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6du4SSXPTxDZKoIVNHVVnT","type":"Entry","createdAt":"2022-10-27T01:20:13.320Z","updatedAt":"2022-10-27T01:20:13.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-4","header":"Learn","body":"- [Blog](/blog)\n- [Demos \u0026 Webinars](/events)\n- [Anyscale Academy](/events)\n- [Anyscale Docs](https://docs.anyscale.com)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1hovCXhGcWlgJmrqNzJ79k","type":"Entry","createdAt":"2022-10-27T01:20:09.895Z","updatedAt":"2022-10-27T01:20:09.895Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-5","header":"Scalable AI and Python","body":"- [Data Ingestion](/data-ingestion)\n- [Reinforcement Learning](/reinforcement-learning)\n- [Ray AIR](/ray-air)\n- [Model Serving](/model-serving)\n- [Hyperparameter Tuning](/hyperparameter-tuning)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"YFy5fUqwOD5ShdMaYEvzB","type":"Entry","createdAt":"2022-10-27T01:20:06.631Z","updatedAt":"2022-10-27T01:20:06.631Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-6","header":"Use Cases","body":"- [Demand Forecasting/Pricing](/demand-forecasting)\n- [Industrial Automation](/industrial-automation)\n- [Scalable ML Platforms](/machine-learning)\n- [NLP](/natural-language-processing)\n- [Recommendation System](/recommendation-system)"}}]}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"KBAZ5OavkdZfNTVjdJkp3","type":"Entry","createdAt":"2022-10-14T10:47:29.682Z","updatedAt":"2022-10-14T10:47:29.682Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Products","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KNN1Kmzo7jxSlxX1k74XQ","type":"Entry","createdAt":"2022-10-07T13:14:07.892Z","updatedAt":"2022-10-07T13:14:07.892Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"52Mmru1brS6ZkmItPeIW86","type":"Entry","createdAt":"2022-10-07T13:13:22.198Z","updatedAt":"2022-11-03T01:33:41.860Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Anyscale Platform","link":"/platform"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6cN6ZwdiFNiPcqIhd7EiQY","type":"Entry","createdAt":"2022-10-07T13:13:49.673Z","updatedAt":"2022-10-07T13:13:49.673Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Open Source","link":"/ray-open-source"}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5R1OTMfuLnPqont0Fjn9Xb","type":"Entry","createdAt":"2022-10-07T13:20:19.578Z","updatedAt":"2022-10-14T10:43:21.550Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Solutions","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1mp3IF3ZJE5JAViu3L9E95","type":"Entry","createdAt":"2022-10-07T13:16:00.346Z","updatedAt":"2022-10-07T13:21:01.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Scalable AI and Python","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4VUyKorAkgQP2V295h8eW3","type":"Entry","createdAt":"2022-10-07T13:15:35.949Z","updatedAt":"2022-10-07T13:15:35.949Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Data Ingestion","link":"/data-ingestion"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"Ouf5WgAzYmLovCZFNWqYm","type":"Entry","createdAt":"2022-10-07T13:17:09.728Z","updatedAt":"2022-10-07T13:17:09.728Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Reinforcement Learning","link":"/reinforcement-learning"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5kVPKqUbXBbKmrkvsImcIi","type":"Entry","createdAt":"2022-10-07T13:17:50.307Z","updatedAt":"2022-10-07T13:17:50.307Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray AIR","link":"/ray-air"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5uUidEvWw1oqjWrRcskEW","type":"Entry","createdAt":"2022-10-07T13:18:21.246Z","updatedAt":"2022-10-07T13:18:21.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Model Serving","link":"/model-serving"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Rbw0CrBN1KecPafCPvF4H","type":"Entry","createdAt":"2022-10-07T13:18:45.601Z","updatedAt":"2022-10-07T13:18:45.601Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Hyperparameter Tuning","link":"/hyperparameter-tuning"}}],"card":"##### Scalable AI and Python\n\nLeverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more."}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2RgWCuIiZe2TI6Nx5Ns7sB","type":"Entry","createdAt":"2022-10-07T13:25:03.101Z","updatedAt":"2023-05-31T12:47:25.274Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Use Cases","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3XMbpU1mPdsU7gktltlAzX","type":"Entry","createdAt":"2023-05-31T12:43:54.011Z","updatedAt":"2023-05-31T12:43:54.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Large Language Models","link":"/large-language-models"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6MAYw6HTSmH5JHcOoYjCzC","type":"Entry","createdAt":"2022-10-07T13:22:29.165Z","updatedAt":"2022-10-07T13:22:29.165Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Demand Forecasting/Pricing","link":"/demand-forecasting"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"30BlKhv6h4neaZwjA2jlxO","type":"Entry","createdAt":"2022-10-07T13:23:09.471Z","updatedAt":"2022-10-07T13:23:09.471Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Industrial Automation","link":"/industrial-automation"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2gGwUVxDuBeUso1uQH2gEW","type":"Entry","createdAt":"2022-10-07T13:23:32.145Z","updatedAt":"2022-10-07T13:23:32.145Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Scalable ML Platforms","link":"/machine-learning"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1cbwbmIvP5ZkV9iNEraQGP","type":"Entry","createdAt":"2022-10-07T13:23:59.732Z","updatedAt":"2022-10-07T13:23:59.732Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"NLP","link":"/natural-language-processing"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2A7FMbckpN5bgW1wp5ZuSh","type":"Entry","createdAt":"2022-10-07T13:24:41.861Z","updatedAt":"2022-10-07T13:24:41.861Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Recommendation System","link":"/recommendation-system"}}],"card":"##### Use Cases\n\nLeverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more."}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QNdrsmqz82oiBJvFTQ26n","type":"Entry","createdAt":"2023-04-13T11:37:45.354Z","updatedAt":"2023-04-13T11:37:45.354Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ecosystem","link":"/ray-ecosystem"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i0nHjF3pmjzGAdEE4V5MG","type":"Entry","createdAt":"2021-12-05T02:08:18.719Z","updatedAt":"2022-03-03T02:01:08.894Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Success Stories","link":"/user-stories"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5mFZvzY56aQCY4xvlBDi9W","type":"Entry","createdAt":"2022-10-14T10:46:35.218Z","updatedAt":"2022-10-14T10:46:35.218Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Learn","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"32tQWjMRAI5qci97W3cuie","type":"Entry","createdAt":"2022-10-07T13:28:46.057Z","updatedAt":"2023-03-22T20:31:55.243Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JQbOCOkxHfH2ZPGNNQVbt","type":"Entry","createdAt":"2021-12-05T14:57:28.664Z","updatedAt":"2021-12-05T14:57:28.664Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Blog","link":"/blog"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4qEkJgYkXZmiDq3lsxg0xW","type":"Entry","createdAt":"2023-03-22T20:31:21.666Z","updatedAt":"2023-03-22T20:31:21.666Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Summit 2023","link":"https://raysummit.anyscale.com/?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=home_nav-pulldown"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48sdeWXptwt3UgEZzeTt3k","type":"Entry","createdAt":"2022-10-07T13:26:28.331Z","updatedAt":"2022-10-07T13:26:28.331Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Videos \u0026 Webinars","link":"/events"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6hMfRHFDZUXcJH1tIq0xhY","type":"Entry","createdAt":"2021-10-21T20:59:36.918Z","updatedAt":"2023-03-06T20:24:26.811Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Training","link":"https://github.com/ray-project/ray-educational-materials"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cVWy7hmqbAzJCW6d7cLUx","type":"Entry","createdAt":"2022-10-07T13:28:33.537Z","updatedAt":"2022-10-07T13:28:33.537Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Anyscale Docs","link":"https://docs.anyscale.com"}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Qjs7zNfhrinHTNnzMpVIO","type":"Entry","createdAt":"2022-10-14T10:44:42.403Z","updatedAt":"2022-10-14T10:44:42.403Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Company","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ZFDrG8YGgU0oyNgBOId7X","type":"Entry","createdAt":"2022-10-07T13:32:38.746Z","updatedAt":"2022-10-07T13:32:38.746Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2RD65wmpn4gmL5Bvxu5VOD","type":"Entry","createdAt":"2020-09-08T17:53:32.413Z","updatedAt":"2021-06-20T15:04:11.482Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"About us","link":"/about"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71EHE1t3QrjnQb8hD9O8Ww","type":"Entry","createdAt":"2022-10-07T13:32:07.593Z","updatedAt":"2022-10-07T13:32:07.593Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"News","link":"/press"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3TNUPaz7ViQ6hbiZciBOQV","type":"Entry","createdAt":"2020-09-28T23:19:39.838Z","updatedAt":"2022-05-12T19:18:18.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Careers","link":"/careers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ubc5oYY5moS7HexIp7AjO","type":"Entry","createdAt":"2022-07-22T20:40:53.600Z","updatedAt":"2022-07-22T20:40:53.600Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Community","link":"/community"}}]}}]}}],"bannerText":"Are you passionate about Ray, scalable AI and distributed computing? Join us at Ray Summit!","bannerLink":"https://raysummit.anyscale.com/?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=homepage_ticker","bannerCta":" Find out more! "}},"title":"Why Third Generation ML Platforms are More Performant","slug":"why-third-generation-ml-platforms-are-more-performant","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12vKMQ3j3q45idk83ZoPYA","type":"Entry","createdAt":"2021-10-01T20:23:37.378Z","updatedAt":"2021-10-01T20:23:37.378Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Chen Shen","slug":"chen-shen"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2021-10-06","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"In a previous blog post, we defined a \"3rd generation ML platform\" as one that offered full programmability for ML workflows. Key to a 3rd generation platform is the concept of a programmable compute layer. In this blog, we report on emerging patterns of distributed compute we see in advanced ML platform workloads. We show how Ray, a leading programmable compute layer, improves performance by 3-9x in relevant production workloads.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"In a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"previous blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we defined a \"3rd generation ML platform\" as one that offered full programmability for ML workflows. Key to a 3rd generation platform is the","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" ","nodeType":"text"},{"data":{},"marks":[],"value":"concept of a ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"programmable compute layer","nodeType":"text"},{"data":{},"marks":[],"value":". In this blog, we report on emerging patterns of distributed compute we see in advanced ML platform workloads. We show how ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a leading programmable compute layer, improves performance by 3-9x in relevant production workloads.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ecYQnlH7n2Tcv8Ovf1IEZ","type":"Asset","createdAt":"2021-10-01T20:28:28.089Z","updatedAt":"2021-10-05T04:17:24.678Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"2ndVS3rdGeneration","description":"A \"2nd generation\" (left) vs \"3rd generation\" (right) ML platform. Light-blue boxes represent clusters, and light-purple represents libraries. The 3rd generation platform eliminates cluster compute silos and improves performance and programmability.","file":{"url":"//images.ctfassets.net/xjan103pcp94/6ecYQnlH7n2Tcv8Ovf1IEZ/79da29e55ec74c834913c2072c6ebf8d/Fig1.png","details":{"size":76050,"image":{"width":2156,"height":420}},"fileName":"Fig1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance Overheads in Second Generation Platforms","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed ML workflows are typically composed from a few types of compute patterns: collective (i.e., a set of processes communicating with each other like in ","nodeType":"text"},{"data":{"uri":"https://github.com/horovod/horovod"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"distributed SGD","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"), chaining (i.e., a ","nodeType":"text"},{"data":{"uri":"https://docs.metaflow.org/metaflow/basics#linear"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"sequential workflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" of tasks run one after the other), and nesting (i.e., tasks that kick off other tasks, commonly seen in ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"hyperparameter tuning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7KYcuMZ98l5DdtDi10vHxp","type":"Asset","createdAt":"2021-10-01T20:43:35.650Z","updatedAt":"2021-10-05T04:21:32.297Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"collectingChainingNesting","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7KYcuMZ98l5DdtDi10vHxp/594784d15b520df3924340af95bc0b86/CollectiveChainingNesting.png","details":{"size":64947,"image":{"width":2180,"height":570}},"fileName":"CollectiveChainingNesting.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"first generation platforms","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ML workflows were implemented as custom-built systems optimized for a specific workflow. In second generation platforms, flexibility was achieved by relying on ","nodeType":"text"},{"data":{"uri":"https://neptune.ai/blog/best-workflow-and-pipeline-orchestration-tools"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"workflow orchestrators","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to provide chaining and nesting, gluing together separate systems that internally implement high-optimized collective operations.","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":" ","nodeType":"text"},{"data":{},"marks":[],"value":"The following figure illustrates a 2nd generation distributed ML platform. Each step is run as a separate cluster by a workflow orchestrator (e.g., ","nodeType":"text"},{"data":{"uri":"https://engineering.fb.com/2016/05/09/core-data/introducing-fblearner-flow-facebook-s-ai-backbone/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FBLearner Flow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker Steps","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://metaflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Metaflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.kubeflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"KubeFlow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"60TSWgITq3zG5xHqP8z3Xi","type":"Asset","createdAt":"2021-10-01T20:44:24.648Z","updatedAt":"2021-10-05T04:24:47.644Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"WorkflowOrchestrator","description":"A \"2nd generation\" distributed ML platform. A workflow orchestrator is needed to connect components implemented as separate distributed systems, limiting the programmability--- and performance--- of the platform.","file":{"url":"//images.ctfassets.net/xjan103pcp94/60TSWgITq3zG5xHqP8z3Xi/badf3cbb8c64666b880e7b28d407e6bf/WorkflowOrchestrator.png","details":{"size":72420,"image":{"width":1676,"height":420}},"fileName":"WorkflowOrchestrator.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, this 2nd generation architecture imposes performance overheads and limits programmability. This is due to:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Scheduling overheads. ","nodeType":"text"},{"data":{},"marks":[],"value":"These platforms rely on separately scheduled VMs or containers per step to distribute the workload. Each step may launch its own distributed framework (e.g., Spark or Distributed PyTorch). This leads to several seconds to minutes of scheduling overhead per step, and prevents optimizations like pipelining.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data movement overheads","nodeType":"text"},{"data":{},"marks":[],"value":". The overhead between steps can be reduced substantially if data is kept in memory between steps when possible and not materialized to storage.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Programmability overheads","nodeType":"text"},{"data":{},"marks":[],"value":". Expressing fine-grained nesting or pipelining can require substantial changes to distributed systems code that are rarely accessible to end-users of existing ML platforms.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Chaining and Nesting can be Performance Bottlenecks","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In many cases, chaining and nesting are ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"not","nodeType":"text"},{"data":{},"marks":[],"value":" performance bottlenecks. This is because the data transferred is small, or scheduling overhead is small compared to execution time. However, there are a growing number of scenarios where bottlenecks do arise. Here we overview several use cases that benefit from optimized chaining and nesting.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BYEdrkn0NDWNOEzPmtSIm","type":"Entry","createdAt":"2021-10-01T21:05:45.584Z","updatedAt":"2021-10-06T20:32:36.631Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":30,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"Compare 2nd vs 3rd Generation","body":"\n|  Use case   | 2nd generation platforms  | 3rd generation platforms | \n| ---------- | ---------- | ---------- |\n| __Data Ingest for Model Training (Chaining):__ Chaining data processing and ML training is simple when the data can fit onto a single machine. It becomes challenging at [large scale](https://arxiv.org/abs/2108.09373), when you want to [shuffle data globally](https://speakerdeck.com/anyscale/per-epoch-shuffling-data-loader-mix-it-up-as-you-train-clark-zinzow-anyscale) during training, or pipeline data preparation with training to [reduce latency](https://docs.ray.io/en/latest/data/dataset-pipeline.html). Although materializing data to cluster storage provides certain benefits, frequent I/O to cluster storage introduces a lot of overhead. | ![01_twineditSimpleChain](//images.contentful.com/xjan103pcp94/45y4oFh98EwSKMRaxzHcJ/93fee7c4c8c217df3a41173f52c6a733/01_twineditSimpleChain.png) | ![01_twineditPipelined](//images.contentful.com/xjan103pcp94/whcbju48HM91FOIIyMHrC/2e2b019b20786f16f84b6479dbeed31b/01_twineditPipelined.png)\n|  __Serving Pipelines (Chaining):__ Similarly, passing data efficiently is critical to the performance of disaggregated [model serving pipelines](https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns). In a disaggregated pipeline, large models are split into multiple parts that can be deployed and scaled separately (e.g., fully-connected networks vs memory-intensive embedding lookups). Predictions can be composed of multiple models in a DAG structure (e.g., [RoboVision](https://robovision.ai/) uses a 5-model stack for vehicle detection pipeline).   | ![02_2nd_4x2_22font_SimpleServing](//images.contentful.com/xjan103pcp94/4UyLFxAEkYrZUIP0AWDXbO/cf95c047d9d4df80066ff766b12ee006/02_2nd_4x2_22font_SimpleServing.png) | ![02_3rd_4x2_22font_DisaggregatedModelServingPipeline](//images.contentful.com/xjan103pcp94/30rncKJXFtnXBK12vcHKyD/7915253df3ff9c68c2faea56e4a45342/02_3rd_4x2_22font_DisaggregatedModelServingPipeline.png)\n|  __Batch Scoring (Nesting):__ Scoring a model on a large dataset can be quite slow without [distributed computation](https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/ai/batch-scoring-python), which naturally generates a nested workload during the course of training. Nesting can be difficult to express, let alone efficiently, in traditional workflow orchestrators.  | ![03_2nd_4x2_22font_SequentuilScoring](//images.contentful.com/xjan103pcp94/5uWbLSWU26EZp5Q9kkEMMh/41c443ee64006e8ee8d88eb20cd65f70/03_2nd_4x2_22font_SequentuilScoring.png)  | ![03_3rd_4x2_22font_DistributedScoring](//images.contentful.com/xjan103pcp94/1IxBk5YiSbdFPRUYneqBKy/8c39b2d0586bfa4af2ab84063a191433/Screen_Shot_2021-10-05_at_6.55.50_PM.png)\n| __Hyperparameter Tuning (Nesting):__ Nesting also occurs naturally during hyperparameter tuning, which seeks to explore many variants of existing workloads. Nesting is needed for [distributed trials](https://docs.ray.io/en/latest/tune/index.html), and efficiency is important for supporting lightweight trials and [population based approaches](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-scheduler-pbt).  | ![04_2nd_4x2_22font_SimpleNesting](//images.contentful.com/xjan103pcp94/6BHEgjIjRWRIymCYfQ1xKc/234d498153325597f75936d9e95cbb96/04_2nd_4x2_22font_SimpleNesting.png)  | ![04_3rd_4x2_22font_DistributedtrialsPBP](//images.contentful.com/xjan103pcp94/2oh4v2FUq0C5RS2FlwBSah/5708b8ccf119b334af0b4584921633a3/04_3rd_4x2_22font_DistributedtrialsPBP.png)\n| __Workflow DAGs (Both Chaining and Nesting):__ Finally, both chaining and nesting occur naturally in higher level workflow DAGs implemented by workflow orchestrators. Here two bottlenecks can occur: (1) if large amounts of data is passed or shared between workflow steps, it is desirable to pass data at memory-speed rather than reading and writing to cluster storage, and (2) if steps are small, scheduling overhead can dominate workflow run time. | ![05_2nd_4x2_22font_DAG](//images.contentful.com/xjan103pcp94/5pHdKYmzsuBM6WAAa2WAur/b45d46823841e0d089d2d80628bb7265/05_2nd_4x2_22font_DAG.png)  | ![05_3rd_4x2_22font_inMemoryLow-overhead](//images.contentful.com/xjan103pcp94/DZmefXHDWCnvBWm0m05wG/aa46b9911cf20c6f4281a8e3763592a9/05_3rd_4x2_22font_inMemoryLow-overhead.png)"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Third Generation Platforms Accelerate Chaining and Nesting","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"A ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"third generation ML platform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" eliminates the above performance bottlenecks. Users and builders of third generation platforms are able to:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Implement chaining and nesting of distributed steps with minimal scheduling and data movement overheads.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Programmatically author ML workflows, weaving together steps like ingest, transform, and training without needing to wrangle separate distributed systems.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is possible with the use of a ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"programmable compute layer ","nodeType":"text"},{"data":{},"marks":[],"value":"such as ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which can serve as a replacement to (or ","nodeType":"text"},{"data":{"uri":"https://www.astronomer.io/blog/airflow-ray-data-science-story"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"accelerator for","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") workflow orchestrators. In a 3rd gen platform, distributed logic such as ","nodeType":"text"},{"data":{"uri":"https://medium.com/distributed-computing-with-ray/data-processing-support-in-ray-ae8da34dce7e"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"data processing","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is implemented as ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/ray-libraries.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"libraries","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" within the compute layer:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5b9o8llgwycHgDtV9c1r8m","type":"Asset","createdAt":"2021-10-04T20:57:30.411Z","updatedAt":"2021-10-04T20:57:30.411Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"3rdGenDistributed","description":"A \"3rd generation\" distributed ML platform. The programmable compute layer allows distributed steps to be tightly woven together in code, eliminating the overheads of separate clusters.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5b9o8llgwycHgDtV9c1r8m/ab3f90039ba3ac7cf2577c1658aad85c/3rdGenDistributed.png","details":{"size":33191,"image":{"width":720,"height":216}},"fileName":"3rdGenDistributed.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Benchmarks","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this section we highlight some of the performance gains users have seen by leveraging Ray's support for efficient chaining and nesting of computations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Uber: Shuffled ML Ingest Pipeline","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a previous blog post, Uber introduced how they were leveraging Ray for ","nodeType":"text"},{"data":{"uri":"https://eng.uber.com/horovod-ray/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"elastic scheduling and training with Horovod","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". In a follow-up project, another team is using Ray's ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/dataset-pipeline.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Dataset Pipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to implement shuffled ML ingest, where data is globally shuffled across cluster CPU workers per iteration of GPU training. Here we analyze the importance of pipelining and in-memory data exchange. We ran a training workload reading 500GB of data on a cluster of 70 CPU nodes and 16 GPU nodes, and find ingest throughput is 3x higher with the pipelining and in-memory data exchange enabled by Ray:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"soBLd3ZStUVqfRH8BCxtR","type":"Asset","createdAt":"2021-10-04T20:59:23.745Z","updatedAt":"2021-10-05T03:52:09.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Shuffled Ingest","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/soBLd3ZStUVqfRH8BCxtR/bafe8e1c993e605a88ae521cdfc99584/Shuffled_Ingest_Throughput.png","details":{"size":88039,"image":{"width":1938,"height":810}},"fileName":"Shuffled Ingest Throughput.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Wildlife Studios: Chained Model Pipelines","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Mobile gaming giant ","nodeType":"text"},{"data":{"uri":"https://wildlifestudios.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Wildlife Studios’","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" legacy system for serving revenue-generating in-game offers was not scaling to meet their latency and cost requirements. After switching to Ray Serve, their Dynamic Offers team was able to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/wildlife-studios-serves-in-game-offers-3x-faster-at-1-10th-the-cost-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"serve offers three times faster","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". The Ray Serve architecture provided support for parallel inference on multiple models in a pipeline, decreasing latency and minimizing idle machines:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1qt32KDRT1K8MzgWLfQ2iq","type":"Asset","createdAt":"2021-10-04T21:00:42.387Z","updatedAt":"2021-10-05T03:52:32.315Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"p95","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1qt32KDRT1K8MzgWLfQ2iq/eaae25d1ae4d466294c568fbe380a0e6/P95_Latency_in_Production_Cluster.png","details":{"size":82299,"image":{"width":1804,"height":784}},"fileName":"P95 Latency in Production Cluster.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Anastasia.ai: Nested Model Evaluation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://anastasia.ai/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Anastasia","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provides a powerful platform that enables organizations to operate AI capacities at scale with a fraction of the resources and effort traditionally required. They were able to accelerate a demand prediction problem using ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and got up to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-anastasia-implements-ray-and-anyscale-to-speed-up-ml-processes-9x"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"9x performance gains","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" due to fine-grained re-use of resources compared to a coarse grained orchestrator:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4ASH8c3Ia98IGL4o1u1FSS","type":"Asset","createdAt":"2021-10-04T21:02:21.018Z","updatedAt":"2021-10-05T03:58:00.087Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"RelativeTimeEvaluate384CoreCluster","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4ASH8c3Ia98IGL4o1u1FSS/5f9db5896146bc065699d6caacb5739e/Relative_Time_to_Evaluate_on_384-core_Cluster.png","details":{"size":75697,"image":{"width":2020,"height":720}},"fileName":"Relative Time to Evaluate on 384-core Cluster.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog, we showed how a programmable compute layer such as Ray can provide 3-9x performance improvements for production ML workloads, eliminating bottlenecks found in 2nd generation production architectures. This is in addition to the productivity and operational benefits of having a programmable architecture.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"As a final note, the ideas presented here apply generally to distributed programming as well as to ML workloads. If you're interested in the performance and programmability of ML applications and distributed computing, check out the ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray project","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and consider ","nodeType":"text"},{"data":{"uri":"https://jobs.lever.co/anyscale"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"joining us","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fTEska4fx1SFm7wlBB7JF","type":"Asset","createdAt":"2021-10-06T16:49:59.640Z","updatedAt":"2021-10-06T16:49:59.640Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"mlplatformCropped","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5fTEska4fx1SFm7wlBB7JF/cb6fc1348b8a28bc0b91ef9dfb080adb/mlplatformCropped.png","details":{"size":84401,"image":{"width":926,"height":500}},"fileName":"mlplatformCropped.png","contentType":"image/png"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3wI6uYJcQZCdFRfR00d2ta","type":"Entry","createdAt":"2022-03-29T00:18:53.787Z","updatedAt":"2022-03-29T00:19:41.793Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Why Third Generation ML Platforms are More Performant","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5YgMgD8vYcB7D6HUNoItHz","type":"Entry","createdAt":"2021-09-15T16:23:49.634Z","updatedAt":"2022-06-22T16:54:14.783Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"The Third Generation of Production ML Architectures","slug":"the-third-generation-of-production-ml-architectures","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2021-09-15","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"As technology has advanced, production ML architectures have evolved. One way to see it is in terms of generations: The first generation involved “fixed function” pipelines, while the second generation involved programmability within the pipeline of particular existing actions.\n\nWhat will the third generation of production ML architectures be like? This post tries to answer that by using history as a guide by looking at the evolution of GPU programming architectures. We’ll then talk about a system called Ray that seems to follow in the same footsteps.","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6S3bvsbfg1wDC80fb1aFXO","type":"Entry","createdAt":"2021-09-14T22:28:07.517Z","updatedAt":"2021-09-15T21:32:52.936Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Third Generation Production ML Architectures","videoUrl":" https://youtu.be/hzW0AKKqew4","caption":"This blog is based on Waleed Kadous’ “Third Generation Production ML Architectures” [talk](https://youtu.be/hzW0AKKqew4) and [slideshow](https://www.slideshare.net/WaleedKadous/third-gen-production-ml-architectures-lessons-from-history-experiences-with-ray) from apply() Conference 2021."}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Where are production ML architectures going? ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Production machine learning architectures are ML systems that are deployed in production environments. These are systems trained on billions of examples, and often serve millions of inferences per second. Because of this mammoth scale, they almost always require distributed training and inference. These systems require careful design to support the massive scale they run at, and so as an industry we have tried to create reusable systems to meet the sometimes difficult requirements. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"As technology has advanced, production ML architectures have evolved. One way to see it is in terms of generations: The ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"first generation","nodeType":"text"},{"data":{},"marks":[],"value":" involved “fixed function” pipelines, while the ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"second generation","nodeType":"text"},{"data":{},"marks":[],"value":" involved programmability within the pipeline of particular existing actions.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What will the third generation of production ML architectures be like? This post tries to answer that by using history as a guide by looking at the evolution of GPU programming architectures. We’ll then talk about a system called ","nodeType":"text"},{"data":{"uri":"https://ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" that seems to follow in the same footsteps. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"GPU Programming History","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let’s quickly look at GPU programming history -- the one that the deep learning revolution is built on. This might give us insight for what the future might hold for production machine learning architectures. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2PPibemMAsHklsf6rbxhYE","type":"Asset","createdAt":"2021-09-14T22:32:10.865Z","updatedAt":"2021-09-14T22:35:25.841Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"1stGenGPU","description":"1st Generation of GPU programming architectures (OpenGL 1.0, Direct 3D)","file":{"url":"//images.ctfassets.net/xjan103pcp94/2PPibemMAsHklsf6rbxhYE/fb3bef34e939d63ea4ec2bb37e8ed465/1stGenGPU.png","details":{"size":28128,"image":{"width":1252,"height":129}},"fileName":"1stGenGPU.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the mid to late 1990’s, we saw the first generation of GPU programming architectures. The input would be things like textures, polygons, meshes, etc. It would go through the pipeline and output be an image shown to the user. The revolutionary aspect of this fixed function pipeline was that it was all hardware accelerated. This was the first time that consumers had hardware accelerated graphics. This enabled the 3D gaming revolution. While this brought many new opportunities, it was also limited in many ways. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"58OjDFJljhp1ev3dCzbowj","type":"Asset","createdAt":"2021-09-15T00:09:37.295Z","updatedAt":"2021-09-15T01:44:35.434Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"ProCons1st","description":"Pros and cons of the 1st GPU programming architecture","file":{"url":"//images.ctfassets.net/xjan103pcp94/58OjDFJljhp1ev3dCzbowj/cbaca85d3dd51414e632774f6014b6a9/ProsCons1st.png","details":{"size":42000,"image":{"width":956,"height":304}},"fileName":"ProsCons1st.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The 2nd generation had the same pipeline, however it was broken up into more programmable sections. This allowed for more flexibility with the vertex and fragment shaders being nothing more than small programs that run in the hardware. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"26fOaclf4se7b8hrvhR3mO","type":"Asset","createdAt":"2021-09-14T22:45:20.741Z","updatedAt":"2021-09-14T22:45:20.741Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2ndGenerationGPU","description":"2nd Generation of GPU programming architectures (OpenGL 2.0, Direct3D 10)","file":{"url":"//images.ctfassets.net/xjan103pcp94/26fOaclf4se7b8hrvhR3mO/b4172c5d5f9cc3d6ada158fbd4a4148a/2ndGeneration.png","details":{"size":56986,"image":{"width":1252,"height":400}},"fileName":"2ndGeneration.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This had some programmability, but it still required conforming to the existing pipeline. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5ixmnmnx5McqhmRwQp3cHY","type":"Asset","createdAt":"2021-09-14T22:46:35.203Z","updatedAt":"2021-09-14T22:46:35.203Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2ndGenerationRigid","description":"The required formats for the different sections were pretty rigid.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5ixmnmnx5McqhmRwQp3cHY/1d250f236acd787eabc4d4d2288c5f50/2ndGenerationRigid.png","details":{"size":78051,"image":{"width":1252,"height":518}},"fileName":"2ndGenerationRigid.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This gave a great deal more flexibility to developers that allowed for amazing effects, and while far more flexible, there were still limits. In particular the inputs and outputs of each stage were well defined, and really could not be changed easily. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The second generation was Turing complete so it was possible to create anything, ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"however possible and easy are not the same thing","nodeType":"text"},{"data":{},"marks":[],"value":". For example, if someone was working on a physics problem (or a machine learning problem), they had to work really hard to make it conform to a combination of vertex and fragment shaders and had to have a deep understanding of how the underlying hardware was designed. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4P5O7R9Cvk5XAhpF68hZv9","type":"Asset","createdAt":"2021-09-14T22:49:44.591Z","updatedAt":"2021-09-14T22:49:44.591Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"thirdGenerationGPU","description":"3rd generation of GPU programming architectures (Cg, OpenCL) are a lot more flexible because there is less of a focus on pipelines and more of a focus on full programmability. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/4P5O7R9Cvk5XAhpF68hZv9/7b0e5c4560c4e779b25aec97b8fb4288/thirdGenerationGPU.png","details":{"size":75364,"image":{"width":1377,"height":508}},"fileName":"thirdGenerationGPU.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The Third generation was the one that brought complete programmability combined with performance to GPU programming architectures. The third generation could do everything the previous generations could do and is a lot more flexible because there is less of a focus on pipelines. It is all about full programmability with a focus on libraries. Indeed the first and second generations literally become nothing more than libraries that can be reused at will (or completely ignored if you want to). ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Originally, games were written on code that runs on Direct3D or OpenGL (first and second generation systems). But with the arrival of the third generation, people started to use engines like Unity or Unreal Engine 4 due to their programmability and flexibility with the GPU being more of an implementation detail. This enabled people without knowledge of pipelines and shaders to use it like a normal code library. This opened up the power of GPUs to a huge number of users and this has led to an explosion of applications. For example, modern deep learning infrastructure is built upon this. For example, CUDA led to cuDNN which led to Caffe then Torch and finally PyTorch. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6rslx90y6tOEPJsr2JXJM4","type":"Asset","createdAt":"2021-09-14T22:53:16.883Z","updatedAt":"2021-09-14T22:53:16.883Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"openGL_Unity","description":"Unity (3rd gen) has become a lot more popular than OpenGL (2nd gen). With Unity, people are worrying less about the architecture and focusing more on the characteristics and the libraries. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/6rslx90y6tOEPJsr2JXJM4/231e5d37ffc5159724827c32e5c33460/openGL_Unity.png","details":{"size":108398,"image":{"width":1047,"height":685}},"fileName":"openGL_Unity.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing GPU and production ML architectures","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"What does any of this have to do with Production ML architectures? There may be a similar evolutionary pattern at play. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To understand some similarities with GPU and ML architectures, let’s first start by looking at the evolution of Uber’s production ML architecture, Michelangelo. Note that this example should also apply very generally to other production ML architectures. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2i3tJAwmYCimATeyB9yLi3","type":"Asset","createdAt":"2021-09-14T22:54:46.537Z","updatedAt":"2021-09-14T22:54:46.537Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"1stGenerationML","description":"Fixed pipeline production ML architecture (image from [2017 Uber blog post](https://eng.uber.com/michelangelo-machine-learning-platform/))","file":{"url":"//images.ctfassets.net/xjan103pcp94/2i3tJAwmYCimATeyB9yLi3/437c63c4ad9ea031d8817f61f39e38a4/1stGenerationML.png","details":{"size":161112,"image":{"width":1183,"height":670}},"fileName":"1stGenerationML.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The diagram above is of a 1st generation fixed pipeline ML production architecture from an ","nodeType":"text"},{"data":{"uri":"https://eng.uber.com/michelangelo-machine-learning-platform/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Uber blog post in 2017","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".  This pipeline has data preparation, data in feature stores (get data), batch training job (train models), repository for the models (eval models), and online as well as offline inference (deploy, predict, and monitor).  If you compare the 1st generation of GPU and ML architectures, they might seem surprisingly familiar. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5oqNtEneu0Y7yJxtYLa8HJ","type":"Asset","createdAt":"2021-09-14T22:55:48.624Z","updatedAt":"2021-09-14T22:55:48.624Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ML_GPU_comparison","description":"The 1st generation of ML and GPU architectures both seem to have fixed function pipelines.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5oqNtEneu0Y7yJxtYLa8HJ/6d8a50bb0c9f6833e120d54d16b49663/ML_GPU_comparison.png","details":{"size":59751,"image":{"width":1502,"height":285}},"fileName":"ML_GPU_comparison.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A ","nodeType":"text"},{"data":{"uri":"https://eng.uber.com/michelangelo-machine-learning-model-representation/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"2019 blog post from Uber","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" discussed the problems with the 1st generation architecture. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"“Michelangelo was initially launched with a monolithic architecture that managed tightly-coupled workflows … made adding support for new Spark transformers difficult and precluded serving of models trained outside of Michelangelo.”","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Essentially, the monolithic architecture limited what they could do and made it difficult to serve models. This motivated the second generation which standardized interfaces and separated out things into components. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UDHcztplSMhsUmnXUBG2G","type":"Asset","createdAt":"2021-09-14T22:58:03.974Z","updatedAt":"2021-09-14T22:58:03.974Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2ndGenerationML","description":"2nd generation Uber Michelangelo architecture (image from [2019 Uber blog post](https://eng.uber.com/michelangelo-machine-learning-model-representation/))","file":{"url":"//images.ctfassets.net/xjan103pcp94/5UDHcztplSMhsUmnXUBG2G/53a36ec9d3d927046134d053963a0525/2ndGenerationMLArchitectures.png","details":{"size":165748,"image":{"width":1500,"height":493}},"fileName":"2ndGenerationMLArchitectures.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The 2nd generation Uber Michelangelo architecture made it easier to replace any of the standard tools that we like to code in (scikit-learn, TensorFlow, Spark) with each other. This is similar to the 2nd generation of GPU architectures which have programmable sections within the pipeline. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray as a 3rd Generation Production ML Architecture","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"If mostly where we are right now is the second generation, let’s ask ourselves what the third generation would look like? We suggest -- using the history of GPU programming architectures as a guide --  that it would need the following qualities: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"It needs to be able to do everything the 1st and 2nd generation can do (they’re just libraries)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The interface for programming the system is just a normal programming language which does not force developers to think about the problem in terms of the underlying architecture","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The focus shifts to libraries: just as people started to talk about Unity and stopped talking about OpenGL itself; in much the same way, people would stop talking about particular architectures for ML and just use libraries.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"With a focus on libraries, the compute engine could just be a detail. This would open the power of ML to a huge number of users and potentially lead to an explosion of applications. That brings us to Ray. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What is Ray","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray can be considered one of these third generation systems. Ray is: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"A simple and flexible framework for distributed computation","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"A cloud-provider independent compute launcher/","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-0.8.2/autoscaling.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"autoscaler","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"An ecosystem of distributed computation libraries built with #1","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"An easy way to “glue” together distributed libraries in code that looks like normal python while still being performant. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What makes Ray simple is that you make simple annotations (i.e., ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"@ray.remote","nodeType":"text"},{"data":{},"marks":[],"value":" decorator in Python) to make functions and classes distributable. What makes Ray flexible is that it is not a batch model. Ray uses the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/actors.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"actor model","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" where tasks and actors can create new tasks and new actors without much cost so you can have a more dynamic creation of functionality.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The image below shows the ray ecosystem. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3UYKLGIOH9JbeSjhPhLM2I","type":"Asset","createdAt":"2021-09-14T23:03:02.844Z","updatedAt":"2021-09-14T23:12:06.362Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"3rd Generation","description":"Ray ecosystem.","file":{"url":"//images.ctfassets.net/xjan103pcp94/3UYKLGIOH9JbeSjhPhLM2I/18823b1c79e7e996913ec6686219cbbf/Screen_Shot_2021-09-14_at_4.10.12_PM.png","details":{"size":494592,"image":{"width":2162,"height":918}},"fileName":"Screen Shot 2021-09-14 at 4.10.12 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"At the bottom layer, you have your favorite compute service provider (AWS, Azure, GCP, K8s, private cluster). At the middle layer, you have Ray which acts as an interface for these machine learning libraries to run on your compute service provider. The top layer contains scalable libraries for machine learning, model serving, data processing, and more. Some are native ray libraries designed from the beginning to run on top of Ray.  There is also a healthy ecosystem of 3rd party libraries like PyTorch, scikit-learn, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/distributed-xgboost-training-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"XGBoost","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/introducing-distributed-lightgbm-training-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LightGBM","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and more with ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-distributed-library-patterns"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"varying levels of Ray integration","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, you can glue together these distributed libraries in a way that feels like a single, normal python script; while underneath it is doing all this distributed computation. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How does Ray “fit” with the historical pattern?  ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Based on the history of GPU architectures, we believe a key goal of 3rd generation production ML architectures is that they are more programmable. Additionally, by moving the focus to libraries, you don’t have to worry about the details of how the distributed computation is happening. You can instead focus on the algorithm. This is similar to the 3rd generation of GPU architectures in which people started to use engines like Unity or Unreal Engine 4 due to their programmability and flexibility with the GPU becoming more of a detail. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How are Customers using Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let’s now go over how Ray is being used by customers to see if it is embodying the characteristics of a 3rd Generation Production ML Architecture. Some ways Ray is being used include: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Being a simpler way to build 1st gen/2nd pipelines","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"A tool to parallelize high performance ML systems","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"A way to build ML applications that make ML accessible to non-specialists","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"A simpler way to build 1st/2nd gen pipelines","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray provides a simpler way to build 1st/2nd gen pipelines because it: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Allows you to implement existing systems more efficiently (programming language to define pipelines)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"More easily allows for shared components (i.e., feature transformation during training vs real time)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"“Out of the box” support for distributed ML","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a recent blog post, Uber shared its use of Ray to show what the 3rd generation Uber Michelangelo architecture would look like. They implemented this architecture in ","nodeType":"text"},{"data":{"uri":"https://medium.com/ludwig-ai/ludwig-ai-v0-4-introducing-declarative-mlops-with-ray-dask-tabnet-and-mlflow-integrations-6509c3875c2e"},"content":[{"data":{},"marks":[],"value":"Ludwig 0.4","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"10cxgivcmEb9igWmIITKoW","type":"Asset","createdAt":"2021-09-14T23:17:42.233Z","updatedAt":"2021-09-17T17:53:35.463Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"UberLeveragingRay","description":"Building 3rd generation Uber Michelangelo architecture (image from [2021 Uber blog post](https://eng.uber.com/horovod-ray/)), implemented in [Ludwig](http://ludwig.ai)","file":{"url":"//images.ctfassets.net/xjan103pcp94/10cxgivcmEb9igWmIITKoW/05ae74c6eece1dc67aeeb4403608bf7e/UberLeveragingRay.png","details":{"size":226672,"image":{"width":1074,"height":586}},"fileName":"UberLeveragingRay.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"They said by leveraging Ray, it allowed them to treat the entire training pipeline as a single script making it far easier to work with and far more intuitive. What is even more interesting is that they mentioned the importance of the machine learning ecosystem/libraries and having a standardized way for people to write machine learning libraries both within Uber and the industry at large. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4Iw4xetfHitr7whm6tj8cN","type":"Asset","createdAt":"2021-09-14T23:22:06.859Z","updatedAt":"2021-09-14T23:22:06.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"UberQuoteRayText","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4Iw4xetfHitr7whm6tj8cN/12dcab7f2107da1760f44e2b57f3dd4e/UberQuoteRayText.png","details":{"size":65906,"image":{"width":1208,"height":220}},"fileName":"UberQuoteRayText.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A Tool for High Performance ML Systems","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The second use case is high performance ML systems. A company using Ray called ","nodeType":"text"},{"data":{"uri":"https://robovision.ai/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Robovision","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" wanted to use vehicle detection using 5 stacked ML models. While they could make their models fit in one GPU or machine, they had to cut their models. They tried to use a vanilla Python implementation to do it which resulted in 5 frames per second. When they used Ray, they got about a 3x performance improvement on the same hardware (16 frames per second). It is important to keep in mind that this is certainly not the first time anyone has worked with 5 stacked ML models. For example, it is definitely possible to do this without Ray by converting each ML model into a microservice, but it is important to keep in mind the complexity. The image below shows the basic pipeline of what Robovision was trying to do. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3Koa7zS4vpRPZ5KWSl5I6j","type":"Asset","createdAt":"2021-09-14T23:23:04.769Z","updatedAt":"2021-09-14T23:23:04.769Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Robovision","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3Koa7zS4vpRPZ5KWSl5I6j/6ab8f1b5e8dfb098e5210b80a6ee271a/Robovision.png","details":{"size":2056026,"image":{"width":2742,"height":1168}},"fileName":"Robovision.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Robovision processes images and outputs classifications of what the vehicles were and what they were doing. When using Ray, they took each of these things and wrapped them in a Ray actor and that gave them a lot more flexibility and an accessible way to the GPU. Code wise implementing a stacked model like this is surprisingly easy to do.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3AgdPEjvrWtxmlq3vJdycT","type":"Entry","createdAt":"2021-09-14T23:23:47.634Z","updatedAt":"2021-09-14T23:23:47.634Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"body":"@ray.remote\nClass Model: \n   def __init__(self, next_actor): \n      self.next = next_actor\n\n   def runmodel(self, inp):\n      out = process(inp); # different for each stage\n      self.next.call.remote(out)\n\n# input_stream -\u003e object_detector -\u003e \n# object_tracker -\u003e speed_calculator -\u003e result_collector\n\nresult_collector = Model.remote()\nspeed_calculator = Model.remote(next_actor=result_collector)\nobject_tracker = Model.remote(next_actor=speed_calculator)\nobject_detector = Model.remote(next_actor=object_tracker)\n\nfor inp in input_stream: \n   object_detector.runmodel.remote(inp)  \n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"At the top of the code, there is an annotation at the top to describe the class called ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Model","nodeType":"text"},{"data":{},"marks":[],"value":" and then you have subclasses that you change the computation for (e.g., ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"speed_calculator","nodeType":"text"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"object_tracker","nodeType":"text"},{"data":{},"marks":[],"value":"). Basically, what you do is for each case is specify who is next in the chain and then you feed the input to the first one and they land at the thing that is actually capturing the data (","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"result_collector","nodeType":"text"},{"data":{},"marks":[],"value":"). The code took a relatively complex stacked model, wrote it in this way, and now it can run on your local machine. By changing the cluster that it runs on, it can easily be run in a distributed manner. Instead of trying to run 5 stacked models on one machine, Ray makes it so you can run 5 stacked models on 5 machines each with their own GPU. Ray takes care of all the coordination and setting up those 5 machines. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Build apps that make ML accessible to non-specialists","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"A third use case is around making machine learning accessible. Say you are looking at images all over the world and you want to find out where piles of garbage are coming into the ocean. ","nodeType":"text"},{"data":{"uri":"https://descarteslabs.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Descartes Labs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" makes this possible by providing an easy to use geospatial data analysis platform with an interface that enables people to analyze petabytes of data at their user’s behest with just a little bit of code. Ray enables this by taking care of the distributed computing details.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"zTVnAfD0itZvJEGd6eg8p","type":"Asset","createdAt":"2021-09-14T23:38:23.262Z","updatedAt":"2021-09-14T23:38:23.262Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"DescartesLabs","description":"If you want to learn more about how Descartes Labs uses Ray, there is an excellent talk on the subject [here](https://www.youtube.com/watch?v=kZCmqnBrUp4) (image courtesy Descartes Labs).","file":{"url":"//images.ctfassets.net/xjan103pcp94/zTVnAfD0itZvJEGd6eg8p/f1eef21bb56ba0dfbed894780dd2b525/DescartesLabsHighResolution.png","details":{"size":4495898,"image":{"width":2860,"height":1554}},"fileName":"DescartesLabsHighResolution.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To undertake ML projects that don’t fit the ML Pipeline","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The final use case are many important and practical ML projects that don’t fit the standard ML pipeline. Some examples of these types of projects include: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Reinforcement Learning: Reinforcement learning mixes the training and testing stages deliberately. We are unaware of any second generation ML system that supports reinforcement learning. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Online Learning: You may want to update your values and behavior online. This means that you have a feedback loop where you are updating your model often on a per minute basis.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Active/Semi-supervised learning.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"For example, ","nodeType":"text"},{"data":{"uri":"https://www.quantumblack.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"QuantumBlack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which is part of McKinsey, used Ray to help build the algorithms that helped that team win the ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/America%27s_Cup"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"America’s Cup ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"- a premiere sailing competition. To learn more about this amazing achievement and how Ray’s ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RLlib","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" library helped enable this, check out the rest of the Tecton talk ","nodeType":"text"},{"data":{"uri":"https://youtu.be/hzW0AKKqew4?t=1564"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". If you would like to learn how other companies are using Ray’s ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RLlib","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" library for reinforcement learning, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"check out this blog","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" highlighting some impressive use cases. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"This post demonstrated how the history of GPU rendering architectures might be giving us hints to where production ML architectures are going. What we saw was the evolution of production ML architectures. This included: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"first generation","nodeType":"text"},{"data":{},"marks":[],"value":" “fixed function” pipelines.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"second generation","nodeType":"text"},{"data":{},"marks":[],"value":" which involves programmability within the pipeline","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"third generation ","nodeType":"text"},{"data":{},"marks":[],"value":"which involves full programmability","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray can be considered an example of a third generation programmable, flexible production ML architecture. It has already led to new and interesting applications like: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Simplifying existing ML architectures ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Parallelizing high performance ML systems","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Making ML accessible to non-specialists","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Highly scalable algorithms for deep reinforcement learning. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you’re interested in learning more about Ray, you can check out the ","nodeType":"text"},{"data":{"uri":"https://ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", join us on ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discourse","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and ","nodeType":"text"},{"data":{"uri":"http://tinyurl.com/ray-white-paper"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"check out the whitepaper!","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" If you're interested in working with us to make it easier to leverage Ray, we're ","nodeType":"text"},{"data":{"uri":"https://jobs.lever.co/anyscale"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"hiring","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"!","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4SA9rDy9K68mQvwo9FwWH7","type":"Asset","createdAt":"2021-09-14T23:42:34.073Z","updatedAt":"2021-09-14T23:42:34.073Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Uber Leveraging Ray","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4SA9rDy9K68mQvwo9FwWH7/ffb157eaf6488353254319049da24238/UberLeveragingRay.png","details":{"size":226672,"image":{"width":1074,"height":586}},"fileName":"UberLeveragingRay.png","contentType":"image/png"}}},"recommendations":[],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true}},"url":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures","ctaText":"Read more","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mI1ydQdbxC8VN9jq5SE9O","type":"Asset","createdAt":"2022-03-24T22:19:37.854Z","updatedAt":"2022-03-24T22:46:25.932Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-recommended-content-rl-robot-bubble-dark","file":{"url":"//images.ctfassets.net/xjan103pcp94/2mI1ydQdbxC8VN9jq5SE9O/b7509b9f4f23ddf47620aa465a919914/blog-recommended-content-rl-robot-bubble-dark.jpg","details":{"size":40730,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-rl-robot-bubble-dark.jpg","contentType":"image/jpeg"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"SFpqDfxhd1Fkl9YvB2WUM","type":"Entry","createdAt":"2022-03-29T00:09:03.580Z","updatedAt":"2022-03-29T00:09:03.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Deep Dive: Data ingest in a third-generation ML architecture","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5945GdUwJCPlj6wHOPlw7Y","type":"Entry","createdAt":"2021-11-30T16:50:41.093Z","updatedAt":"2022-06-22T16:09:22.594Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Deep Dive: Data Ingest in a Third Generation ML Architecture","slug":"deep-dive-data-ingest-in-a-third-generation-ml-architecture","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12vKMQ3j3q45idk83ZoPYA","type":"Entry","createdAt":"2021-10-01T20:23:37.378Z","updatedAt":"2021-10-01T20:23:37.378Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Chen Shen","slug":"chen-shen"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48lK0gPUqCqrmxWiQi8ve8","type":"Entry","createdAt":"2021-02-16T07:25:36.331Z","updatedAt":"2021-02-16T07:25:36.331Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Clark Zinzow","slug":"clark-zinzow","link":"https://www.linkedin.com/in/clarkzinzow/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2021-11-30","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Distributed libraries allow improved performance by exploiting the full bandwidth of distributed memory, and giving greater programmability. But how does that actually work? What does the code look like?\n\nIn this post, we’ll be looking at a concrete example with code samples: ML ingest with Ray Datasets and Ray Train.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"This is part 3 of our series on ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"third generation ML architectures","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". In the previous post, we talked about how distributed libraries allow improved ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"performance","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" by exploiting the full bandwidth of distributed memory, and giving greater programmability. But how does that ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"actually work","nodeType":"text"},{"data":{},"marks":[],"value":"? What does the code look like?","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this post, we’ll be looking at a concrete example with code samples: ML ingest with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/user_guide.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"We show how these distributed libraries can be woven together with just a few lines of Python--- a key capability not possible in 2nd gen architectures.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"We examine how Datasets and Train use the interoperable primitives of Ray tasks, actors, and objects to enable this composable architecture.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Runnable scripts are available that can be adapted for use on your own Ray cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Small Data Training","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To set the stage, let's consider ML training in the small-data setting. These kinds of pipelines are quite simple since all data fits in memory, and the overhead of shuffling is minimal. You can express it as just a few lines of pseudocode:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3ZZZM2MDEx3oRdMKvIblVb","type":"Entry","createdAt":"2021-11-29T23:28:30.694Z","updatedAt":"2021-11-29T23:28:50.762Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"small data training","body":"data = load_data()\npreprocess(data)\nfor each epoch:\n    random_shuffle(data)\n    train_one_epoch(data)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's review the steps above:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Loading","nodeType":"text"},{"data":{},"marks":[],"value":": Small data is typically read from files on local disk into memory. It may be streamed from files in some cases.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Preprocessing","nodeType":"text"},{"data":{},"marks":[],"value":": Apply simple transformations (i.e., feature engineering).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Shuffling","nodeType":"text"},{"data":{},"marks":[],"value":": Randomly permute the order of items in the dataset. Shuffling randomly for each epoch is ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1709.10432"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"important","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for stochastic gradient descent.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Training","nodeType":"text"},{"data":{},"marks":[],"value":": Fit the model over the data (e.g., using a framework like ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://horovod.readthedocs.io/en/stable/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Horovod","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":").","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Big Data Training Challenges","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Training models over big data adds additional needs around (1) distributed preprocessing, (2) distributed shuffling to improve convergence rates, and (3) pipelined execution with ML training:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed preprocessing","nodeType":"text"},{"data":{},"marks":[],"value":": The data ingestion requirements of large-scale training can be substantial, motivating specialized systems such as ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/2108.09373"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"DPP","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" from Facebook and ","nodeType":"text"},{"data":{"uri":"https://eng.uber.com/petastorm/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Petastorm","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" from Uber. The systems allow for preprocessing to be off-loaded to separate nodes in the cluster distinct from the GPU machines. Some portion of preprocessing can be done offline, but it is desirable for data to be \"minimally preprocessed\" for flexibility.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed shuffling","nodeType":"text"},{"data":{},"marks":[],"value":": It is important for the dataset to be shuffled (randomly re-ordered) for each epoch of training. This can significantly ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1709.10432"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"improve the convergence of SGD","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", but is challenging in the ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/MapReduce"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"distributed setting","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". While ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"global shuffling","nodeType":"text"},{"data":{},"marks":[],"value":" is optimal, typically solutions like TensorFlow/Pytorch data loaders and Petastorm only perform local shuffling due to the engineering complexity of stitching together large-scale data shuffles with ML training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Pipelining data processing and training","nodeType":"text"},{"data":{},"marks":[],"value":": Due to limited cluster memory sizes and the need for random per-epoch shuffles, we see that preprocessing and shuffle computations may need to be interleaved with training. This is only possible today in specialized systems like DPP (e.g., you cannot trivially connect Spark's distributed shuffle with Horovod, since they are separate distributed systems).\nIn other words, our simple pipeline became hard since components necessarily become distributed and pipelined for performance:","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In other words, our simple pipeline became hard since components necessarily become distributed and pipelined for performance:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"10WGpzv003QrG6Mz73dzUo","type":"Entry","createdAt":"2021-11-29T23:32:46.395Z","updatedAt":"2021-11-29T23:32:46.395Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"data = load_data()         # larger than cluster memory :(","body":"data = load_data()         # larger than cluster memory :(\npreprocess(data)           # distributed transforms :(\nfor each epoch:\n    random_shuffle(data)   # distributed shuffle :(\n    train_one_epoch(data)  # pipelined with above distributed steps :(","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Second Generation Approach","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's briefly consider how we could compose existing distributed systems to solve this distributed ingest problem. We need to set up a Spark cluster for data processing, a Horovod cluster for training, a coordinator service for control plane operations, and external storage for data plane communication.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6jWNBb88EkWu12CCvGYDst","type":"Asset","createdAt":"2021-11-30T00:22:56.327Z","updatedAt":"2021-11-30T21:06:31.778Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"2nd Generation - Data Ingest Problem","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6jWNBb88EkWu12CCvGYDst/88488a650b7eb5eb6d88bda7e37ca7c1/dataIngestProblem.png","details":{"size":90348,"image":{"width":1406,"height":666}},"fileName":"dataIngestProblem.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The training pipeline would work in the following steps. First, the coordinator service would (1) submit a shuffle job to the Spark cluster, which (2) reads and writes data out to external storage. Next, the Horovod data reader (e.g., Petastorm) would (3) fetch the written dataset location from the coordinator and (4) read the shuffle data for training. These steps would repeat for each epoch of training, and can run concurrently to optimize execution latencies.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The disadvantages of the 2nd generation approach are:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Lack of programmability","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": need to setup and manage 3+ separate distributed systems. It's also hard to orchestrate with workflow systems due to the interleaving of shuffle with training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Performance overhead","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": intermediate data written to external storage since it needs to cross between distributed systems.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Third Generation Approach","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In contrast, with a 3rd gen architecture we can compose the entire data ingest pipeline using distributed ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"libraries","nodeType":"text"},{"data":{},"marks":[],"value":". In the snippet below ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/examples/train_linear_dataset_example.html#train-linear-dataset-example"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"(see here for a full runnable example)","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we compose a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset-pipeline.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Dataset Pipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with a distributed ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/user_guide.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train Job","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"78pALMO2tFH7mvFI7WL9wK","type":"Entry","createdAt":"2021-11-30T00:24:17.532Z","updatedAt":"2021-11-30T00:24:17.532Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Third Generation Approach","body":"from ray.train import Trainer, get_dataset_shard\n\n# Distributed Preprocessing and Shuffle\npipe = ray.data.read_parquet(path).window(size).repeat()\npipe = pipe.map_batches(preprocess)\npipe = pipe.random_shuffle_each_window()\n\n# Ray Train Function\ndef train_func():\n    model = NeuralNetworkModel(...)\n    model = train.torch.prepare_model(model)\n    for epoch_data in get_dataset_shard().iter_epochs():\n        model.fit(epoch_data.to_torch(...))\n\n# Compose and Run\ntrainer = Trainer(num_workers=3, backend=\"torch\", use_gpu=True)\nresult = trainer.run(train_func, dataset=dataset_pipeline)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The above snippet, while simplified, is able to express the aforementioned ML ingest and training pipeline with just a few lines of Python code--- without any need to wrangle distributed systems. Under the hood, the Dataset and Train libraries leverage Ray Tasks and Actors respectively to execute distributed data preprocessing and ML training. We are able to compose them by just passing a reference to the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"dataset_pipeline","nodeType":"text"},{"data":{},"marks":[],"value":" object to Train:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"n0VLbvBnx8WyFKvQJDDyC","type":"Asset","createdAt":"2021-11-30T00:27:03.418Z","updatedAt":"2021-11-30T21:09:59.864Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"3rdGenTasks andActors","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/n0VLbvBnx8WyFKvQJDDyC/b40de05028d9740c1b3f51af4e20eaeb/3rdGenTasks_andActors.png","details":{"size":148336,"image":{"width":1366,"height":922}},"fileName":"3rdGenTasks andActors.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The above figure illustrates the tasks and actors created by the above code snippet.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Compared to the 2nd gen approach, the 3rd gen approach achieves:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Lower operational and development overheads","nodeType":"text"},{"data":{},"marks":[],"value":": developers can compose and ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"customize","nodeType":"text"},{"data":{},"marks":[],"value":" the entire distributed training system in a single script thanks to the programmability of a 3rd gen architecture.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Better performance","nodeType":"text"},{"data":{},"marks":[],"value":": as we'll see in the case studies, this approach reduces overheads by allowing data to be passed in-memory.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Code Walkthrough","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"So how does it work? Let's walk through the above example starting with the system requirements.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Requirements for the Example","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"For performance, we want data to be passed in-memory between preprocessing, shuffle, and training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"We should support ingestion of a dataset that is larger than memory. In the example below, we'll assume a 2TB dataset, and a cluster with 1TB of memory.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Support for heterogeneous clusters (e.g., a cluster with GPU training nodes and CPU preprocessing nodes).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Part 1: Windowed data loading","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's look at the first part of the code above, which creates a data loading pipeline.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xvSklEPrmcWNId9RfBs5j","type":"Entry","createdAt":"2021-11-30T00:28:20.053Z","updatedAt":"2021-11-30T00:28:20.053Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"pipe = ray.data.read_parquet(path).window(size).repeat()","body":"pipe = ray.data.read_parquet(path).window(size).repeat()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This uses the Ray Dataset library to create a DatasetPipeline reading our parquet data from disk. Since the dataset (2TB) is larger than our cluster memory (1TB), we use the .window() function to process windows of size=200GB at a time, leaving extra memory headroom for execution. Since we want to loop over the dataset indefinitely, we use the .repeat() operator after that.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Part 2: Preprocessing and shuffle pipeline","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The second part of the pipeline is applying the distributed transform and shuffling operations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"K22lS3qMNQAVVakEyi5mO","type":"Entry","createdAt":"2021-11-30T00:29:09.699Z","updatedAt":"2021-11-30T00:29:09.699Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Preprocessing and shuffle pipeline","body":"pipe = pipe.map_batches(preprocess)\npipe = pipe.random_shuffle()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is telling Ray to transform records in the pipeline with a given ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"preprocess","nodeType":"text"},{"data":{},"marks":[],"value":" function, and then shuffling the entire window randomly (e.g., 200GB at a time), to avoid going out of core. So far, nothing has been executed beyond reading the file metadata--- we're building up a logical pipeline.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Part 3: Ray Train Setup","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next we define the code that is run on each GPU worker and implements distributed training. Each worker can read a particular split of the pipeline we defined by calling ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"get_dataset_shard","nodeType":"text"},{"data":{},"marks":[],"value":". It sets up a model using ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"train.torch.prepare_model","nodeType":"text"},{"data":{},"marks":[],"value":" to participate in distributed training. Then, it trains over the data in each epoch (repeat) of the dataset.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4OVxuAmQZAAr2zL5vrYVnO","type":"Entry","createdAt":"2021-11-30T00:29:55.709Z","updatedAt":"2021-11-30T00:29:55.709Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"train_func","body":"def train_func():\n    model = NeuralNetworkModel(...)\n    model = train.torch.prepare_model(model)\n    for epoch_data in get_dataset_shard().iter_epochs():\n        model.fit(epoch_data.to_torch(...))","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To create the training actors, we create a ray.train.Trainer that requires 3 GPU workers:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"BIl5itQv7ZPDqNfgrNZSP","type":"Entry","createdAt":"2021-11-30T00:30:22.961Z","updatedAt":"2021-11-30T00:30:22.961Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"trainer = Trainer(num_workers=3, backend=\"torch\", use_gpu=True)","body":"trainer = Trainer(num_workers=3, backend=\"torch\", use_gpu=True)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"At this point, our pipeline is fully defined, and our training actors have been created and assigned GPUs in the cluster, we just need to run it.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Part 3: Running everything","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"This line of code triggers the execution of the entire pipeline:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"j221k8qfddcLQDzWQVpcA","type":"Entry","createdAt":"2021-11-30T00:31:25.195Z","updatedAt":"2021-11-30T00:31:25.195Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Part 3: Running everything","body":"result = trainer.run(train_func, dataset=dataset_pipeline)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"So what's happening in the cluster?","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train sends actor method calls to each actor to run its given training function.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Each actor pulls data from the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"DatasetPipeline","nodeType":"text"},{"data":{},"marks":[],"value":" shard given to it (each pipeline shard contains a handle to a coordinator actor created by Datasets for this DatasetPipeline instance).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"This triggers actor calls to the coordinator actor.\na. The coordinator schedules execution of the next window of the pipeline, e.g., Ray tasks that use CPU nodes in the cluster to:\n      i. load data for the window (200GB)\n      ii. preprocess the data\n      iii. randomly shuffle the data\n      iv. split up the data and assign splits to trainer actors\nb. The coordinator returns to the trainer actors object references to their assigned Dataset split.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The trainer actors ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.get()","nodeType":"text"},{"data":{},"marks":[],"value":" data blocks from their Dataset split and generate mini-batches to pass to the underlying learning library (i.e., PyTorch).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can visualize the overall dataflow in the following timeline diagram. Once data is loaded, shuffle and execution proceed in a fully pipelined way, leveraging tasks running on CPU nodes to implement shuffling, and actors running on GPUs for training:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"51nJaf8KaCjBBQOzS4W6mU","type":"Asset","createdAt":"2021-11-30T05:53:49.763Z","updatedAt":"2021-11-30T05:53:49.763Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"dataflow","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/51nJaf8KaCjBBQOzS4W6mU/d490293c792df1d7803eb30f13682b1d/dataflow.png","details":{"size":90897,"image":{"width":1210,"height":432}},"fileName":"dataflow.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Try it out yourself with these examples in Ray 1.8:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/examples/train_linear_dataset_example.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://docs.ray.io/en/master/train/examples/train_linear_dataset_example.html","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/examples/big_data_ingestion.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://docs.ray.io/en/master/data/examples/big_data_ingestion.html","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Benchmarks","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a previous blog post, we discussed the ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"performance advantages","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" of passing data in-memory and with pipelining and showed improvements in an ablation study. Since then, Datasets has been used by several open source users to implement large-scale shuffled ML ingest. We present two case studies from our ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/presentation/d/1zANPlmrxQkjPU62I-p92oFO3rJrmjVhs73hL4YbM4C4"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyData Dataset talk","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" showing significant performance improvements:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Case Study 1: high-tech ML platform startup","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Dask-on-Ray → Datasets → Horovod","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Dask-on-Ray and Datasets was ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"8x faster","nodeType":"text"},{"data":{},"marks":[],"value":" than Pandas + S3+ Petastorm, even on a single machine.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Benchmark:","nodeType":"text"},{"data":{},"marks":[],"value":" ","nodeType":"text"},{"data":{"uri":"http://ludwig.ai/"},"content":[{"data":{},"marks":[],"value":"Ludwig AI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" model, NYC Taxi dataset (5 GB subset), single g4dn.4xlarge instance","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UdwUzwL7HNFGOSUgHydvq","type":"Asset","createdAt":"2021-11-30T07:55:45.520Z","updatedAt":"2021-11-30T21:14:24.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Shuffled Data Benchmark","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2UdwUzwL7HNFGOSUgHydvq/3bdea36e01ae221aa6d802001c57a528/shuffledDataBenchmark.png","details":{"size":79641,"image":{"width":1108,"height":656}},"fileName":"shuffledDataBenchmark.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Case Study 2: large transport tech company","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"S3 → Datasets → Horovod","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Datasets from S3 was ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"4x faster","nodeType":"text"},{"data":{},"marks":[],"value":" than Petastorm from S3","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Benchmark:","nodeType":"text"},{"data":{},"marks":[],"value":" 1.5 TB synthetic tabular dataset, 16 nodes (40 vCPUs, 180 GB RAM), 2 shuffle windows ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Gl6shcoyCtt7P1xHuYWO0","type":"Asset","createdAt":"2021-11-30T07:57:18.136Z","updatedAt":"2021-11-30T07:57:18.136Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Petastorm Datasets","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6Gl6shcoyCtt7P1xHuYWO0/63479b61e30fb33db13df73f72e42da4/petastormDatasets.png","details":{"size":28808,"image":{"width":626,"height":290}},"fileName":"petastormDatasets.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We believe 3rd gen ML architectures will help engineers develop and standardize infrastructure for large-scale ML apps. This blog demonstrated that with just a single Python script, we can connect distributed data preprocessing with training in a highly performant way.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Moreover, in true 3rd-gen fashion, we were able to do the above without building a specialized system. We used Ray to interleave execution of two independent distributed libraries--- a key capability not possible in 2nd gen architectures. This composability is possible since both libraries are built on the common and interoperable primitives of Ray tasks, actors, and objects.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While we're just getting started with ML ingest-- look for new examples and performance enhancements as Ray Datasets graduates from beta in the next few months--- this is just one aspect of programmable distributed compute with Ray. Check out other use cases in Tuning, Training, Serving, and more here: ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://www.ray.io/","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7hIMsn7WDpX9zGqRwfZnvu","type":"Asset","createdAt":"2021-11-30T21:13:58.335Z","updatedAt":"2021-11-30T21:13:58.335Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"3rdGenTasks andActors","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7hIMsn7WDpX9zGqRwfZnvu/0f04538b77d8cc926e7021a3be138720/3rdGenTasks_andActors.png","details":{"size":148336,"image":{"width":1366,"height":922}},"fileName":"3rdGenTasks andActors.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}}],"hideIntro":true,"recommendations":[]}},"url":"https://www.anyscale.com/blog/deep-dive-data-ingest-in-a-third-generation-ml-architecture","ctaText":"Read more","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"qyo3m0dc8CvMNCfhxDy5e","type":"Asset","createdAt":"2022-03-24T21:30:36.648Z","updatedAt":"2022-03-24T21:30:36.648Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-code-dark","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/qyo3m0dc8CvMNCfhxDy5e/409ed6f79aea5822b10aaa365318a005/blog-recommended-content-code-dark.jpg","details":{"size":39745,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-code-dark.jpg","contentType":"image/jpeg"}}},"recommendations":null}},{"fields":{"content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7xkHJWpQ3mRxrgAQw3oYFk","type":"Entry","createdAt":"2023-06-15T13:47:45.558Z","updatedAt":"2023-06-16T13:24:48.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Introducing the Anyscale Databricks Connector","seoTitle":"Integrating Anyscale Ray with Databricks","slug":"introducing-the-anyscale-databricks-connector","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3e6JquwKuIz6QvCWO0BAz","type":"Entry","createdAt":"2023-06-15T13:16:27.453Z","updatedAt":"2023-06-15T13:16:27.453Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Greene","slug":"eric-greene","link":"https://www.linkedin.com/in/greene/"}}],"publishedDate":"2023-06-15","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Introducing the Anyscale Databricks connector. Simple Data Transfer and Efficient AI Workflows with Anyscale and Databricks.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The Anyscale Databricks Connector is a new Ray Datasets capability that facilitates easy data transfer between Databricks clusters and Anyscale hosted Ray clusters. The connector makes it easy for Data Scientists using Anyscale to leverage their Databricks data lake during machine learning and discovery.  It also enables a simpler way for machine learning engineers to create end-to-end workloads by allowing the entire ML pipeline to be executed within a single Python script. By taking advantage of the highly-scalable nature of Ray and Ray Datasets, machine learning workloads such as training, tuning and batch serving jobs can be executed more quickly and at lower cost, all while taking advantage of the latest advancements in AI through other Ray integrations with Hugging Face, XGBoost, LightGBM and many AI frameworks and libraries.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Key Benefits","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simple access to data","nodeType":"text"},{"data":{},"marks":[],"value":" - Access Databricks data through SQL queries within Anyscale Workspaces Visual Code or JupyterHub environments, empowering data science discovery and development.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Improved data security and governance","nodeType":"text"},{"data":{},"marks":[],"value":" - Ensure data security and governance by directly copying data from Databricks data lakes to Ray clusters, eliminating the need for intermediate steps and maintaining separate controls over sensitive data. All data is encrypted in transit and at rest.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Highly-scalable data exchange","nodeType":"text"},{"data":{},"marks":[],"value":" - Take advantage of the parallel read and write capabilities of Ray Datasets, enabling the exchange of terabytes of data in minutes. This capability significantly speeds up data-intensive operations and enhances scalability while reducing job run times and overall costs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simplified workload development","nodeType":"text"},{"data":{},"marks":[],"value":" - Simplify machine learning workflows by consolidating all the necessary logic into a single script. This script can query features, train and tune models, and score and materialize results back into the data lake, streamlining the entire process.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Unlock the latest AI Innovations","nodeType":"text"},{"data":{},"marks":[],"value":" - Leverage the power of Databricks for querying and joining data, while benefiting from the scalability and simplicity of Ray AIR for machine learning and AI development. Ray AIR provides integration to the latest AI innovations such as pretrained Hugging Face language models.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Simple, Secure and Scalable Data Access","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing the Anyscale Databricks connector, large datasets can be queried from a Databricks SQL warehouse, and quickly transferred into a Ray Dataset distributed across the Ray cluster. The Databricks connector reads query results in chunks of data, in parallel across the Ray cluster. The size of the dataset and the speed at which it can be transferred scales based on the size of your Ray cluster and the number of simultaneous requests the underlying Databricks SQL warehouse supports.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6taTlZK9ffeivO5RCUJiQ4","type":"Asset","createdAt":"2023-06-15T13:27:43.116Z","updatedAt":"2023-06-15T13:27:43.116Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"1 - Anyscale Databricks Architecture","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6taTlZK9ffeivO5RCUJiQ4/c5fc0bfef768f0fb6e8785bf71ded1e2/1_-_Anyscale_Databricks_Arch.jpg","details":{"size":95989,"image":{"width":761,"height":478}},"fileName":"1 - Anyscale Databricks Arch.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Anyscale Databricks connectors, parallel reads and writes and overall speed will scale with the size of the Ray cluster. The benchmarks for reading and writing demonstrate how the cluster scales out to support larger data sets.  ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2KimYEe845rk6fb2oYaCmz","type":"Asset","createdAt":"2023-06-15T13:35:36.597Z","updatedAt":"2023-06-15T13:35:36.597Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2 - Anyscale Databricks Table Example","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2KimYEe845rk6fb2oYaCmz/8302017d6535b5d676d3dfb50dc12cbd/2_-_Anyscale_Databricks_Table_Example.jpg","details":{"size":218942,"image":{"width":2178,"height":1002}},"fileName":"2 - Anyscale Databricks Table Example.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Simplified ML Development","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing the Databricks connector from within an Anyscale Workspaces, Data Scientists and Machine Learning Engineers have a unified experience while developing workloads. The data query, feature engineering, training, tuning and inference can all be executed within a single Python script that scales across a Ray cluster. Ray integrates with the latest AI and Machine learning libraries, enabling the most advanced ML workloads that work with 3rd party libs  like Hugging Face, XGBoost, LightGBM, and most PyTorch and TensorFlow model architectures.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7c7IX8TpJKbaXdnLIwoD88","type":"Asset","createdAt":"2023-06-15T13:39:22.298Z","updatedAt":"2023-06-15T13:39:22.298Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"3 - Anyscale Databricks UI","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7c7IX8TpJKbaXdnLIwoD88/f17fb8c8884ce197c9ad35a8ce3f6b4e/3_-_Anyscale_Databricks_Product_UI.jpg","details":{"size":242920,"image":{"width":2226,"height":1230}},"fileName":"3 - Anyscale Databricks Product UI.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2muX02EO70X2MXN8wBRyUJ","type":"Asset","createdAt":"2023-06-15T13:41:08.580Z","updatedAt":"2023-06-15T13:41:08.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"4 - Anyscale Databricks Product UI 2","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2muX02EO70X2MXN8wBRyUJ/cc5301c192c66e632cb3141a9e5bde55/4_-_Anyscale_Databricks_Product_UI_2.jpg","details":{"size":383181,"image":{"width":1860,"height":1282}},"fileName":"4 - Anyscale Databricks Product UI 2.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlock new use cases with AI and ML ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nAnyscale and Ray integrate with most open source AI and ML libraries, enabling the latest innovation in AI to be applied to Databricks data. Ray makes working with Hugging face, XGBoost, LightGBM, TensorFlow and PyTorch and SciKit Learn a unified experience, with the added benefits of scaling with distributed training, tuning and serving.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical ML workload to train a LightGBM can be implemented in 20 lines of code.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6rUeSQ88fvbTd68jkhrii0","type":"Asset","createdAt":"2023-06-15T13:42:58.984Z","updatedAt":"2023-06-15T13:42:58.984Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"5 - Anyscale Databricks deployment","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6rUeSQ88fvbTd68jkhrii0/43de8dc64eaf8792ad73ac6faf8b4551/5_-_Anyscale_Databricks_deployment.jpg","details":{"size":272363,"image":{"width":1470,"height":1402}},"fileName":"5 - Anyscale Databricks deployment.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Want to see for yourself? ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/signup"},"content":[{"data":{},"marks":[],"value":"Request a Trial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" today!\n","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71IpYQvsxplh7okIL7RJVx","type":"Asset","createdAt":"2023-06-15T13:50:54.646Z","updatedAt":"2023-06-16T13:19:09.178Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"locale":"en-US"},"fields":{"title":"0 -Anyscale Databricks headline image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/71IpYQvsxplh7okIL7RJVx/b193e65fbc2d6500e3ccb2162d323080/0_-_Updated_Databricks_Anyscale_Image.png","details":{"size":90723,"image":{"width":1500,"height":1000}},"fileName":"0 - Updated Databricks Anyscale Image.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false,"recommendations":[]}}}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}}],"hideIntro":true,"bannerText":null,"bannerLink":null},"__N_SSG":true},"page":"/blog/[id]","query":{"id":"why-third-generation-ml-platforms-are-more-performant"},"buildId":"-oT8_WeW9m_lAxr26cJvY","isFallback":false,"gsp":true,"customServer":true,"scriptLoader":[]}</script><script type="text/javascript" id="hs-script-loader" async="" defer="" src="https://js.hs-scripts.com/20523749.js"></script><script type="text/javascript">
  _linkedin_partner_id = "4210313";
  window._linkedin_data_partner_ids = window._linkedin_data_partner_ids || [];
  window._linkedin_data_partner_ids.push(_linkedin_partner_id);
</script><script type="text/javascript">
  (function(l) { if (!l){window.lintrk = function(a,b){window.lintrk.q.push([a,b])};
  window.lintrk.q=[]} var s = document.getElementsByTagName("script")[0];
  var b = document.createElement("script");
  b.type = "text/javascript";b.async = true;
  b.src = "https://snap.licdn.com/li.lms-analytics/insight.min.js";
  s.parentNode.insertBefore(b, s);})(window.lintrk);
</script><noscript><img height="1" width="1" style="display:none" alt="" src="https://px.ads.linkedin.com/collect/?pid=4210313&amp;fmt=gif"/></noscript></body></html>