<!DOCTYPE html><html lang="en-US"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>An Introduction to Reinforcement Learning with OpenAI Gym, RLlib, and Google Colab | Anyscale</title><meta name="description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta property="og:title" content="An Introduction to Reinforcement Learning with OpenAI Gym, RLlib, and Google Colab | Anyscale"/><meta property="og:description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta property="og:site_name" content="Anyscale"/><meta property="og:image" content="https://images.ctfassets.net/xjan103pcp94/3ies13ZkanQdfdD5onpL6C/88151cd3a715a9dabc4f5adff8c66b30/InteractionCommunicationEnvironment.jpg"/><meta property="fb:app_id" content=""/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta name="twitter:site" content=""/><meta name="twitter:title" content="An Introduction to Reinforcement Learning with OpenAI Gym, RLlib, and Google Colab | Anyscale"/><meta name="twitter:image" content="https://images.ctfassets.net/xjan103pcp94/3ies13ZkanQdfdD5onpL6C/88151cd3a715a9dabc4f5adff8c66b30/InteractionCommunicationEnvironment.jpg"/><meta name="next-head-count" content="14"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><link rel="icon" href="../static/favicon.ico"/><script>
(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-P8H6KQG');
</script><script src="https://tag.clearbitscripts.com/v1/pk_7a96be585f731437b35c7bad25c432b7/tags.js"></script><script src="https://www.googleoptimize.com/optimize.js?id=OPT-NPGB9W3"></script><script>
  !function(f,b,e,v,n,t,s)
  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
  n.queue=[];t=b.createElement(e);t.async=!0;
  t.src=v;s=b.getElementsByTagName(e)[0];
  s.parentNode.insertBefore(t,s)}(window,document,'script',
  'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '168099398554987');
  fbq('track', 'PageView');
</script><link rel="stylesheet" href="../static/font-awesome-4.7.0/css/font-awesome.min.css"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="true"/><script src="https://fast.wistia.com/assets/external/E-v1.js" async=""></script><meta name="ahrefs-site-verification" content="4be3c5d78cdad8823d038256ac06d4336ae7a84632dac0ed57975703fc14fe6b"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="../_next/static/css/dbba207740458c19.css" as="style"/><link rel="stylesheet" href="../_next/static/css/dbba207740458c19.css" data-n-g=""/><link rel="preload" href="../_next/static/css/8a274f3d4edb6eb9.css" as="style"/><link rel="stylesheet" href="../_next/static/css/8a274f3d4edb6eb9.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="../_next/static/chunks/webpack-94adde92c5e1c75b.js" defer=""></script><script src="../_next/static/chunks/framework-95c3ca6de27174dd.js" defer=""></script><script src="../_next/static/chunks/main-774d7304f1973b2a.js" defer=""></script><script src="../_next/static/chunks/pages/_app-fd8cb5a9775605f5.js" defer=""></script><script src="../_next/static/chunks/0f1ac474-1cd8c801d37caef0.js" defer=""></script><script src="../_next/static/chunks/1812-a871a62a8e8aa88c.js" defer=""></script><script src="../_next/static/chunks/9065-e24d5adee6cbba35.js" defer=""></script><script src="../_next/static/chunks/9614-b709f46f6d53dd35.js" defer=""></script><script src="../_next/static/chunks/8423-007cfdc371b27898.js" defer=""></script><script src="../_next/static/chunks/7846-ec0724342ae15ba3.js" defer=""></script><script src="../_next/static/chunks/pages/blog/[id]-4165e8a9d7b23d0c.js" defer=""></script><script src="../_next/static/-oT8_WeW9m_lAxr26cJvY/_buildManifest.js" defer=""></script><script src="../_next/static/-oT8_WeW9m_lAxr26cJvY/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Rubik&display=swap">@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4i1Uw.woff) format('woff')}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFWkUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFU0UzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFVUUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFWUUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFV0UzdYPFkZVO.woff) format('woff');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href="https://fonts.googleapis.com/css2?family=Outfit:wght@400;500;600;700&display=swap">@font-face{font-family:'Outfit';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4TC1C4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4QK1C4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4e6yC4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4deyC4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,400;0,700;1,400;1,700&display=swap">@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTeEw.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINscg.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKtdU.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7MIU.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGqZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuE6ZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuFKZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGKZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGaZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuF6ZJW9XjDg.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINce_fuJGl18QRY.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINccvfuJGl18QRY.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcdffuJGl18QRY.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINceffuJGl18QRY.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcePfuJGl18QRY.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcdvfuJGl18Q.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdzeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdXeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdLeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhd7eFaxOedfTDw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhd_eFaxOedfTDw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdHeFaxOedc.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIxsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIVsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIJsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AI5sdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AI9sdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIFsdP3pBms.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body class=""><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-P8H6KQG" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next" data-reactroot=""><svg xmlns="http://www.w3.org/2000/svg" class="sr-only"><symbol id="svg-arrow-right-gradient"><path d="M1.5 7.5h18" stroke="url(#paint0_linear_3927_132230)" stroke-width="1.5" stroke-linecap="round"></path><path d="m16.6562 3.65625 3.8442 3.84412-3.8442 3.84413" stroke="url(#paint1_linear_3927_132230)" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><defs><linearGradient id="paint0_linear_3927_132230" x1="1.06972" y1="8.58929" x2="12.2319" y2="-1.53819" gradientUnits="userSpaceOnUse"><stop stop-color="#8B20BE"></stop><stop offset="1" stop-color="#3579FF" stop-opacity=".97"></stop></linearGradient><linearGradient id="paint1_linear_3927_132230" x1="12.377" y1="7.75171" x2="16.9307" y2="11.8684" gradientUnits="userSpaceOnUse"><stop stop-color="#8B20BE"></stop><stop offset="1" stop-color="#3579FF" stop-opacity=".97"></stop></linearGradient></defs></symbol></svg><div class="Banner_container__zEmFV"><a href="https://raysummit.anyscale.com/?utm_source=anyscale&amp;utm_medium=website&amp;utm_campaign=ray_summit_2023&amp;utm_content=homepage_ticker" rel="noreferrer nofollow" target="_blank"><span class="Banner_banner-text__wr6Xi"><span class="Banner_main-text__cpJ6H">Are you passionate about Ray, scalable AI and distributed computing? Join us at Ray Summit!</span><span class="Banner_cta-button__0JCtt"> Find out more! <!-- --> <svg width="23" height="14" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1.5 6.75a.75.75 0 0 0 0 1.5v-1.5Zm18 1.5a.75.75 0 0 0 0-1.5v1.5Zm-18 0h18v-1.5h-18v1.5Z" fill="#fff"></path><path d="M17.187 3.126a.75.75 0 0 0-1.061 1.06l1.06-1.06ZM20.5 7.5l.53.53a.75.75 0 0 0 0-1.06l-.53.53Zm-4.374 3.314a.75.75 0 1 0 1.06 1.06l-1.06-1.06Zm0-6.627L19.97 8.03l1.06-1.061-3.843-3.844-1.061 1.06ZM19.97 6.97l-3.844 3.844 1.06 1.06 3.845-3.843L19.97 6.97Z" fill="#fff"></path></svg></span></span></a><button class="Banner_close-button__9ksfY" type="button" aria-label="Close Banner"></button></div><header class="inner-shadow sticky top-0 z-[1000] h-24 transform py-5 transition-transform duration-500 bg-primary translate-y-0"><div class="container mx-auto flex items-center justify-between"><a href="../index.html" class="text-white"><svg width="141" height="32" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><g clip-path="url(#anyscale-logo_svg__a)" fill="currentColor"><path d="M10.653 17.46H3.865C1.697 17.46 0 19.16 0 21.332v6.797c0 2.17 1.697 3.87 3.865 3.87h6.788c2.169 0 3.866-1.7 3.866-3.87V21.33c0-2.171-1.792-3.87-3.866-3.87Zm1.509 10.573c0 .85-.66 1.51-1.509 1.51H3.865a1.49 1.49 0 0 1-1.508-1.51v-6.796c0-.85.66-1.51 1.508-1.51h6.788c.849 0 1.509.66 1.509 1.51v6.796Z"></path><path d="M28.095 11.988h-8.108V3.87C19.892 1.7 18.196 0 16.027 0H3.865C1.697 0 0 1.7 0 3.87v6.608c0 2.171 1.697 3.87 3.865 3.87h13.67v13.687c0 2.171 1.698 3.87 3.866 3.87h6.6c2.168 0 3.865-1.699 3.865-3.87V15.858c0-2.076-1.697-3.775-3.771-3.87Zm-24.23 0a1.49 1.49 0 0 1-1.508-1.51V3.87c0-.85.66-1.51 1.508-1.51h12.162c.849 0 1.509.66 1.509 1.51v8.118H3.866Zm25.738 16.047c0 .85-.66 1.51-1.508 1.51h-6.6a1.49 1.49 0 0 1-1.508-1.51V14.348h8.108c.848 0 1.508.66 1.508 1.51v12.177ZM79.194 17.462l-3.017-8.118h-3.3l4.714 11.327-2.546 5.853h3.112l7.165-17.18H82.21l-3.016 8.118ZM93.335 14.819c-.66-.284-1.414-.472-2.075-.661-.66-.189-1.225-.378-1.79-.66-.378-.19-.566-.473-.566-.85 0-.378.188-.661.472-.85.847-.472 1.79-.378 2.639.094.377.284.566.661.566 1.133h2.828c0-1.133-.566-2.171-1.414-2.832-.943-.755-2.169-1.133-3.3-1.038-.849 0-1.697.094-2.45.472-.66.283-1.227.66-1.604 1.227-.377.472-.565 1.133-.565 1.794 0 .66.188 1.321.565 1.887.377.472.849.85 1.414 1.039.66.283 1.414.472 2.168.66.66.19 1.226.378 1.886.661.377.19.566.472.566.85 0 .377-.189.66-.471.85-.377.283-.85.377-1.32.377-.472 0-1.038-.094-1.415-.472-.377-.283-.565-.66-.66-1.133h-2.922c0 .755.282 1.416.754 1.982.472.567 1.131 1.133 1.791 1.416.849.378 1.698.567 2.546.567.849 0 1.697-.095 2.45-.472.661-.283 1.227-.66 1.604-1.227.377-.567.565-1.133.565-1.794 0-.66-.188-1.321-.66-1.888-.565-.66-1.037-.944-1.602-1.132ZM102.575 11.702c.566 0 1.036.095 1.508.472.377.283.66.755.849 1.227h3.205a5.366 5.366 0 0 0-1.886-3.02 5.348 5.348 0 0 0-3.582-1.039c-1.037 0-2.074.284-3.017.755a5.486 5.486 0 0 0-2.074 2.077 7.427 7.427 0 0 0 0 6.325c.472.85 1.131 1.604 2.074 2.076.943.472 1.98.756 3.017.756 1.32 0 2.546-.378 3.582-1.133a5.366 5.366 0 0 0 1.886-3.02h-3.111c-.283 1.037-1.32 1.698-2.357 1.604-.754 0-1.509-.283-1.98-.944a5.119 5.119 0 0 1 0-5.192 1.914 1.914 0 0 1 1.886-.944ZM118.412 10.1a.405.405 0 0 1-.377.378c-.094 0-.188 0-.188-.095a3.382 3.382 0 0 0-.944-.66c-.659-.378-1.507-.567-2.262-.567-.943 0-1.885.283-2.734.755a4.459 4.459 0 0 0-1.885 2.077c-.472.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.376.85 1.037 1.604 1.885 2.17.849.473 1.697.756 2.64.756.754 0 1.603-.189 2.263-.567.659-.283 1.131-.755 1.602-1.321v1.699h2.923V9.345h-2.923v.755Zm-.377 6.891c-.283.472-.66.944-1.132 1.227-.47.283-1.037.378-1.507.378-.567 0-1.037-.189-1.509-.378-.472-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.094-1.228.472-1.794.848-1.51 2.64-1.982 4.147-1.227h.095c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM55.907 10.1a.406.406 0 0 1-.377.378c-.095 0-.19 0-.19-.095a3.384 3.384 0 0 0-.942-.66c-.66-.378-1.508-.567-2.263-.567-.942 0-1.885.283-2.733.755a4.463 4.463 0 0 0-1.886 2.077c-.471.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.377.85 1.037 1.604 1.886 2.17.848.473 1.697.756 2.64.756.753 0 1.602-.189 2.262-.567.66-.283 1.131-.755 1.603-1.321v1.699h2.922V9.345h-2.922v.755Zm-.472 6.891c-.283.472-.66.944-1.131 1.227-.471.283-1.037.378-1.509.378-.565 0-1.037-.189-1.508-.378-.471-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.095-1.228.472-1.794.848-1.51 2.64-1.982 4.148-1.227h.094c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM126.708 5.477h-2.923v15.575h2.923V5.477ZM140.191 14.914c0-1.038-.189-2.077-.755-3.02-.471-.85-1.131-1.605-2.074-1.983a6.78 6.78 0 0 0-6.033 0 5.485 5.485 0 0 0-2.075 2.077c-.565 1.038-.754 2.077-.754 3.21 0 1.132.189 2.17.754 3.114a5.485 5.485 0 0 0 2.075 2.077c.942.472 1.979.755 3.016.755a6.56 6.56 0 0 0 3.489-1.038c.943-.661 1.602-1.7 1.885-2.738h-3.016c-.377.944-1.321 1.51-2.358 1.416-.659 0-1.414-.283-1.885-.755-.566-.472-.848-1.227-.848-1.982h8.484c.095-.378.095-.755.095-1.133Zm-8.579-.85c.093-.755.377-1.415.942-1.887.566-.472 1.226-.661 1.886-.661.754 0 1.414.189 1.979.66.566.473.849 1.133.849 1.794l-5.656.095ZM67.692 9.159a4.7 4.7 0 0 0-1.98.472c-.377.188-.754.377-1.037.66a.286.286 0 0 1-.377 0c-.094-.094-.094-.094-.094-.188v-.756H61.28v11.705h2.923v-6.418c-.094-.755.188-1.51.754-2.171.471-.472 1.226-.756 1.886-.756.66 0 1.414.284 1.885.755.472.567.754 1.322.66 2.172v6.418h2.923v-6.89c.094-1.322-.283-2.644-1.226-3.682-.848-.85-2.168-1.416-3.394-1.321Z"></path></g><defs><clipPath id="anyscale-logo_svg__a"><path fill="currentColor" d="M0 0h140.19v32H0z"></path></clipPath></defs></svg></a><div class="lg:relative lg:flex lg:flex-row hidden"><div class="max-h-[calc(100vh-192px)] items-center overflow-y-auto py-[1px] shadow-[0_-1px_0_#DAE3F2] lg:flex lg:overflow-visible lg:p-0 lg:shadow-none"><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Products</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="../platform.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Anyscale Platform</div></a><a href="../ray-open-source.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Open Source</div></a></div></div></div></div></div></div><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Solutions</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="container mx-auto mb-4 lg:mb-0 mt-2 lg:mt-0 lg:mr-12"><div class="header-card h-full rounded-[5px] p-6 lg:w-[296px]"><h5 id="scalable-ai-and-python">Scalable AI and Python</h5>
<p>Leverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more.</p></div></div><div class="grid lg:flex grid-cols-1 md:grid-cols-2 gap-x-10 lg:gap-x-12 container mx-auto"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><p class="text-base leading-normal font-normal text-darkGrey-300 px-6 lg:px-4 mb-4">Scalable AI and Python</p><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2"><a href="../data-ingestion.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Data Ingestion</div></a><a href="../reinforcement-learning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Reinforcement Learning</div></a><a href="../ray-air.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray AIR</div></a><a href="../model-serving.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Model Serving</div></a><a href="../hyperparameter-tuning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Hyperparameter Tuning</div></a></div></div></div><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><p class="text-base leading-normal font-normal text-darkGrey-300 px-6 lg:px-4 mb-4">Use Cases</p><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2"><a href="../large-language-models.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Large Language Models</div></a><a href="../demand-forecasting.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Demand Forecasting/Pricing</div></a><a href="../industrial-automation.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Industrial Automation</div></a><a href="../machine-learning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Scalable ML Platforms</div></a><a href="../natural-language-processing.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">NLP</div></a><a href="../recommendation-system.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Recommendation System</div></a></div></div></div></div></div></div></div><a class="block font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4 shadow-[0_1px_0_#DAE3F2] lg:shadow-none" href="../ray-ecosystem.html"><div class="w-full container mx-auto">Ecosystem</div></a><a class="block font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4 shadow-[0_1px_0_#DAE3F2] lg:shadow-none" href="../user-stories.html"><div class="w-full container mx-auto">Success Stories</div></a><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Learn</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="how-ray-solves-common-production-challenges-for-generative-ai.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Blog</div></a><a href="https://raysummit.anyscale.com/?utm_source=anyscale&amp;utm_medium=website&amp;utm_campaign=ray_summit_2023&amp;utm_content=home_nav-pulldown"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Summit 2023</div></a><a href="../event-category/rl-summit.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Videos &amp; Webinars</div></a><a href="https://github.com/ray-project/ray-educational-materials"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Training</div></a><a href="https://docs.anyscale.com"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Anyscale Docs</div></a></div></div></div></div></div></div><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Company</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="../about.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">About us</div></a><a href="../press.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">News</div></a><a href="../careers.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Careers</div></a><a href="../community.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Community</div></a></div></div></div></div></div></div></div><div class="absolute bottom-0 left-0 right-0 bg-white py-5 shadow-[0_-1px_0_#DAE3F2] lg:relative lg:ml-5 lg:bg-transparent lg:p-0 lg:shadow-none"><div class="container mx-auto flex justify-end"><a class="btn bg-gradient min-w-[320px] justify-center bg-blue-900 text-center lg:min-w-0" href="../beta.html">Try It Now</a></div></div></div><div class="cursor-pointer py-4 pl-6 lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M20.05 11H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95ZM20.05 16H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95ZM20.05 6H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95Z" fill="#fff"></path></svg></div></div></header><div class="Layout_page-sections__2__gb overflow-x-hidden"><div class="BlogPost_root__ly_um"><div class="Breadcrumbs_root__53EXt"><a class="Breadcrumbs_link__HBYkg" href="../index.html">Home</a><a class="Breadcrumbs_link__HBYkg" href="how-ray-solves-common-production-challenges-for-generative-ai.html">Blog</a><span class="Breadcrumbs_link__HBYkg">Blog Detail</span></div><div class="BlogPost_inner__70rgb"><div class="BlogPost_main__husly"><div class="container"><div class="ArticleHero_inner__lg98I"><h1 class="ArticleHero_title__JB2va">An Introduction to Reinforcement Learning with OpenAI Gym, RLlib, and Google Colab</h1><div class="ArticleHero_details__DZgPt"><div class="container"><span class="ArticleDetails_authors__aqDWy"><span>By </span><a href="../blog@author=michael-galarnyk.html">Michael Galarnyk</a><span> and </span><a href="../blog@author=sven-mika.html">Sven Mika</a><span>   </span></span><span class="ArticleDetails_article-published-tags__9VxRY"><span class="article-published">|   <!-- -->August 26, 2021</span></span></div></div></div></div><div class="ArticleBody_container__QoiWj ArticleBody_page_article__R6nNl"><div class="ArticleBody_inner__ml2H8"><div class="EmbeddedContent_root__oulKc"><div class="EmbeddedContent_inner__IE4jT"><div class="YoutubePlayer_container__xfelL"><div class="YoutubePlayer_inner__Hkl4E"><div class="YoutubePlayer_wrapper__Oq7Ea"></div></div></div></div><span class="EmbeddedContent_caption__D3wUT EmbeddedContent_align_left__IRH6R">This tutorial will use reinforcement learning (RL) to help balance a virtual CartPole. The <a href="https://www.youtube.com/watch?v=XiigTGKZfks&amp;t=106s">video</a> above from PilcoLearner shows the results of using RL in a real-life CartPole environment.</span></div><p>One possible definition of reinforcement learning (RL) is a computational approach to learning how to maximize the total sum of rewards when interacting with an environment. While a definition is useful, this tutorial aims to illustrate what reinforcement learning is through images, code, and video examples and along the way introduce reinforcement learning terms like agents and environments.</p><p>In particular, this tutorial explores:</p><ul><li><p>What is Reinforcement Learning </p></li><li><p>The OpenAI Gym CartPole Environment</p></li><li><p>The Role of Agents in Reinforcement Learning</p></li><li><p>How to Train an Agent by using the Python Library RLlib</p></li><li><p>How to use a GPU to Speed Up Training</p></li><li><p>Hyperparameter Tuning with Ray Tune</p></li></ul><h2>What is Reinforcement Learning</h2><p>As a <a href="reinforcement-learning-with-rllib-in-the-unity-game-engine.html"><u>previous post noted</u></a>, machine learning (ML), a sub-field of AI, uses neural networks or other types of mathematical models to learn how to interpret complex patterns. Two areas of ML that have recently become very popular due to their high level of maturity are supervised learning (SL), in which neural networks learn to make predictions based on large amounts of data, and reinforcement learning (RL), where the networks learn to make good action decisions in a trial-and-error fashion, using a simulator.</p><div class="ArticleBody_image__rd3Dj" style="width:512px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/1uWlGSCvzjk6hY3PNw5NdA/92e09a3d2351f8672b3bb46df9144cc7/Blog_-_intro_reinforcement.png" alt="intro-to-rl-1"/></div></div></div><p>RL is the tech behind mind-boggling successes such as DeepMind’s AlphaGo Zero and the StarCraft II AI (AlphaStar) or OpenAI’s DOTA 2 AI (“OpenAI Five”). Note that there are <a href="best-reinforcement-learning-talks-from-ray-summit-2021.html"><u>many impressive uses of reinforcement learning</u></a> and the reason why it is so powerful and promising for real-life decision making problems is because RL is capable of learning continuously — sometimes even in ever changing environments — starting with no knowledge of which decisions to make whatsoever (random behavior). </p><div><p><a href="../production-rl-summit.html#agenda?utm_source=anyscale&utm_medium=blog&utm_content=openaigym-blog&utm_campaign=rl_summit"><img src="https://images.ctfassets.net/xjan103pcp94/1hu1Zwg1RQ0ifg9Lfp98bA/7d913772436d45666a80e83f629b4815/RL_Summit__12_.png" style="width: 100%"></a></p>
</div><h2>Agents and Environments</h2><div class="ArticleBody_image__rd3Dj" style="width:512px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/5h5ZwNAqLHAIRZ9jPGvRU1/6ceb65f718883cf2e7b8ca9dcd0a5fc4/Blog_-_intro_reinforcement_2.png" alt="agents-and-environments"/></div></div></div><p>The diagram above shows the interactions and communications between an agent and an environment. In reinforcement learning, one or more agents interact within an environment which may be either a simulation like CartPole in this tutorial or a connection to real-world sensors and actuators. At each step, the agent receives an observation (i.e., the state of the environment), takes an action, and usually receives a reward (the frequency at which an agent receives a reward depends on a given task or problem). Agents learn from repeated trials, and a sequence of those is called an episode — the sequence of actions from an initial observation up to either a “success” or “failure” causing the environment to reach its “done” state. The learning portion of an RL framework trains a policy about which actions (i.e., sequential decisions) cause agents to maximize their long-term, cumulative rewards. </p><h2>The OpenAI Gym Cartpole Environment</h2><div class="ArticleBody_image__rd3Dj" style="width:1052px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/4hLHnMXJN2EwwAXq2yYx9v/41b16121290d6c46b6b85492a572a4cf/cartPoleRemade.png" alt="CartPole Remade"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">CartPole</span></div><p>The problem we are trying to solve is trying to keep a pole upright. Specifically, the pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart&#x27;s velocity.</p><div class="ArticleBody_image__rd3Dj" style="width:512px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/1g6s6IMlS4IqUCZrpXZGUM/2493f114da86c08108f3b763343e83a9/Blog_-_intro_reinforcement_4.png" alt="policy "/></div></div></div><p>Rather than code this environment from scratch, this tutorial will use <a href="http://gym.openai.com/"><u>OpenAI Gym</u></a> which is a toolkit that provides a wide variety of simulated environments (Atari games, board games, 2D and 3D physical simulations, and so on). Gym makes no assumptions about the structure of your agent (what pushes the cart left or right in this cartpole example), and is compatible with any numerical computation library, such as numpy.</p><p>The code below loads the cartpole environment.</p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span></code><span style="display:block"><span style="color:#c678dd">import</span><span> gym
</span></span><span style="display:block"><span>env = gym.make(</span><span style="color:#98c379">&quot;CartPole-v0&quot;</span><span>)</span></span></code></pre></div><p>Let&#x27;s now start to understand this environment by looking at the action space.</p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span></code><span style="display:block"><span>env.action_space</span></span></code></pre></div><div class="ArticleBody_image__rd3Dj" style="width:290px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/4h49FDvVX7yC4W132pIojC/d0299dd7ceae93f7ef70b12a2b9b5b44/Blog_-_intro_reinforcement_5.png" alt="env.action_space"/></div></div></div><p>The output Discrete(2) means that there are two actions. In cartpole, 0 corresponds to &quot;push cart to the left&quot; and 1 corresponds to &quot;push cart to the right&quot;. Note that in this particular example, standing still is not an option. In reinforcement learning, the agent produces an action output and this action is sent to an environment which then reacts. The environment produces an observation (along with a reward signal, not shown here) which we can see below:</p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span></code><span style="display:block"><span>env.reset()</span></span></code></pre></div><div class="ArticleBody_image__rd3Dj" style="width:512px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/rxba5zIqvEqMBkOs8CFEF/532a32e7eb27763d8d656bf2b8747f0f/Blog_-_intro_reinforcement_6.png" alt="env.reset ()"/></div></div></div><p>The observation is a vector of dim=4, containing the cart&#x27;s x position, cart x velocity, the pole angle in radians (1 radian = 57.295 degrees), and the angular velocity of the pole. The numbers shown above are the initial observation after starting a new episode (`env.reset()`). With each timestep (and action), the observation values will change, depending on the state of the cart and pole.</p><h2>Training an Agent</h2><p>In reinforcement learning, the goal of the agent is to produce smarter and smarter actions over time. It does so with a policy. In deep reinforcement learning, this policy is represented with a neural network. Let&#x27;s first interact with the gym environment without a neural network or machine learning algorithm of any kind. Instead we&#x27;ll start with random movement (left or right). This is just to understand the mechanisms.</p><div class="ArticleBody_image__rd3Dj" style="width:512px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/52y6RzXWOM5ntjn1BZwUYP/94044c6727443aab584f449dfb419cf0/Blog_-_intro_reinforcement_7.png" alt="policy random movement"/></div></div></div><p>The code below resets the environment and takes 20 steps (20 cycles), always taking a random action and printing the results.</p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">3
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">4
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">5
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">6
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">7
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">8
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">9
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">10
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">11
</span></code><span style="display:block"><span style="color:#5c6370"># returns an initial observation</span><span>
</span></span><span style="display:block">env.reset()
</span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#c678dd">for</span><span> i </span><span style="color:#c678dd">in</span><span> </span><span style="color:#e6c07b">range</span><span>(</span><span style="color:#d19a66">20</span><span>):
</span></span><span style="display:block">
</span><span style="display:block"><span>  </span><span style="color:#5c6370"># env.action_space.sample() produces either 0 (left) or 1 (right).</span><span>
</span></span><span style="display:block">  observation, reward, done, info = env.step(env.action_space.sample())
</span><span style="display:block">
</span><span style="display:block"><span>  </span><span style="color:#e6c07b">print</span><span>(</span><span style="color:#98c379">&quot;step&quot;</span><span>, i, observation, reward, done, info)
</span></span><span style="display:block">
</span><span style="display:block">env.close()
</span><span style="display:block">
</span></code></pre></div><div class="ArticleBody_image__rd3Dj" style="width:512px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/74TmunIlHtkSEvH0nxtnjg/44ed9a28256c676eb01b365afd499328/Blog_-_intro_reinforcement_8.png" alt="steps"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">Sample output. There are multiple conditions for episode termination in cartpole. In the image, the episode is terminated because it is over 12 degrees (0.20944 rad). Other conditions for episode termination are cart position is more than 2.4 (center of the cart reaches the edge of the display), episode length is greater than 200, or the solved requirement which is when the average return is greater than or equal to 195.0 over 100 consecutive trials.
</span></div><p>The printed output above shows the following things: </p><ul><li><p>step (how many times it has cycled through the environment). In each timestep, an agent chooses an action, and the environment returns an observation and a reward</p></li><li><p>observation of the environment [x cart position, x cart velocity, pole angle (rad), pole angular velocity]</p></li><li><p>reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward. The reward is 1 for every step taken for cartpole, including the termination step. After it is 0 (step 18 and 19 in the image).</p></li><li><p>done is a boolean. It indicates whether it&#x27;s time to reset the environment again. Most tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. In cart pole, it could be that the pole tipped too far (more than 12 degrees/0.20944 radians), position is more than 2.4 meaning the center of the cart reaches the edge of the display, episode length is greater than 200, or the solved requirement which is when the average return is greater than or equal to 195.0 over 100 consecutive trials.</p></li><li><p>info which is diagnostic information useful for debugging. It is empty for this cartpole environment.</p></li></ul><p>While these numbers are useful outputs, a video might be clearer. If you are running this code in Google Colab,  it is important to note that there is no display driver available for generating videos. However, it is possible to install a virtual display driver to get it to work.</p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">3
</span></code><span style="display:block"><span style="color:#5c6370"># install dependencies needed for recording videos</span><span>
</span></span><span style="display:block"><span>!apt-</span><span class="hljs-builtin-name">get</span><span> install -y xvfb x11-utils
</span></span><span style="display:block"><span>!pip install </span><span style="color:#98c379">pyvirtualdisplay</span><span>==0.2.*
</span></span><span style="display:block">
</span></code></pre></div><p>The next step is to start an instance of the virtual display.</p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">3
</span></code><span style="display:block"><span style="color:#c678dd">from</span><span> pyvirtualdisplay </span><span style="color:#c678dd">import</span><span> Display
</span></span><span style="display:block"><span>display = Display(visible=</span><span style="color:#56b6c2">False</span><span>, size=(</span><span style="color:#d19a66">1400</span><span>, </span><span style="color:#d19a66">900</span><span>))
</span></span><span style="display:block">_ = display.start()
</span></code></pre></div><p>OpenAI gym has a VideoRecorder wrapper that can record a video of the running environment in MP4 format. The code below is the same as before except that it is for 200 steps and is recording.</p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">3
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">4
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">5
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">6
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">7
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">8
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">9
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">10
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">11
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">12
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">13
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">14
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">15
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">16
</span></code><span style="display:block"><span style="color:#c678dd">from</span><span> gym.wrappers.monitoring.video_recorder </span><span style="color:#c678dd">import</span><span> VideoRecorder
</span></span><span style="display:block"><span>before_training = </span><span style="color:#98c379">&quot;before_training.mp4&quot;</span><span>
</span></span><span style="display:block">
</span><span style="display:block">video = VideoRecorder(env, before_training)
</span><span style="display:block"><span></span><span style="color:#5c6370"># returns an initial observation</span><span>
</span></span><span style="display:block">env.reset()
</span><span style="display:block"><span></span><span style="color:#c678dd">for</span><span> i </span><span style="color:#c678dd">in</span><span> </span><span style="color:#e6c07b">range</span><span>(</span><span style="color:#d19a66">200</span><span>):
</span></span><span style="display:block">  env.render()
</span><span style="display:block">  video.capture_frame()
</span><span style="display:block"><span>  </span><span style="color:#5c6370"># env.action_space.sample() produces either 0 (left) or 1 (right).</span><span>
</span></span><span style="display:block">  observation, reward, done, info = env.step(env.action_space.sample())
</span><span style="display:block"><span>  </span><span style="color:#5c6370"># Not printing this time</span><span>
</span></span><span style="display:block"><span>  </span><span style="color:#5c6370">#print(&quot;step&quot;, i, observation, reward, done, info)</span><span>
</span></span><span style="display:block">
</span><span style="display:block">video.close()
</span><span style="display:block">env.close()
</span></code></pre></div><div class="ArticleBody_image__rd3Dj" style="width:512px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/1mDEqsrizwjyoV3YSBWZkH/5867b700eb021a594fa0b1d34120ebe6/Blog_-_intro_reinforcement_9.png" alt="user warning"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">Usually you end the simulation when done is 1 (True). The code above let the environment keep on going after a termination condition was reached. For example, in CartPole, this could be when the pole tips over, pole goes off-screen, or reaches other termination conditions.</span></div><p>The code above saved the video file into the Colab disk. In order to display it in the notebook, you need a helper function. </p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">3
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">4
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">5
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">6
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">7
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">8
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">9
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">10
</span></code><span style="display:block"><span style="color:#c678dd">from</span><span> base64 </span><span style="color:#c678dd">import</span><span> b64encode
</span></span><span style="display:block"><span></span><span class="hljs-function" style="color:#c678dd">def</span><span class="hljs-function"> </span><span class="hljs-function" style="color:#61aeee">render_mp4</span><span class="hljs-function">(</span><span class="hljs-function hljs-params">videopath: </span><span class="hljs-function hljs-params" style="color:#e6c07b">str</span><span class="hljs-function">) -&gt; </span><span class="hljs-function" style="color:#e6c07b">str</span><span class="hljs-function">:</span><span>
</span></span><span style="display:block"><span>  </span><span style="color:#98c379">&quot;&quot;&quot;
</span></span><span style="display:block;color:#98c379">  Gets a string containing a b4-encoded version of the MP4 video
</span><span style="display:block;color:#98c379">  at the specified path.
</span><span style="display:block"><span style="color:#98c379">  &quot;&quot;&quot;</span><span>
</span></span><span style="display:block"><span>  mp4 = </span><span style="color:#e6c07b">open</span><span>(videopath, </span><span style="color:#98c379">&#x27;rb&#x27;</span><span>).read()
</span></span><span style="display:block">  base64_encoded_mp4 = b64encode(mp4).decode()
</span><span style="display:block"><span>  </span><span style="color:#c678dd">return</span><span> </span><span style="color:#98c379">f&#x27;&lt;video width=400 controls&gt;&lt;source src=&quot;data:video/mp4;&#x27;</span><span> \
</span></span><span style="display:block"><span>         </span><span style="color:#98c379">f&#x27;base64,</span><span style="color:#e06c75">{base64_encoded_mp4}</span><span style="color:#98c379">&quot; type=&quot;video/mp4&quot;&gt;&lt;/video&gt;&#x27;</span><span>
</span></span><span style="display:block">
</span></code></pre></div><p>The code below renders the results. You should get a video similar to the one below. </p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">3
</span></code><span style="display:block"><span style="color:#c678dd">from</span><span> IPython.display </span><span style="color:#c678dd">import</span><span> HTML
</span></span><span style="display:block">html = render_mp4(before_training)
</span><span style="display:block">HTML(html)
</span></code></pre></div><div class="EmbeddedContent_root__oulKc"><div class="EmbeddedContent_inner__IE4jT"><div class="YoutubePlayer_container__xfelL"><div class="YoutubePlayer_inner__Hkl4E"><div class="YoutubePlayer_wrapper__Oq7Ea"></div></div></div></div></div><p>Playing the video demonstrates that randomly choosing an action is not a good policy for keeping the CartPole upright.</p><h2>How to Train an Agent using Ray&#x27;s RLlib</h2><p>The previous section of the tutorial had our agent make random actions disregarding the observations and rewards from the environment. The goal of having an agent is to produce smarter and smarter actions over time and random actions don&#x27;t accomplish that. To make an agent make smarter actions over time, itl needs a better policy. In deep reinforcement learning, the policy is represented with a neural network.</p><div class="ArticleBody_image__rd3Dj" style="width:512px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/3su83sKnDVpzt0r1OJO1DD/8a59845c574723c1b9ec0d5ebf9cacc4/Blog_-_intro_reinforcement_10.png" alt="policy deep-rl"/></div></div></div><p>This tutorial will use the <a href="https://docs.ray.io/en/master/rllib.html"><u>RLlib library</u></a> to train a smarter agent. RLlib has many advantages like: </p><ul><li><p>Extreme flexibility. It allows you to customize every aspect of the RL cycle. For instance, this section of the tutorial will make a custom neural network policy using PyTorch (RLlib also has native support for TensorFlow). </p></li><li><p>Scalability. Reinforcement learning applications can be quite compute intensive and often need to scale-out to a cluster for faster training. RLlib not only has first-class support for GPUs, but it is also built on <a href="https://ray.io/">Ray</a> which is an open source library for <a href="parallelizing-python-code.html">parallel</a> and <a href="writing-your-first-distributed-python-application-with-ray.html">distributed</a> Python. This makes scaling Python programs from a laptop to a cluster easy. </p></li><li><p>A unified API and support for offline, model-based, model-free, multi-agent algorithms, and more (these algorithms won’t be explored in this tutorial). </p></li><li><p>Being part of the <a href="https://github.com/ray-project/ray"><u>Ray Project ecosystem</u></a>. One advantage of this is that RLlib can be run with other libraries in the ecosystem like <a href="https://docs.ray.io/en/master/tune/index.html"><u>Ray Tune</u></a>, a library for experiment execution and hyperparameter tuning at any scale (more on this later).</p></li></ul><p>While some of these features won&#x27;t be fully utilized in this post, they are highly useful for when you want to do something more complicated and solve real world problems. You can learn about some impressive use-cases of RLlib <a href="best-reinforcement-learning-talks-from-ray-summit-2021.html"><u>here</u></a>.</p><p>To get started with RLlib, you need to first install it.</p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span></code><span style="display:block"><span>!pip install </span><span style="color:#98c379">&#x27;ray[rllib]&#x27;</span><span>==</span><span style="color:#d19a66">1.6</span></span></code></pre></div><p>Now you can train a PyTorch model using the Proximal Policy Optimization (PPO) algorithm. It is a very well rounded, one size fits all type of algorithm which you can learn more about <a href="https://docs.ray.io/en/master/rllib-algorithms.html#proximal-policy-optimization-ppo"><u>here</u></a>. The code below uses a neural network consisting of a single hidden layer of 32 neurons and linear activation functions.<br/></p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">3
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">4
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">5
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">6
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">7
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">8
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">9
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">10
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">11
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">12
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">13
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">14
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">15
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">16
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">17
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">18
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">19
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">20
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">21
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">22
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">23
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">24
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">25
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">26
</span></code><span style="display:block"><span style="color:#c678dd">import</span><span> ray
</span></span><span style="display:block"><span></span><span style="color:#c678dd">from</span><span> ray.rllib.agents.ppo </span><span style="color:#c678dd">import</span><span> PPOTrainer
</span></span><span style="display:block">config = {
</span><span style="display:block"><span>    </span><span style="color:#98c379">&quot;env&quot;</span><span>: </span><span style="color:#98c379">&quot;CartPole-v0&quot;</span><span>,
</span></span><span style="display:block"><span>    </span><span style="color:#5c6370"># Change the following line to `“framework”: “tf”` to use tensorflow</span><span>
</span></span><span style="display:block"><span>    </span><span style="color:#98c379">&quot;framework&quot;</span><span>: </span><span style="color:#98c379">&quot;torch&quot;</span><span>,
</span></span><span style="display:block"><span>    </span><span style="color:#98c379">&quot;model&quot;</span><span>: {
</span></span><span style="display:block"><span>      </span><span style="color:#98c379">&quot;fcnet_hiddens&quot;</span><span>: [</span><span style="color:#d19a66">32</span><span>],
</span></span><span style="display:block"><span>      </span><span style="color:#98c379">&quot;fcnet_activation&quot;</span><span>: </span><span style="color:#98c379">&quot;linear&quot;</span><span>,
</span></span><span style="display:block">    },
</span><span style="display:block">}
</span><span style="display:block"><span> stop = {</span><span style="color:#98c379">&quot;episode_reward_mean&quot;</span><span>: </span><span style="color:#d19a66">195</span><span>}
</span></span><span style="display:block"> ray.shutdown()
</span><span style="display:block">ray.init(
</span><span style="display:block"><span>  num_cpus=</span><span style="color:#d19a66">3</span><span>,
</span></span><span style="display:block"><span>  include_dashboard=</span><span style="color:#56b6c2">False</span><span>,
</span></span><span style="display:block"><span>  ignore_reinit_error=</span><span style="color:#56b6c2">True</span><span>,
</span></span><span style="display:block"><span>  log_to_driver=</span><span style="color:#56b6c2">False</span><span>,
</span></span><span style="display:block">)
</span><span style="display:block"><span></span><span style="color:#5c6370"># execute training </span><span>
</span></span><span style="display:block">analysis = ray.tune.run(
</span><span style="display:block"><span>  </span><span style="color:#98c379">&quot;PPO&quot;</span><span>,
</span></span><span style="display:block">  config=config,
</span><span style="display:block">  stop=stop,
</span><span style="display:block"><span>  checkpoint_at_end=</span><span style="color:#56b6c2">True</span><span>,
</span></span><span style="display:block">)
</span></code></pre></div><p>This code should produce quite a bit of output. The final entry should look something like this:</p><div class="ArticleBody_image__rd3Dj" style="width:512px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/1ABB5vepqo4Iy7UOOCJawT/906249ef28e0138ab98df13b79dbe955/Blog_-_intro_reinforcement_13.png" alt="status"/></div></div></div><p>The entry shows it took 35 iterations, running over 258 seconds, to solve the environment. This will be different each time, but will probably be about 7 seconds per iteration (258 / 35 = 7.3). Note that if you like to learn the Ray API and see what commands like ray.shutdown and ray.init do, you can <a href="writing-your-first-distributed-python-application-with-ray.html"><u>check out this tutorial</u></a>. </p><h2>How to use a GPU to Speed Up Training</h2><p>While the rest of the tutorial utilizes CPUs, it is important to note that you can speed up model training by using a GPU in Google Colab. This can be done by selecting <b>Runtime &gt; Change runtime type</b> and set hardware accelerator to <b>GPU</b>. Then select <b>Runtime &gt; Restart and run all</b>.</p><div class="ArticleBody_image__rd3Dj" style="width:512px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/67hYTruOvuO7Y8L0rKwG8g/c6f8e508ce2c0c5a5d6db907edb2c9f7/Blog_-_intro_reinforcement_14.png" alt="status 2"/></div></div></div><p>Notice that, although the number of training iterations might be about the same, the time per iteration has come down significantly (from 7 seconds to 5.5 seconds).</p><h2>Creating a Video of the Trained Model in Action</h2><p>RLlib provides a Trainer class which holds a policy for environment interaction. Through the trainer interface, a policy can be trained, action computed, and checkpointed. While the analysis object returned from <i>ray.tune.run</i> earlier did not contain any trainer instances, it has all the information needed to reconstruct one from a saved checkpoint because  <i>checkpoint_at_end=True</i> was passed as a parameter. The code below shows this. </p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">3
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">4
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">5
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">6
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">7
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">8
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">9
</span></code><span style="display:block"><span style="color:#5c6370"># restore a trainer from the last checkpoint</span><span>
</span></span><span style="display:block"><span>trial = analysis.get_best_logdir(</span><span style="color:#98c379">&quot;episode_reward_mean&quot;</span><span>, </span><span style="color:#98c379">&quot;max&quot;</span><span>)
</span></span><span style="display:block">checkpoint = analysis.get_best_checkpoint(
</span><span style="display:block">  trial,
</span><span style="display:block"><span>  </span><span style="color:#98c379">&quot;training_iteration&quot;</span><span>,
</span></span><span style="display:block"><span>  </span><span style="color:#98c379">&quot;max&quot;</span><span>,
</span></span><span style="display:block">)
</span><span style="display:block">trainer = PPOTrainer(config=config)
</span><span style="display:block">trainer.restore(checkpoint)
</span></code></pre></div><p>Let’s now create another video, but this time choose the action recommended by the trained model instead of acting randomly.</p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">3
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">4
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">5
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">6
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">7
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">8
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">9
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">10
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">11
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">12
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">13
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">14
</span></code><span style="display:block"><span>after_training = </span><span style="color:#98c379">&quot;after_training.mp4&quot;</span><span>
</span></span><span style="display:block">after_video = VideoRecorder(env, after_training)
</span><span style="display:block">observation = env.reset()
</span><span style="display:block"><span>done = </span><span style="color:#56b6c2">False</span><span>
</span></span><span style="display:block"><span></span><span style="color:#c678dd">while</span><span> </span><span style="color:#c678dd">not</span><span> done:
</span></span><span style="display:block">  env.render()
</span><span style="display:block">  after_video.capture_frame()
</span><span style="display:block">  action = trainer.compute_action(observation)
</span><span style="display:block">  observation, reward, done, info = env.step(action)
</span><span style="display:block">after_video.close()
</span><span style="display:block">env.close()
</span><span style="display:block"><span></span><span style="color:#5c6370"># You should get a video similar to the one below. </span><span>
</span></span><span style="display:block">html = render_mp4(after_training)
</span><span style="display:block">HTML(html)
</span></code></pre></div><div class="EmbeddedContent_root__oulKc"><div class="EmbeddedContent_inner__IE4jT"><div class="YoutubePlayer_container__xfelL"><div class="YoutubePlayer_inner__Hkl4E"><div class="YoutubePlayer_wrapper__Oq7Ea"></div></div></div></div></div><p>This time, the pole balances nicely which means the agent has solved the cartpole environment! </p><h2>Hyperparameter Tuning with Ray Tune</h2><div class="ArticleBody_image__rd3Dj" style="width:512px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/1EGuORfezOpLR5ywb0Hjlq/86b3d2ba725ba7a355597b5055659794/Blog_-_intro_reinforcement_11.jpeg" alt="libraries"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">The Ray Ecosystem</span></div><p><i>For a deep-dive on hyperparameter tuning, from the basics to how to distribute hyperparameter tuning using Ray Tune, </i><a href="what-is-hyperparameter-tuning.html"><i>check our our blog series on hyperparameter optimization</i></a><i>.</i></p><p>RLlib is a reinforcement learning library that is part of the Ray Ecosystem. Ray is a highly scalable universal framework for parallel and distributed python. It is very general and that generality is important for supporting its library ecosystem. The ecosystem covers everything from <a href="https://docs.ray.io/en/latest/tune/index.html"><u>training</u></a>, to <a href="https://docs.ray.io/en/master/serve/index.html"><u>production serving</u></a>, to <a href="data-processing-support-in-ray.html"><u>data processing</u></a> and more. You can use multiple libraries together and build applications that do all of these things.</p><p>This part of the tutorial utilizes <a href="https://docs.ray.io/en/latest/tune/index.html"><u>Ray Tune</u></a> which is another library in the Ray Ecosystem. It is a library for experiment execution and hyperparameter tuning at any scale. While this tutorial will only use grid search, note that Ray Tune also gives you access to more efficient hyperparameter tuning algorithms like population based training, BayesOptSearch, and HyperBand/ASHA.  </p><p>Let’s now try to find hyperparameters that can solve the CartPole environment in the fewest timesteps.</p><p>Enter the following code, and be prepared for it to take a while to run:</p><div class="CodeSnippet_container__wu5pt"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ccc;background:#222025"><code class="language-python"><code style="float:left;padding-right:10px"><span class="react-syntax-highlighter-line-number" style="opacity:0.35">1
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">2
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">3
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">4
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">5
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">6
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">7
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">8
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">9
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">10
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">11
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">12
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">13
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">14
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">15
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">16
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">17
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">18
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">19
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">20
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">21
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">22
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">23
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">24
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">25
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">26
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">27
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">28
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">29
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">30
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">31
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">32
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">33
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">34
</span><span class="react-syntax-highlighter-line-number" style="opacity:0.35">35
</span></code><span style="display:block"><span>parameter_search_config = {
</span></span><span style="display:block"><span>    </span><span style="color:#98c379">&quot;env&quot;</span><span>: </span><span style="color:#98c379">&quot;CartPole-v0&quot;</span><span>,
</span></span><span style="display:block"><span>    </span><span style="color:#98c379">&quot;framework&quot;</span><span>: </span><span style="color:#98c379">&quot;torch&quot;</span><span>,
</span></span><span style="display:block">
</span><span style="display:block"><span>    </span><span style="color:#5c6370"># Hyperparameter tuning</span><span>
</span></span><span style="display:block"><span>    </span><span style="color:#98c379">&quot;model&quot;</span><span>: {
</span></span><span style="display:block"><span>      </span><span style="color:#98c379">&quot;fcnet_hiddens&quot;</span><span>: ray.tune.grid_search([[</span><span style="color:#d19a66">32</span><span>], [</span><span style="color:#d19a66">64</span><span>]]),
</span></span><span style="display:block"><span>      </span><span style="color:#98c379">&quot;fcnet_activation&quot;</span><span>: ray.tune.grid_search([</span><span style="color:#98c379">&quot;linear&quot;</span><span>, </span><span style="color:#98c379">&quot;relu&quot;</span><span>]),
</span></span><span style="display:block">    },
</span><span style="display:block"><span>    </span><span style="color:#98c379">&quot;lr&quot;</span><span>: ray.tune.uniform(</span><span style="color:#d19a66">1e-7</span><span>, </span><span style="color:#d19a66">1e-2</span><span>)
</span></span><span style="display:block">}
</span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#5c6370"># To explicitly stop or restart Ray, use the shutdown API.</span><span>
</span></span><span style="display:block">ray.shutdown()
</span><span style="display:block">
</span><span style="display:block">ray.init(
</span><span style="display:block"><span>  num_cpus=</span><span style="color:#d19a66">12</span><span>,
</span></span><span style="display:block"><span>  include_dashboard=</span><span style="color:#56b6c2">False</span><span>,
</span></span><span style="display:block"><span>  ignore_reinit_error=</span><span style="color:#56b6c2">True</span><span>,
</span></span><span style="display:block"><span>  log_to_driver=</span><span style="color:#56b6c2">False</span><span>,
</span></span><span style="display:block">)
</span><span style="display:block">
</span><span style="display:block">parameter_search_analysis = ray.tune.run(
</span><span style="display:block"><span>  </span><span style="color:#98c379">&quot;PPO&quot;</span><span>,
</span></span><span style="display:block">  config=parameter_search_config,
</span><span style="display:block">  stop=stop,
</span><span style="display:block"><span>  num_samples=</span><span style="color:#d19a66">5</span><span>,
</span></span><span style="display:block"><span>  metric=</span><span style="color:#98c379">&quot;timesteps_total&quot;</span><span>,
</span></span><span style="display:block"><span>  mode=</span><span style="color:#98c379">&quot;min&quot;</span><span>,
</span></span><span style="display:block">)
</span><span style="display:block">
</span><span style="display:block"><span></span><span style="color:#e6c07b">print</span><span>(
</span></span><span style="display:block"><span>  </span><span style="color:#98c379">&quot;Best hyperparameters found:&quot;</span><span>,
</span></span><span style="display:block">  parameter_search_analysis.best_config,
</span><span style="display:block">)
</span><span style="display:block">
</span></code></pre></div><p>By asking for 12 CPU cores by passing in num_cpus=12 to ray.init, four trials get run in parallel across three cpus each. If this doesn’t work, perhaps Google has changed the VMs available on Colab. Any value of three or more should work. If Colab errors by running out of RAM, you might need to do <b>Runtime &gt; Factory reset runtime</b>, followed by <b>Runtime &gt; Run all</b>. Note that there is an area in the top right of the Colab notebook showing the RAM and disk use.</p><p>Specifying num_samples=5 means that you will get five random samples for the learning rate. For each of those, there are two values for the size of the hidden layer, and two values for the activation function. So, there will be 5 * 2 * 2 = 20 trials, shown with their statuses in the output of the cell as the calculation runs.</p><p>Note that Ray prints the current best configuration as it goes. This includes all the default values that have been set, which is a good place to find other parameters that could be tweaked.</p><p>After running this, the final output might be similar to the following output: </p><p>INFO tune.py:549 -- Total run time: 3658.24 seconds (3657.45 seconds for the tuning loop).</p><p>Best hyperparameters found: {&#x27;env&#x27;: &#x27;CartPole-v0&#x27;, &#x27;framework&#x27;: &#x27;torch&#x27;, &#x27;model&#x27;: {&#x27;fcnet_hiddens&#x27;: [64], &#x27;fcnet_activation&#x27;: &#x27;relu&#x27;}, &#x27;lr&#x27;: 0.006733929096170726};&#x27;&#x27;&#x27;</p><p>So, of the twenty sets of hyperparameters, the one with 64 neurons, the ReLU activation function, and a learning rate around 6.7e-3 performed best.</p><h2>Conclusion</h2><div class="ArticleBody_image__rd3Dj" style="width:512px"><div class="Image_wrapper__beWl0 Image_plain__m5QAa"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/601dfdJWuaH4znfvrjIDRI/31fb646652ae4332b2fe0c75fb76f1cd/Blog_-_intro_reinforcement_12.jpeg" alt="neural mmo"/></div></div><span class="Image_caption__outy2 Image_align_left__VA3hB">Neural MMO is an environment modeled from Massively Multiplayer Online games — a genre supporting hundreds to thousands of concurrent players. You can learn how Ray and RLlib help enable some key features of this and other projects <a href="best-reinforcement-learning-talks-from-ray-summit-2021.html">here</a>. </span></div><p>This tutorial illustrated what reinforcement learning is by introducing reinforcement learning terminology, by showing how agents and environments interact, and by demonstrating these concepts through code and video examples. If you would like to learn more about reinforcement learning, check out the <a href="../events/2021/06/24/hands-on-reinforcement-learning-with-rays-rllib.html"><u>RLlib tutorial by Sven Mika</u></a>. It is a great way to learn about RLlib’s best practices, multi-agent algorithms, and much more. If you would like to keep up to date with all things RLlib and Ray, consider <a href="https://twitter.com/raydistributed"><u>following @raydistributed on twitter</u></a> and <a href="https://anyscale.us5.list-manage.com/subscribe?u=524b25758d03ad7ec4f64105f&amp;id=d94e960a03"><u>sign up for the Ray newsletter</u></a>. </p></div></div></div><div class="BlogPost_aside__BK_Wk"><div class="root"><h4 class="ArticleExtras_label__JQEEO">Sharing</h4><div class="ArticleExtras_sharing__LSXs1 ArticleExtras_section__26jYL"><a target="_blank" rel="noreferrer" href="an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google.html" aria-label="Share on Facebook"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M24 12c0-6.628-5.372-12-12-12S0 5.372 0 12c0 5.99 4.388 10.955 10.125 11.855v-8.386H7.078V12h3.047V9.356c0-3.007 1.79-4.668 4.533-4.668 1.312 0 2.686.234 2.686.234v2.953H15.83c-1.49 0-1.955.926-1.955 1.875V12h3.328l-.532 3.469h-2.796v8.386C19.613 22.955 24 17.99 24 12Z" fill="#999"></path></svg><div class="ArticleExtras_active__cv689"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M24 12c0-6.628-5.372-12-12-12S0 5.372 0 12c0 5.99 4.388 10.955 10.125 11.855v-8.386H7.078V12h3.047V9.356c0-3.007 1.79-4.668 4.533-4.668 1.312 0 2.686.234 2.686.234v2.953H15.83c-1.49 0-1.955.926-1.955 1.875V12h3.328l-.532 3.469h-2.796v8.386C19.613 22.955 24 17.99 24 12Z" fill="#234999"></path></svg></div></a><a target="_blank" rel="noreferrer" href="an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google.html" aria-label="Share on Twitter"><svg width="24" height="20" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M24 2.436c-.955.358-1.791.597-2.866.716 1.075-.597 1.791-1.552 2.15-2.746-.956.597-2.03.955-3.105 1.194-1.79-2.03-4.895-2.15-6.925-.239-1.314 1.194-1.91 2.985-1.433 4.776C7.88 5.9 4.299 3.988 1.79 1.003a4.305 4.305 0 0 0-.716 2.388c0 1.672.835 3.105 2.149 4.06-.717 0-1.433-.12-2.15-.597v.12c0 2.268 1.672 4.298 3.94 4.775-.477.12-.835.24-1.313.24-.358 0-.597 0-.955-.12.597 2.03 2.508 3.343 4.538 3.343-1.672 1.313-3.821 2.15-6.09 2.15-.358 0-.836 0-1.194-.12 2.269 1.433 4.896 2.268 7.522 2.268 7.642.12 13.851-6.089 13.97-13.73v-.837c1.075-.716 1.911-1.552 2.508-2.507Z" fill="#999"></path></svg><div class="ArticleExtras_active__cv689"><svg width="24" height="20" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M24 2.436c-.955.358-1.791.597-2.866.716 1.075-.597 1.791-1.552 2.15-2.746-.956.597-2.03.955-3.105 1.194-1.79-2.03-4.895-2.15-6.925-.239-1.314 1.194-1.91 2.985-1.433 4.776C7.88 5.9 4.299 3.988 1.79 1.003a4.305 4.305 0 0 0-.716 2.388c0 1.672.835 3.105 2.149 4.06-.717 0-1.433-.12-2.15-.597v.12c0 2.268 1.672 4.298 3.94 4.775-.477.12-.835.24-1.313.24-.358 0-.597 0-.955-.12.597 2.03 2.508 3.343 4.538 3.343-1.672 1.313-3.821 2.15-6.09 2.15-.358 0-.836 0-1.194-.12 2.269 1.433 4.896 2.268 7.522 2.268 7.642.12 13.851-6.089 13.97-13.73v-.837c1.075-.716 1.911-1.552 2.508-2.507Z" fill="#234999"></path></svg></div></a><a target="_blank" rel="noreferrer" href="an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google.html" aria-label="Share on Linkedin"><svg width="24" height="25" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M22.191 0H1.809C.844 0 0 .724 0 1.688v20.624c0 1.085.844 1.809 1.809 1.809h20.382c.965 0 1.809-.724 1.809-1.689V1.688C24 .724 23.156 0 22.191 0ZM7.236 20.14H3.618V9.287h3.618v10.855ZM5.427 7.84c-.965 0-1.809-.845-1.809-1.93 0-1.086.844-1.93 1.93-1.93 1.085-.12 1.93.603 2.05 1.688.12 1.086-.724 2.05-1.688 2.171h-.483Zm14.955 12.3h-3.618v-5.788c0-1.447-.483-2.412-1.81-2.412-.843 0-1.567.482-1.808 1.326-.12.242-.12.603-.12.845v6.03H9.406V9.286h3.618v1.568c.724-1.206 1.93-1.809 3.256-1.809 2.413 0 4.222 1.568 4.222 4.945v6.15h-.121Z" fill="#999"></path></svg><div class="ArticleExtras_active__cv689"><svg width="24" height="25" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M22.191 0H1.809C.844 0 0 .724 0 1.688v20.624c0 1.085.844 1.809 1.809 1.809h20.382c.965 0 1.809-.724 1.809-1.689V1.688C24 .724 23.156 0 22.191 0ZM7.236 20.14H3.618V9.287h3.618v10.855ZM5.427 7.84c-.965 0-1.809-.845-1.809-1.93 0-1.086.844-1.93 1.93-1.93 1.085-.12 1.93.603 2.05 1.688.12 1.086-.724 2.05-1.688 2.171h-.483Zm14.955 12.3h-3.618v-5.788c0-1.447-.483-2.412-1.81-2.412-.843 0-1.567.482-1.808 1.326-.12.242-.12.603-.12.845v6.03H9.406V9.286h3.618v1.568c.724-1.206 1.93-1.809 3.256-1.809 2.413 0 4.222 1.568 4.222 4.945v6.15h-.121Z" fill="#234999"></path></svg></div></a></div><div class="tags-wrapper ArticleExtras_section__26jYL"><h4 class="ArticleExtras_label__JQEEO">Tags</h4><span class="ArticleExtras_tags__Q_xZs"><span class="tag">Ray RLlib</span></span></div><div class="ArticleExtras_form-wrapper__rTr1C"><h4 class="ArticleExtras_label__JQEEO">Sign up for product updates</h4><div class="HubspotEmailForm_root__Rwc_O HubspotEmailForm_condensed___nNtN"><div class="HubspotEmailForm_form___3bhH"><div id="hubspotForm"></div></div></div></div></div><div class="root"><h4 class="RecommendedContent_header__3Cuyf">Recommended content</h4><a class="RecommendedContent_item__e9DDs" href="../ray-summit-2022.html"><div class="RecommendedContent_thumbnail__bD3bd"><div class="Image_wrapper__beWl0"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/5nTlh9josVs5zpZqU3vTbn/e66043f62ed4a4b53d7e182e50289ec3/blog-recommended-content-ray-summit-2022.png" alt="blog-recommended-content-ray-summit-2022"/></div></div></div><div class="RecommendedContent_item-content__rXTae"><h4>Ray Summit: Aug 23-24 in SF!</h4><span>Register now<svg width="15" height="12" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1 6h12.222m0 0-5-5m5 5-5 5" stroke="#234999" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></div></a><a class="RecommendedContent_item__e9DDs" href="an-informal-introduction-to-reinforcement-learning.html"><div class="RecommendedContent_thumbnail__bD3bd"><div class="Image_wrapper__beWl0"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/3tbbqYD9I2NRmPfTlTH1Io/d4d8b09181c1d59d7f64c6ee3d009ba2/blog-recommended-content-rl-robot-gear-light.jpg" alt="blog-recommended-content-rl-robot-gear-light"/></div></div></div><div class="RecommendedContent_item-content__rXTae"><h4>An informal introduction to RL</h4><span>Read blog series<svg width="15" height="12" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1 6h12.222m0 0-5-5m5 5-5 5" stroke="#234999" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></div></a><a class="RecommendedContent_item__e9DDs" href="introducing-the-anyscale-databricks-connector.html"><div class="RecommendedContent_thumbnail__bD3bd"><div class="Image_wrapper__beWl0"><div class="Image_image__EAuYA"><img src="https://images.ctfassets.net/xjan103pcp94/71IpYQvsxplh7okIL7RJVx/b193e65fbc2d6500e3ccb2162d323080/0_-_Updated_Databricks_Anyscale_Image.png" alt="0 -Anyscale Databricks headline image"/></div></div></div><div class="RecommendedContent_item-content__rXTae"><h4>Introducing the Anyscale Databricks Connector</h4><span>Read more<svg width="15" height="12" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1 6h12.222m0 0-5-5m5 5-5 5" stroke="#234999" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></div></a></div></div></div></div></div><footer class="relative bg-primary pt-30 pb-30 md:pt-36 md:pb-[180px] lg:pt-20"><div class="container mx-auto flex flex-col space-y-12 md:space-y-16 lg:flex-row lg:justify-between lg:space-y-0"><div class="flex flex-col gap-16"><div class="flex flex-col gap-6"><div class="h-10 text-white"><svg width="141" height="32" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="origin-top-left scale-125"><g clip-path="url(#anyscale-logo_svg__a)" fill="currentColor"><path d="M10.653 17.46H3.865C1.697 17.46 0 19.16 0 21.332v6.797c0 2.17 1.697 3.87 3.865 3.87h6.788c2.169 0 3.866-1.7 3.866-3.87V21.33c0-2.171-1.792-3.87-3.866-3.87Zm1.509 10.573c0 .85-.66 1.51-1.509 1.51H3.865a1.49 1.49 0 0 1-1.508-1.51v-6.796c0-.85.66-1.51 1.508-1.51h6.788c.849 0 1.509.66 1.509 1.51v6.796Z"></path><path d="M28.095 11.988h-8.108V3.87C19.892 1.7 18.196 0 16.027 0H3.865C1.697 0 0 1.7 0 3.87v6.608c0 2.171 1.697 3.87 3.865 3.87h13.67v13.687c0 2.171 1.698 3.87 3.866 3.87h6.6c2.168 0 3.865-1.699 3.865-3.87V15.858c0-2.076-1.697-3.775-3.771-3.87Zm-24.23 0a1.49 1.49 0 0 1-1.508-1.51V3.87c0-.85.66-1.51 1.508-1.51h12.162c.849 0 1.509.66 1.509 1.51v8.118H3.866Zm25.738 16.047c0 .85-.66 1.51-1.508 1.51h-6.6a1.49 1.49 0 0 1-1.508-1.51V14.348h8.108c.848 0 1.508.66 1.508 1.51v12.177ZM79.194 17.462l-3.017-8.118h-3.3l4.714 11.327-2.546 5.853h3.112l7.165-17.18H82.21l-3.016 8.118ZM93.335 14.819c-.66-.284-1.414-.472-2.075-.661-.66-.189-1.225-.378-1.79-.66-.378-.19-.566-.473-.566-.85 0-.378.188-.661.472-.85.847-.472 1.79-.378 2.639.094.377.284.566.661.566 1.133h2.828c0-1.133-.566-2.171-1.414-2.832-.943-.755-2.169-1.133-3.3-1.038-.849 0-1.697.094-2.45.472-.66.283-1.227.66-1.604 1.227-.377.472-.565 1.133-.565 1.794 0 .66.188 1.321.565 1.887.377.472.849.85 1.414 1.039.66.283 1.414.472 2.168.66.66.19 1.226.378 1.886.661.377.19.566.472.566.85 0 .377-.189.66-.471.85-.377.283-.85.377-1.32.377-.472 0-1.038-.094-1.415-.472-.377-.283-.565-.66-.66-1.133h-2.922c0 .755.282 1.416.754 1.982.472.567 1.131 1.133 1.791 1.416.849.378 1.698.567 2.546.567.849 0 1.697-.095 2.45-.472.661-.283 1.227-.66 1.604-1.227.377-.567.565-1.133.565-1.794 0-.66-.188-1.321-.66-1.888-.565-.66-1.037-.944-1.602-1.132ZM102.575 11.702c.566 0 1.036.095 1.508.472.377.283.66.755.849 1.227h3.205a5.366 5.366 0 0 0-1.886-3.02 5.348 5.348 0 0 0-3.582-1.039c-1.037 0-2.074.284-3.017.755a5.486 5.486 0 0 0-2.074 2.077 7.427 7.427 0 0 0 0 6.325c.472.85 1.131 1.604 2.074 2.076.943.472 1.98.756 3.017.756 1.32 0 2.546-.378 3.582-1.133a5.366 5.366 0 0 0 1.886-3.02h-3.111c-.283 1.037-1.32 1.698-2.357 1.604-.754 0-1.509-.283-1.98-.944a5.119 5.119 0 0 1 0-5.192 1.914 1.914 0 0 1 1.886-.944ZM118.412 10.1a.405.405 0 0 1-.377.378c-.094 0-.188 0-.188-.095a3.382 3.382 0 0 0-.944-.66c-.659-.378-1.507-.567-2.262-.567-.943 0-1.885.283-2.734.755a4.459 4.459 0 0 0-1.885 2.077c-.472.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.376.85 1.037 1.604 1.885 2.17.849.473 1.697.756 2.64.756.754 0 1.603-.189 2.263-.567.659-.283 1.131-.755 1.602-1.321v1.699h2.923V9.345h-2.923v.755Zm-.377 6.891c-.283.472-.66.944-1.132 1.227-.47.283-1.037.378-1.507.378-.567 0-1.037-.189-1.509-.378-.472-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.094-1.228.472-1.794.848-1.51 2.64-1.982 4.147-1.227h.095c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM55.907 10.1a.406.406 0 0 1-.377.378c-.095 0-.19 0-.19-.095a3.384 3.384 0 0 0-.942-.66c-.66-.378-1.508-.567-2.263-.567-.942 0-1.885.283-2.733.755a4.463 4.463 0 0 0-1.886 2.077c-.471.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.377.85 1.037 1.604 1.886 2.17.848.473 1.697.756 2.64.756.753 0 1.602-.189 2.262-.567.66-.283 1.131-.755 1.603-1.321v1.699h2.922V9.345h-2.922v.755Zm-.472 6.891c-.283.472-.66.944-1.131 1.227-.471.283-1.037.378-1.509.378-.565 0-1.037-.189-1.508-.378-.471-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.095-1.228.472-1.794.848-1.51 2.64-1.982 4.148-1.227h.094c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM126.708 5.477h-2.923v15.575h2.923V5.477ZM140.191 14.914c0-1.038-.189-2.077-.755-3.02-.471-.85-1.131-1.605-2.074-1.983a6.78 6.78 0 0 0-6.033 0 5.485 5.485 0 0 0-2.075 2.077c-.565 1.038-.754 2.077-.754 3.21 0 1.132.189 2.17.754 3.114a5.485 5.485 0 0 0 2.075 2.077c.942.472 1.979.755 3.016.755a6.56 6.56 0 0 0 3.489-1.038c.943-.661 1.602-1.7 1.885-2.738h-3.016c-.377.944-1.321 1.51-2.358 1.416-.659 0-1.414-.283-1.885-.755-.566-.472-.848-1.227-.848-1.982h8.484c.095-.378.095-.755.095-1.133Zm-8.579-.85c.093-.755.377-1.415.942-1.887.566-.472 1.226-.661 1.886-.661.754 0 1.414.189 1.979.66.566.473.849 1.133.849 1.794l-5.656.095ZM67.692 9.159a4.7 4.7 0 0 0-1.98.472c-.377.188-.754.377-1.037.66a.286.286 0 0 1-.377 0c-.094-.094-.094-.094-.094-.188v-.756H61.28v11.705h2.923v-6.418c-.094-.755.188-1.51.754-2.171.471-.472 1.226-.756 1.886-.756.66 0 1.414.284 1.885.755.472.567.754 1.322.66 2.172v6.418h2.923v-6.89c.094-1.322-.283-2.644-1.226-3.682-.848-.85-2.168-1.416-3.394-1.321Z"></path></g><defs><clipPath id="anyscale-logo_svg__a"><path fill="currentColor" d="M0 0h140.19v32H0z"></path></clipPath></defs></svg></div><div class="font-light leading-normal text-lightBlue-300 opacity-70 hidden lg:flex"><span>© Anyscale, Inc 2022</span></div></div><div class="z-10 hidden flex-col gap-10 lg:flex"><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Anyscale</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://www.linkedin.com/company/joinanyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M15.784 14.168a1.667 1.667 0 1 1-3.334-.002 1.667 1.667 0 0 1 3.334.002Zm.05 2.9H12.5V27.5h3.334V17.067Zm5.266 0h-3.317V27.5h3.284v-5.475c0-3.05 3.975-3.334 3.975 0V27.5h3.291v-6.609c0-5.141-5.883-4.95-7.266-2.424l.033-1.4Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://www.facebook.com/AnyscaleCompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M21.667 21.247h2.083l.834-3.333h-2.917v-1.667c0-.858 0-1.666 1.666-1.666h1.25v-2.8a23.49 23.49 0 0 0-2.38-.117c-2.263 0-3.87 1.38-3.87 3.917v2.333h-2.5v3.333h2.5v7.084h3.334v-7.084Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://twitter.com/anyscalecompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/anyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Ray</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://twitter.com/raydistributed"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/ray-project/ray"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div></div></div><div class="grid grid-cols-2 gap-x-8 gap-y-12 md:grid-cols-3 md:gap-y-10 md:gap-x-16"><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Products</div><div><ul>
<li><a href="../platform.html">Anyscale Compute Platform</a></li>
<li><a href="../ray-open-source.html">Ray Open Source</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Events</div><div><ul>
<li><a href="../event-category/rl-summit.html">Webinars</a></li>
<li><a href="../event-category/rl-summit.html">Meetups</a></li>
<li><a href="../event-category/rl-summit.html">Summits</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Company</div><div><ul>
<li><a href="../about.html">About Us</a></li>
<li><a href="../press.html">News</a></li>
<li><a href="../careers.html">Careers</a></li>
<li><a href="../community.html">Community</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Learn</div><div><ul>
<li><a href="how-ray-solves-common-production-challenges-for-generative-ai.html">Blog</a></li>
<li><a href="../event-category/rl-summit.html">Demos &amp; Webinars</a></li>
<li><a href="../event-category/rl-summit.html">Anyscale Academy</a></li>
<li><a href="https://docs.anyscale.com">Anyscale Docs</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Scalable AI and Python</div><div><ul>
<li><a href="../data-ingestion.html">Data Ingestion</a></li>
<li><a href="../reinforcement-learning.html">Reinforcement Learning</a></li>
<li><a href="../ray-air.html">Ray AIR</a></li>
<li><a href="../model-serving.html">Model Serving</a></li>
<li><a href="../hyperparameter-tuning.html">Hyperparameter Tuning</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Use Cases</div><div><ul>
<li><a href="../demand-forecasting.html">Demand Forecasting/Pricing</a></li>
<li><a href="../industrial-automation.html">Industrial Automation</a></li>
<li><a href="../machine-learning.html">Scalable ML Platforms</a></li>
<li><a href="../natural-language-processing.html">NLP</a></li>
<li><a href="../recommendation-system.html">Recommendation System</a></li>
</ul>
</div></div></div><div class="grid-col-1 grid gap-12 md:grid-cols-3 md:gap-16 lg:hidden"><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Anyscale</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://www.linkedin.com/company/joinanyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M15.784 14.168a1.667 1.667 0 1 1-3.334-.002 1.667 1.667 0 0 1 3.334.002Zm.05 2.9H12.5V27.5h3.334V17.067Zm5.266 0h-3.317V27.5h3.284v-5.475c0-3.05 3.975-3.334 3.975 0V27.5h3.291v-6.609c0-5.141-5.883-4.95-7.266-2.424l.033-1.4Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://www.facebook.com/AnyscaleCompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M21.667 21.247h2.083l.834-3.333h-2.917v-1.667c0-.858 0-1.666 1.666-1.666h1.25v-2.8a23.49 23.49 0 0 0-2.38-.117c-2.263 0-3.87 1.38-3.87 3.917v2.333h-2.5v3.333h2.5v7.084h3.334v-7.084Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://twitter.com/anyscalecompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/anyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Ray</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://twitter.com/raydistributed"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/ray-project/ray"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="md:self-end md:text-right"><div class="font-light leading-normal text-lightBlue-300 opacity-70"><span>© Anyscale, Inc 2022</span></div></div></div></div><div class="pointer-events-none absolute bottom-0 left-0"><svg width="612" height="297" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="hidden lg:block"><g opacity="0.5" clip-path="url(#dots_svg__a)"><mask id="dots_svg__c" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="0" y="0" width="612" height="297"><path transform="rotate(90 611.5 .5)" fill="url(#dots_svg__b)" stroke="#F2F3F7" d="M611.5.5h296v611h-296z"></path></mask><g mask="url(#dots_svg__c)" stroke="#A2B7D4"><g opacity="0.2" stroke-linecap="round"><path d="M598 350v-688M582 350v-688M566 350v-688M550 350v-688M534 350v-688M518 350v-688M502 350v-688M486 350v-688M470 350v-688M454 350v-688M438 350v-688M422 350v-688M406 350v-688M390 350v-688M374 350v-688M358 350v-688M342 350v-688M326 350v-688M310 350v-688M294 350v-688M278 350v-688M262 350v-688M246 350v-688M230 350v-688M214 350v-688M198 350v-688M182 350v-688M166 350v-688M150 350v-688M134 350v-688M118 350v-688M102 350v-688M86 350v-688M70 350v-688M54 350v-688M38 350v-688M22 350v-688M6 350v-688"></path></g><g opacity="0.2" stroke-linecap="round"><path d="M1014 14H0M1014 30H0M1014 46H0M1014 62H0M1014 78H0M1014 94H0M1014 110H0M1014 126H0M1014 142H0M1014 158H0M1014 174H0M1014 190H4M1014 206H4M1014 222H4M1014 238H4M1014 254H4M1014 270H4M1014 286H4"></path></g><path fill="#4C6996" d="M615.5 12.5v3h-3v-3zM615.5 28.5v3h-3v-3zM615.5 44.5v3h-3v-3zM615.5 60.5v3h-3v-3zM615.5 76.5v3h-3v-3zM615.5 92.5v3h-3v-3zM615.5 108.5v3h-3v-3zM615.5 124.5v3h-3v-3zM615.5 140.5v3h-3v-3zM615.5 156.5v3h-3v-3zM615.5 172.5v3h-3v-3zM615.5 188.5v3h-3v-3zM615.5 204.5v3h-3v-3zM615.5 220.5v3h-3v-3zM615.5 236.5v3h-3v-3zM615.5 252.5v3h-3v-3zM615.5 268.5v3h-3v-3zM615.5 284.5v3h-3v-3zM599.5 12.5v3h-3v-3zM599.5 28.5v3h-3v-3zM599.5 44.5v3h-3v-3zM599.5 60.5v3h-3v-3zM599.5 76.5v3h-3v-3zM599.5 92.5v3h-3v-3zM599.5 108.5v3h-3v-3zM599.5 124.5v3h-3v-3zM599.5 140.5v3h-3v-3zM599.5 156.5v3h-3v-3zM599.5 172.5v3h-3v-3zM599.5 188.5v3h-3v-3zM599.5 204.5v3h-3v-3zM599.5 220.5v3h-3v-3zM599.5 236.5v3h-3v-3zM599.5 252.5v3h-3v-3zM599.5 268.5v3h-3v-3zM599.5 284.5v3h-3v-3zM583.5 12.5v3h-3v-3zM583.5 28.5v3h-3v-3zM583.5 44.5v3h-3v-3zM583.5 60.5v3h-3v-3zM583.5 76.5v3h-3v-3zM583.5 92.5v3h-3v-3zM583.5 108.5v3h-3v-3zM583.5 124.5v3h-3v-3zM583.5 140.5v3h-3v-3zM583.5 156.5v3h-3v-3zM583.5 172.5v3h-3v-3zM583.5 188.5v3h-3v-3zM583.5 204.5v3h-3v-3zM583.5 220.5v3h-3v-3zM583.5 236.5v3h-3v-3zM583.5 252.5v3h-3v-3zM583.5 268.5v3h-3v-3zM583.5 284.5v3h-3v-3zM567.5 12.5v3h-3v-3zM567.5 28.5v3h-3v-3zM567.5 44.5v3h-3v-3zM567.5 60.5v3h-3v-3zM567.5 76.5v3h-3v-3zM567.5 92.5v3h-3v-3zM567.5 108.5v3h-3v-3zM567.5 124.5v3h-3v-3zM567.5 140.5v3h-3v-3zM567.5 156.5v3h-3v-3zM567.5 172.5v3h-3v-3zM567.5 188.5v3h-3v-3zM567.5 204.5v3h-3v-3zM567.5 220.5v3h-3v-3zM567.5 236.5v3h-3v-3zM567.5 252.5v3h-3v-3zM567.5 268.5v3h-3v-3zM567.5 284.5v3h-3v-3zM551.5 12.5v3h-3v-3zM551.5 28.5v3h-3v-3zM551.5 44.5v3h-3v-3zM551.5 60.5v3h-3v-3zM551.5 76.5v3h-3v-3zM551.5 92.5v3h-3v-3zM551.5 108.5v3h-3v-3zM551.5 124.5v3h-3v-3zM551.5 140.5v3h-3v-3zM551.5 156.5v3h-3v-3zM551.5 172.5v3h-3v-3zM551.5 188.5v3h-3v-3zM551.5 204.5v3h-3v-3zM551.5 220.5v3h-3v-3zM551.5 236.5v3h-3v-3zM551.5 252.5v3h-3v-3zM551.5 268.5v3h-3v-3zM551.5 284.5v3h-3v-3zM535.5 12.5v3h-3v-3zM535.5 28.5v3h-3v-3zM535.5 44.5v3h-3v-3zM535.5 60.5v3h-3v-3zM535.5 76.5v3h-3v-3zM535.5 92.5v3h-3v-3zM535.5 108.5v3h-3v-3zM535.5 124.5v3h-3v-3zM535.5 140.5v3h-3v-3zM535.5 156.5v3h-3v-3zM535.5 172.5v3h-3v-3zM535.5 188.5v3h-3v-3zM535.5 204.5v3h-3v-3zM535.5 220.5v3h-3v-3zM535.5 236.5v3h-3v-3zM535.5 252.5v3h-3v-3zM535.5 268.5v3h-3v-3zM535.5 284.5v3h-3v-3zM519.5 12.5v3h-3v-3zM519.5 28.5v3h-3v-3zM519.5 44.5v3h-3v-3zM519.5 60.5v3h-3v-3zM519.5 76.5v3h-3v-3zM519.5 92.5v3h-3v-3zM519.5 108.5v3h-3v-3zM519.5 124.5v3h-3v-3zM519.5 140.5v3h-3v-3zM519.5 156.5v3h-3v-3zM519.5 172.5v3h-3v-3zM519.5 188.5v3h-3v-3zM519.5 204.5v3h-3v-3zM519.5 220.5v3h-3v-3zM519.5 236.5v3h-3v-3zM519.5 252.5v3h-3v-3zM519.5 268.5v3h-3v-3zM519.5 284.5v3h-3v-3zM503.5 12.5v3h-3v-3zM503.5 28.5v3h-3v-3zM503.5 44.5v3h-3v-3zM503.5 60.5v3h-3v-3zM503.5 76.5v3h-3v-3zM503.5 92.5v3h-3v-3zM503.5 108.5v3h-3v-3zM503.5 124.5v3h-3v-3zM503.5 140.5v3h-3v-3zM503.5 156.5v3h-3v-3zM503.5 172.5v3h-3v-3zM503.5 188.5v3h-3v-3zM503.5 204.5v3h-3v-3zM503.5 220.5v3h-3v-3zM503.5 236.5v3h-3v-3zM503.5 252.5v3h-3v-3zM503.5 268.5v3h-3v-3zM503.5 284.5v3h-3v-3zM487.5 12.5v3h-3v-3zM487.5 28.5v3h-3v-3zM487.5 44.5v3h-3v-3zM487.5 60.5v3h-3v-3zM487.5 76.5v3h-3v-3zM487.5 92.5v3h-3v-3zM487.5 108.5v3h-3v-3zM487.5 124.5v3h-3v-3zM487.5 140.5v3h-3v-3zM487.5 156.5v3h-3v-3zM487.5 172.5v3h-3v-3zM487.5 188.5v3h-3v-3zM487.5 204.5v3h-3v-3zM487.5 220.5v3h-3v-3zM487.5 236.5v3h-3v-3zM487.5 252.5v3h-3v-3zM487.5 268.5v3h-3v-3zM487.5 284.5v3h-3v-3zM471.5 12.5v3h-3v-3zM471.5 28.5v3h-3v-3zM471.5 44.5v3h-3v-3zM471.5 60.5v3h-3v-3zM471.5 76.5v3h-3v-3zM471.5 92.5v3h-3v-3zM471.5 108.5v3h-3v-3zM471.5 124.5v3h-3v-3zM471.5 140.5v3h-3v-3zM471.5 156.5v3h-3v-3zM471.5 172.5v3h-3v-3zM471.5 188.5v3h-3v-3zM471.5 204.5v3h-3v-3zM471.5 220.5v3h-3v-3zM471.5 236.5v3h-3v-3zM471.5 252.5v3h-3v-3zM471.5 268.5v3h-3v-3zM471.5 284.5v3h-3v-3zM455.5 12.5v3h-3v-3zM455.5 28.5v3h-3v-3zM455.5 44.5v3h-3v-3zM455.5 60.5v3h-3v-3zM455.5 76.5v3h-3v-3zM455.5 92.5v3h-3v-3zM455.5 108.5v3h-3v-3zM455.5 124.5v3h-3v-3zM455.5 140.5v3h-3v-3zM455.5 156.5v3h-3v-3zM455.5 172.5v3h-3v-3zM455.5 188.5v3h-3v-3zM455.5 204.5v3h-3v-3zM455.5 220.5v3h-3v-3zM455.5 236.5v3h-3v-3zM455.5 252.5v3h-3v-3zM455.5 268.5v3h-3v-3zM455.5 284.5v3h-3v-3zM439.5 12.5v3h-3v-3zM439.5 28.5v3h-3v-3zM439.5 44.5v3h-3v-3zM439.5 60.5v3h-3v-3zM439.5 76.5v3h-3v-3zM439.5 92.5v3h-3v-3zM439.5 108.5v3h-3v-3zM439.5 124.5v3h-3v-3zM439.5 140.5v3h-3v-3zM439.5 156.5v3h-3v-3zM439.5 172.5v3h-3v-3zM439.5 188.5v3h-3v-3zM439.5 204.5v3h-3v-3zM439.5 220.5v3h-3v-3zM439.5 236.5v3h-3v-3zM439.5 252.5v3h-3v-3zM439.5 268.5v3h-3v-3zM439.5 284.5v3h-3v-3zM423.5 12.5v3h-3v-3zM423.5 28.5v3h-3v-3zM423.5 44.5v3h-3v-3zM423.5 60.5v3h-3v-3zM423.5 76.5v3h-3v-3zM423.5 92.5v3h-3v-3zM423.5 108.5v3h-3v-3zM423.5 124.5v3h-3v-3zM423.5 140.5v3h-3v-3zM423.5 156.5v3h-3v-3zM423.5 172.5v3h-3v-3zM423.5 188.5v3h-3v-3zM423.5 204.5v3h-3v-3zM423.5 220.5v3h-3v-3zM423.5 236.5v3h-3v-3zM423.5 252.5v3h-3v-3zM423.5 268.5v3h-3v-3zM423.5 284.5v3h-3v-3zM407.5 12.5v3h-3v-3zM407.5 28.5v3h-3v-3zM407.5 44.5v3h-3v-3zM407.5 60.5v3h-3v-3zM407.5 76.5v3h-3v-3zM407.5 92.5v3h-3v-3zM407.5 108.5v3h-3v-3zM407.5 124.5v3h-3v-3zM407.5 140.5v3h-3v-3zM407.5 156.5v3h-3v-3zM407.5 172.5v3h-3v-3zM407.5 188.5v3h-3v-3zM407.5 204.5v3h-3v-3zM407.5 220.5v3h-3v-3zM407.5 236.5v3h-3v-3zM407.5 252.5v3h-3v-3zM407.5 268.5v3h-3v-3zM407.5 284.5v3h-3v-3zM391.5 12.5v3h-3v-3zM391.5 28.5v3h-3v-3zM391.5 44.5v3h-3v-3zM391.5 60.5v3h-3v-3zM391.5 76.5v3h-3v-3zM391.5 92.5v3h-3v-3zM391.5 108.5v3h-3v-3zM391.5 124.5v3h-3v-3zM391.5 140.5v3h-3v-3zM391.5 156.5v3h-3v-3zM391.5 172.5v3h-3v-3zM391.5 188.5v3h-3v-3zM391.5 204.5v3h-3v-3zM391.5 220.5v3h-3v-3zM391.5 236.5v3h-3v-3zM391.5 252.5v3h-3v-3zM391.5 268.5v3h-3v-3zM391.5 284.5v3h-3v-3zM375.5 12.5v3h-3v-3zM375.5 28.5v3h-3v-3zM375.5 44.5v3h-3v-3zM375.5 60.5v3h-3v-3zM375.5 76.5v3h-3v-3zM375.5 92.5v3h-3v-3zM375.5 108.5v3h-3v-3zM375.5 124.5v3h-3v-3zM375.5 140.5v3h-3v-3zM375.5 156.5v3h-3v-3zM375.5 172.5v3h-3v-3zM375.5 188.5v3h-3v-3zM375.5 204.5v3h-3v-3zM375.5 220.5v3h-3v-3z"></path><rect x="375.5" y="236.5" width="3" height="3" rx="1.5" transform="rotate(90 375.5 236.5)" fill="#4C6996"></rect><path fill="#4C6996" d="M375.5 252.5v3h-3v-3zM375.5 268.5v3h-3v-3zM375.5 284.5v3h-3v-3zM359.5 12.5v3h-3v-3zM359.5 28.5v3h-3v-3zM359.5 44.5v3h-3v-3zM359.5 60.5v3h-3v-3zM359.5 76.5v3h-3v-3zM359.5 92.5v3h-3v-3zM359.5 108.5v3h-3v-3zM359.5 124.5v3h-3v-3zM359.5 140.5v3h-3v-3zM359.5 156.5v3h-3v-3zM359.5 172.5v3h-3v-3zM359.5 188.5v3h-3v-3zM359.5 204.5v3h-3v-3zM359.5 220.5v3h-3v-3zM359.5 236.5v3h-3v-3zM359.5 252.5v3h-3v-3zM359.5 268.5v3h-3v-3zM359.5 284.5v3h-3v-3zM343.5 12.5v3h-3v-3zM343.5 28.5v3h-3v-3zM343.5 44.5v3h-3v-3zM343.5 60.5v3h-3v-3zM343.5 76.5v3h-3v-3zM343.5 92.5v3h-3v-3zM343.5 108.5v3h-3v-3zM343.5 124.5v3h-3v-3zM343.5 140.5v3h-3v-3zM343.5 156.5v3h-3v-3zM343.5 172.5v3h-3v-3zM343.5 188.5v3h-3v-3zM343.5 204.5v3h-3v-3zM343.5 220.5v3h-3v-3zM343.5 236.5v3h-3v-3zM343.5 252.5v3h-3v-3zM343.5 268.5v3h-3v-3zM343.5 284.5v3h-3v-3zM327.5 12.5v3h-3v-3zM327.5 28.5v3h-3v-3zM327.5 44.5v3h-3v-3zM327.5 60.5v3h-3v-3zM327.5 76.5v3h-3v-3zM327.5 92.5v3h-3v-3zM327.5 108.5v3h-3v-3zM327.5 124.5v3h-3v-3zM327.5 140.5v3h-3v-3zM327.5 156.5v3h-3v-3zM327.5 172.5v3h-3v-3zM327.5 188.5v3h-3v-3zM327.5 204.5v3h-3v-3zM327.5 220.5v3h-3v-3zM327.5 236.5v3h-3v-3zM327.5 252.5v3h-3v-3zM327.5 268.5v3h-3v-3zM327.5 284.5v3h-3v-3zM311.5 12.5v3h-3v-3zM311.5 28.5v3h-3v-3zM311.5 44.5v3h-3v-3zM311.5 60.5v3h-3v-3zM311.5 76.5v3h-3v-3zM311.5 92.5v3h-3v-3zM311.5 108.5v3h-3v-3zM311.5 124.5v3h-3v-3zM311.5 140.5v3h-3v-3zM311.5 156.5v3h-3v-3zM311.5 172.5v3h-3v-3zM311.5 188.5v3h-3v-3zM311.5 204.5v3h-3v-3zM311.5 220.5v3h-3v-3zM311.5 236.5v3h-3v-3zM311.5 252.5v3h-3v-3zM311.5 268.5v3h-3v-3zM311.5 284.5v3h-3v-3zM295.5 12.5v3h-3v-3zM295.5 28.5v3h-3v-3zM295.5 44.5v3h-3v-3zM295.5 60.5v3h-3v-3zM295.5 76.5v3h-3v-3zM295.5 92.5v3h-3v-3zM295.5 108.5v3h-3v-3zM295.5 124.5v3h-3v-3zM295.5 140.5v3h-3v-3zM295.5 156.5v3h-3v-3zM295.5 172.5v3h-3v-3zM295.5 188.5v3h-3v-3zM295.5 204.5v3h-3v-3zM295.5 220.5v3h-3v-3zM295.5 236.5v3h-3v-3zM295.5 252.5v3h-3v-3zM295.5 268.5v3h-3v-3zM295.5 284.5v3h-3v-3zM279.5 12.5v3h-3v-3zM279.5 28.5v3h-3v-3zM279.5 44.5v3h-3v-3zM279.5 60.5v3h-3v-3zM279.5 76.5v3h-3v-3zM279.5 92.5v3h-3v-3zM279.5 108.5v3h-3v-3zM279.5 124.5v3h-3v-3zM279.5 140.5v3h-3v-3zM279.5 156.5v3h-3v-3zM279.5 172.5v3h-3v-3zM279.5 188.5v3h-3v-3zM279.5 204.5v3h-3v-3zM279.5 220.5v3h-3v-3zM279.5 236.5v3h-3v-3zM279.5 252.5v3h-3v-3zM279.5 268.5v3h-3v-3zM279.5 284.5v3h-3v-3zM263.5 12.5v3h-3v-3zM263.5 28.5v3h-3v-3zM263.5 44.5v3h-3v-3zM263.5 60.5v3h-3v-3zM263.5 76.5v3h-3v-3zM263.5 92.5v3h-3v-3zM263.5 108.5v3h-3v-3zM263.5 124.5v3h-3v-3zM263.5 140.5v3h-3v-3zM263.5 156.5v3h-3v-3zM263.5 172.5v3h-3v-3zM263.5 188.5v3h-3v-3zM263.5 204.5v3h-3v-3zM263.5 220.5v3h-3v-3zM263.5 236.5v3h-3v-3zM263.5 252.5v3h-3v-3zM263.5 268.5v3h-3v-3zM263.5 284.5v3h-3v-3zM247.5 12.5v3h-3v-3zM247.5 28.5v3h-3v-3zM247.5 44.5v3h-3v-3zM247.5 60.5v3h-3v-3zM247.5 76.5v3h-3v-3zM247.5 92.5v3h-3v-3zM247.5 108.5v3h-3v-3zM247.5 124.5v3h-3v-3zM247.5 140.5v3h-3v-3zM247.5 156.5v3h-3v-3zM247.5 172.5v3h-3v-3zM247.5 188.5v3h-3v-3zM247.5 204.5v3h-3v-3zM247.5 220.5v3h-3v-3zM247.5 236.5v3h-3v-3zM247.5 252.5v3h-3v-3zM247.5 268.5v3h-3v-3zM247.5 284.5v3h-3v-3zM231.5 12.5v3h-3v-3zM231.5 28.5v3h-3v-3zM231.5 44.5v3h-3v-3zM231.5 60.5v3h-3v-3zM231.5 76.5v3h-3v-3zM231.5 92.5v3h-3v-3zM231.5 108.5v3h-3v-3zM231.5 124.5v3h-3v-3zM231.5 140.5v3h-3v-3zM231.5 156.5v3h-3v-3zM231.5 172.5v3h-3v-3zM231.5 188.5v3h-3v-3zM231.5 204.5v3h-3v-3zM231.5 220.5v3h-3v-3zM231.5 236.5v3h-3v-3zM231.5 252.5v3h-3v-3zM231.5 268.5v3h-3v-3zM231.5 284.5v3h-3v-3zM215.5 12.5v3h-3v-3zM215.5 28.5v3h-3v-3zM215.5 44.5v3h-3v-3zM215.5 60.5v3h-3v-3zM215.5 76.5v3h-3v-3zM215.5 92.5v3h-3v-3zM215.5 108.5v3h-3v-3zM215.5 124.5v3h-3v-3zM215.5 140.5v3h-3v-3zM215.5 156.5v3h-3v-3zM215.5 172.5v3h-3v-3zM215.5 188.5v3h-3v-3zM215.5 204.5v3h-3v-3zM215.5 220.5v3h-3v-3zM215.5 236.5v3h-3v-3zM215.5 252.5v3h-3v-3zM215.5 268.5v3h-3v-3zM215.5 284.5v3h-3v-3zM199.5 12.5v3h-3v-3zM199.5 28.5v3h-3v-3zM199.5 44.5v3h-3v-3zM199.5 60.5v3h-3v-3zM199.5 76.5v3h-3v-3zM199.5 92.5v3h-3v-3zM199.5 108.5v3h-3v-3zM199.5 124.5v3h-3v-3zM199.5 140.5v3h-3v-3zM199.5 156.5v3h-3v-3zM199.5 172.5v3h-3v-3zM199.5 188.5v3h-3v-3zM199.5 204.5v3h-3v-3zM199.5 220.5v3h-3v-3zM199.5 236.5v3h-3v-3zM199.5 252.5v3h-3v-3zM199.5 268.5v3h-3v-3zM199.5 284.5v3h-3v-3zM183.5 12.5v3h-3v-3zM183.5 28.5v3h-3v-3zM183.5 44.5v3h-3v-3zM183.5 60.5v3h-3v-3zM183.5 76.5v3h-3v-3zM183.5 92.5v3h-3v-3zM183.5 108.5v3h-3v-3zM183.5 124.5v3h-3v-3zM183.5 140.5v3h-3v-3zM183.5 156.5v3h-3v-3zM183.5 172.5v3h-3v-3zM183.5 188.5v3h-3v-3zM183.5 204.5v3h-3v-3zM183.5 220.5v3h-3v-3zM183.5 236.5v3h-3v-3zM183.5 252.5v3h-3v-3zM183.5 268.5v3h-3v-3zM183.5 284.5v3h-3v-3zM167.5 12.5v3h-3v-3zM167.5 28.5v3h-3v-3zM167.5 44.5v3h-3v-3zM167.5 60.5v3h-3v-3zM167.5 76.5v3h-3v-3zM167.5 92.5v3h-3v-3zM167.5 108.5v3h-3v-3zM167.5 124.5v3h-3v-3zM167.5 140.5v3h-3v-3zM167.5 156.5v3h-3v-3zM167.5 172.5v3h-3v-3zM167.5 188.5v3h-3v-3zM167.5 204.5v3h-3v-3zM167.5 220.5v3h-3v-3zM167.5 236.5v3h-3v-3zM167.5 252.5v3h-3v-3zM167.5 268.5v3h-3v-3zM167.5 284.5v3h-3v-3zM151.5 12.5v3h-3v-3zM151.5 28.5v3h-3v-3zM151.5 44.5v3h-3v-3zM151.5 60.5v3h-3v-3zM151.5 76.5v3h-3v-3zM151.5 92.5v3h-3v-3zM151.5 108.5v3h-3v-3zM151.5 124.5v3h-3v-3zM151.5 140.5v3h-3v-3zM151.5 156.5v3h-3v-3zM151.5 172.5v3h-3v-3zM151.5 188.5v3h-3v-3zM151.5 204.5v3h-3v-3zM151.5 220.5v3h-3v-3zM151.5 236.5v3h-3v-3zM151.5 252.5v3h-3v-3zM151.5 268.5v3h-3v-3zM151.5 284.5v3h-3v-3zM135.5 12.5v3h-3v-3zM135.5 28.5v3h-3v-3zM135.5 44.5v3h-3v-3zM135.5 60.5v3h-3v-3zM135.5 76.5v3h-3v-3zM135.5 92.5v3h-3v-3zM135.5 108.5v3h-3v-3zM135.5 124.5v3h-3v-3zM135.5 140.5v3h-3v-3zM135.5 156.5v3h-3v-3zM135.5 172.5v3h-3v-3zM135.5 188.5v3h-3v-3zM135.5 204.5v3h-3v-3zM135.5 220.5v3h-3v-3zM135.5 236.5v3h-3v-3zM135.5 252.5v3h-3v-3zM135.5 268.5v3h-3v-3zM135.5 284.5v3h-3v-3zM119.5 12.5v3h-3v-3zM119.5 28.5v3h-3v-3zM119.5 44.5v3h-3v-3zM119.5 60.5v3h-3v-3zM119.5 76.5v3h-3v-3zM119.5 92.5v3h-3v-3zM119.5 108.5v3h-3v-3zM119.5 124.5v3h-3v-3zM119.5 140.5v3h-3v-3zM119.5 156.5v3h-3v-3zM119.5 172.5v3h-3v-3zM119.5 188.5v3h-3v-3zM119.5 204.5v3h-3v-3zM119.5 220.5v3h-3v-3zM119.5 236.5v3h-3v-3zM119.5 252.5v3h-3v-3zM119.5 268.5v3h-3v-3zM119.5 284.5v3h-3v-3zM103.5 12.5v3h-3v-3zM103.5 28.5v3h-3v-3zM103.5 44.5v3h-3v-3zM103.5 60.5v3h-3v-3zM103.5 76.5v3h-3v-3zM103.5 92.5v3h-3v-3zM103.5 108.5v3h-3v-3zM103.5 124.5v3h-3v-3zM103.5 140.5v3h-3v-3zM103.5 156.5v3h-3v-3zM103.5 172.5v3h-3v-3zM103.5 188.5v3h-3v-3zM103.5 204.5v3h-3v-3zM103.5 220.5v3h-3v-3zM103.5 236.5v3h-3v-3zM103.5 252.5v3h-3v-3zM103.5 268.5v3h-3v-3zM103.5 284.5v3h-3v-3zM87.5 12.5v3h-3v-3zM87.5 28.5v3h-3v-3zM87.5 44.5v3h-3v-3zM87.5 60.5v3h-3v-3zM87.5 76.5v3h-3v-3zM87.5 92.5v3h-3v-3zM87.5 108.5v3h-3v-3zM87.5 124.5v3h-3v-3zM87.5 140.5v3h-3v-3zM87.5 156.5v3h-3v-3zM87.5 172.5v3h-3v-3zM87.5 188.5v3h-3v-3zM87.5 204.5v3h-3v-3zM87.5 220.5v3h-3v-3zM87.5 236.5v3h-3v-3zM87.5 252.5v3h-3v-3zM87.5 268.5v3h-3v-3zM87.5 284.5v3h-3v-3zM71.5 12.5v3h-3v-3zM71.5 28.5v3h-3v-3zM71.5 44.5v3h-3v-3zM71.5 60.5v3h-3v-3zM71.5 76.5v3h-3v-3zM71.5 92.5v3h-3v-3zM71.5 108.5v3h-3v-3zM71.5 124.5v3h-3v-3zM71.5 140.5v3h-3v-3zM71.5 156.5v3h-3v-3zM71.5 172.5v3h-3v-3zM71.5 188.5v3h-3v-3zM71.5 204.5v3h-3v-3zM71.5 220.5v3h-3v-3zM71.5 236.5v3h-3v-3zM71.5 252.5v3h-3v-3zM71.5 268.5v3h-3v-3zM71.5 284.5v3h-3v-3zM55.5 12.5v3h-3v-3zM55.5 28.5v3h-3v-3zM55.5 44.5v3h-3v-3zM55.5 60.5v3h-3v-3zM55.5 76.5v3h-3v-3zM55.5 92.5v3h-3v-3zM55.5 108.5v3h-3v-3zM55.5 124.5v3h-3v-3zM55.5 140.5v3h-3v-3zM55.5 156.5v3h-3v-3zM55.5 172.5v3h-3v-3zM55.5 188.5v3h-3v-3zM55.5 204.5v3h-3v-3zM55.5 220.5v3h-3v-3zM55.5 236.5v3h-3v-3zM55.5 252.5v3h-3v-3zM55.5 268.5v3h-3v-3zM55.5 284.5v3h-3v-3zM39.5 12.5v3h-3v-3zM39.5 28.5v3h-3v-3zM39.5 44.5v3h-3v-3zM39.5 60.5v3h-3v-3zM39.5 76.5v3h-3v-3zM39.5 92.5v3h-3v-3zM39.5 108.5v3h-3v-3zM39.5 124.5v3h-3v-3zM39.5 140.5v3h-3v-3zM39.5 156.5v3h-3v-3zM39.5 172.5v3h-3v-3zM39.5 188.5v3h-3v-3zM39.5 204.5v3h-3v-3zM39.5 220.5v3h-3v-3zM39.5 236.5v3h-3v-3zM39.5 252.5v3h-3v-3zM39.5 268.5v3h-3v-3zM39.5 284.5v3h-3v-3zM23.5 12.5v3h-3v-3zM23.5 28.5v3h-3v-3zM23.5 44.5v3h-3v-3zM23.5 60.5v3h-3v-3zM23.5 76.5v3h-3v-3zM23.5 92.5v3h-3v-3zM23.5 108.5v3h-3v-3zM23.5 124.5v3h-3v-3zM23.5 140.5v3h-3v-3zM23.5 156.5v3h-3v-3zM23.5 172.5v3h-3v-3zM23.5 188.5v3h-3v-3zM23.5 204.5v3h-3v-3zM23.5 220.5v3h-3v-3zM23.5 236.5v3h-3v-3zM23.5 252.5v3h-3v-3zM23.5 268.5v3h-3v-3zM23.5 284.5v3h-3v-3zM7.5 12.5v3h-3v-3zM7.5 28.5v3h-3v-3zM7.5 44.5v3h-3v-3zM7.5 60.5v3h-3v-3zM7.5 76.5v3h-3v-3zM7.5 92.5v3h-3v-3zM7.5 108.5v3h-3v-3zM7.5 124.5v3h-3v-3zM7.5 140.5v3h-3v-3zM7.5 156.5v3h-3v-3zM7.5 172.5v3h-3v-3zM7.5 188.5v3h-3v-3zM7.5 204.5v3h-3v-3zM7.5 220.5v3h-3v-3zM7.5 236.5v3h-3v-3zM7.5 252.5v3h-3v-3zM7.5 268.5v3h-3v-3zM7.5 284.5v3h-3v-3z"></path></g></g><defs><radialGradient id="dots_svg__b" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="rotate(-136.519 627.256 92.637) scale(929.292 677.674)"><stop stop-color="#D9D9D9"></stop><stop offset="0.574" stop-color="#D9D9D9" stop-opacity="0"></stop></radialGradient><clipPath id="dots_svg__a"><path fill="#fff" transform="rotate(90 306 306)" d="M0 0h297v612H0z"></path></clipPath></defs></svg></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"site":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ke4xTi5OD9gnn2YtRXyCV","type":"Entry","createdAt":"2022-10-14T08:31:23.161Z","updatedAt":"2023-06-06T20:01:24.167Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"site"}},"locale":"en-US"},"fields":{"title":"Anyscale","description":"From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab.","identifier":"site-master-new","footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"16eb9I0yhaIALNOPR8WKTY","type":"Entry","createdAt":"2020-09-27T16:03:59.581Z","updatedAt":"2022-10-27T01:20:31.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer","body":"© Anyscale, Inc 2022","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"C7AXPFHtleWPOZ9BuT73K","type":"Asset","createdAt":"2020-09-28T14:40:50.925Z","updatedAt":"2020-09-28T14:40:50.925Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"anyscale_logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/C7AXPFHtleWPOZ9BuT73K/927f5130df1357db041d7a7908ab572e/Graphic_only__transparent_background_.png","details":{"size":1415,"image":{"width":331,"height":331}},"fileName":"Graphic only (transparent background).png","contentType":"image/png"}}}],"subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1cDdKq2eUfklc6w52dU6dg","type":"Entry","createdAt":"2020-09-27T16:05:42.307Z","updatedAt":"2022-10-27T01:20:26.714Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-1","header":"Products","body":"- [Anyscale Compute Platform](/platform)\n- [Ray Open Source](/ray-open-source)\n"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5tpXhcAJwSdhX8Wqw9HYTt","type":"Entry","createdAt":"2020-09-27T16:06:37.409Z","updatedAt":"2022-10-27T01:20:21.887Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":10,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-2","header":"Events","body":"- [Webinars](/events)\n- [Meetups](/events)\n- [Summits](/events)\n"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3c3LEomScM93hIg3i2zItU","type":"Entry","createdAt":"2020-09-27T16:07:42.856Z","updatedAt":"2022-10-27T01:20:18.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-3","header":"Company","body":"- [About Us](/about)\n- [News](/press)\n- [Careers](/careers)\n- [Community](/community)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6du4SSXPTxDZKoIVNHVVnT","type":"Entry","createdAt":"2022-10-27T01:20:13.320Z","updatedAt":"2022-10-27T01:20:13.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-4","header":"Learn","body":"- [Blog](/blog)\n- [Demos \u0026 Webinars](/events)\n- [Anyscale Academy](/events)\n- [Anyscale Docs](https://docs.anyscale.com)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1hovCXhGcWlgJmrqNzJ79k","type":"Entry","createdAt":"2022-10-27T01:20:09.895Z","updatedAt":"2022-10-27T01:20:09.895Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-5","header":"Scalable AI and Python","body":"- [Data Ingestion](/data-ingestion)\n- [Reinforcement Learning](/reinforcement-learning)\n- [Ray AIR](/ray-air)\n- [Model Serving](/model-serving)\n- [Hyperparameter Tuning](/hyperparameter-tuning)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"YFy5fUqwOD5ShdMaYEvzB","type":"Entry","createdAt":"2022-10-27T01:20:06.631Z","updatedAt":"2022-10-27T01:20:06.631Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-6","header":"Use Cases","body":"- [Demand Forecasting/Pricing](/demand-forecasting)\n- [Industrial Automation](/industrial-automation)\n- [Scalable ML Platforms](/machine-learning)\n- [NLP](/natural-language-processing)\n- [Recommendation System](/recommendation-system)"}}]}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"KBAZ5OavkdZfNTVjdJkp3","type":"Entry","createdAt":"2022-10-14T10:47:29.682Z","updatedAt":"2022-10-14T10:47:29.682Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Products","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KNN1Kmzo7jxSlxX1k74XQ","type":"Entry","createdAt":"2022-10-07T13:14:07.892Z","updatedAt":"2022-10-07T13:14:07.892Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"52Mmru1brS6ZkmItPeIW86","type":"Entry","createdAt":"2022-10-07T13:13:22.198Z","updatedAt":"2022-11-03T01:33:41.860Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Anyscale Platform","link":"/platform"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6cN6ZwdiFNiPcqIhd7EiQY","type":"Entry","createdAt":"2022-10-07T13:13:49.673Z","updatedAt":"2022-10-07T13:13:49.673Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Open Source","link":"/ray-open-source"}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5R1OTMfuLnPqont0Fjn9Xb","type":"Entry","createdAt":"2022-10-07T13:20:19.578Z","updatedAt":"2022-10-14T10:43:21.550Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Solutions","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1mp3IF3ZJE5JAViu3L9E95","type":"Entry","createdAt":"2022-10-07T13:16:00.346Z","updatedAt":"2022-10-07T13:21:01.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Scalable AI and Python","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4VUyKorAkgQP2V295h8eW3","type":"Entry","createdAt":"2022-10-07T13:15:35.949Z","updatedAt":"2022-10-07T13:15:35.949Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Data Ingestion","link":"/data-ingestion"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"Ouf5WgAzYmLovCZFNWqYm","type":"Entry","createdAt":"2022-10-07T13:17:09.728Z","updatedAt":"2022-10-07T13:17:09.728Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Reinforcement Learning","link":"/reinforcement-learning"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5kVPKqUbXBbKmrkvsImcIi","type":"Entry","createdAt":"2022-10-07T13:17:50.307Z","updatedAt":"2022-10-07T13:17:50.307Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray AIR","link":"/ray-air"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5uUidEvWw1oqjWrRcskEW","type":"Entry","createdAt":"2022-10-07T13:18:21.246Z","updatedAt":"2022-10-07T13:18:21.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Model Serving","link":"/model-serving"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Rbw0CrBN1KecPafCPvF4H","type":"Entry","createdAt":"2022-10-07T13:18:45.601Z","updatedAt":"2022-10-07T13:18:45.601Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Hyperparameter Tuning","link":"/hyperparameter-tuning"}}],"card":"##### Scalable AI and Python\n\nLeverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more."}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2RgWCuIiZe2TI6Nx5Ns7sB","type":"Entry","createdAt":"2022-10-07T13:25:03.101Z","updatedAt":"2023-05-31T12:47:25.274Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Use Cases","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3XMbpU1mPdsU7gktltlAzX","type":"Entry","createdAt":"2023-05-31T12:43:54.011Z","updatedAt":"2023-05-31T12:43:54.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Large Language Models","link":"/large-language-models"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6MAYw6HTSmH5JHcOoYjCzC","type":"Entry","createdAt":"2022-10-07T13:22:29.165Z","updatedAt":"2022-10-07T13:22:29.165Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Demand Forecasting/Pricing","link":"/demand-forecasting"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"30BlKhv6h4neaZwjA2jlxO","type":"Entry","createdAt":"2022-10-07T13:23:09.471Z","updatedAt":"2022-10-07T13:23:09.471Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Industrial Automation","link":"/industrial-automation"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2gGwUVxDuBeUso1uQH2gEW","type":"Entry","createdAt":"2022-10-07T13:23:32.145Z","updatedAt":"2022-10-07T13:23:32.145Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Scalable ML Platforms","link":"/machine-learning"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1cbwbmIvP5ZkV9iNEraQGP","type":"Entry","createdAt":"2022-10-07T13:23:59.732Z","updatedAt":"2022-10-07T13:23:59.732Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"NLP","link":"/natural-language-processing"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2A7FMbckpN5bgW1wp5ZuSh","type":"Entry","createdAt":"2022-10-07T13:24:41.861Z","updatedAt":"2022-10-07T13:24:41.861Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Recommendation System","link":"/recommendation-system"}}],"card":"##### Use Cases\n\nLeverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more."}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QNdrsmqz82oiBJvFTQ26n","type":"Entry","createdAt":"2023-04-13T11:37:45.354Z","updatedAt":"2023-04-13T11:37:45.354Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ecosystem","link":"/ray-ecosystem"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i0nHjF3pmjzGAdEE4V5MG","type":"Entry","createdAt":"2021-12-05T02:08:18.719Z","updatedAt":"2022-03-03T02:01:08.894Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Success Stories","link":"/user-stories"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5mFZvzY56aQCY4xvlBDi9W","type":"Entry","createdAt":"2022-10-14T10:46:35.218Z","updatedAt":"2022-10-14T10:46:35.218Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Learn","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"32tQWjMRAI5qci97W3cuie","type":"Entry","createdAt":"2022-10-07T13:28:46.057Z","updatedAt":"2023-03-22T20:31:55.243Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JQbOCOkxHfH2ZPGNNQVbt","type":"Entry","createdAt":"2021-12-05T14:57:28.664Z","updatedAt":"2021-12-05T14:57:28.664Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Blog","link":"/blog"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4qEkJgYkXZmiDq3lsxg0xW","type":"Entry","createdAt":"2023-03-22T20:31:21.666Z","updatedAt":"2023-03-22T20:31:21.666Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Summit 2023","link":"https://raysummit.anyscale.com/?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=home_nav-pulldown"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48sdeWXptwt3UgEZzeTt3k","type":"Entry","createdAt":"2022-10-07T13:26:28.331Z","updatedAt":"2022-10-07T13:26:28.331Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Videos \u0026 Webinars","link":"/events"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6hMfRHFDZUXcJH1tIq0xhY","type":"Entry","createdAt":"2021-10-21T20:59:36.918Z","updatedAt":"2023-03-06T20:24:26.811Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Training","link":"https://github.com/ray-project/ray-educational-materials"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cVWy7hmqbAzJCW6d7cLUx","type":"Entry","createdAt":"2022-10-07T13:28:33.537Z","updatedAt":"2022-10-07T13:28:33.537Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Anyscale Docs","link":"https://docs.anyscale.com"}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Qjs7zNfhrinHTNnzMpVIO","type":"Entry","createdAt":"2022-10-14T10:44:42.403Z","updatedAt":"2022-10-14T10:44:42.403Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Company","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ZFDrG8YGgU0oyNgBOId7X","type":"Entry","createdAt":"2022-10-07T13:32:38.746Z","updatedAt":"2022-10-07T13:32:38.746Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2RD65wmpn4gmL5Bvxu5VOD","type":"Entry","createdAt":"2020-09-08T17:53:32.413Z","updatedAt":"2021-06-20T15:04:11.482Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"About us","link":"/about"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71EHE1t3QrjnQb8hD9O8Ww","type":"Entry","createdAt":"2022-10-07T13:32:07.593Z","updatedAt":"2022-10-07T13:32:07.593Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"News","link":"/press"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3TNUPaz7ViQ6hbiZciBOQV","type":"Entry","createdAt":"2020-09-28T23:19:39.838Z","updatedAt":"2022-05-12T19:18:18.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Careers","link":"/careers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ubc5oYY5moS7HexIp7AjO","type":"Entry","createdAt":"2022-07-22T20:40:53.600Z","updatedAt":"2022-07-22T20:40:53.600Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Community","link":"/community"}}]}}]}}],"bannerText":"Are you passionate about Ray, scalable AI and distributed computing? Join us at Ray Summit!","bannerLink":"https://raysummit.anyscale.com/?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=homepage_ticker","bannerCta":" Find out more! "}},"title":"An Introduction to Reinforcement Learning with OpenAI Gym, RLlib, and Google Colab","slug":"an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4MRdnDXbzFHCnXPJLKM6yu","type":"Entry","createdAt":"2021-01-19T01:14:31.681Z","updatedAt":"2021-01-19T01:14:31.681Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Sven Mika","slug":"sven-mika","link":"https://twitter.com/sven_mika"}}],"publishedDate":"2021-08-26","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"An introductory tutorial on reinforcement learning with OpenAI Gym, RLlib, and Google Colab. ","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4vx7G22RlSS9WfUsYRnWXP","type":"Entry","createdAt":"2021-08-26T02:23:06.709Z","updatedAt":"2021-08-26T02:23:06.709Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"CartPole Video Embed","videoUrl":"https://www.youtube.com/watch?v=XiigTGKZfks\u0026t=106s ","caption":"This tutorial will use reinforcement learning (RL) to help balance a virtual CartPole. The [video](https://www.youtube.com/watch?v=XiigTGKZfks\u0026t=106s ) above from PilcoLearner shows the results of using RL in a real-life CartPole environment."}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"One possible definition of reinforcement learning (RL) is a computational approach to learning how to maximize the total sum of rewards when interacting with an environment. While a definition is useful, this tutorial aims to illustrate what reinforcement learning is through images, code, and video examples and along the way introduce reinforcement learning terms like agents and environments.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In particular, this tutorial explores:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"What is Reinforcement Learning ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The OpenAI Gym CartPole Environment","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The Role of Agents in Reinforcement Learning","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"How to Train an Agent by using the Python Library RLlib","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"How to use a GPU to Speed Up Training","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Hyperparameter Tuning with Ray Tune","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What is Reinforcement Learning","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/reinforcement-learning-with-rllib-in-the-unity-game-engine"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"previous post noted","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", machine learning (ML), a sub-field of AI, uses neural networks or other types of mathematical models to learn how to interpret complex patterns. Two areas of ML that have recently become very popular due to their high level of maturity are supervised learning (SL), in which neural networks learn to make predictions based on large amounts of data, and reinforcement learning (RL), where the networks learn to make good action decisions in a trial-and-error fashion, using a simulator.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1uWlGSCvzjk6hY3PNw5NdA","type":"Asset","createdAt":"2021-08-25T20:35:29.798Z","updatedAt":"2021-08-25T20:35:29.798Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"intro-to-rl-1","file":{"url":"//images.ctfassets.net/xjan103pcp94/1uWlGSCvzjk6hY3PNw5NdA/92e09a3d2351f8672b3bb46df9144cc7/Blog_-_intro_reinforcement.png","details":{"size":102498,"image":{"width":512,"height":417}},"fileName":"Blog - intro reinforcement.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"RL is the tech behind mind-boggling successes such as DeepMind’s AlphaGo Zero and the StarCraft II AI (AlphaStar) or OpenAI’s DOTA 2 AI (“OpenAI Five”). Note that there are ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"many impressive uses of reinforcement learning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the reason why it is so powerful and promising for real-life decision making problems is because RL is capable of learning continuously — sometimes even in ever changing environments — starting with no knowledge of which decisions to make whatsoever (random behavior). ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1lBFXrIrR4PZT2kwg1ewQW","type":"Entry","createdAt":"2022-02-17T18:22:16.717Z","updatedAt":"2022-02-17T23:55:42.067Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"rl-summit-cta","body":"\u003ca href=\"/production-rl-summit#agenda?utm_source=anyscale\u0026utm_medium=blog\u0026utm_content=openaigym-blog\u0026utm_campaign=rl_summit\"\u003e\u003cimg src=\"//images.ctfassets.net/xjan103pcp94/1hu1Zwg1RQ0ifg9Lfp98bA/7d913772436d45666a80e83f629b4815/RL_Summit__12_.png\" style=\"width: 100%\"\u003e\u003c/a\u003e"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Agents and Environments","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5h5ZwNAqLHAIRZ9jPGvRU1","type":"Asset","createdAt":"2021-08-25T20:48:58.153Z","updatedAt":"2021-08-25T20:48:58.153Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"agents-and-environments","file":{"url":"//images.ctfassets.net/xjan103pcp94/5h5ZwNAqLHAIRZ9jPGvRU1/6ceb65f718883cf2e7b8ca9dcd0a5fc4/Blog_-_intro_reinforcement_2.png","details":{"size":81263,"image":{"width":512,"height":309}},"fileName":"Blog - intro reinforcement 2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The diagram above shows the interactions and communications between an agent and an environment. In reinforcement learning, one or more agents interact within an environment which may be either a simulation like CartPole in this tutorial or a connection to real-world sensors and actuators. At each step, the agent receives an observation (i.e., the state of the environment), takes an action, and usually receives a reward (the frequency at which an agent receives a reward depends on a given task or problem). Agents learn from repeated trials, and a sequence of those is called an episode — the sequence of actions from an initial observation up to either a “success” or “failure” causing the environment to reach its “done” state. The learning portion of an RL framework trains a policy about which actions (i.e., sequential decisions) cause agents to maximize their long-term, cumulative rewards. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The OpenAI Gym Cartpole Environment","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4hLHnMXJN2EwwAXq2yYx9v","type":"Asset","createdAt":"2021-09-02T17:53:23.032Z","updatedAt":"2021-09-02T17:54:36.660Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"CartPole Remade","description":"CartPole","file":{"url":"//images.ctfassets.net/xjan103pcp94/4hLHnMXJN2EwwAXq2yYx9v/41b16121290d6c46b6b85492a572a4cf/cartPoleRemade.png","details":{"size":29009,"image":{"width":1052,"height":781}},"fileName":"cartPoleRemade.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The problem we are trying to solve is trying to keep a pole upright. Specifically, the pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1g6s6IMlS4IqUCZrpXZGUM","type":"Asset","createdAt":"2021-08-25T20:56:09.733Z","updatedAt":"2021-08-25T20:56:09.733Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"policy ","file":{"url":"//images.ctfassets.net/xjan103pcp94/1g6s6IMlS4IqUCZrpXZGUM/2493f114da86c08108f3b763343e83a9/Blog_-_intro_reinforcement_4.png","details":{"size":81222,"image":{"width":512,"height":310}},"fileName":"Blog - intro reinforcement 4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Rather than code this environment from scratch, this tutorial will use ","nodeType":"text"},{"data":{"uri":"http://gym.openai.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"OpenAI Gym","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" which is a toolkit that provides a wide variety of simulated environments (Atari games, board games, 2D and 3D physical simulations, and so on). Gym makes no assumptions about the structure of your agent (what pushes the cart left or right in this cartpole example), and is compatible with any numerical computation library, such as numpy.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The code below loads the cartpole environment.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"248NFtYGwnp83dCT5z5g2O","type":"Entry","createdAt":"2021-08-26T02:26:23.264Z","updatedAt":"2021-08-26T02:26:23.264Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"import gym rllib","body":"import gym\nenv = gym.make(\"CartPole-v0\")","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's now start to understand this environment by looking at the action space.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3tOSAPhzPsECwFrbpl9rmr","type":"Entry","createdAt":"2021-08-26T02:27:02.351Z","updatedAt":"2021-08-26T02:27:26.263Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"env.action_space","body":"env.action_space","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4h49FDvVX7yC4W132pIojC","type":"Asset","createdAt":"2021-08-25T20:59:15.616Z","updatedAt":"2021-08-25T20:59:15.616Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"env.action_space","file":{"url":"//images.ctfassets.net/xjan103pcp94/4h49FDvVX7yC4W132pIojC/d0299dd7ceae93f7ef70b12a2b9b5b44/Blog_-_intro_reinforcement_5.png","details":{"size":11312,"image":{"width":290,"height":118}},"fileName":"Blog - intro reinforcement 5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The output Discrete(2) means that there are two actions. In cartpole, 0 corresponds to \"push cart to the left\" and 1 corresponds to \"push cart to the right\". Note that in this particular example, standing still is not an option. In reinforcement learning, the agent produces an action output and this action is sent to an environment which then reacts. The environment produces an observation (along with a reward signal, not shown here) which we can see below:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1tocyYPjnLSMgmc0g6CBcq","type":"Entry","createdAt":"2021-08-26T02:27:56.995Z","updatedAt":"2021-08-26T02:27:56.995Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"env.reset()","body":"env.reset()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"rxba5zIqvEqMBkOs8CFEF","type":"Asset","createdAt":"2021-08-25T21:00:08.346Z","updatedAt":"2021-08-25T21:00:08.346Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"env.reset ()","file":{"url":"//images.ctfassets.net/xjan103pcp94/rxba5zIqvEqMBkOs8CFEF/532a32e7eb27763d8d656bf2b8747f0f/Blog_-_intro_reinforcement_6.png","details":{"size":15292,"image":{"width":512,"height":140}},"fileName":"Blog - intro reinforcement 6.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The observation is a vector of dim=4, containing the cart's x position, cart x velocity, the pole angle in radians (1 radian = 57.295 degrees), and the angular velocity of the pole. The numbers shown above are the initial observation after starting a new episode (`env.reset()`). With each timestep (and action), the observation values will change, depending on the state of the cart and pole.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Training an Agent","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In reinforcement learning, the goal of the agent is to produce smarter and smarter actions over time. It does so with a policy. In deep reinforcement learning, this policy is represented with a neural network. Let's first interact with the gym environment without a neural network or machine learning algorithm of any kind. Instead we'll start with random movement (left or right). This is just to understand the mechanisms.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"52y6RzXWOM5ntjn1BZwUYP","type":"Asset","createdAt":"2021-08-25T21:01:08.772Z","updatedAt":"2021-08-25T21:01:08.772Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"policy random movement","file":{"url":"//images.ctfassets.net/xjan103pcp94/52y6RzXWOM5ntjn1BZwUYP/94044c6727443aab584f449dfb419cf0/Blog_-_intro_reinforcement_7.png","details":{"size":73994,"image":{"width":512,"height":312}},"fileName":"Blog - intro reinforcement 7.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The code below resets the environment and takes 20 steps (20 cycles), always taking a random action and printing the results.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1po0JCIIyh2uhzuI0QtWIl","type":"Entry","createdAt":"2021-08-26T02:33:42.667Z","updatedAt":"2021-08-26T02:33:42.667Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"CartPole 20 steps and take random actions","body":"# returns an initial observation\nenv.reset()\n\nfor i in range(20):\n\n  # env.action_space.sample() produces either 0 (left) or 1 (right).\n  observation, reward, done, info = env.step(env.action_space.sample())\n\n  print(\"step\", i, observation, reward, done, info)\n\nenv.close()\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"74TmunIlHtkSEvH0nxtnjg","type":"Asset","createdAt":"2021-08-25T21:03:22.070Z","updatedAt":"2021-08-25T21:03:36.648Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"steps","description":"Sample output. There are multiple conditions for episode termination in cartpole. In the image, the episode is terminated because it is over 12 degrees (0.20944 rad). Other conditions for episode termination are cart position is more than 2.4 (center of the cart reaches the edge of the display), episode length is greater than 200, or the solved requirement which is when the average return is greater than or equal to 195.0 over 100 consecutive trials.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/74TmunIlHtkSEvH0nxtnjg/44ed9a28256c676eb01b365afd499328/Blog_-_intro_reinforcement_8.png","details":{"size":147352,"image":{"width":512,"height":280}},"fileName":"Blog - intro reinforcement 8.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The printed output above shows the following things: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"step (how many times it has cycled through the environment). In each timestep, an agent chooses an action, and the environment returns an observation and a reward","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"observation of the environment [x cart position, x cart velocity, pole angle (rad), pole angular velocity]","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward. The reward is 1 for every step taken for cartpole, including the termination step. After it is 0 (step 18 and 19 in the image).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"done is a boolean. It indicates whether it's time to reset the environment again. Most tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. In cart pole, it could be that the pole tipped too far (more than 12 degrees/0.20944 radians), position is more than 2.4 meaning the center of the cart reaches the edge of the display, episode length is greater than 200, or the solved requirement which is when the average return is greater than or equal to 195.0 over 100 consecutive trials.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"info which is diagnostic information useful for debugging. It is empty for this cartpole environment.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"While these numbers are useful outputs, a video might be clearer. If you are running this code in Google Colab,  it is important to note that there is no display driver available for generating videos. However, it is possible to install a virtual display driver to get it to work.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"BMC1nfuvsamFESKvCOZjK","type":"Entry","createdAt":"2021-08-26T02:34:55.826Z","updatedAt":"2021-08-26T02:34:55.826Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"install dependencies for recording videos in jupyter","body":"# install dependencies needed for recording videos\n!apt-get install -y xvfb x11-utils\n!pip install pyvirtualdisplay==0.2.*\n"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The next step is to start an instance of the virtual display.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Z83g1VInTf4XsjvJKtCsW","type":"Entry","createdAt":"2021-08-26T02:35:55.777Z","updatedAt":"2021-08-26T02:35:55.777Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"from pyvirtualdisplay import Display","body":"from pyvirtualdisplay import Display\ndisplay = Display(visible=False, size=(1400, 900))\n_ = display.start()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"OpenAI gym has a VideoRecorder wrapper that can record a video of the running environment in MP4 format. The code below is the same as before except that it is for 200 steps and is recording.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3JvjQ0rhBqiacofcsb0cCZ","type":"Entry","createdAt":"2021-08-26T02:36:57.451Z","updatedAt":"2021-08-26T02:37:11.181Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"from gym.wrappers.monitoring.video_recorder import VideoRecorder","body":"from gym.wrappers.monitoring.video_recorder import VideoRecorder\nbefore_training = \"before_training.mp4\"\n\nvideo = VideoRecorder(env, before_training)\n# returns an initial observation\nenv.reset()\nfor i in range(200):\n  env.render()\n  video.capture_frame()\n  # env.action_space.sample() produces either 0 (left) or 1 (right).\n  observation, reward, done, info = env.step(env.action_space.sample())\n  # Not printing this time\n  #print(\"step\", i, observation, reward, done, info)\n\nvideo.close()\nenv.close()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1mDEqsrizwjyoV3YSBWZkH","type":"Asset","createdAt":"2021-08-25T21:06:40.749Z","updatedAt":"2021-08-25T21:06:58.240Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"user warning","description":"Usually you end the simulation when done is 1 (True). The code above let the environment keep on going after a termination condition was reached. For example, in CartPole, this could be when the pole tips over, pole goes off-screen, or reaches other termination conditions.","file":{"url":"//images.ctfassets.net/xjan103pcp94/1mDEqsrizwjyoV3YSBWZkH/5867b700eb021a594fa0b1d34120ebe6/Blog_-_intro_reinforcement_9.png","details":{"size":8732,"image":{"width":512,"height":11}},"fileName":"Blog - intro reinforcement 9.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The code above saved the video file into the Colab disk. In order to display it in the notebook, you need a helper function. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4FGtJPhCDOla8Eeg1uff2q","type":"Entry","createdAt":"2021-08-26T02:39:16.921Z","updatedAt":"2021-08-26T02:39:16.921Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"from base64 import b64encode","body":"from base64 import b64encode\ndef render_mp4(videopath: str) -\u003e str:\n  \"\"\"\n  Gets a string containing a b4-encoded version of the MP4 video\n  at the specified path.\n  \"\"\"\n  mp4 = open(videopath, 'rb').read()\n  base64_encoded_mp4 = b64encode(mp4).decode()\n  return f'\u003cvideo width=400 controls\u003e\u003csource src=\"data:video/mp4;' \\\n         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"\u003e\u003c/video\u003e'\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The code below renders the results. You should get a video similar to the one below. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"17lz8NAyS0WFEMHGEGeRtH","type":"Entry","createdAt":"2021-08-26T18:22:43.337Z","updatedAt":"2021-08-26T18:22:43.337Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Render Results","body":"from IPython.display import HTML\nhtml = render_mp4(before_training)\nHTML(html)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"25DKSpb5hbLqZyjB3kx1YE","type":"Entry","createdAt":"2021-08-26T02:41:53.727Z","updatedAt":"2021-08-26T02:41:53.727Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"after training cartpole","videoUrl":"https://www.youtube.com/watch?v=ZF4IJotq4ac"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Playing the video demonstrates that randomly choosing an action is not a good policy for keeping the CartPole upright.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How to Train an Agent using Ray's RLlib","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The previous section of the tutorial had our agent make random actions disregarding the observations and rewards from the environment. The goal of having an agent is to produce smarter and smarter actions over time and random actions don't accomplish that. To make an agent make smarter actions over time, itl needs a better policy. In deep reinforcement learning, the policy is represented with a neural network.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3su83sKnDVpzt0r1OJO1DD","type":"Asset","createdAt":"2021-08-25T21:08:18.790Z","updatedAt":"2021-08-25T21:08:18.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"policy deep-rl","file":{"url":"//images.ctfassets.net/xjan103pcp94/3su83sKnDVpzt0r1OJO1DD/8a59845c574723c1b9ec0d5ebf9cacc4/Blog_-_intro_reinforcement_10.png","details":{"size":91950,"image":{"width":512,"height":298}},"fileName":"Blog - intro reinforcement 10.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This tutorial will use the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/rllib.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RLlib library","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to train a smarter agent. RLlib has many advantages like: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Extreme flexibility. It allows you to customize every aspect of the RL cycle. For instance, this section of the tutorial will make a custom neural network policy using PyTorch (RLlib also has native support for TensorFlow). ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scalability. Reinforcement learning applications can be quite compute intensive and often need to scale-out to a cluster for faster training. RLlib not only has first-class support for GPUs, but it is also built on ","nodeType":"text"},{"data":{"uri":"https://ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" which is an open source library for ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/parallelizing-python-code"},"content":[{"data":{},"marks":[],"value":"parallel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray"},"content":[{"data":{},"marks":[],"value":"distributed","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" Python. This makes scaling Python programs from a laptop to a cluster easy. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"A unified API and support for offline, model-based, model-free, multi-agent algorithms, and more (these algorithms won’t be explored in this tutorial). ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Being part of the ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Project ecosystem","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". One advantage of this is that RLlib can be run with other libraries in the ecosystem like ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a library for experiment execution and hyperparameter tuning at any scale (more on this later).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"While some of these features won't be fully utilized in this post, they are highly useful for when you want to do something more complicated and solve real world problems. You can learn about some impressive use-cases of RLlib ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To get started with RLlib, you need to first install it.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"llPmdpicPEh4sktQLClkN","type":"Entry","createdAt":"2021-08-26T02:43:18.133Z","updatedAt":"2021-08-26T02:44:10.138Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"pip install 'ray[rllib]'==1.6","body":"!pip install 'ray[rllib]'==1.6","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now you can train a PyTorch model using the Proximal Policy Optimization (PPO) algorithm. It is a very well rounded, one size fits all type of algorithm which you can learn more about ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/rllib-algorithms.html#proximal-policy-optimization-ppo"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". The code below uses a neural network consisting of a single hidden layer of 32 neurons and linear activation functions.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3CEIlpc4TZNTT5rhjNXXmk","type":"Entry","createdAt":"2021-08-26T02:45:30.639Z","updatedAt":"2021-08-26T02:45:30.639Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"PPOTrainer RLlib ","body":"import ray\nfrom ray.rllib.agents.ppo import PPOTrainer\nconfig = {\n    \"env\": \"CartPole-v0\",\n    # Change the following line to `“framework”: “tf”` to use tensorflow\n    \"framework\": \"torch\",\n    \"model\": {\n      \"fcnet_hiddens\": [32],\n      \"fcnet_activation\": \"linear\",\n    },\n}\n stop = {\"episode_reward_mean\": 195}\n ray.shutdown()\nray.init(\n  num_cpus=3,\n  include_dashboard=False,\n  ignore_reinit_error=True,\n  log_to_driver=False,\n)\n# execute training \nanalysis = ray.tune.run(\n  \"PPO\",\n  config=config,\n  stop=stop,\n  checkpoint_at_end=True,\n)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This code should produce quite a bit of output. The final entry should look something like this:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1ABB5vepqo4Iy7UOOCJawT","type":"Asset","createdAt":"2021-08-25T21:11:05.878Z","updatedAt":"2021-08-25T21:11:05.878Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"status","file":{"url":"//images.ctfassets.net/xjan103pcp94/1ABB5vepqo4Iy7UOOCJawT/906249ef28e0138ab98df13b79dbe955/Blog_-_intro_reinforcement_13.png","details":{"size":40211,"image":{"width":512,"height":128}},"fileName":"Blog - intro reinforcement 13.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The entry shows it took 35 iterations, running over 258 seconds, to solve the environment. This will be different each time, but will probably be about 7 seconds per iteration (258 / 35 = 7.3). Note that if you like to learn the Ray API and see what commands like ray.shutdown and ray.init do, you can ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"check out this tutorial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How to use a GPU to Speed Up Training","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"While the rest of the tutorial utilizes CPUs, it is important to note that you can speed up model training by using a GPU in Google Colab. This can be done by selecting ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Runtime \u003e Change runtime type","nodeType":"text"},{"data":{},"marks":[],"value":" and set hardware accelerator to ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"GPU","nodeType":"text"},{"data":{},"marks":[],"value":". Then select ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Runtime \u003e Restart and run all","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"67hYTruOvuO7Y8L0rKwG8g","type":"Asset","createdAt":"2021-08-25T21:14:07.578Z","updatedAt":"2021-08-25T21:14:07.578Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"status 2","file":{"url":"//images.ctfassets.net/xjan103pcp94/67hYTruOvuO7Y8L0rKwG8g/c6f8e508ce2c0c5a5d6db907edb2c9f7/Blog_-_intro_reinforcement_14.png","details":{"size":38211,"image":{"width":512,"height":95}},"fileName":"Blog - intro reinforcement 14.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Notice that, although the number of training iterations might be about the same, the time per iteration has come down significantly (from 7 seconds to 5.5 seconds).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Creating a Video of the Trained Model in Action","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"RLlib provides a Trainer class which holds a policy for environment interaction. Through the trainer interface, a policy can be trained, action computed, and checkpointed. While the analysis object returned from ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"ray.tune.run","nodeType":"text"},{"data":{},"marks":[],"value":" earlier did not contain any trainer instances, it has all the information needed to reconstruct one from a saved checkpoint because  ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"checkpoint_at_end=True","nodeType":"text"},{"data":{},"marks":[],"value":" was passed as a parameter. The code below shows this. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"53jTRj0oqUQCU2FqdhRQR1","type":"Entry","createdAt":"2021-08-26T02:46:16.825Z","updatedAt":"2021-08-26T02:46:16.825Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"restore a trainer from the last checkpoint","body":"# restore a trainer from the last checkpoint\ntrial = analysis.get_best_logdir(\"episode_reward_mean\", \"max\")\ncheckpoint = analysis.get_best_checkpoint(\n  trial,\n  \"training_iteration\",\n  \"max\",\n)\ntrainer = PPOTrainer(config=config)\ntrainer.restore(checkpoint)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let’s now create another video, but this time choose the action recommended by the trained model instead of acting randomly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Poa61wCX6geBXS9dKLRdy","type":"Entry","createdAt":"2021-08-26T02:47:13.048Z","updatedAt":"2021-08-26T02:47:13.048Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"after_training = \"after_training.mp4\"","body":"after_training = \"after_training.mp4\"\nafter_video = VideoRecorder(env, after_training)\nobservation = env.reset()\ndone = False\nwhile not done:\n  env.render()\n  after_video.capture_frame()\n  action = trainer.compute_action(observation)\n  observation, reward, done, info = env.step(action)\nafter_video.close()\nenv.close()\n# You should get a video similar to the one below. \nhtml = render_mp4(after_training)\nHTML(html)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7j1W4H6wFrdoem88Ff0pM2","type":"Entry","createdAt":"2021-08-26T02:48:20.776Z","updatedAt":"2021-08-26T02:48:20.776Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"After Training CartPole","videoUrl":"https://youtu.be/6VJvXM-KSYY"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This time, the pole balances nicely which means the agent has solved the cartpole environment! ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hyperparameter Tuning with Ray Tune","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1EGuORfezOpLR5ywb0Hjlq","type":"Asset","createdAt":"2021-08-25T21:20:39.972Z","updatedAt":"2021-08-25T21:20:39.972Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"libraries","description":"The Ray Ecosystem","file":{"url":"//images.ctfassets.net/xjan103pcp94/1EGuORfezOpLR5ywb0Hjlq/86b3d2ba725ba7a355597b5055659794/Blog_-_intro_reinforcement_11.jpeg","details":{"size":41774,"image":{"width":512,"height":288}},"fileName":"Blog - intro reinforcement 11.jpeg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"For a deep-dive on hyperparameter tuning, from the basics to how to distribute hyperparameter tuning using Ray Tune, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/what-is-hyperparameter-tuning"},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"check our our blog series on hyperparameter optimization","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"RLlib is a reinforcement learning library that is part of the Ray Ecosystem. Ray is a highly scalable universal framework for parallel and distributed python. It is very general and that generality is important for supporting its library ecosystem. The ecosystem covers everything from ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"training","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", to ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"production serving","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/data-processing-support-in-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"data processing","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and more. You can use multiple libraries together and build applications that do all of these things.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This part of the tutorial utilizes ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" which is another library in the Ray Ecosystem. It is a library for experiment execution and hyperparameter tuning at any scale. While this tutorial will only use grid search, note that Ray Tune also gives you access to more efficient hyperparameter tuning algorithms like population based training, BayesOptSearch, and HyperBand/ASHA.  ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let’s now try to find hyperparameters that can solve the CartPole environment in the fewest timesteps.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Enter the following code, and be prepared for it to take a while to run:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"57rjoEkvVk1pYlLGGoQImw","type":"Entry","createdAt":"2021-08-26T02:51:39.184Z","updatedAt":"2021-08-26T02:51:39.184Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"parameter_search_config = {     \"env\": \"CartPole-v0\",","body":"parameter_search_config = {\n    \"env\": \"CartPole-v0\",\n    \"framework\": \"torch\",\n\n    # Hyperparameter tuning\n    \"model\": {\n      \"fcnet_hiddens\": ray.tune.grid_search([[32], [64]]),\n      \"fcnet_activation\": ray.tune.grid_search([\"linear\", \"relu\"]),\n    },\n    \"lr\": ray.tune.uniform(1e-7, 1e-2)\n}\n\n# To explicitly stop or restart Ray, use the shutdown API.\nray.shutdown()\n\nray.init(\n  num_cpus=12,\n  include_dashboard=False,\n  ignore_reinit_error=True,\n  log_to_driver=False,\n)\n\nparameter_search_analysis = ray.tune.run(\n  \"PPO\",\n  config=parameter_search_config,\n  stop=stop,\n  num_samples=5,\n  metric=\"timesteps_total\",\n  mode=\"min\",\n)\n\nprint(\n  \"Best hyperparameters found:\",\n  parameter_search_analysis.best_config,\n)\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"By asking for 12 CPU cores by passing in num_cpus=12 to ray.init, four trials get run in parallel across three cpus each. If this doesn’t work, perhaps Google has changed the VMs available on Colab. Any value of three or more should work. If Colab errors by running out of RAM, you might need to do ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Runtime \u003e Factory reset runtime","nodeType":"text"},{"data":{},"marks":[],"value":", followed by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Runtime \u003e Run all","nodeType":"text"},{"data":{},"marks":[],"value":". Note that there is an area in the top right of the Colab notebook showing the RAM and disk use.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Specifying num_samples=5 means that you will get five random samples for the learning rate. For each of those, there are two values for the size of the hidden layer, and two values for the activation function. So, there will be 5 * 2 * 2 = 20 trials, shown with their statuses in the output of the cell as the calculation runs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Note that Ray prints the current best configuration as it goes. This includes all the default values that have been set, which is a good place to find other parameters that could be tweaked.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"After running this, the final output might be similar to the following output: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"INFO tune.py:549 -- Total run time: 3658.24 seconds (3657.45 seconds for the tuning loop).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Best hyperparameters found: {'env': 'CartPole-v0', 'framework': 'torch', 'model': {'fcnet_hiddens': [64], 'fcnet_activation': 'relu'}, 'lr': 0.006733929096170726};'''","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"So, of the twenty sets of hyperparameters, the one with 64 neurons, the ReLU activation function, and a learning rate around 6.7e-3 performed best.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"601dfdJWuaH4znfvrjIDRI","type":"Asset","createdAt":"2021-08-25T21:22:37.638Z","updatedAt":"2021-08-25T21:23:15.054Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"neural mmo","description":"Neural MMO is an environment modeled from Massively Multiplayer Online games — a genre supporting hundreds to thousands of concurrent players. You can learn how Ray and RLlib help enable some key features of this and other projects [here](https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021). ","file":{"url":"//images.ctfassets.net/xjan103pcp94/601dfdJWuaH4znfvrjIDRI/31fb646652ae4332b2fe0c75fb76f1cd/Blog_-_intro_reinforcement_12.jpeg","details":{"size":97412,"image":{"width":512,"height":288}},"fileName":"Blog - intro reinforcement 12.jpeg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This tutorial illustrated what reinforcement learning is by introducing reinforcement learning terminology, by showing how agents and environments interact, and by demonstrating these concepts through code and video examples. If you would like to learn more about reinforcement learning, check out the ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/events/2021/06/24/hands-on-reinforcement-learning-with-rays-rllib"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RLlib tutorial by Sven Mika","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". It is a great way to learn about RLlib’s best practices, multi-agent algorithms, and much more. If you would like to keep up to date with all things RLlib and Ray, consider ","nodeType":"text"},{"data":{"uri":"https://twitter.com/raydistributed"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"following @raydistributed on twitter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://anyscale.us5.list-manage.com/subscribe?u=524b25758d03ad7ec4f64105f\u0026id=d94e960a03"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"sign up for the Ray newsletter","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3ies13ZkanQdfdD5onpL6C","type":"Asset","createdAt":"2021-08-26T15:50:13.877Z","updatedAt":"2021-08-26T15:50:13.877Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Introduction to Reinforcement Learning ","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3ies13ZkanQdfdD5onpL6C/88151cd3a715a9dabc4f5adff8c66b30/InteractionCommunicationEnvironment.jpg","details":{"size":61078,"image":{"width":782,"height":471}},"fileName":"InteractionCommunicationEnvironment.jpg","contentType":"image/jpeg"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Zk7jixFrLqHf3IJLzrO2B","type":"Entry","createdAt":"2022-05-03T18:47:31.422Z","updatedAt":"2022-05-03T18:47:31.422Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Ray Summit: Aug 23-24 in SF!","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5LTWfPAScfUtzXj2KOUALz","type":"Entry","createdAt":"2022-04-12T15:29:42.674Z","updatedAt":"2022-10-04T22:06:07.976Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":17,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"summit"}},"locale":"en-US"},"fields":{"title":"Ray Summit 2022","slug":"ray-summit-2022","description":"Centered on Ray, the open-source distributed computing framework, Ray Summit brings together ML practitioners, data scientists, and more to discuss the growing demand for distributed computing.","sections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xIJZXdAqhOEizUCUuQaiV","type":"Entry","createdAt":"2022-03-31T16:04:49.746Z","updatedAt":"2022-09-12T20:58:36.991Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":20,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-hero","header":"The Place for _Everything Ray_","body":"Missed us in San Francisco for Ray Summit 2022? Don't fret, you can watch your favorite sessions on demand! ","ctaText":"View Sessions","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"9F9S7NLaIdPL4ThYwVlFW","type":"Asset","createdAt":"2022-09-06T14:49:38.936Z","updatedAt":"2022-09-06T14:53:02.350Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Presented by Anyscale","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/9F9S7NLaIdPL4ThYwVlFW/8ecd09b334942298f0b8c2d5567ec032/presented-by-anyscale.svg","details":{"size":22813,"image":{"width":289,"height":32}},"fileName":"presented-by-anyscale.svg","contentType":"image/svg+xml"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4A0d4TUfp0TCqUA60CXopR","type":"Asset","createdAt":"2022-09-06T14:53:48.720Z","updatedAt":"2022-09-06T14:53:48.720Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4A0d4TUfp0TCqUA60CXopR/2e4240c268d074d840f7a00d069a768a/rs-collage_2x.jpg","details":{"size":568547,"image":{"width":2400,"height":1376}},"fileName":"rs-collage@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6FGUP0cXunTbgs2UemeAU","type":"Asset","createdAt":"2022-09-06T14:54:09.975Z","updatedAt":"2022-09-06T14:54:09.975Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event, stacked vertically","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6FGUP0cXunTbgs2UemeAU/16ed175024bc9cc0a2fae3cb13538dff/rs-collage-mobile_2x.jpg","details":{"size":243282,"image":{"width":750,"height":1420}},"fileName":"rs-collage-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4RKa7qVhWoIaoNSa1ZH2SK","type":"Entry","createdAt":"2022-09-07T20:34:03.567Z","updatedAt":"2022-09-07T20:34:03.567Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"summit-2022-hero_cta-section","header":"Don't miss the next Ray Summit!","body":"Sign up for updates","ctaText":"Sign up","ctaLink":"https://share.hsforms.com/1Ee3Gh8c9TY69ZQib-yZJvgc7w85"}}],"type":"2022 Summit Hero","styling":["transparent-header"]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6GL7ZzuSLadWdVFKHj7yIH","type":"Entry","createdAt":"2022-04-20T18:00:12.964Z","updatedAt":"2022-08-08T22:50:29.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-keynote-speakers","header":"Keynote Speakers","ctaText":"See all speakers","ctaLink":"/ray-summit-2022/speakers","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6VhewtcICU9SCXjwd5qhHJ","type":"Entry","createdAt":"2022-04-21T21:17:09.321Z","updatedAt":"2022-04-21T21:17:09.321Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"list"}},"locale":"en-US"},"fields":{"title":"Ray Summit 2022 - Keynote Speakers","identifier":"keynote_speakers","items":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2sPwgp5zouLwisCkxTHBaK","type":"Entry","createdAt":"2022-04-21T21:18:34.974Z","updatedAt":"2022-05-16T18:02:46.776Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Smitha Shyam","position":"Director of Engineering, Uber","affiliation":"Uber","bio":"Smitha Shyam is the director of engineering of UberAI and leads all the centralized AI/ML efforts for Uber. She has extensive experience building and leading high-performing teams in startups and in large companies. She graduated from the University of Michigan, Ann Arbor.\n\nOver the years, Smitha has mentored several engineers and managers across companies. She is also an advocate for diversity and inclusion efforts at Uber. As the chair of Ladyeng, she has started several foundational programs in hiring and retention for women engineers.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7nKqcsiTCDon14Io4uBbxU","type":"Asset","createdAt":"2022-04-21T23:16:14.499Z","updatedAt":"2022-04-21T23:16:14.499Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"smitha-shyam","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7nKqcsiTCDon14Io4uBbxU/c31357a945609abf816da8072173c166/smitha-shyam.jpg","details":{"size":62469,"image":{"width":460,"height":460}},"fileName":"smitha-shyam.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5NVzGePWp1VKrXxNwrMLLn","type":"Entry","createdAt":"2022-04-21T21:18:27.224Z","updatedAt":"2022-05-11T14:50:45.477Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Soumith Chintala","position":"Engineer, Meta AI","affiliation":"Meta AI","bio":"Soumith Chintala is a researcher in AI, currently at Meta. Soumith is well known for creating and leading PyTorch, a widely used AI framework. Soumith has published widely on high-performance machine learning systems, generative models, and computer vision algorithms. He holds a Masters in computer science from NYU, and spent time in Yann LeCun’s NYU lab building deep learning models for robotics, pedestrian detection, natural image OCR, and depth-images, among others.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2yUmUNyyr0HkVQsHDF3stM","type":"Asset","createdAt":"2022-04-21T23:27:30.566Z","updatedAt":"2022-04-21T23:27:30.566Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"soumith-chintala","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2yUmUNyyr0HkVQsHDF3stM/7b952bb4e193509ef768dc3cbb9cb087/soumith-chintala.jpg","details":{"size":39483,"image":{"width":460,"height":460}},"fileName":"soumith-chintala.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"37cqTe8nXtU8AxdvJ8vTYy","type":"Entry","createdAt":"2022-04-21T21:18:23.531Z","updatedAt":"2022-04-21T23:16:37.788Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Anca Dragan","position":"Assistant Professor, UC Berkeley","affiliation":"UC Berkeley","bio":"Anca Dragan is an assistant professor in the EECS Department at UC Berkeley. Her goal is to enable robots and AI agents to work for and around people. She runs the InterACT laboratory, where she focuses on algorithmic human-robot interaction: algorithms that move beyond the robot's function in isolation, and generate robot behavior that coordinates well with human actions, and that is aligned with what humans actually want the robot to do. \n\nAnca got her PhD from CMU's Robotics Institute. She helped found the Berkeley AI Research Laboratory, and is a co-PI of the Center for Human-Compatible AI. She has been honored by the Presidential Early Career Award for Scientists and Engineers (PECASE), NSF CAREER, Sloan, Okawa, ONR Young Investigator Award, MIT TR35, IEEE RAS Early Career Award, and IJCAI Early Career Spotlight.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7I5fn9lcmg6OiQjTv1xaUE","type":"Asset","createdAt":"2022-04-21T23:16:34.804Z","updatedAt":"2022-04-21T23:16:34.804Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"anca-dragan","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7I5fn9lcmg6OiQjTv1xaUE/70ca76a137e11e23be9738de68666958/anca-dragan.jpg","details":{"size":62919,"image":{"width":460,"height":460}},"fileName":"anca-dragan.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1fkvgM9siP5eSUM5rUyYuF","type":"Entry","createdAt":"2022-04-21T21:18:20.046Z","updatedAt":"2022-05-10T23:50:09.775Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Darío Gil","position":"Senior Vice President and Director of Research, IBM","affiliation":"IBM","bio":"Dr. Darío Gil leads the technology roadmap and the technical community of IBM, directing innovation strategies in areas including hybrid cloud, AI, semiconductors, quantum computing, and exploratory science. Dr. Gil is responsible for IBM Research, one of the world’s largest and most influential corporate research labs, with over 3,000 researchers. He is the 12th Director in its 76-year history. He is also responsible for IBM's intellectual property strategy and business.\n\nDr. Gil is a globally recognized leader of the quantum computing industry. Under his leadership, IBM was the first company in the world to build programmable quantum computers and make them universally available through the cloud. \n\nAn advocate of collaborative research models, Dr. Gil co-chairs the MIT-IBM Watson AI Lab, which advances fundamental AI research to the broad benefit of industry and society. He also co-chairs the COVID-19 High-Performance Computing Consortium, which provides access to the world’s most powerful high-performance computing resources in support of COVID-19 research.\n\nDr. Gil is a member of the National Science Board (NSB), the governing body of the National Science Foundation (NSF), serves on the President’s Research Council of the Canadian Institute for Advanced Research (CIFAR), and the MIT School of Engineering Dean's Advisory Council.\n\nDr. Gil is on the boards of the Semiconductor Industry Association (SIA), New York Academy of Sciences, New York Hall of Science, and Research!America.\n\nDr. Gil received his Ph.D. in Electrical Engineering and Computer Science from MIT.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6QKfUR1rSDbkIZHGup5VuC","type":"Asset","createdAt":"2022-04-21T23:16:54.705Z","updatedAt":"2022-04-21T23:16:54.705Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"dario-gil","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6QKfUR1rSDbkIZHGup5VuC/8dca5bbb06e136005ce99ba0a4c93796/dario-gil.jpg","details":{"size":56712,"image":{"width":460,"height":460}},"fileName":"dario-gil.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5yMOMOzdaNWgm2K0NqI6L7","type":"Entry","createdAt":"2022-04-21T21:17:16.279Z","updatedAt":"2022-05-10T23:46:06.170Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Kim Hazelwood","position":"Engineering Director, Meta AI","affiliation":"Meta AI","bio":"Kim Hazelwood is an engineering director at Meta AI, where she focuses on research and engineering challenges at the intersection of scalable computer systems and applied machine learning. Prior to joining Meta in 2015, Kim held positions across academia and industry, including a software engineering role on the Google TPU project, and a tenured associate professor position at the University of Virginia. She holds a PhD in Computer Science from Harvard University, and is a recipient of the MIT “Top 35 Innovators under 35” award, the ACM SIGPLAN “Test of Time” Award, the CRA’s Anita Borg Early Career Award, and an NSF Career Award. She currently serves on the Board of Directors for the Computing Research Association (CRA).","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2P9aceRqaPownumGV8msld","type":"Asset","createdAt":"2022-04-21T23:24:00.346Z","updatedAt":"2022-04-21T23:24:00.346Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"kim-hazelwood","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2P9aceRqaPownumGV8msld/6728735db479c4190a7b9b966529865f/kim-hazelwood.jpg","details":{"size":54731,"image":{"width":460,"height":460}},"fileName":"kim-hazelwood.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1aUNAEWD3ayCJYGJOkFV9A","type":"Entry","createdAt":"2022-04-21T21:18:17.218Z","updatedAt":"2022-05-10T23:47:19.882Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Greg Brockman","position":"CTO \u0026 Co-Founder, OpenAI","affiliation":"OpenAI","bio":"Greg Brockman is the co-founder and CTO of OpenAI, a research and deployment company whose mission is to ensure general-purpose artificial intelligence benefits all of humanity. Before OpenAI, he was the founding engineer and CTO of Stripe, which he helped build from 4 to 250 employees.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3NhKoloJSZ0gfPz7sfBWg7","type":"Asset","createdAt":"2022-04-21T23:26:22.296Z","updatedAt":"2022-04-21T23:26:22.296Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"greg-brockman","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3NhKoloJSZ0gfPz7sfBWg7/23a97680cf61fa23644e4a8529677d6b/greg-brockman.jpg","details":{"size":46303,"image":{"width":460,"height":460}},"fileName":"greg-brockman.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1YO2ge9ofskZb9WkfqqYIA","type":"Entry","createdAt":"2021-03-24T23:31:42.231Z","updatedAt":"2022-05-10T23:48:01.485Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Ion Stoica","position":"Co-Founder, Executive Chairman \u0026 President, Anyscale | Professor, UC Berkeley","affiliation":"Anyscale | UC Berkeley","bio":"Ion Stoica is the Executive Chairman and President and co–founder of Anyscale. Ion also serves as a professor in the EECS Department at UC Berkeley, and as a co-director of the AMPLab. At AMPLab Ion has been leading the software systems effort, which included the development of Apache Spark, as well as two other high-profile open source projects: Apache Mesos and Tachyon. In 2006, Ion co-founded Conviva, a startup to commercialize technologies for large-scale video distribution, where he is serving as CTO. Ion is an ACM Fellow and has received numerous awards, including the SIGCOMM Test of Time Award (2011), and the ACM doctoral dissertation award (2001). Ion holds a PhD in electrical and computer engineering from Carnegie Mellon University and an MS in computer science and control engineering from Polytechnic University Bucharest.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6FWTqgUWOcopW8uaLdWIf8","type":"Asset","createdAt":"2021-03-24T23:31:35.666Z","updatedAt":"2022-04-29T21:18:37.571Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Ion Stoica ","file":{"url":"//images.ctfassets.net/xjan103pcp94/6FWTqgUWOcopW8uaLdWIf8/20666928e67f153617ca2fc272b03175/ion.jpg","details":{"size":207543,"image":{"width":460,"height":460}},"fileName":"ion.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4JeGywZu9yJGzzbbj4l9O8","type":"Entry","createdAt":"2021-03-24T23:39:26.442Z","updatedAt":"2022-05-10T23:48:24.968Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"speaker"}},"locale":"en-US"},"fields":{"name":"Robert Nishihara","position":"Co-founder and CEO, Anyscale","affiliation":"Anyscale","bio":"Robert Nishihara is one of the creators of Ray, a distributed system for scaling Python and machine learning applications. He is one of the co-founders and CEO of Anyscale, which is the company behind Ray. He did his PhD in machine learning and distributed systems in the computer science department at UC Berkeley. Before that, he majored in math at Harvard.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1VqfLPGACWStT9rW7Xqxpe","type":"Asset","createdAt":"2021-03-24T23:39:14.408Z","updatedAt":"2021-03-24T23:39:14.408Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Robert Nishihara","file":{"url":"//images.ctfassets.net/xjan103pcp94/1VqfLPGACWStT9rW7Xqxpe/e391eda0916e251d296bee4dd925c7ff/Robert_Nishihara.jpeg","details":{"size":17176,"image":{"width":264,"height":264}},"fileName":"Robert_Nishihara.jpeg","contentType":"image/jpeg"}}}}}]}}],"type":"2022 Summit Speakers","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JTi5fsyo0Q3YrF3SAQglg","type":"Entry","createdAt":"2022-08-08T22:49:55.163Z","updatedAt":"2022-08-09T07:03:38.615Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-ray-summit-selected-speakers","header":"Highlighted Talks","ctaText":"Full Agenda","ctaLink":"/ray-summit-2022/agenda","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"50eS4cKXBNE4GHezdZ8Rws","type":"Entry","createdAt":"2022-08-08T16:57:39.885Z","updatedAt":"2022-08-09T07:03:38.710Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-selected-speakers_ray-2","header":"What is new in Ray","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46dPYcmkIPhas8NMEKCoQm","type":"Entry","createdAt":"2022-08-08T17:07:12.318Z","updatedAt":"2022-08-09T19:40:41.050Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"Scaling AI workloads with Ray","header":"Scaling AI workloads with Ray","body":"__Richard Liaw__ (Engineering Manager, Anyscale)\n__Eric Liang__ (Software Engineer, Anyscale)","ctaLink":"#","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6YU0defxEs5aqfD3HCv2NZ","type":"Asset","createdAt":"2022-02-10T17:55:41.125Z","updatedAt":"2022-08-09T07:03:38.477Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"Richard Liaw","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6YU0defxEs5aqfD3HCv2NZ/bcb17037b7d09348cfa1350eae24aeac/Richard_Liaw_new.jpg","details":{"size":610536,"image":{"width":1613,"height":1613}},"fileName":"Richard_Liaw_new.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6wr2QAJmY5Aqfglyun2uJU","type":"Entry","createdAt":"2022-08-08T17:47:52.795Z","updatedAt":"2022-08-09T19:40:56.493Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"What's new in Ray Serve","header":"What's new in Ray Serve","body":"__Edward Oakes__ (Software Engineer, Anyscale)\n__Simon Mo__ (Software Engineer, Anyscale)","ctaLink":"#","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1g6W4MigVyeRH4XIpXswSz","type":"Asset","createdAt":"2021-05-17T22:00:38.657Z","updatedAt":"2022-08-09T07:03:38.490Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Edward Oakes","file":{"url":"//images.ctfassets.net/xjan103pcp94/1g6W4MigVyeRH4XIpXswSz/cfcf55d83c2e4de5021dffa809decb8d/Edward_Oakes.jpeg","details":{"size":40029,"image":{"width":473,"height":473}},"fileName":"Edward_Oakes.jpeg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3CGiDcLVGYXKNwcN5yfdSt","type":"Asset","createdAt":"2021-03-24T23:46:01.359Z","updatedAt":"2022-08-09T07:03:38.495Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Simon Mo","file":{"url":"//images.ctfassets.net/xjan103pcp94/3CGiDcLVGYXKNwcN5yfdSt/d5962b24d4855a008b42484679f331d3/Simon_Mo.jpeg","details":{"size":9447,"image":{"width":196,"height":196}},"fileName":"Simon_Mo.jpeg","contentType":"image/jpeg"}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6tM4M4NRKokLCwRaUM7KIC","type":"Entry","createdAt":"2022-08-08T17:51:58.818Z","updatedAt":"2022-08-09T19:41:09.943Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"A Kubernetes Ray clustering solution","header":"A Kubernetes Ray clustering solution","body":"__Ali Kanso__ (Principal Software Engineer, Microsoft)\n__Jiaxin Shan__ (Software Engineer, ByteDance)","ctaLink":"#","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"54hxJPEtsFnq0JE3r5d6fV","type":"Asset","createdAt":"2022-08-08T17:50:22.249Z","updatedAt":"2022-08-09T07:03:38.500Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Ali Kanso headshot","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/54hxJPEtsFnq0JE3r5d6fV/88625653b709d1a3f42863caef25a7aa/Ali_Kanso_headshot.jpeg","details":{"size":79668,"image":{"width":888,"height":803}},"fileName":"Ali Kanso headshot.jpeg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Xa0EaqBpa6sAcdx3QkL8c","type":"Asset","createdAt":"2022-08-08T17:51:31.117Z","updatedAt":"2022-08-09T07:03:38.505Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Jiaxin Shan headshot","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2Xa0EaqBpa6sAcdx3QkL8c/ee49fea3f6bdad95459fb91b9d9cbea5/Jiaxin_headshot.jpeg","details":{"size":3743,"image":{"width":119,"height":160}},"fileName":"Jiaxin headshot.jpeg","contentType":"image/jpeg"}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"JjhNQlOHMj9ntHVo4Lp7T","type":"Entry","createdAt":"2022-08-08T17:52:59.573Z","updatedAt":"2022-08-09T19:41:25.882Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"What’s new in RLlib","header":"What’s new in RLlib","body":"__Sven Mika__ (Machine Learning Engineer, Anyscale)\n__Jun Gong__ (Software Engineer, Anyscale)","ctaLink":"#","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4ZPD68Zc09phhDQ5bAZW5N","type":"Asset","createdAt":"2021-05-04T16:21:35.282Z","updatedAt":"2022-08-09T07:03:38.511Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Sven Mika","file":{"url":"//images.ctfassets.net/xjan103pcp94/4ZPD68Zc09phhDQ5bAZW5N/76ebfb116ed3cd4408926cf6efcd27b3/Sven_Mika.jpeg","details":{"size":50339,"image":{"width":500,"height":500}},"fileName":"Sven Mika.jpeg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3Knoljp9ItOJ06eOnnXFNO","type":"Asset","createdAt":"2022-08-08T17:54:32.123Z","updatedAt":"2022-08-09T07:03:38.516Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Jun Gong headshot","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3Knoljp9ItOJ06eOnnXFNO/e8b68cdd3ca2d183517f1142a7dbbe7f/Jun_Gong_headshot.jpeg","details":{"size":376455,"image":{"width":1513,"height":1757}},"fileName":"Jun Gong headshot.jpeg","contentType":"image/jpeg"}}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5iioKfOgEqVzgQ1DMmA1Wy","type":"Entry","createdAt":"2022-08-08T16:57:36.716Z","updatedAt":"2022-08-09T07:03:39.318Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-selected-speakers_deep-dives","header":"Ray Deep Dives","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4mHBAHsxGuq6ozbPGoX9Lj","type":"Entry","createdAt":"2022-08-08T18:02:11.323Z","updatedAt":"2022-08-09T19:42:11.121Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":" Large Scale Data Shuffle in Ray with Exoshuffle","header":" Large Scale Data Shuffle in Ray with Exoshuffle","body":"__Stephanie Wang__ (Software Engineer, Anyscale)\n__Jiajun Yao__ (Software Engineer, Anyscale)","ctaLink":"#","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6mnlqhGKX31TuowRLvl0Oy","type":"Asset","createdAt":"2021-05-26T03:04:24.446Z","updatedAt":"2022-08-09T07:03:38.521Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Stephanie Wang","file":{"url":"//images.ctfassets.net/xjan103pcp94/6mnlqhGKX31TuowRLvl0Oy/5a17b3c27c51b1e804ec353f858ef505/Stephanie_Wang.jpg","details":{"size":2694552,"image":{"width":3229,"height":3229}},"fileName":"Stephanie_Wang.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3FvavfFPerjx7EEbh1Zeqn","type":"Asset","createdAt":"2022-08-08T18:02:08.573Z","updatedAt":"2022-08-09T07:03:38.526Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Jiajun Yao headshot","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3FvavfFPerjx7EEbh1Zeqn/334d745170d3edb2d9b5e41cc6e75ab2/Jiajun_Yao_headshot.jpeg","details":{"size":11334,"image":{"width":220,"height":220}},"fileName":"Jiajun Yao headshot.jpeg","contentType":"image/jpeg"}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Jm6rWe7bvHgA8XFa5J8HB","type":"Entry","createdAt":"2022-08-08T18:03:07.078Z","updatedAt":"2022-08-09T19:42:23.127Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"Deep dive into data ingest with AIR + Datasets","header":"Deep dive into data ingest with AIR + Datasets","body":"__Clark Zinzow__ (Software Engineer, Anyscale)","ctaLink":"#","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3oNt1w83l2HoPSieGzPRL","type":"Asset","createdAt":"2021-05-06T00:58:34.305Z","updatedAt":"2022-08-09T07:03:38.535Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Clark Zinzow","file":{"url":"//images.ctfassets.net/xjan103pcp94/3oNt1w83l2HoPSieGzPRL/bc76a4ac776859abd9abca21ef20b5a7/Clark_Zinzow.jpeg","details":{"size":11694,"image":{"width":263,"height":263}},"fileName":"Clark_Zinzow.jpeg","contentType":"image/jpeg"}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5iOeR5eRoXkA94qEtFLo3","type":"Entry","createdAt":"2022-08-08T18:05:08.469Z","updatedAt":"2022-08-09T19:42:35.641Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"Ray Observability: Present and future","header":"Ray Observability: Present and future","body":"__SangBin Cho__ (Software Engineer, Anyscale)\n__Ricky Xu__ (Software Engineer, Anyscale)","ctaLink":"#","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5nL8L0YYaxoxGuXZdju6PE","type":"Asset","createdAt":"2021-03-24T23:43:29.973Z","updatedAt":"2022-08-09T07:03:38.541Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"SangBin Cho","file":{"url":"//images.ctfassets.net/xjan103pcp94/5nL8L0YYaxoxGuXZdju6PE/05c5b14e9c930423d1ed6b6deb579bed/SangBin_Cho.png","details":{"size":96919,"image":{"width":230,"height":230}},"fileName":"SangBin_Cho.png","contentType":"image/png"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"n69ZqOiZHkCDsYsnrxudc","type":"Asset","createdAt":"2022-08-08T18:04:53.359Z","updatedAt":"2022-08-09T07:03:38.546Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Ricky Xu headshot","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/n69ZqOiZHkCDsYsnrxudc/0d80a33aac6594893be8da442b56aaed/Ricky_Xu_headshot.jpeg","details":{"size":16754,"image":{"width":250,"height":250}},"fileName":"Ricky Xu headshot.jpeg","contentType":"image/jpeg"}}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"YCBUZOXJGB0qcJF8TKjfI","type":"Entry","createdAt":"2022-08-08T16:57:30.714Z","updatedAt":"2022-08-09T07:03:39.412Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-selected-speakers_use-cases","header":"Ray Use Cases \u0026 Ecosystem","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1xVZTuPMy5V1QQz49bznoM","type":"Entry","createdAt":"2022-08-08T18:24:18.211Z","updatedAt":"2022-08-09T19:42:58.604Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":" Large-scale deep learning training and tuning with Ray at Uber","header":" Large-scale deep learning training and tuning with Ray at Uber","body":"__Xu Ning__ (Senior Engineering Manager, Uber)\n__Michael Mui__ (Staff Software Engineer, Uber AI)\n__Di Yu__ (Sr. Software Engineer, Uber Technologies, Inc.)","ctaLink":"#","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1Nph5bOL8ZFS5Ul0agh4NF","type":"Asset","createdAt":"2021-05-27T18:49:49.565Z","updatedAt":"2022-08-09T07:03:38.553Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Xu Ning","file":{"url":"//images.ctfassets.net/xjan103pcp94/1Nph5bOL8ZFS5Ul0agh4NF/b956df7626604846df590e3d3f7f13c6/Xu_Ning_profile_picture.jpg","details":{"size":1941774,"image":{"width":4419,"height":4419}},"fileName":"Xu_Ning_profile_picture.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6JviXujUZVxS7U9EpL3LYj","type":"Asset","createdAt":"2021-03-24T21:03:05.225Z","updatedAt":"2022-08-09T07:03:38.560Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Michael Mui","file":{"url":"//images.ctfassets.net/xjan103pcp94/6JviXujUZVxS7U9EpL3LYj/91c438149d89638f5b26acbff454ab00/Michael_Mui.jpeg","details":{"size":53365,"image":{"width":529,"height":529}},"fileName":"Michael_Mui.jpeg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2DNZwaunEE9n2fqgwQEF7F","type":"Asset","createdAt":"2022-08-08T18:24:15.048Z","updatedAt":"2022-08-09T07:03:38.566Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Di Yu headshot","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2DNZwaunEE9n2fqgwQEF7F/879a4c80e491818a64ec7b62c7b0fd9d/Di_Yu_headshot.png","details":{"size":471605,"image":{"width":800,"height":800}},"fileName":"Di Yu headshot.png","contentType":"image/png"}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2DvQnSV1cjYN8BR4WaxIT0","type":"Entry","createdAt":"2022-08-08T18:26:10.088Z","updatedAt":"2022-08-09T19:43:10.553Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"The Magic of Merlin: Shopify's new machine learning platform","header":"The Magic of Merlin: Shopify's new machine learning platform","body":"__Isaac Vidas__ (Machine Learning Platform Tech Lead, Shopify)","ctaLink":"#","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3EzAbgxd9xuszpZCEiWFnY","type":"Asset","createdAt":"2022-08-08T18:26:07.224Z","updatedAt":"2022-08-09T07:03:38.571Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Isaac Vidas headshot","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3EzAbgxd9xuszpZCEiWFnY/dfbb2f4c24c1186b6b9caa43a771ec0e/Isaac_Vidas_headshot.jpeg","details":{"size":72098,"image":{"width":500,"height":500}},"fileName":"Isaac Vidas headshot.jpeg","contentType":"image/jpeg"}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5ftzOr2cpQNgvt6UcHN5vu","type":"Entry","createdAt":"2022-08-08T18:30:16.122Z","updatedAt":"2022-08-09T19:43:31.718Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"Ray + Weights \u0026 Biases: Build and deploy real-world ML models","header":"Ray + Weights \u0026 Biases: Build and deploy real-world ML models","body":"__Lukas Biewald__ (CEO/Founder, Weights \u0026 Biases)\t","ctaLink":"#","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"33lo7KUtgVc8jRflLXamuZ","type":"Asset","createdAt":"2022-08-08T18:30:10.511Z","updatedAt":"2022-08-09T07:03:38.576Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Lukas Biewald headshot","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/33lo7KUtgVc8jRflLXamuZ/f0eebdf85e4e6217a05843ec242b59f0/Lukas_Biewald_headshot.png","details":{"size":4655993,"image":{"width":1890,"height":1508}},"fileName":"Lukas Biewald headshot.png","contentType":"image/png"}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6oHnFu3sKJwYgj4LCKGjoj","type":"Entry","createdAt":"2022-08-08T18:31:01.072Z","updatedAt":"2022-08-09T19:43:44.326Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"Large-scale distributed training with TorchX and Ray","header":"Large-scale distributed training with TorchX and Ray","body":"__Mark Saroufim__ (Machine Learning Engineer, Meta)","ctaLink":"#","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"s8NUJftUBLqJlrH8vzgsE","type":"Asset","createdAt":"2022-02-03T01:41:12.770Z","updatedAt":"2022-08-09T07:03:38.582Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Mark Saroufim","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/s8NUJftUBLqJlrH8vzgsE/12b99d44fa0a7507205dc1068d5c0476/Mark_Saroufim.jpeg","details":{"size":21630,"image":{"width":230,"height":222}},"fileName":"Mark_Saroufim.jpeg","contentType":"image/jpeg"}}}]}}]}}],"type":"2022 Summit Agenda Preview"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5cgHuoJ713DewhVY6iUYrG","type":"Entry","createdAt":"2022-05-03T21:57:45.147Z","updatedAt":"2022-05-09T13:41:03.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-trainings","header":"Exclusive Ray Training Sessions","body":"Ray Summit is a great opportunity to level up your Ray skills through hands-on training sessions with the builders and maintainers of Ray. We’re offering several pre-conference training sessions on August 22. Space is limited — [reserve your spot now](https://www.myeventi.events/raysummit22/attendee/)!","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3sYQTUmD5X5ncvGRFyIRR6","type":"Asset","createdAt":"2022-05-03T04:40:25.040Z","updatedAt":"2022-05-03T04:40:25.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image desktop","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3sYQTUmD5X5ncvGRFyIRR6/a77c675b07d63e73dd79ada9d182498a/training-image_2x.jpg","details":{"size":313509,"image":{"width":1136,"height":1480}},"fileName":"training-image@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7rsC56a4YCgENLyxsHyeIb","type":"Asset","createdAt":"2022-05-03T04:40:54.089Z","updatedAt":"2022-05-03T04:40:54.089Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image mobile","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7rsC56a4YCgENLyxsHyeIb/e2592c9b4424048b3c916fbbbc313065/training-image-mobile_2x.jpg","details":{"size":153801,"image":{"width":830,"height":874}},"fileName":"training-image-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4Hb3J9SrCFTDXqIhIdSIwa","type":"Entry","createdAt":"2022-05-03T18:55:51.104Z","updatedAt":"2022-05-12T17:05:47.310Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-trainings_session-1","header":"Training 1:","subheader":"Introduction to Ray for distributed applications","ctaText":"Learn more","ctaLink":"https://www.anyscale.com/ray-summit-2022/agenda/sessions/184","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2cQEsjUrAByUPA31Wwj72D","type":"Asset","createdAt":"2022-05-03T18:55:45.375Z","updatedAt":"2022-05-03T18:55:45.375Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"training1","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2cQEsjUrAByUPA31Wwj72D/ccf8331db8b525944d2a9a30c87b857e/training1.svg","details":{"size":992,"image":{"width":100,"height":100}},"fileName":"training1.svg","contentType":"image/svg+xml"}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"hZ4Tl1wvj7CwF00slwpLg","type":"Entry","createdAt":"2022-05-03T18:58:52.831Z","updatedAt":"2022-05-12T17:06:01.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-trainings_session-2","header":"Training 2:","subheader":"Machine learning model deployment and serving with Ray Serve","ctaText":"Learn more","ctaLink":"https://www.anyscale.com/ray-summit-2022/agenda/sessions/189","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"561LrSUXdICliHshu5CYIE","type":"Asset","createdAt":"2022-05-03T18:58:48.343Z","updatedAt":"2022-05-03T18:58:48.343Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"training2","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/561LrSUXdICliHshu5CYIE/836b47ab88a434d512a46812fecc2a92/training2.svg","details":{"size":873,"image":{"width":100,"height":100}},"fileName":"training2.svg","contentType":"image/svg+xml"}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"YJ5KWyjg4NGf29NrfHcJX","type":"Entry","createdAt":"2022-05-03T18:59:15.953Z","updatedAt":"2022-05-12T17:07:11.981Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-trainings_session-3","header":"Training 3:","subheader":"Introduction to reinforcement learning and RLlib","ctaText":"Learn more","ctaLink":"https://www.anyscale.com/ray-summit-2022/agenda/sessions/190","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4AZwccv9yM6gccHUYJJDoI","type":"Asset","createdAt":"2022-05-03T18:59:12.095Z","updatedAt":"2022-05-03T18:59:12.095Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"training3","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4AZwccv9yM6gccHUYJJDoI/a235431c2b4a1290c2045defaa0f4320/training3.svg","details":{"size":966,"image":{"width":100,"height":100}},"fileName":"training3.svg","contentType":"image/svg+xml"}}}]}}],"type":"2022 Summit Trainings"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mRJQtuDbIhAuEbwLT2V1K","type":"Entry","createdAt":"2022-03-31T21:42:56.117Z","updatedAt":"2022-04-11T23:46:07.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-reasons-to-attend","header":"Why Attend?","body":"As the annual user conference for the [open source Ray community](https://www.ray.io/community), Ray Summit is a unique opportunity to enhance your Ray skills, build your professional network, and learn what's on the Ray roadmap.","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5f6gxk4a4ujE2y6FZ8ATQ5","type":"Entry","createdAt":"2022-03-31T21:42:39.377Z","updatedAt":"2022-04-11T20:40:54.526Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-reasons-to-attend_2","header":"Enhance Your Skills","body":"Extend what you can do with Ray by attending hands-on, expert-led workshops and deep dives","ctaText":"See the agenda","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5tnjeoCJZqpvwopKiopgVF","type":"Asset","createdAt":"2022-04-11T20:40:52.647Z","updatedAt":"2022-04-11T20:40:52.647Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Outline icon of wrench and gear","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5tnjeoCJZqpvwopKiopgVF/b2e73034db7a96b3671e9d681ea3e9ac/enhance-icon.svg","details":{"size":6196,"image":{"width":64,"height":64}},"fileName":"enhance-icon.svg","contentType":"image/svg+xml"}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"20QwyiOmkHCEOkMvp0Jhtk","type":"Entry","createdAt":"2022-03-31T21:42:47.478Z","updatedAt":"2022-04-11T20:41:36.679Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-reasons-to-attend_3","header":"Build Your Network","body":"Mingle with the world’s top minds in distributed computing, machine learning, and AI","ctaLink":"/","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Dg4T7AgnMXWg0pfVBHkyr","type":"Asset","createdAt":"2022-04-11T20:41:34.246Z","updatedAt":"2022-04-11T20:41:34.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Outline icon with avatar and connecting dots","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Dg4T7AgnMXWg0pfVBHkyr/9a8ce56e1166a499fbee94b36b64d379/build-icon.svg","details":{"size":5951,"image":{"width":64,"height":64}},"fileName":"build-icon.svg","contentType":"image/svg+xml"}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6QxivsuhhLoVSloRQs9ZlQ","type":"Entry","createdAt":"2022-03-31T21:42:32.048Z","updatedAt":"2022-04-11T20:42:06.224Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-reasons-to-attend_1","header":"Get Involved","body":"Learn what’s on the Ray roadmap and connect with the builders and maintainers of Ray and Ray libraries","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1zZBDYPiNGvROhJdCIM9RS","type":"Asset","createdAt":"2022-04-11T20:42:01.094Z","updatedAt":"2022-04-11T20:42:01.094Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Line icon with bending line and dots","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1zZBDYPiNGvROhJdCIM9RS/9cc283fd3f6568ca1ca924b4a99215cf/involved.svg","details":{"size":4461,"image":{"width":64,"height":64}},"fileName":"involved.svg","contentType":"image/svg+xml"}}}]}}],"type":"2022 Summit Content Tiles"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"15k0SGqB2ZKOUfAsM3JnyV","type":"Entry","createdAt":"2022-03-31T16:05:55.739Z","updatedAt":"2022-08-05T00:25:20.225Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":16,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-about","header":"Celebrate the Ray community","body":"Get ready for our epic community bash at the Art of Tinkering exhibit at the San Francisco Exploratorium on August 23 from 5:30 to 9:30 p.m. Come explore, play, and celebrate with the Ray Community. This is an exclusive opportunity to experience this amazing interactive exhibition before it closes in September.","ctaText":"Check out the agenda","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UcZT4h1P7uFDz6cn6JH8l","type":"Asset","createdAt":"2022-06-02T20:37:22.299Z","updatedAt":"2022-06-02T20:37:22.299Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Man taking a photo on his phone","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2UcZT4h1P7uFDz6cn6JH8l/5040a9909f18ab015c27b220077f916e/taking-picture.jpg","details":{"size":107846,"image":{"width":980,"height":772}},"fileName":"taking-picture.jpg","contentType":"image/jpeg"}}}],"type":"2022 Summit Split Image","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4P8Jn0q7nBU05iD6DmcQCC","type":"Entry","createdAt":"2022-04-01T19:15:31.735Z","updatedAt":"2022-04-11T16:36:59.573Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-social-tiles","header":"Join the Conversation","body":"Ready to get involved in the Ray community before the conference? Ask a question in the forums. Open a pull request. Or share why you’re excited with the hashtag __#RaySummit__ on Twitter.","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6OMTlXkJ1OHXNkEjyPrXTH","type":"Entry","createdAt":"2022-04-01T19:13:42.654Z","updatedAt":"2022-06-03T19:34:02.162Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-social-tiles_slack","ctaText":"Join the channel","ctaLink":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform?usp=send_form","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7yv4V9V1gq9W8mEnQ1NWyQ","type":"Asset","createdAt":"2022-06-01T15:32:01.924Z","updatedAt":"2022-06-03T15:27:05.893Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Slack logo gray tint","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7yv4V9V1gq9W8mEnQ1NWyQ/cbdedcaef6273a8217e7224a55b380b0/slack-blue.svg","details":{"size":2888,"image":{"width":64,"height":64}},"fileName":"slack-blue.svg","contentType":"image/svg+xml"}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7EE28ee1ATQtodoLloT6mg","type":"Entry","createdAt":"2022-04-01T19:14:07.157Z","updatedAt":"2022-06-03T19:33:59.129Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-social-tiles_twitter","ctaText":"Follow us","ctaLink":"https://twitter.com/Ray_Summit_Live","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"45pAOzVnLahEfDFH2mdSqS","type":"Asset","createdAt":"2022-06-01T15:32:42.788Z","updatedAt":"2022-06-03T15:27:33.679Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Twitter logo gray tint","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/45pAOzVnLahEfDFH2mdSqS/a9ce3f9d370199869330783070d55178/twitter-blue.svg","details":{"size":2120,"image":{"width":64,"height":52}},"fileName":"twitter-blue.svg","contentType":"image/svg+xml"}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1CRAzFxndL37wiNg4x8WKV","type":"Entry","createdAt":"2022-04-01T19:14:52.573Z","updatedAt":"2022-06-03T19:33:56.091Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-social-tiles_github","ctaText":"Contribute to Ray","ctaLink":"https://github.com/ray-project/ray","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6u2wlYxqwNCz6Pqf6JTY1e","type":"Asset","createdAt":"2022-06-01T15:35:02.130Z","updatedAt":"2022-06-03T15:27:48.074Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Github logo gray tint","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6u2wlYxqwNCz6Pqf6JTY1e/eba535c1420380626dacedb1093ebd82/git-blue.svg","details":{"size":2412,"image":{"width":64,"height":61}},"fileName":"git-blue.svg","contentType":"image/svg+xml"}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2qrec5rR8kabZxCHaPS7NH","type":"Entry","createdAt":"2022-04-01T19:15:25.354Z","updatedAt":"2022-06-03T19:34:05.213Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-social-tiles_discuss","ctaText":"Ask a question","ctaLink":"https://discuss.ray.io/","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2p8v202IyEQmNWf5sRBMlp","type":"Asset","createdAt":"2022-06-01T15:35:26.650Z","updatedAt":"2022-06-03T15:28:18.820Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Discuss icon gray tint","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2p8v202IyEQmNWf5sRBMlp/8c45d5f85941049a94cf807e2d060dba/ask-blue.svg","details":{"size":1476,"image":{"width":64,"height":70}},"fileName":"ask-blue.svg","contentType":"image/svg+xml"}}}]}}],"type":"2022 Summit Social Tiles"}}],"headerLogo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70DIdxwT8XY96aw7vMUHOu","type":"Asset","createdAt":"2022-06-01T15:21:46.804Z","updatedAt":"2022-06-01T15:21:46.804Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo Dark Blue","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/70DIdxwT8XY96aw7vMUHOu/5ca0a8b7533c8df70bc0e032b889862a/rs-logo-nav.svg","details":{"size":9954,"image":{"width":108,"height":48}},"fileName":"rs-logo-nav.svg","contentType":"image/svg+xml"}}},"headerLogoWhite":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3qyLXKGgGfWhv4oPqYl0SE","type":"Asset","createdAt":"2022-04-20T21:50:58.774Z","updatedAt":"2022-04-20T21:50:58.774Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3qyLXKGgGfWhv4oPqYl0SE/c4dee9e7a344c39db11fefafbf255b5b/rs-logo-horizontal.svg","details":{"size":9975,"image":{"width":108,"height":48}},"fileName":"rs-logo-horizontal.svg","contentType":"image/svg+xml"}}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KUtRQbdqJo1RrWMrVyfEm","type":"Entry","createdAt":"2022-03-31T16:21:13.402Z","updatedAt":"2022-04-08T17:25:33.228Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Home","link":"/ray-summit-2022"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4MvQcRSkHLMqIJoO9O2cQe","type":"Entry","createdAt":"2022-03-31T16:17:27.336Z","updatedAt":"2022-09-12T16:59:51.221Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"On-Demand","link":"/ray-summit-2022/agenda","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3N9kRDz36tieTI4YGVWKBJ","type":"Entry","createdAt":"2022-06-30T21:34:59.987Z","updatedAt":"2022-09-09T21:10:16.159Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"ALL SESSIONS","link":"/ray-summit-2022/agenda"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46B5diwEDDr3kB2XDoGfDP","type":"Entry","createdAt":"2022-05-11T17:13:42.174Z","updatedAt":"2022-05-11T17:13:42.174Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Speakers","link":"/ray-summit-2022/speakers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Dt5fy6A7rBpXdG7HeKMGd","type":"Entry","createdAt":"2022-03-31T16:17:43.592Z","updatedAt":"2022-03-31T16:17:43.592Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Sponsors","link":"/ray-summit-2022/sponsors"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SvRE9pnyq3BXUPGKRWx6W","type":"Entry","createdAt":"2022-06-03T19:29:31.009Z","updatedAt":"2022-06-03T19:29:31.009Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Who Attends","link":"/ray-summit-2022/who-attends"}}],"mainSponsors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"xMQIOU9n7gVS4mkX2Nlag","type":"Entry","createdAt":"2021-01-31T16:33:06.495Z","updatedAt":"2021-05-11T13:28:31.312Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Ray","url":"https://ray.io","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jLroqm98RCMLGASthyuPv","type":"Asset","createdAt":"2021-05-11T13:28:26.930Z","updatedAt":"2021-05-11T13:28:26.930Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/jLroqm98RCMLGASthyuPv/f76cc04827316de5d3f8b529f6a3f02c/Ray_BM.svg","details":{"size":4742,"image":{"width":93,"height":36}},"fileName":"Ray_BM.svg","contentType":"image/svg+xml"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11syIHhtX8JajSlXHSHE1O","type":"Entry","createdAt":"2021-01-31T16:31:24.509Z","updatedAt":"2021-05-11T13:28:00.309Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Anyscale","url":"https://www.anyscale.com","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UkDmTzmZHIOG7yxFP4sVL","type":"Asset","createdAt":"2021-05-11T13:27:55.219Z","updatedAt":"2021-05-11T13:27:55.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Anyscale","file":{"url":"//images.ctfassets.net/xjan103pcp94/5UkDmTzmZHIOG7yxFP4sVL/db10aebda9a9aad10903470781a6f2e6/ANSC_BM.svg","details":{"size":7431,"image":{"width":158,"height":36}},"fileName":"ANSC_BM.svg","contentType":"image/svg+xml"}}}}}],"footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cfwECWrOqJ5Fegr3YpReS","type":"Entry","createdAt":"2022-04-01T21:22:12.001Z","updatedAt":"2022-04-13T17:35:29.511Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-footer","body":"©Anyscale, Inc 2022\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/privacy-policy\"\u003ePrivacy Policy\u003c/a\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/code-of-conduct\"\u003eCode of Conduct\u003c/a\u003e"}},"recommendations":[]}},"ctaText":"Register now","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5nTlh9josVs5zpZqU3vTbn","type":"Asset","createdAt":"2022-05-03T18:47:25.125Z","updatedAt":"2022-05-03T18:47:25.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-ray-summit-2022","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5nTlh9josVs5zpZqU3vTbn/e66043f62ed4a4b53d7e182e50289ec3/blog-recommended-content-ray-summit-2022.png","details":{"size":17234,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-ray-summit-2022.png","contentType":"image/png"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2EQzVvFklHtrDGOZF9gmhJ","type":"Entry","createdAt":"2022-03-21T17:46:21.879Z","updatedAt":"2022-03-21T17:47:09.167Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"An informal introduction to RL","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2q2uFmxMeiYtjxODaW61uy","type":"Entry","createdAt":"2022-02-22T16:24:28.106Z","updatedAt":"2022-06-22T15:56:22.597Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"An informal introduction to reinforcement learning","slug":"an-informal-introduction-to-reinforcement-learning","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2AgYlFYAtLGaS7VgbwcoVa","type":"Entry","createdAt":"2022-02-15T19:00:59.347Z","updatedAt":"2022-02-15T19:00:59.347Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Misha Laskin","slug":"misha-laskin"}}],"publishedDate":"2022-02-22","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Reinforcement learning (RL) has played a critical role in the rapid pace of AI advances over the last decade. In this post, we'll cover what RL is and why it's important, both as a research subject and for a diverse set of practical applications.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"This series on reinforcement learning was guest-authored by ","nodeType":"text"},{"data":{"uri":"https://twitter.com/MishaLaskin"},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Misha Laskin","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" while he was at UC Berkeley. Misha's focus areas are unsupervised learning and reinforcement learning.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Reinforcement learning (RL) has played a critical role in the rapid pace of AI advances over the last decade. In this post, we will explain in simple terms what RL is and why it is important, not only as a research subject but also for a diverse set of practical applications.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What is reinforcement learning?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To understand what RL is, it’s easier to first understand what it is not. Most well-known machine learning algorithms make predictions but do not need to reason over a long time period or interact with the world. For example, given an image of an object, an image classifier will make a prediction about the identity of the object (e.g., an apple or an orange). This image classifier is passive. It doesn’t interact with the world and it doesn’t need to reason about how previous images influence its current prediction. Image classification is categorized as supervised learning but it is not RL.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlike passive predictive models like image classifiers, RL algorithms both interact with the world and need to reason over periods of time to achieve their goals. An RL algorithm’s goal is to maximize rewards over a given time period. Intuitively, it works similar to classical conditioning ideas from psychology. For example, a common way to train a dog to learn a new trick is by rewarding it with a treat when it does something right. The dog wants to maximize the number of treats it gets and will learn new skills to maximize its reward.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1KRtNT9VBGTuSUYZJuat8m","type":"Asset","createdAt":"2022-02-15T19:10:38.146Z","updatedAt":"2022-02-15T19:21:05.973Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"blog-intro-to-rl-1","description":"RL agents continually learn from experience, whereas supervised learning agents learn to make predictions based on a static dataset.","file":{"url":"//images.ctfassets.net/xjan103pcp94/1KRtNT9VBGTuSUYZJuat8m/83c09bb7810cb3ca4ccb0efb19c45723/blog-intro-to-rl-1.png","details":{"size":1494705,"image":{"width":3306,"height":1908}},"fileName":"blog-intro-to-rl-1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5l1F2FF0i2PCHFy9mG040i","type":"Entry","createdAt":"2022-02-17T22:42:00.588Z","updatedAt":"2022-02-17T23:17:30.524Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"RL Summit CTA (Misha Laskin RL blog series)","body":"\u003ca href=\"/production-rl-summit#agenda?utm_source=anyscale\u0026utm_medium=blog\u0026utm_content=rl-blog-series\u0026utm_campaign=rl_summit\"\u003e\u003cimg src=\"//images.ctfassets.net/xjan103pcp94/1hu1Zwg1RQ0ifg9Lfp98bA/7d913772436d45666a80e83f629b4815/RL_Summit__12_.png\" style=\"width: 100%\"\u003e\u003c/a\u003e"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Apple-picking robot","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"RL algorithms work in the same way as training dogs to learn new tricks. Instead of food treats, RL agents receive numerical rewards. For example, suppose we drop a robot into a maze and want it to maximize the number of apples it picks up. We can assign a +1 reward each time the robot picks up an apple. At the end of a fixed time period, we count all the apples the robot picked up to determine its total reward. RL algorithms are a mathematical framework for maximizing the total reward. An RL algorithm, given some set of assumptions about its environment, will  guarantee that the robot picks up the maximal possible number of apples in its environment.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"With this apple-picking robot example we can also illustrate a fundamental challenge with RL algorithms: the exploration problem. Suppose the robot is in a room with a moderate number of apples, but there exists another room down a long corridor with an entire apple tree. The best place for the robot to be is in the room with the apple tree, but it first has to traverse a long corridor with no rewards to get there. RL algorithms will discover the optimal solution on their own, but they can take a long time to train if exploration is hard.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Li6bNTzAGrhzQM6YbsJlE","type":"Asset","createdAt":"2022-02-15T19:14:41.480Z","updatedAt":"2022-02-15T19:17:58.196Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"blog-intro-to-rl-2","description":"The exploration problem in RL: Room A has few apples, but it's nearby. Room B has many more apples, but it's far away. How does the robot know that it's better to explore to get to Room B?","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Li6bNTzAGrhzQM6YbsJlE/2ca06c8f8e0f76e0f1e356860d38b211/blog-intro-to-rl-2.png","details":{"size":661464,"image":{"width":2903,"height":1096}},"fileName":"blog-intro-to-rl-2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A brief history of reinforcement learning","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"While parts of RL theory started to be developed as early as the mid 1950s, the ideas that have shaped modern RL weren’t brought together until 1989 when Chris Watkins formalized Q-learning — the mathematical framework for training RL agents. But it wasn’t until after the breakthrough AlexNet paper in 2012 that RL took off in the modern era. The ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"ImageNet moment","nodeType":"text"},{"data":{},"marks":[],"value":" marked the first time a deep neural network substantially outperformed prior approaches to image classification on complex real-world images.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Shortly after in 2013, Vlad Mnih and collaborators at DeepMind released the Deep Q Network (DQN), which was the first system to play Atari video games autonomously from pixel inputs. This groundbreaking achievement paved the way for modern RL called Deep RL because it utilized deep neural networks to estimate how likely particular actions are to lead to high rewards. The DQN algorithm was at the core of subsequent advances like AlphaGo and AlphaStar.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Practical applications of reinforcement learning","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"While most RL research today focuses on algorithms to develop reward maximizing agents in simulated settings, the RL framework is quite general and is already starting to be applied to solve a variety of practical problems.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Recommender systems:","nodeType":"text"},{"data":{},"marks":[],"value":" Recommender systems aim to recommend products or content to a user that will have higher likelihood to convert or engage. The number of possible products a system could recommend is quite large and efficient algorithms are needed to not only select products a user will like most, but also make sure these selections will increase the company’s desired long-term metrics. Examples of RL being used to power recommender systems include ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/pdf/2008.09369.pdf"},"content":[{"data":{},"marks":[],"value":"product recommendation at Alibaba","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/pdf/1812.02353.pdf"},"content":[{"data":{},"marks":[],"value":"video recommendations at YouTube","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Over the last few years, new RL algorithms have been developed specifically tailored for recommender systems. One example is ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1905.12767"},"content":[{"data":{},"marks":[],"value":"SlateQ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which uses RL to present a slate of items to the user. SlateQ is designed to deal with the combinatorially large space of potential product recommendations to drive overall user satisfaction.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4CHdygpw6pWLrmvkKT5Z86","type":"Asset","createdAt":"2022-02-15T19:30:06.069Z","updatedAt":"2022-02-15T19:30:06.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-intro-to-rl-5","description":"SlateQ increases user engagement over time. Source: https://arxiv.org/abs/1905.12767","file":{"url":"//images.ctfassets.net/xjan103pcp94/4CHdygpw6pWLrmvkKT5Z86/873aaff5f08eedbb4ad3cdef46fe43c7/blog-intro-to-rl-5.png","details":{"size":362621,"image":{"width":1200,"height":696}},"fileName":"blog-intro-to-rl-5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"One important aspect to keep in mind when deploying RL algorithms is to make sure that the reward function is set up to maximize the company’s long-term metrics, such as satisfaction with the company’s product. For example, in the mid 2010s YouTube learned that while their recommendations were increasing user engagement, they were not increasing long-term user satisfaction. This was due to implicit bias, when it is unclear if a user is clicking on a video because it is their intent or simply because the recommendation was present on the screen. Since then Google released ","nodeType":"text"},{"data":{"uri":"https://dl.acm.org/doi/10.1145/3298689.3346997"},"content":[{"data":{},"marks":[],"value":"a new RL algorithm","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" that addresses implicit bias.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Autonomous vehicle navigation:","nodeType":"text"},{"data":{},"marks":[],"value":" RL is being used today to plan paths and control autonomous vehicles (AVs) at Tesla. One example of RL usage for AVs is to plan a path to autonomously park a car. The number of possible paths a car could explore is exponential, which results in slow and inefficient path planning with traditional methods. At AI day, ","nodeType":"text"},{"data":{"uri":"https://www.youtube.com/watch?v=j0z4FweCy4M"},"content":[{"data":{},"marks":[],"value":"Tesla showcased","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" an algorithm based on AlphaGo to find faster and more efficient plans.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Cooling data centers","nodeType":"text"},{"data":{},"marks":[],"value":": Another useful application of RL has been ","nodeType":"text"},{"data":{"uri":"https://deepmind.com/blog/article/deepmind-ai-reduces-google-data-centre-cooling-bill-40"},"content":[{"data":{},"marks":[],"value":"cooling data centers at Google","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Data centers consume a lot of energy and the computers they house dissipate it as heat. Maintaining an operational data center requires sophisticated cooling systems, which are expensive to run. With RL, Google was able to deploy cooling on-demand to eliminate wasteful periods where cooling was not needed.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this post, we covered an informal introduction to what is RL and why it’s useful. These are just three of many practical applications of RL. It’s exciting that one algorithmic framework is general enough to apply to such a broad range of problems.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[],"nodeType":"hr"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next up, we'll explore ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-reinforcement-learning-framework"},"content":[{"data":{},"marks":[],"value":"how the mathematical framework for RL actually works","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Or, check out our other resources below for more on RL:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/production-rl-summit#agenda?utm_source=anyscale\u0026utm_medium=blog\u0026utm_content=rl-blog-series\u0026utm_campaign=rl_summit"},"content":[{"data":{},"marks":[],"value":"Register for the upcoming Production RL Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a free virtual event that brings together ML engineers, data scientists, and researchers pioneering the use of RL to solve real-world business problems","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/index.html"},"content":[{"data":{},"marks":[],"value":"Learn more about RLlib: industry-grade reinforcement learning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google"},"content":[{"data":{},"marks":[],"value":"Check out our introduction to reinforcement learning with OpenAI Gym, RLlib, and Google Colab","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021"},"content":[{"data":{},"marks":[],"value":"Get an overview of some of the best reinforcement learning talks presented at Ray Summit 2021","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ZV5flSIzNMWJXtg09FEZM","type":"Asset","createdAt":"2022-02-18T15:14:44.998Z","updatedAt":"2022-05-24T19:56:36.761Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-intro-to-rl-thumb","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/ZV5flSIzNMWJXtg09FEZM/19745e2831a018f600a4e3eba68389e1/1371476_Blog_Image-Illustration_-4_052022.jpg","details":{"size":817851,"image":{"width":1500,"height":1000}},"fileName":"1371476_Blog Image-Illustration -4_052022.jpg","contentType":"image/jpeg"}}},"mainImageFit":"cover","recommendations":[],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}}],"hideIntro":true}},"ctaText":"Read blog series","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3tbbqYD9I2NRmPfTlTH1Io","type":"Asset","createdAt":"2022-03-21T17:46:16.833Z","updatedAt":"2022-03-23T18:51:22.476Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"locale":"en-US"},"fields":{"title":"blog-recommended-content-rl-robot-gear-light","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3tbbqYD9I2NRmPfTlTH1Io/d4d8b09181c1d59d7f64c6ee3d009ba2/blog-recommended-content-rl-robot-gear-light.jpg","details":{"size":50047,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-rl-robot-gear-light.jpg","contentType":"image/jpeg"}}},"recommendations":null}},{"fields":{"content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7xkHJWpQ3mRxrgAQw3oYFk","type":"Entry","createdAt":"2023-06-15T13:47:45.558Z","updatedAt":"2023-06-16T13:24:48.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Introducing the Anyscale Databricks Connector","seoTitle":"Integrating Anyscale Ray with Databricks","slug":"introducing-the-anyscale-databricks-connector","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3e6JquwKuIz6QvCWO0BAz","type":"Entry","createdAt":"2023-06-15T13:16:27.453Z","updatedAt":"2023-06-15T13:16:27.453Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Greene","slug":"eric-greene","link":"https://www.linkedin.com/in/greene/"}}],"publishedDate":"2023-06-15","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Introducing the Anyscale Databricks connector. Simple Data Transfer and Efficient AI Workflows with Anyscale and Databricks.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The Anyscale Databricks Connector is a new Ray Datasets capability that facilitates easy data transfer between Databricks clusters and Anyscale hosted Ray clusters. The connector makes it easy for Data Scientists using Anyscale to leverage their Databricks data lake during machine learning and discovery.  It also enables a simpler way for machine learning engineers to create end-to-end workloads by allowing the entire ML pipeline to be executed within a single Python script. By taking advantage of the highly-scalable nature of Ray and Ray Datasets, machine learning workloads such as training, tuning and batch serving jobs can be executed more quickly and at lower cost, all while taking advantage of the latest advancements in AI through other Ray integrations with Hugging Face, XGBoost, LightGBM and many AI frameworks and libraries.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Key Benefits","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simple access to data","nodeType":"text"},{"data":{},"marks":[],"value":" - Access Databricks data through SQL queries within Anyscale Workspaces Visual Code or JupyterHub environments, empowering data science discovery and development.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Improved data security and governance","nodeType":"text"},{"data":{},"marks":[],"value":" - Ensure data security and governance by directly copying data from Databricks data lakes to Ray clusters, eliminating the need for intermediate steps and maintaining separate controls over sensitive data. All data is encrypted in transit and at rest.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Highly-scalable data exchange","nodeType":"text"},{"data":{},"marks":[],"value":" - Take advantage of the parallel read and write capabilities of Ray Datasets, enabling the exchange of terabytes of data in minutes. This capability significantly speeds up data-intensive operations and enhances scalability while reducing job run times and overall costs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simplified workload development","nodeType":"text"},{"data":{},"marks":[],"value":" - Simplify machine learning workflows by consolidating all the necessary logic into a single script. This script can query features, train and tune models, and score and materialize results back into the data lake, streamlining the entire process.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Unlock the latest AI Innovations","nodeType":"text"},{"data":{},"marks":[],"value":" - Leverage the power of Databricks for querying and joining data, while benefiting from the scalability and simplicity of Ray AIR for machine learning and AI development. Ray AIR provides integration to the latest AI innovations such as pretrained Hugging Face language models.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Simple, Secure and Scalable Data Access","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing the Anyscale Databricks connector, large datasets can be queried from a Databricks SQL warehouse, and quickly transferred into a Ray Dataset distributed across the Ray cluster. The Databricks connector reads query results in chunks of data, in parallel across the Ray cluster. The size of the dataset and the speed at which it can be transferred scales based on the size of your Ray cluster and the number of simultaneous requests the underlying Databricks SQL warehouse supports.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6taTlZK9ffeivO5RCUJiQ4","type":"Asset","createdAt":"2023-06-15T13:27:43.116Z","updatedAt":"2023-06-15T13:27:43.116Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"1 - Anyscale Databricks Architecture","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6taTlZK9ffeivO5RCUJiQ4/c5fc0bfef768f0fb6e8785bf71ded1e2/1_-_Anyscale_Databricks_Arch.jpg","details":{"size":95989,"image":{"width":761,"height":478}},"fileName":"1 - Anyscale Databricks Arch.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Anyscale Databricks connectors, parallel reads and writes and overall speed will scale with the size of the Ray cluster. The benchmarks for reading and writing demonstrate how the cluster scales out to support larger data sets.  ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2KimYEe845rk6fb2oYaCmz","type":"Asset","createdAt":"2023-06-15T13:35:36.597Z","updatedAt":"2023-06-15T13:35:36.597Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2 - Anyscale Databricks Table Example","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2KimYEe845rk6fb2oYaCmz/8302017d6535b5d676d3dfb50dc12cbd/2_-_Anyscale_Databricks_Table_Example.jpg","details":{"size":218942,"image":{"width":2178,"height":1002}},"fileName":"2 - Anyscale Databricks Table Example.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Simplified ML Development","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing the Databricks connector from within an Anyscale Workspaces, Data Scientists and Machine Learning Engineers have a unified experience while developing workloads. The data query, feature engineering, training, tuning and inference can all be executed within a single Python script that scales across a Ray cluster. Ray integrates with the latest AI and Machine learning libraries, enabling the most advanced ML workloads that work with 3rd party libs  like Hugging Face, XGBoost, LightGBM, and most PyTorch and TensorFlow model architectures.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7c7IX8TpJKbaXdnLIwoD88","type":"Asset","createdAt":"2023-06-15T13:39:22.298Z","updatedAt":"2023-06-15T13:39:22.298Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"3 - Anyscale Databricks UI","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7c7IX8TpJKbaXdnLIwoD88/f17fb8c8884ce197c9ad35a8ce3f6b4e/3_-_Anyscale_Databricks_Product_UI.jpg","details":{"size":242920,"image":{"width":2226,"height":1230}},"fileName":"3 - Anyscale Databricks Product UI.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2muX02EO70X2MXN8wBRyUJ","type":"Asset","createdAt":"2023-06-15T13:41:08.580Z","updatedAt":"2023-06-15T13:41:08.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"4 - Anyscale Databricks Product UI 2","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2muX02EO70X2MXN8wBRyUJ/cc5301c192c66e632cb3141a9e5bde55/4_-_Anyscale_Databricks_Product_UI_2.jpg","details":{"size":383181,"image":{"width":1860,"height":1282}},"fileName":"4 - Anyscale Databricks Product UI 2.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlock new use cases with AI and ML ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nAnyscale and Ray integrate with most open source AI and ML libraries, enabling the latest innovation in AI to be applied to Databricks data. Ray makes working with Hugging face, XGBoost, LightGBM, TensorFlow and PyTorch and SciKit Learn a unified experience, with the added benefits of scaling with distributed training, tuning and serving.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical ML workload to train a LightGBM can be implemented in 20 lines of code.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6rUeSQ88fvbTd68jkhrii0","type":"Asset","createdAt":"2023-06-15T13:42:58.984Z","updatedAt":"2023-06-15T13:42:58.984Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"5 - Anyscale Databricks deployment","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6rUeSQ88fvbTd68jkhrii0/43de8dc64eaf8792ad73ac6faf8b4551/5_-_Anyscale_Databricks_deployment.jpg","details":{"size":272363,"image":{"width":1470,"height":1402}},"fileName":"5 - Anyscale Databricks deployment.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Want to see for yourself? ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/signup"},"content":[{"data":{},"marks":[],"value":"Request a Trial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" today!\n","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71IpYQvsxplh7okIL7RJVx","type":"Asset","createdAt":"2023-06-15T13:50:54.646Z","updatedAt":"2023-06-16T13:19:09.178Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"locale":"en-US"},"fields":{"title":"0 -Anyscale Databricks headline image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/71IpYQvsxplh7okIL7RJVx/b193e65fbc2d6500e3ccb2162d323080/0_-_Updated_Databricks_Anyscale_Image.png","details":{"size":90723,"image":{"width":1500,"height":1000}},"fileName":"0 - Updated Databricks Anyscale Image.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false,"recommendations":[]}}}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}}],"hideIntro":true,"bannerText":null,"bannerLink":null},"__N_SSG":true},"page":"/blog/[id]","query":{"id":"an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google"},"buildId":"-oT8_WeW9m_lAxr26cJvY","isFallback":false,"gsp":true,"customServer":true,"scriptLoader":[]}</script><script type="text/javascript" id="hs-script-loader" async="" defer="" src="https://js.hs-scripts.com/20523749.js"></script><script type="text/javascript">
  _linkedin_partner_id = "4210313";
  window._linkedin_data_partner_ids = window._linkedin_data_partner_ids || [];
  window._linkedin_data_partner_ids.push(_linkedin_partner_id);
</script><script type="text/javascript">
  (function(l) { if (!l){window.lintrk = function(a,b){window.lintrk.q.push([a,b])};
  window.lintrk.q=[]} var s = document.getElementsByTagName("script")[0];
  var b = document.createElement("script");
  b.type = "text/javascript";b.async = true;
  b.src = "https://snap.licdn.com/li.lms-analytics/insight.min.js";
  s.parentNode.insertBefore(b, s);})(window.lintrk);
</script><noscript><img height="1" width="1" style="display:none" alt="" src="https://px.ads.linkedin.com/collect/?pid=4210313&amp;fmt=gif"/></noscript></body></html>