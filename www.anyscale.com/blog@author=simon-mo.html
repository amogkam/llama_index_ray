<!DOCTYPE html><html lang="en-US"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Blog | Anyscale</title><meta name="description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta property="og:title" content="Blog | Anyscale"/><meta property="og:description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta property="og:site_name" content="Anyscale"/><meta property="og:image" content=""/><meta property="fb:app_id" content=""/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta name="twitter:site" content=""/><meta name="twitter:title" content="Blog | Anyscale"/><meta name="twitter:image" content=""/><meta name="next-head-count" content="14"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><link rel="icon" href="static/favicon.ico"/><script>
(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-P8H6KQG');
</script><script src="https://tag.clearbitscripts.com/v1/pk_7a96be585f731437b35c7bad25c432b7/tags.js"></script><script src="https://www.googleoptimize.com/optimize.js?id=OPT-NPGB9W3"></script><script>
  !function(f,b,e,v,n,t,s)
  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
  n.queue=[];t=b.createElement(e);t.async=!0;
  t.src=v;s=b.getElementsByTagName(e)[0];
  s.parentNode.insertBefore(t,s)}(window,document,'script',
  'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '168099398554987');
  fbq('track', 'PageView');
</script><link rel="stylesheet" href="static/font-awesome-4.7.0/css/font-awesome.min.css"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="true"/><script src="https://fast.wistia.com/assets/external/E-v1.js" async=""></script><meta name="ahrefs-site-verification" content="4be3c5d78cdad8823d038256ac06d4336ae7a84632dac0ed57975703fc14fe6b"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="_next/static/css/dbba207740458c19.css" as="style"/><link rel="stylesheet" href="_next/static/css/dbba207740458c19.css" data-n-g=""/><link rel="preload" href="_next/static/css/13fbfc51931a4b43.css" as="style"/><link rel="stylesheet" href="_next/static/css/13fbfc51931a4b43.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="_next/static/chunks/webpack-94adde92c5e1c75b.js" defer=""></script><script src="_next/static/chunks/framework-95c3ca6de27174dd.js" defer=""></script><script src="_next/static/chunks/main-774d7304f1973b2a.js" defer=""></script><script src="_next/static/chunks/pages/_app-fd8cb5a9775605f5.js" defer=""></script><script src="_next/static/chunks/0f1ac474-1cd8c801d37caef0.js" defer=""></script><script src="_next/static/chunks/1812-a871a62a8e8aa88c.js" defer=""></script><script src="_next/static/chunks/9065-e24d5adee6cbba35.js" defer=""></script><script src="_next/static/chunks/6139-f3c4647afbd26b94.js" defer=""></script><script src="_next/static/chunks/8423-007cfdc371b27898.js" defer=""></script><script src="_next/static/chunks/3167-65b612e959dd1945.js" defer=""></script><script src="_next/static/chunks/9027-e83e1bb65c284840.js" defer=""></script><script src="_next/static/chunks/pages/blog-1eafbb689a124ac5.js" defer=""></script><script src="_next/static/-oT8_WeW9m_lAxr26cJvY/_buildManifest.js" defer=""></script><script src="_next/static/-oT8_WeW9m_lAxr26cJvY/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Rubik&display=swap">@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4i1Uw.woff) format('woff')}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFWkUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFU0UzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFVUUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFWUUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFV0UzdYPFkZVO.woff) format('woff');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href="https://fonts.googleapis.com/css2?family=Outfit:wght@400;500;600;700&display=swap">@font-face{font-family:'Outfit';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4TC1C4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4QK1C4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4e6yC4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4deyC4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,400;0,700;1,400;1,700&display=swap">@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTeEw.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINscg.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKtdU.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7MIU.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGqZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuE6ZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuFKZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGKZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGaZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuF6ZJW9XjDg.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINce_fuJGl18QRY.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINccvfuJGl18QRY.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcdffuJGl18QRY.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINceffuJGl18QRY.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcePfuJGl18QRY.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcdvfuJGl18Q.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdzeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdXeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdLeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhd7eFaxOedfTDw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhd_eFaxOedfTDw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdHeFaxOedc.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIxsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIVsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIJsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AI5sdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AI9sdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIFsdP3pBms.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body class=""><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-P8H6KQG" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next" data-reactroot=""><svg xmlns="http://www.w3.org/2000/svg" class="sr-only"><symbol id="svg-arrow-right-gradient"><path d="M1.5 7.5h18" stroke="url(#paint0_linear_3927_132230)" stroke-width="1.5" stroke-linecap="round"></path><path d="m16.6562 3.65625 3.8442 3.84412-3.8442 3.84413" stroke="url(#paint1_linear_3927_132230)" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><defs><linearGradient id="paint0_linear_3927_132230" x1="1.06972" y1="8.58929" x2="12.2319" y2="-1.53819" gradientUnits="userSpaceOnUse"><stop stop-color="#8B20BE"></stop><stop offset="1" stop-color="#3579FF" stop-opacity=".97"></stop></linearGradient><linearGradient id="paint1_linear_3927_132230" x1="12.377" y1="7.75171" x2="16.9307" y2="11.8684" gradientUnits="userSpaceOnUse"><stop stop-color="#8B20BE"></stop><stop offset="1" stop-color="#3579FF" stop-opacity=".97"></stop></linearGradient></defs></symbol></svg><div class="Banner_container__zEmFV"><a href="https://raysummit.anyscale.com/?utm_source=anyscale&amp;utm_medium=website&amp;utm_campaign=ray_summit_2023&amp;utm_content=homepage_ticker" rel="noreferrer nofollow" target="_blank"><span class="Banner_banner-text__wr6Xi"><span class="Banner_main-text__cpJ6H">Are you passionate about Ray, scalable AI and distributed computing? Join us at Ray Summit!</span><span class="Banner_cta-button__0JCtt"> Find out more! <!-- --> <svg width="23" height="14" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1.5 6.75a.75.75 0 0 0 0 1.5v-1.5Zm18 1.5a.75.75 0 0 0 0-1.5v1.5Zm-18 0h18v-1.5h-18v1.5Z" fill="#fff"></path><path d="M17.187 3.126a.75.75 0 0 0-1.061 1.06l1.06-1.06ZM20.5 7.5l.53.53a.75.75 0 0 0 0-1.06l-.53.53Zm-4.374 3.314a.75.75 0 1 0 1.06 1.06l-1.06-1.06Zm0-6.627L19.97 8.03l1.06-1.061-3.843-3.844-1.061 1.06ZM19.97 6.97l-3.844 3.844 1.06 1.06 3.845-3.843L19.97 6.97Z" fill="#fff"></path></svg></span></span></a><button class="Banner_close-button__9ksfY" type="button" aria-label="Close Banner"></button></div><header class="inner-shadow sticky top-0 z-[1000] h-24 transform py-5 transition-transform duration-500 bg-primary translate-y-0"><div class="container mx-auto flex items-center justify-between"><a href="index.html" class="text-white"><svg width="141" height="32" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><g clip-path="url(#anyscale-logo_svg__a)" fill="currentColor"><path d="M10.653 17.46H3.865C1.697 17.46 0 19.16 0 21.332v6.797c0 2.17 1.697 3.87 3.865 3.87h6.788c2.169 0 3.866-1.7 3.866-3.87V21.33c0-2.171-1.792-3.87-3.866-3.87Zm1.509 10.573c0 .85-.66 1.51-1.509 1.51H3.865a1.49 1.49 0 0 1-1.508-1.51v-6.796c0-.85.66-1.51 1.508-1.51h6.788c.849 0 1.509.66 1.509 1.51v6.796Z"></path><path d="M28.095 11.988h-8.108V3.87C19.892 1.7 18.196 0 16.027 0H3.865C1.697 0 0 1.7 0 3.87v6.608c0 2.171 1.697 3.87 3.865 3.87h13.67v13.687c0 2.171 1.698 3.87 3.866 3.87h6.6c2.168 0 3.865-1.699 3.865-3.87V15.858c0-2.076-1.697-3.775-3.771-3.87Zm-24.23 0a1.49 1.49 0 0 1-1.508-1.51V3.87c0-.85.66-1.51 1.508-1.51h12.162c.849 0 1.509.66 1.509 1.51v8.118H3.866Zm25.738 16.047c0 .85-.66 1.51-1.508 1.51h-6.6a1.49 1.49 0 0 1-1.508-1.51V14.348h8.108c.848 0 1.508.66 1.508 1.51v12.177ZM79.194 17.462l-3.017-8.118h-3.3l4.714 11.327-2.546 5.853h3.112l7.165-17.18H82.21l-3.016 8.118ZM93.335 14.819c-.66-.284-1.414-.472-2.075-.661-.66-.189-1.225-.378-1.79-.66-.378-.19-.566-.473-.566-.85 0-.378.188-.661.472-.85.847-.472 1.79-.378 2.639.094.377.284.566.661.566 1.133h2.828c0-1.133-.566-2.171-1.414-2.832-.943-.755-2.169-1.133-3.3-1.038-.849 0-1.697.094-2.45.472-.66.283-1.227.66-1.604 1.227-.377.472-.565 1.133-.565 1.794 0 .66.188 1.321.565 1.887.377.472.849.85 1.414 1.039.66.283 1.414.472 2.168.66.66.19 1.226.378 1.886.661.377.19.566.472.566.85 0 .377-.189.66-.471.85-.377.283-.85.377-1.32.377-.472 0-1.038-.094-1.415-.472-.377-.283-.565-.66-.66-1.133h-2.922c0 .755.282 1.416.754 1.982.472.567 1.131 1.133 1.791 1.416.849.378 1.698.567 2.546.567.849 0 1.697-.095 2.45-.472.661-.283 1.227-.66 1.604-1.227.377-.567.565-1.133.565-1.794 0-.66-.188-1.321-.66-1.888-.565-.66-1.037-.944-1.602-1.132ZM102.575 11.702c.566 0 1.036.095 1.508.472.377.283.66.755.849 1.227h3.205a5.366 5.366 0 0 0-1.886-3.02 5.348 5.348 0 0 0-3.582-1.039c-1.037 0-2.074.284-3.017.755a5.486 5.486 0 0 0-2.074 2.077 7.427 7.427 0 0 0 0 6.325c.472.85 1.131 1.604 2.074 2.076.943.472 1.98.756 3.017.756 1.32 0 2.546-.378 3.582-1.133a5.366 5.366 0 0 0 1.886-3.02h-3.111c-.283 1.037-1.32 1.698-2.357 1.604-.754 0-1.509-.283-1.98-.944a5.119 5.119 0 0 1 0-5.192 1.914 1.914 0 0 1 1.886-.944ZM118.412 10.1a.405.405 0 0 1-.377.378c-.094 0-.188 0-.188-.095a3.382 3.382 0 0 0-.944-.66c-.659-.378-1.507-.567-2.262-.567-.943 0-1.885.283-2.734.755a4.459 4.459 0 0 0-1.885 2.077c-.472.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.376.85 1.037 1.604 1.885 2.17.849.473 1.697.756 2.64.756.754 0 1.603-.189 2.263-.567.659-.283 1.131-.755 1.602-1.321v1.699h2.923V9.345h-2.923v.755Zm-.377 6.891c-.283.472-.66.944-1.132 1.227-.47.283-1.037.378-1.507.378-.567 0-1.037-.189-1.509-.378-.472-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.094-1.228.472-1.794.848-1.51 2.64-1.982 4.147-1.227h.095c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM55.907 10.1a.406.406 0 0 1-.377.378c-.095 0-.19 0-.19-.095a3.384 3.384 0 0 0-.942-.66c-.66-.378-1.508-.567-2.263-.567-.942 0-1.885.283-2.733.755a4.463 4.463 0 0 0-1.886 2.077c-.471.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.377.85 1.037 1.604 1.886 2.17.848.473 1.697.756 2.64.756.753 0 1.602-.189 2.262-.567.66-.283 1.131-.755 1.603-1.321v1.699h2.922V9.345h-2.922v.755Zm-.472 6.891c-.283.472-.66.944-1.131 1.227-.471.283-1.037.378-1.509.378-.565 0-1.037-.189-1.508-.378-.471-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.095-1.228.472-1.794.848-1.51 2.64-1.982 4.148-1.227h.094c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM126.708 5.477h-2.923v15.575h2.923V5.477ZM140.191 14.914c0-1.038-.189-2.077-.755-3.02-.471-.85-1.131-1.605-2.074-1.983a6.78 6.78 0 0 0-6.033 0 5.485 5.485 0 0 0-2.075 2.077c-.565 1.038-.754 2.077-.754 3.21 0 1.132.189 2.17.754 3.114a5.485 5.485 0 0 0 2.075 2.077c.942.472 1.979.755 3.016.755a6.56 6.56 0 0 0 3.489-1.038c.943-.661 1.602-1.7 1.885-2.738h-3.016c-.377.944-1.321 1.51-2.358 1.416-.659 0-1.414-.283-1.885-.755-.566-.472-.848-1.227-.848-1.982h8.484c.095-.378.095-.755.095-1.133Zm-8.579-.85c.093-.755.377-1.415.942-1.887.566-.472 1.226-.661 1.886-.661.754 0 1.414.189 1.979.66.566.473.849 1.133.849 1.794l-5.656.095ZM67.692 9.159a4.7 4.7 0 0 0-1.98.472c-.377.188-.754.377-1.037.66a.286.286 0 0 1-.377 0c-.094-.094-.094-.094-.094-.188v-.756H61.28v11.705h2.923v-6.418c-.094-.755.188-1.51.754-2.171.471-.472 1.226-.756 1.886-.756.66 0 1.414.284 1.885.755.472.567.754 1.322.66 2.172v6.418h2.923v-6.89c.094-1.322-.283-2.644-1.226-3.682-.848-.85-2.168-1.416-3.394-1.321Z"></path></g><defs><clipPath id="anyscale-logo_svg__a"><path fill="currentColor" d="M0 0h140.19v32H0z"></path></clipPath></defs></svg></a><div class="lg:relative lg:flex lg:flex-row hidden"><div class="max-h-[calc(100vh-192px)] items-center overflow-y-auto py-[1px] shadow-[0_-1px_0_#DAE3F2] lg:flex lg:overflow-visible lg:p-0 lg:shadow-none"><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Products</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="platform.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Anyscale Platform</div></a><a href="ray-open-source.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Open Source</div></a></div></div></div></div></div></div><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Solutions</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="container mx-auto mb-4 lg:mb-0 mt-2 lg:mt-0 lg:mr-12"><div class="header-card h-full rounded-[5px] p-6 lg:w-[296px]"><h5 id="scalable-ai-and-python">Scalable AI and Python</h5>
<p>Leverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more.</p></div></div><div class="grid lg:flex grid-cols-1 md:grid-cols-2 gap-x-10 lg:gap-x-12 container mx-auto"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><p class="text-base leading-normal font-normal text-darkGrey-300 px-6 lg:px-4 mb-4">Scalable AI and Python</p><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2"><a href="data-ingestion.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Data Ingestion</div></a><a href="reinforcement-learning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Reinforcement Learning</div></a><a href="ray-air.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray AIR</div></a><a href="model-serving.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Model Serving</div></a><a href="hyperparameter-tuning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Hyperparameter Tuning</div></a></div></div></div><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><p class="text-base leading-normal font-normal text-darkGrey-300 px-6 lg:px-4 mb-4">Use Cases</p><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2"><a href="large-language-models.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Large Language Models</div></a><a href="demand-forecasting.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Demand Forecasting/Pricing</div></a><a href="industrial-automation.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Industrial Automation</div></a><a href="machine-learning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Scalable ML Platforms</div></a><a href="natural-language-processing.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">NLP</div></a><a href="recommendation-system.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Recommendation System</div></a></div></div></div></div></div></div></div><a class="block font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4 shadow-[0_1px_0_#DAE3F2] lg:shadow-none" href="ray-ecosystem.html"><div class="w-full container mx-auto">Ecosystem</div></a><a class="block font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4 shadow-[0_1px_0_#DAE3F2] lg:shadow-none" href="user-stories.html"><div class="w-full container mx-auto">Success Stories</div></a><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Learn</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="blog/how-ray-solves-common-production-challenges-for-generative-ai.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Blog</div></a><a href="https://raysummit.anyscale.com/?utm_source=anyscale&amp;utm_medium=website&amp;utm_campaign=ray_summit_2023&amp;utm_content=home_nav-pulldown"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Summit 2023</div></a><a href="event-category/rl-summit.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Videos &amp; Webinars</div></a><a href="https://github.com/ray-project/ray-educational-materials"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Training</div></a><a href="https://docs.anyscale.com"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Anyscale Docs</div></a></div></div></div></div></div></div><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Company</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="about.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">About us</div></a><a href="press.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">News</div></a><a href="careers.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Careers</div></a><a href="community.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Community</div></a></div></div></div></div></div></div></div><div class="absolute bottom-0 left-0 right-0 bg-white py-5 shadow-[0_-1px_0_#DAE3F2] lg:relative lg:ml-5 lg:bg-transparent lg:p-0 lg:shadow-none"><div class="container mx-auto flex justify-end"><a class="btn bg-gradient min-w-[320px] justify-center bg-blue-900 text-center lg:min-w-0" href="beta.html">Try It Now</a></div></div></div><div class="cursor-pointer py-4 pl-6 lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M20.05 11H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95ZM20.05 16H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95ZM20.05 6H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95Z" fill="#fff"></path></svg></div></div></header><div class="Layout_page-sections__2__gb overflow-x-hidden"><div class="ArticlesList_container__mBEpW"><div class="ArticlesList_inner__QWc69"><div class="ArticlesList_header__45BKa"><h1>Posts by Simon Mo</h1><div class="ArticlesList_spacer__8l_nL"></div></div><div class="BlogFilters_root__mrUMs"><div class="BlogFilters_inner__87PZK"><div class="BlogFilters_item__1GpID"><div class="wrapper"><select aria-label="Types" class="SelectDropdown_select__hNpf2"><option selected="">All Types</option><option value="news">News</option><option value="culture">Culture</option><option value="engineering">Engineering</option><option value="user-story">User Story</option></select><div class="SelectDropdown_select__hNpf2 SelectDropdown_select-dropdown__Jabke"><button type="button" aria-haspopup="true" aria-expanded="false" style="width:245px">All Types</button><ul role="menu" class="SelectDropdown_menu__3wacL"><li class="SelectDropdown_item__LO5La SelectDropdown_active__YhRi7" tabindex="0" role="menuitem">All Types</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">News</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Culture</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Engineering</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">User Story</li></ul></div></div></div><div class="BlogFilters_item__1GpID"><div class="wrapper"><select aria-label="Tags" class="SelectDropdown_select__hNpf2"><option selected="">All Products / Libraries</option><option value="anyscale">Anyscale</option><option value="ray_core">Ray Core</option><option value="ray-datasets">Ray Datasets</option><option value="ray_train">Ray Train</option><option value="ray-tune">Ray Tune</option><option value="ray_serve">Ray Serve</option><option value="rllib">Ray RLlib</option></select><div class="SelectDropdown_select__hNpf2 SelectDropdown_select-dropdown__Jabke"><button type="button" aria-haspopup="true" aria-expanded="false" style="width:245px">All Products / Libraries</button><ul role="menu" class="SelectDropdown_menu__3wacL"><li class="SelectDropdown_item__LO5La SelectDropdown_active__YhRi7" tabindex="0" role="menuitem">All Products / Libraries</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Anyscale</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Core</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Datasets</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Train</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Tune</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Serve</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray RLlib</li></ul></div></div></div></div></div><div class="empty"><h2>No posts found.</h2><p><a href="blog/how-ray-solves-common-production-challenges-for-generative-ai.html">View all</a></p></div><div class="ArticlesList_list__uP0RC"></div><div class="Pagination_container__FdBHw"></div></div></div></div><footer class="relative bg-primary pt-30 pb-30 md:pt-36 md:pb-[180px] lg:pt-20"><div class="container mx-auto flex flex-col space-y-12 md:space-y-16 lg:flex-row lg:justify-between lg:space-y-0"><div class="flex flex-col gap-16"><div class="flex flex-col gap-6"><div class="h-10 text-white"><svg width="141" height="32" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="origin-top-left scale-125"><g clip-path="url(#anyscale-logo_svg__a)" fill="currentColor"><path d="M10.653 17.46H3.865C1.697 17.46 0 19.16 0 21.332v6.797c0 2.17 1.697 3.87 3.865 3.87h6.788c2.169 0 3.866-1.7 3.866-3.87V21.33c0-2.171-1.792-3.87-3.866-3.87Zm1.509 10.573c0 .85-.66 1.51-1.509 1.51H3.865a1.49 1.49 0 0 1-1.508-1.51v-6.796c0-.85.66-1.51 1.508-1.51h6.788c.849 0 1.509.66 1.509 1.51v6.796Z"></path><path d="M28.095 11.988h-8.108V3.87C19.892 1.7 18.196 0 16.027 0H3.865C1.697 0 0 1.7 0 3.87v6.608c0 2.171 1.697 3.87 3.865 3.87h13.67v13.687c0 2.171 1.698 3.87 3.866 3.87h6.6c2.168 0 3.865-1.699 3.865-3.87V15.858c0-2.076-1.697-3.775-3.771-3.87Zm-24.23 0a1.49 1.49 0 0 1-1.508-1.51V3.87c0-.85.66-1.51 1.508-1.51h12.162c.849 0 1.509.66 1.509 1.51v8.118H3.866Zm25.738 16.047c0 .85-.66 1.51-1.508 1.51h-6.6a1.49 1.49 0 0 1-1.508-1.51V14.348h8.108c.848 0 1.508.66 1.508 1.51v12.177ZM79.194 17.462l-3.017-8.118h-3.3l4.714 11.327-2.546 5.853h3.112l7.165-17.18H82.21l-3.016 8.118ZM93.335 14.819c-.66-.284-1.414-.472-2.075-.661-.66-.189-1.225-.378-1.79-.66-.378-.19-.566-.473-.566-.85 0-.378.188-.661.472-.85.847-.472 1.79-.378 2.639.094.377.284.566.661.566 1.133h2.828c0-1.133-.566-2.171-1.414-2.832-.943-.755-2.169-1.133-3.3-1.038-.849 0-1.697.094-2.45.472-.66.283-1.227.66-1.604 1.227-.377.472-.565 1.133-.565 1.794 0 .66.188 1.321.565 1.887.377.472.849.85 1.414 1.039.66.283 1.414.472 2.168.66.66.19 1.226.378 1.886.661.377.19.566.472.566.85 0 .377-.189.66-.471.85-.377.283-.85.377-1.32.377-.472 0-1.038-.094-1.415-.472-.377-.283-.565-.66-.66-1.133h-2.922c0 .755.282 1.416.754 1.982.472.567 1.131 1.133 1.791 1.416.849.378 1.698.567 2.546.567.849 0 1.697-.095 2.45-.472.661-.283 1.227-.66 1.604-1.227.377-.567.565-1.133.565-1.794 0-.66-.188-1.321-.66-1.888-.565-.66-1.037-.944-1.602-1.132ZM102.575 11.702c.566 0 1.036.095 1.508.472.377.283.66.755.849 1.227h3.205a5.366 5.366 0 0 0-1.886-3.02 5.348 5.348 0 0 0-3.582-1.039c-1.037 0-2.074.284-3.017.755a5.486 5.486 0 0 0-2.074 2.077 7.427 7.427 0 0 0 0 6.325c.472.85 1.131 1.604 2.074 2.076.943.472 1.98.756 3.017.756 1.32 0 2.546-.378 3.582-1.133a5.366 5.366 0 0 0 1.886-3.02h-3.111c-.283 1.037-1.32 1.698-2.357 1.604-.754 0-1.509-.283-1.98-.944a5.119 5.119 0 0 1 0-5.192 1.914 1.914 0 0 1 1.886-.944ZM118.412 10.1a.405.405 0 0 1-.377.378c-.094 0-.188 0-.188-.095a3.382 3.382 0 0 0-.944-.66c-.659-.378-1.507-.567-2.262-.567-.943 0-1.885.283-2.734.755a4.459 4.459 0 0 0-1.885 2.077c-.472.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.376.85 1.037 1.604 1.885 2.17.849.473 1.697.756 2.64.756.754 0 1.603-.189 2.263-.567.659-.283 1.131-.755 1.602-1.321v1.699h2.923V9.345h-2.923v.755Zm-.377 6.891c-.283.472-.66.944-1.132 1.227-.47.283-1.037.378-1.507.378-.567 0-1.037-.189-1.509-.378-.472-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.094-1.228.472-1.794.848-1.51 2.64-1.982 4.147-1.227h.095c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM55.907 10.1a.406.406 0 0 1-.377.378c-.095 0-.19 0-.19-.095a3.384 3.384 0 0 0-.942-.66c-.66-.378-1.508-.567-2.263-.567-.942 0-1.885.283-2.733.755a4.463 4.463 0 0 0-1.886 2.077c-.471.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.377.85 1.037 1.604 1.886 2.17.848.473 1.697.756 2.64.756.753 0 1.602-.189 2.262-.567.66-.283 1.131-.755 1.603-1.321v1.699h2.922V9.345h-2.922v.755Zm-.472 6.891c-.283.472-.66.944-1.131 1.227-.471.283-1.037.378-1.509.378-.565 0-1.037-.189-1.508-.378-.471-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.095-1.228.472-1.794.848-1.51 2.64-1.982 4.148-1.227h.094c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM126.708 5.477h-2.923v15.575h2.923V5.477ZM140.191 14.914c0-1.038-.189-2.077-.755-3.02-.471-.85-1.131-1.605-2.074-1.983a6.78 6.78 0 0 0-6.033 0 5.485 5.485 0 0 0-2.075 2.077c-.565 1.038-.754 2.077-.754 3.21 0 1.132.189 2.17.754 3.114a5.485 5.485 0 0 0 2.075 2.077c.942.472 1.979.755 3.016.755a6.56 6.56 0 0 0 3.489-1.038c.943-.661 1.602-1.7 1.885-2.738h-3.016c-.377.944-1.321 1.51-2.358 1.416-.659 0-1.414-.283-1.885-.755-.566-.472-.848-1.227-.848-1.982h8.484c.095-.378.095-.755.095-1.133Zm-8.579-.85c.093-.755.377-1.415.942-1.887.566-.472 1.226-.661 1.886-.661.754 0 1.414.189 1.979.66.566.473.849 1.133.849 1.794l-5.656.095ZM67.692 9.159a4.7 4.7 0 0 0-1.98.472c-.377.188-.754.377-1.037.66a.286.286 0 0 1-.377 0c-.094-.094-.094-.094-.094-.188v-.756H61.28v11.705h2.923v-6.418c-.094-.755.188-1.51.754-2.171.471-.472 1.226-.756 1.886-.756.66 0 1.414.284 1.885.755.472.567.754 1.322.66 2.172v6.418h2.923v-6.89c.094-1.322-.283-2.644-1.226-3.682-.848-.85-2.168-1.416-3.394-1.321Z"></path></g><defs><clipPath id="anyscale-logo_svg__a"><path fill="currentColor" d="M0 0h140.19v32H0z"></path></clipPath></defs></svg></div><div class="font-light leading-normal text-lightBlue-300 opacity-70 hidden lg:flex"><span>Â© Anyscale, Inc 2022</span></div></div><div class="z-10 hidden flex-col gap-10 lg:flex"><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Anyscale</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://www.linkedin.com/company/joinanyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M15.784 14.168a1.667 1.667 0 1 1-3.334-.002 1.667 1.667 0 0 1 3.334.002Zm.05 2.9H12.5V27.5h3.334V17.067Zm5.266 0h-3.317V27.5h3.284v-5.475c0-3.05 3.975-3.334 3.975 0V27.5h3.291v-6.609c0-5.141-5.883-4.95-7.266-2.424l.033-1.4Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://www.facebook.com/AnyscaleCompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M21.667 21.247h2.083l.834-3.333h-2.917v-1.667c0-.858 0-1.666 1.666-1.666h1.25v-2.8a23.49 23.49 0 0 0-2.38-.117c-2.263 0-3.87 1.38-3.87 3.917v2.333h-2.5v3.333h2.5v7.084h3.334v-7.084Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://twitter.com/anyscalecompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/anyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Ray</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://twitter.com/raydistributed"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/ray-project/ray"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div></div></div><div class="grid grid-cols-2 gap-x-8 gap-y-12 md:grid-cols-3 md:gap-y-10 md:gap-x-16"><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Products</div><div><ul>
<li><a href="platform.html">Anyscale Compute Platform</a></li>
<li><a href="ray-open-source.html">Ray Open Source</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Events</div><div><ul>
<li><a href="event-category/rl-summit.html">Webinars</a></li>
<li><a href="event-category/rl-summit.html">Meetups</a></li>
<li><a href="event-category/rl-summit.html">Summits</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Company</div><div><ul>
<li><a href="about.html">About Us</a></li>
<li><a href="press.html">News</a></li>
<li><a href="careers.html">Careers</a></li>
<li><a href="community.html">Community</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Learn</div><div><ul>
<li><a href="blog/how-ray-solves-common-production-challenges-for-generative-ai.html">Blog</a></li>
<li><a href="event-category/rl-summit.html">Demos &amp; Webinars</a></li>
<li><a href="event-category/rl-summit.html">Anyscale Academy</a></li>
<li><a href="https://docs.anyscale.com">Anyscale Docs</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Scalable AI and Python</div><div><ul>
<li><a href="data-ingestion.html">Data Ingestion</a></li>
<li><a href="reinforcement-learning.html">Reinforcement Learning</a></li>
<li><a href="ray-air.html">Ray AIR</a></li>
<li><a href="model-serving.html">Model Serving</a></li>
<li><a href="hyperparameter-tuning.html">Hyperparameter Tuning</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Use Cases</div><div><ul>
<li><a href="demand-forecasting.html">Demand Forecasting/Pricing</a></li>
<li><a href="industrial-automation.html">Industrial Automation</a></li>
<li><a href="machine-learning.html">Scalable ML Platforms</a></li>
<li><a href="natural-language-processing.html">NLP</a></li>
<li><a href="recommendation-system.html">Recommendation System</a></li>
</ul>
</div></div></div><div class="grid-col-1 grid gap-12 md:grid-cols-3 md:gap-16 lg:hidden"><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Anyscale</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://www.linkedin.com/company/joinanyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M15.784 14.168a1.667 1.667 0 1 1-3.334-.002 1.667 1.667 0 0 1 3.334.002Zm.05 2.9H12.5V27.5h3.334V17.067Zm5.266 0h-3.317V27.5h3.284v-5.475c0-3.05 3.975-3.334 3.975 0V27.5h3.291v-6.609c0-5.141-5.883-4.95-7.266-2.424l.033-1.4Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://www.facebook.com/AnyscaleCompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M21.667 21.247h2.083l.834-3.333h-2.917v-1.667c0-.858 0-1.666 1.666-1.666h1.25v-2.8a23.49 23.49 0 0 0-2.38-.117c-2.263 0-3.87 1.38-3.87 3.917v2.333h-2.5v3.333h2.5v7.084h3.334v-7.084Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://twitter.com/anyscalecompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/anyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Ray</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://twitter.com/raydistributed"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/ray-project/ray"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="md:self-end md:text-right"><div class="font-light leading-normal text-lightBlue-300 opacity-70"><span>Â© Anyscale, Inc 2022</span></div></div></div></div><div class="pointer-events-none absolute bottom-0 left-0"><svg width="612" height="297" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="hidden lg:block"><g opacity="0.5" clip-path="url(#dots_svg__a)"><mask id="dots_svg__c" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="0" y="0" width="612" height="297"><path transform="rotate(90 611.5 .5)" fill="url(#dots_svg__b)" stroke="#F2F3F7" d="M611.5.5h296v611h-296z"></path></mask><g mask="url(#dots_svg__c)" stroke="#A2B7D4"><g opacity="0.2" stroke-linecap="round"><path d="M598 350v-688M582 350v-688M566 350v-688M550 350v-688M534 350v-688M518 350v-688M502 350v-688M486 350v-688M470 350v-688M454 350v-688M438 350v-688M422 350v-688M406 350v-688M390 350v-688M374 350v-688M358 350v-688M342 350v-688M326 350v-688M310 350v-688M294 350v-688M278 350v-688M262 350v-688M246 350v-688M230 350v-688M214 350v-688M198 350v-688M182 350v-688M166 350v-688M150 350v-688M134 350v-688M118 350v-688M102 350v-688M86 350v-688M70 350v-688M54 350v-688M38 350v-688M22 350v-688M6 350v-688"></path></g><g opacity="0.2" stroke-linecap="round"><path d="M1014 14H0M1014 30H0M1014 46H0M1014 62H0M1014 78H0M1014 94H0M1014 110H0M1014 126H0M1014 142H0M1014 158H0M1014 174H0M1014 190H4M1014 206H4M1014 222H4M1014 238H4M1014 254H4M1014 270H4M1014 286H4"></path></g><path fill="#4C6996" d="M615.5 12.5v3h-3v-3zM615.5 28.5v3h-3v-3zM615.5 44.5v3h-3v-3zM615.5 60.5v3h-3v-3zM615.5 76.5v3h-3v-3zM615.5 92.5v3h-3v-3zM615.5 108.5v3h-3v-3zM615.5 124.5v3h-3v-3zM615.5 140.5v3h-3v-3zM615.5 156.5v3h-3v-3zM615.5 172.5v3h-3v-3zM615.5 188.5v3h-3v-3zM615.5 204.5v3h-3v-3zM615.5 220.5v3h-3v-3zM615.5 236.5v3h-3v-3zM615.5 252.5v3h-3v-3zM615.5 268.5v3h-3v-3zM615.5 284.5v3h-3v-3zM599.5 12.5v3h-3v-3zM599.5 28.5v3h-3v-3zM599.5 44.5v3h-3v-3zM599.5 60.5v3h-3v-3zM599.5 76.5v3h-3v-3zM599.5 92.5v3h-3v-3zM599.5 108.5v3h-3v-3zM599.5 124.5v3h-3v-3zM599.5 140.5v3h-3v-3zM599.5 156.5v3h-3v-3zM599.5 172.5v3h-3v-3zM599.5 188.5v3h-3v-3zM599.5 204.5v3h-3v-3zM599.5 220.5v3h-3v-3zM599.5 236.5v3h-3v-3zM599.5 252.5v3h-3v-3zM599.5 268.5v3h-3v-3zM599.5 284.5v3h-3v-3zM583.5 12.5v3h-3v-3zM583.5 28.5v3h-3v-3zM583.5 44.5v3h-3v-3zM583.5 60.5v3h-3v-3zM583.5 76.5v3h-3v-3zM583.5 92.5v3h-3v-3zM583.5 108.5v3h-3v-3zM583.5 124.5v3h-3v-3zM583.5 140.5v3h-3v-3zM583.5 156.5v3h-3v-3zM583.5 172.5v3h-3v-3zM583.5 188.5v3h-3v-3zM583.5 204.5v3h-3v-3zM583.5 220.5v3h-3v-3zM583.5 236.5v3h-3v-3zM583.5 252.5v3h-3v-3zM583.5 268.5v3h-3v-3zM583.5 284.5v3h-3v-3zM567.5 12.5v3h-3v-3zM567.5 28.5v3h-3v-3zM567.5 44.5v3h-3v-3zM567.5 60.5v3h-3v-3zM567.5 76.5v3h-3v-3zM567.5 92.5v3h-3v-3zM567.5 108.5v3h-3v-3zM567.5 124.5v3h-3v-3zM567.5 140.5v3h-3v-3zM567.5 156.5v3h-3v-3zM567.5 172.5v3h-3v-3zM567.5 188.5v3h-3v-3zM567.5 204.5v3h-3v-3zM567.5 220.5v3h-3v-3zM567.5 236.5v3h-3v-3zM567.5 252.5v3h-3v-3zM567.5 268.5v3h-3v-3zM567.5 284.5v3h-3v-3zM551.5 12.5v3h-3v-3zM551.5 28.5v3h-3v-3zM551.5 44.5v3h-3v-3zM551.5 60.5v3h-3v-3zM551.5 76.5v3h-3v-3zM551.5 92.5v3h-3v-3zM551.5 108.5v3h-3v-3zM551.5 124.5v3h-3v-3zM551.5 140.5v3h-3v-3zM551.5 156.5v3h-3v-3zM551.5 172.5v3h-3v-3zM551.5 188.5v3h-3v-3zM551.5 204.5v3h-3v-3zM551.5 220.5v3h-3v-3zM551.5 236.5v3h-3v-3zM551.5 252.5v3h-3v-3zM551.5 268.5v3h-3v-3zM551.5 284.5v3h-3v-3zM535.5 12.5v3h-3v-3zM535.5 28.5v3h-3v-3zM535.5 44.5v3h-3v-3zM535.5 60.5v3h-3v-3zM535.5 76.5v3h-3v-3zM535.5 92.5v3h-3v-3zM535.5 108.5v3h-3v-3zM535.5 124.5v3h-3v-3zM535.5 140.5v3h-3v-3zM535.5 156.5v3h-3v-3zM535.5 172.5v3h-3v-3zM535.5 188.5v3h-3v-3zM535.5 204.5v3h-3v-3zM535.5 220.5v3h-3v-3zM535.5 236.5v3h-3v-3zM535.5 252.5v3h-3v-3zM535.5 268.5v3h-3v-3zM535.5 284.5v3h-3v-3zM519.5 12.5v3h-3v-3zM519.5 28.5v3h-3v-3zM519.5 44.5v3h-3v-3zM519.5 60.5v3h-3v-3zM519.5 76.5v3h-3v-3zM519.5 92.5v3h-3v-3zM519.5 108.5v3h-3v-3zM519.5 124.5v3h-3v-3zM519.5 140.5v3h-3v-3zM519.5 156.5v3h-3v-3zM519.5 172.5v3h-3v-3zM519.5 188.5v3h-3v-3zM519.5 204.5v3h-3v-3zM519.5 220.5v3h-3v-3zM519.5 236.5v3h-3v-3zM519.5 252.5v3h-3v-3zM519.5 268.5v3h-3v-3zM519.5 284.5v3h-3v-3zM503.5 12.5v3h-3v-3zM503.5 28.5v3h-3v-3zM503.5 44.5v3h-3v-3zM503.5 60.5v3h-3v-3zM503.5 76.5v3h-3v-3zM503.5 92.5v3h-3v-3zM503.5 108.5v3h-3v-3zM503.5 124.5v3h-3v-3zM503.5 140.5v3h-3v-3zM503.5 156.5v3h-3v-3zM503.5 172.5v3h-3v-3zM503.5 188.5v3h-3v-3zM503.5 204.5v3h-3v-3zM503.5 220.5v3h-3v-3zM503.5 236.5v3h-3v-3zM503.5 252.5v3h-3v-3zM503.5 268.5v3h-3v-3zM503.5 284.5v3h-3v-3zM487.5 12.5v3h-3v-3zM487.5 28.5v3h-3v-3zM487.5 44.5v3h-3v-3zM487.5 60.5v3h-3v-3zM487.5 76.5v3h-3v-3zM487.5 92.5v3h-3v-3zM487.5 108.5v3h-3v-3zM487.5 124.5v3h-3v-3zM487.5 140.5v3h-3v-3zM487.5 156.5v3h-3v-3zM487.5 172.5v3h-3v-3zM487.5 188.5v3h-3v-3zM487.5 204.5v3h-3v-3zM487.5 220.5v3h-3v-3zM487.5 236.5v3h-3v-3zM487.5 252.5v3h-3v-3zM487.5 268.5v3h-3v-3zM487.5 284.5v3h-3v-3zM471.5 12.5v3h-3v-3zM471.5 28.5v3h-3v-3zM471.5 44.5v3h-3v-3zM471.5 60.5v3h-3v-3zM471.5 76.5v3h-3v-3zM471.5 92.5v3h-3v-3zM471.5 108.5v3h-3v-3zM471.5 124.5v3h-3v-3zM471.5 140.5v3h-3v-3zM471.5 156.5v3h-3v-3zM471.5 172.5v3h-3v-3zM471.5 188.5v3h-3v-3zM471.5 204.5v3h-3v-3zM471.5 220.5v3h-3v-3zM471.5 236.5v3h-3v-3zM471.5 252.5v3h-3v-3zM471.5 268.5v3h-3v-3zM471.5 284.5v3h-3v-3zM455.5 12.5v3h-3v-3zM455.5 28.5v3h-3v-3zM455.5 44.5v3h-3v-3zM455.5 60.5v3h-3v-3zM455.5 76.5v3h-3v-3zM455.5 92.5v3h-3v-3zM455.5 108.5v3h-3v-3zM455.5 124.5v3h-3v-3zM455.5 140.5v3h-3v-3zM455.5 156.5v3h-3v-3zM455.5 172.5v3h-3v-3zM455.5 188.5v3h-3v-3zM455.5 204.5v3h-3v-3zM455.5 220.5v3h-3v-3zM455.5 236.5v3h-3v-3zM455.5 252.5v3h-3v-3zM455.5 268.5v3h-3v-3zM455.5 284.5v3h-3v-3zM439.5 12.5v3h-3v-3zM439.5 28.5v3h-3v-3zM439.5 44.5v3h-3v-3zM439.5 60.5v3h-3v-3zM439.5 76.5v3h-3v-3zM439.5 92.5v3h-3v-3zM439.5 108.5v3h-3v-3zM439.5 124.5v3h-3v-3zM439.5 140.5v3h-3v-3zM439.5 156.5v3h-3v-3zM439.5 172.5v3h-3v-3zM439.5 188.5v3h-3v-3zM439.5 204.5v3h-3v-3zM439.5 220.5v3h-3v-3zM439.5 236.5v3h-3v-3zM439.5 252.5v3h-3v-3zM439.5 268.5v3h-3v-3zM439.5 284.5v3h-3v-3zM423.5 12.5v3h-3v-3zM423.5 28.5v3h-3v-3zM423.5 44.5v3h-3v-3zM423.5 60.5v3h-3v-3zM423.5 76.5v3h-3v-3zM423.5 92.5v3h-3v-3zM423.5 108.5v3h-3v-3zM423.5 124.5v3h-3v-3zM423.5 140.5v3h-3v-3zM423.5 156.5v3h-3v-3zM423.5 172.5v3h-3v-3zM423.5 188.5v3h-3v-3zM423.5 204.5v3h-3v-3zM423.5 220.5v3h-3v-3zM423.5 236.5v3h-3v-3zM423.5 252.5v3h-3v-3zM423.5 268.5v3h-3v-3zM423.5 284.5v3h-3v-3zM407.5 12.5v3h-3v-3zM407.5 28.5v3h-3v-3zM407.5 44.5v3h-3v-3zM407.5 60.5v3h-3v-3zM407.5 76.5v3h-3v-3zM407.5 92.5v3h-3v-3zM407.5 108.5v3h-3v-3zM407.5 124.5v3h-3v-3zM407.5 140.5v3h-3v-3zM407.5 156.5v3h-3v-3zM407.5 172.5v3h-3v-3zM407.5 188.5v3h-3v-3zM407.5 204.5v3h-3v-3zM407.5 220.5v3h-3v-3zM407.5 236.5v3h-3v-3zM407.5 252.5v3h-3v-3zM407.5 268.5v3h-3v-3zM407.5 284.5v3h-3v-3zM391.5 12.5v3h-3v-3zM391.5 28.5v3h-3v-3zM391.5 44.5v3h-3v-3zM391.5 60.5v3h-3v-3zM391.5 76.5v3h-3v-3zM391.5 92.5v3h-3v-3zM391.5 108.5v3h-3v-3zM391.5 124.5v3h-3v-3zM391.5 140.5v3h-3v-3zM391.5 156.5v3h-3v-3zM391.5 172.5v3h-3v-3zM391.5 188.5v3h-3v-3zM391.5 204.5v3h-3v-3zM391.5 220.5v3h-3v-3zM391.5 236.5v3h-3v-3zM391.5 252.5v3h-3v-3zM391.5 268.5v3h-3v-3zM391.5 284.5v3h-3v-3zM375.5 12.5v3h-3v-3zM375.5 28.5v3h-3v-3zM375.5 44.5v3h-3v-3zM375.5 60.5v3h-3v-3zM375.5 76.5v3h-3v-3zM375.5 92.5v3h-3v-3zM375.5 108.5v3h-3v-3zM375.5 124.5v3h-3v-3zM375.5 140.5v3h-3v-3zM375.5 156.5v3h-3v-3zM375.5 172.5v3h-3v-3zM375.5 188.5v3h-3v-3zM375.5 204.5v3h-3v-3zM375.5 220.5v3h-3v-3z"></path><rect x="375.5" y="236.5" width="3" height="3" rx="1.5" transform="rotate(90 375.5 236.5)" fill="#4C6996"></rect><path fill="#4C6996" d="M375.5 252.5v3h-3v-3zM375.5 268.5v3h-3v-3zM375.5 284.5v3h-3v-3zM359.5 12.5v3h-3v-3zM359.5 28.5v3h-3v-3zM359.5 44.5v3h-3v-3zM359.5 60.5v3h-3v-3zM359.5 76.5v3h-3v-3zM359.5 92.5v3h-3v-3zM359.5 108.5v3h-3v-3zM359.5 124.5v3h-3v-3zM359.5 140.5v3h-3v-3zM359.5 156.5v3h-3v-3zM359.5 172.5v3h-3v-3zM359.5 188.5v3h-3v-3zM359.5 204.5v3h-3v-3zM359.5 220.5v3h-3v-3zM359.5 236.5v3h-3v-3zM359.5 252.5v3h-3v-3zM359.5 268.5v3h-3v-3zM359.5 284.5v3h-3v-3zM343.5 12.5v3h-3v-3zM343.5 28.5v3h-3v-3zM343.5 44.5v3h-3v-3zM343.5 60.5v3h-3v-3zM343.5 76.5v3h-3v-3zM343.5 92.5v3h-3v-3zM343.5 108.5v3h-3v-3zM343.5 124.5v3h-3v-3zM343.5 140.5v3h-3v-3zM343.5 156.5v3h-3v-3zM343.5 172.5v3h-3v-3zM343.5 188.5v3h-3v-3zM343.5 204.5v3h-3v-3zM343.5 220.5v3h-3v-3zM343.5 236.5v3h-3v-3zM343.5 252.5v3h-3v-3zM343.5 268.5v3h-3v-3zM343.5 284.5v3h-3v-3zM327.5 12.5v3h-3v-3zM327.5 28.5v3h-3v-3zM327.5 44.5v3h-3v-3zM327.5 60.5v3h-3v-3zM327.5 76.5v3h-3v-3zM327.5 92.5v3h-3v-3zM327.5 108.5v3h-3v-3zM327.5 124.5v3h-3v-3zM327.5 140.5v3h-3v-3zM327.5 156.5v3h-3v-3zM327.5 172.5v3h-3v-3zM327.5 188.5v3h-3v-3zM327.5 204.5v3h-3v-3zM327.5 220.5v3h-3v-3zM327.5 236.5v3h-3v-3zM327.5 252.5v3h-3v-3zM327.5 268.5v3h-3v-3zM327.5 284.5v3h-3v-3zM311.5 12.5v3h-3v-3zM311.5 28.5v3h-3v-3zM311.5 44.5v3h-3v-3zM311.5 60.5v3h-3v-3zM311.5 76.5v3h-3v-3zM311.5 92.5v3h-3v-3zM311.5 108.5v3h-3v-3zM311.5 124.5v3h-3v-3zM311.5 140.5v3h-3v-3zM311.5 156.5v3h-3v-3zM311.5 172.5v3h-3v-3zM311.5 188.5v3h-3v-3zM311.5 204.5v3h-3v-3zM311.5 220.5v3h-3v-3zM311.5 236.5v3h-3v-3zM311.5 252.5v3h-3v-3zM311.5 268.5v3h-3v-3zM311.5 284.5v3h-3v-3zM295.5 12.5v3h-3v-3zM295.5 28.5v3h-3v-3zM295.5 44.5v3h-3v-3zM295.5 60.5v3h-3v-3zM295.5 76.5v3h-3v-3zM295.5 92.5v3h-3v-3zM295.5 108.5v3h-3v-3zM295.5 124.5v3h-3v-3zM295.5 140.5v3h-3v-3zM295.5 156.5v3h-3v-3zM295.5 172.5v3h-3v-3zM295.5 188.5v3h-3v-3zM295.5 204.5v3h-3v-3zM295.5 220.5v3h-3v-3zM295.5 236.5v3h-3v-3zM295.5 252.5v3h-3v-3zM295.5 268.5v3h-3v-3zM295.5 284.5v3h-3v-3zM279.5 12.5v3h-3v-3zM279.5 28.5v3h-3v-3zM279.5 44.5v3h-3v-3zM279.5 60.5v3h-3v-3zM279.5 76.5v3h-3v-3zM279.5 92.5v3h-3v-3zM279.5 108.5v3h-3v-3zM279.5 124.5v3h-3v-3zM279.5 140.5v3h-3v-3zM279.5 156.5v3h-3v-3zM279.5 172.5v3h-3v-3zM279.5 188.5v3h-3v-3zM279.5 204.5v3h-3v-3zM279.5 220.5v3h-3v-3zM279.5 236.5v3h-3v-3zM279.5 252.5v3h-3v-3zM279.5 268.5v3h-3v-3zM279.5 284.5v3h-3v-3zM263.5 12.5v3h-3v-3zM263.5 28.5v3h-3v-3zM263.5 44.5v3h-3v-3zM263.5 60.5v3h-3v-3zM263.5 76.5v3h-3v-3zM263.5 92.5v3h-3v-3zM263.5 108.5v3h-3v-3zM263.5 124.5v3h-3v-3zM263.5 140.5v3h-3v-3zM263.5 156.5v3h-3v-3zM263.5 172.5v3h-3v-3zM263.5 188.5v3h-3v-3zM263.5 204.5v3h-3v-3zM263.5 220.5v3h-3v-3zM263.5 236.5v3h-3v-3zM263.5 252.5v3h-3v-3zM263.5 268.5v3h-3v-3zM263.5 284.5v3h-3v-3zM247.5 12.5v3h-3v-3zM247.5 28.5v3h-3v-3zM247.5 44.5v3h-3v-3zM247.5 60.5v3h-3v-3zM247.5 76.5v3h-3v-3zM247.5 92.5v3h-3v-3zM247.5 108.5v3h-3v-3zM247.5 124.5v3h-3v-3zM247.5 140.5v3h-3v-3zM247.5 156.5v3h-3v-3zM247.5 172.5v3h-3v-3zM247.5 188.5v3h-3v-3zM247.5 204.5v3h-3v-3zM247.5 220.5v3h-3v-3zM247.5 236.5v3h-3v-3zM247.5 252.5v3h-3v-3zM247.5 268.5v3h-3v-3zM247.5 284.5v3h-3v-3zM231.5 12.5v3h-3v-3zM231.5 28.5v3h-3v-3zM231.5 44.5v3h-3v-3zM231.5 60.5v3h-3v-3zM231.5 76.5v3h-3v-3zM231.5 92.5v3h-3v-3zM231.5 108.5v3h-3v-3zM231.5 124.5v3h-3v-3zM231.5 140.5v3h-3v-3zM231.5 156.5v3h-3v-3zM231.5 172.5v3h-3v-3zM231.5 188.5v3h-3v-3zM231.5 204.5v3h-3v-3zM231.5 220.5v3h-3v-3zM231.5 236.5v3h-3v-3zM231.5 252.5v3h-3v-3zM231.5 268.5v3h-3v-3zM231.5 284.5v3h-3v-3zM215.5 12.5v3h-3v-3zM215.5 28.5v3h-3v-3zM215.5 44.5v3h-3v-3zM215.5 60.5v3h-3v-3zM215.5 76.5v3h-3v-3zM215.5 92.5v3h-3v-3zM215.5 108.5v3h-3v-3zM215.5 124.5v3h-3v-3zM215.5 140.5v3h-3v-3zM215.5 156.5v3h-3v-3zM215.5 172.5v3h-3v-3zM215.5 188.5v3h-3v-3zM215.5 204.5v3h-3v-3zM215.5 220.5v3h-3v-3zM215.5 236.5v3h-3v-3zM215.5 252.5v3h-3v-3zM215.5 268.5v3h-3v-3zM215.5 284.5v3h-3v-3zM199.5 12.5v3h-3v-3zM199.5 28.5v3h-3v-3zM199.5 44.5v3h-3v-3zM199.5 60.5v3h-3v-3zM199.5 76.5v3h-3v-3zM199.5 92.5v3h-3v-3zM199.5 108.5v3h-3v-3zM199.5 124.5v3h-3v-3zM199.5 140.5v3h-3v-3zM199.5 156.5v3h-3v-3zM199.5 172.5v3h-3v-3zM199.5 188.5v3h-3v-3zM199.5 204.5v3h-3v-3zM199.5 220.5v3h-3v-3zM199.5 236.5v3h-3v-3zM199.5 252.5v3h-3v-3zM199.5 268.5v3h-3v-3zM199.5 284.5v3h-3v-3zM183.5 12.5v3h-3v-3zM183.5 28.5v3h-3v-3zM183.5 44.5v3h-3v-3zM183.5 60.5v3h-3v-3zM183.5 76.5v3h-3v-3zM183.5 92.5v3h-3v-3zM183.5 108.5v3h-3v-3zM183.5 124.5v3h-3v-3zM183.5 140.5v3h-3v-3zM183.5 156.5v3h-3v-3zM183.5 172.5v3h-3v-3zM183.5 188.5v3h-3v-3zM183.5 204.5v3h-3v-3zM183.5 220.5v3h-3v-3zM183.5 236.5v3h-3v-3zM183.5 252.5v3h-3v-3zM183.5 268.5v3h-3v-3zM183.5 284.5v3h-3v-3zM167.5 12.5v3h-3v-3zM167.5 28.5v3h-3v-3zM167.5 44.5v3h-3v-3zM167.5 60.5v3h-3v-3zM167.5 76.5v3h-3v-3zM167.5 92.5v3h-3v-3zM167.5 108.5v3h-3v-3zM167.5 124.5v3h-3v-3zM167.5 140.5v3h-3v-3zM167.5 156.5v3h-3v-3zM167.5 172.5v3h-3v-3zM167.5 188.5v3h-3v-3zM167.5 204.5v3h-3v-3zM167.5 220.5v3h-3v-3zM167.5 236.5v3h-3v-3zM167.5 252.5v3h-3v-3zM167.5 268.5v3h-3v-3zM167.5 284.5v3h-3v-3zM151.5 12.5v3h-3v-3zM151.5 28.5v3h-3v-3zM151.5 44.5v3h-3v-3zM151.5 60.5v3h-3v-3zM151.5 76.5v3h-3v-3zM151.5 92.5v3h-3v-3zM151.5 108.5v3h-3v-3zM151.5 124.5v3h-3v-3zM151.5 140.5v3h-3v-3zM151.5 156.5v3h-3v-3zM151.5 172.5v3h-3v-3zM151.5 188.5v3h-3v-3zM151.5 204.5v3h-3v-3zM151.5 220.5v3h-3v-3zM151.5 236.5v3h-3v-3zM151.5 252.5v3h-3v-3zM151.5 268.5v3h-3v-3zM151.5 284.5v3h-3v-3zM135.5 12.5v3h-3v-3zM135.5 28.5v3h-3v-3zM135.5 44.5v3h-3v-3zM135.5 60.5v3h-3v-3zM135.5 76.5v3h-3v-3zM135.5 92.5v3h-3v-3zM135.5 108.5v3h-3v-3zM135.5 124.5v3h-3v-3zM135.5 140.5v3h-3v-3zM135.5 156.5v3h-3v-3zM135.5 172.5v3h-3v-3zM135.5 188.5v3h-3v-3zM135.5 204.5v3h-3v-3zM135.5 220.5v3h-3v-3zM135.5 236.5v3h-3v-3zM135.5 252.5v3h-3v-3zM135.5 268.5v3h-3v-3zM135.5 284.5v3h-3v-3zM119.5 12.5v3h-3v-3zM119.5 28.5v3h-3v-3zM119.5 44.5v3h-3v-3zM119.5 60.5v3h-3v-3zM119.5 76.5v3h-3v-3zM119.5 92.5v3h-3v-3zM119.5 108.5v3h-3v-3zM119.5 124.5v3h-3v-3zM119.5 140.5v3h-3v-3zM119.5 156.5v3h-3v-3zM119.5 172.5v3h-3v-3zM119.5 188.5v3h-3v-3zM119.5 204.5v3h-3v-3zM119.5 220.5v3h-3v-3zM119.5 236.5v3h-3v-3zM119.5 252.5v3h-3v-3zM119.5 268.5v3h-3v-3zM119.5 284.5v3h-3v-3zM103.5 12.5v3h-3v-3zM103.5 28.5v3h-3v-3zM103.5 44.5v3h-3v-3zM103.5 60.5v3h-3v-3zM103.5 76.5v3h-3v-3zM103.5 92.5v3h-3v-3zM103.5 108.5v3h-3v-3zM103.5 124.5v3h-3v-3zM103.5 140.5v3h-3v-3zM103.5 156.5v3h-3v-3zM103.5 172.5v3h-3v-3zM103.5 188.5v3h-3v-3zM103.5 204.5v3h-3v-3zM103.5 220.5v3h-3v-3zM103.5 236.5v3h-3v-3zM103.5 252.5v3h-3v-3zM103.5 268.5v3h-3v-3zM103.5 284.5v3h-3v-3zM87.5 12.5v3h-3v-3zM87.5 28.5v3h-3v-3zM87.5 44.5v3h-3v-3zM87.5 60.5v3h-3v-3zM87.5 76.5v3h-3v-3zM87.5 92.5v3h-3v-3zM87.5 108.5v3h-3v-3zM87.5 124.5v3h-3v-3zM87.5 140.5v3h-3v-3zM87.5 156.5v3h-3v-3zM87.5 172.5v3h-3v-3zM87.5 188.5v3h-3v-3zM87.5 204.5v3h-3v-3zM87.5 220.5v3h-3v-3zM87.5 236.5v3h-3v-3zM87.5 252.5v3h-3v-3zM87.5 268.5v3h-3v-3zM87.5 284.5v3h-3v-3zM71.5 12.5v3h-3v-3zM71.5 28.5v3h-3v-3zM71.5 44.5v3h-3v-3zM71.5 60.5v3h-3v-3zM71.5 76.5v3h-3v-3zM71.5 92.5v3h-3v-3zM71.5 108.5v3h-3v-3zM71.5 124.5v3h-3v-3zM71.5 140.5v3h-3v-3zM71.5 156.5v3h-3v-3zM71.5 172.5v3h-3v-3zM71.5 188.5v3h-3v-3zM71.5 204.5v3h-3v-3zM71.5 220.5v3h-3v-3zM71.5 236.5v3h-3v-3zM71.5 252.5v3h-3v-3zM71.5 268.5v3h-3v-3zM71.5 284.5v3h-3v-3zM55.5 12.5v3h-3v-3zM55.5 28.5v3h-3v-3zM55.5 44.5v3h-3v-3zM55.5 60.5v3h-3v-3zM55.5 76.5v3h-3v-3zM55.5 92.5v3h-3v-3zM55.5 108.5v3h-3v-3zM55.5 124.5v3h-3v-3zM55.5 140.5v3h-3v-3zM55.5 156.5v3h-3v-3zM55.5 172.5v3h-3v-3zM55.5 188.5v3h-3v-3zM55.5 204.5v3h-3v-3zM55.5 220.5v3h-3v-3zM55.5 236.5v3h-3v-3zM55.5 252.5v3h-3v-3zM55.5 268.5v3h-3v-3zM55.5 284.5v3h-3v-3zM39.5 12.5v3h-3v-3zM39.5 28.5v3h-3v-3zM39.5 44.5v3h-3v-3zM39.5 60.5v3h-3v-3zM39.5 76.5v3h-3v-3zM39.5 92.5v3h-3v-3zM39.5 108.5v3h-3v-3zM39.5 124.5v3h-3v-3zM39.5 140.5v3h-3v-3zM39.5 156.5v3h-3v-3zM39.5 172.5v3h-3v-3zM39.5 188.5v3h-3v-3zM39.5 204.5v3h-3v-3zM39.5 220.5v3h-3v-3zM39.5 236.5v3h-3v-3zM39.5 252.5v3h-3v-3zM39.5 268.5v3h-3v-3zM39.5 284.5v3h-3v-3zM23.5 12.5v3h-3v-3zM23.5 28.5v3h-3v-3zM23.5 44.5v3h-3v-3zM23.5 60.5v3h-3v-3zM23.5 76.5v3h-3v-3zM23.5 92.5v3h-3v-3zM23.5 108.5v3h-3v-3zM23.5 124.5v3h-3v-3zM23.5 140.5v3h-3v-3zM23.5 156.5v3h-3v-3zM23.5 172.5v3h-3v-3zM23.5 188.5v3h-3v-3zM23.5 204.5v3h-3v-3zM23.5 220.5v3h-3v-3zM23.5 236.5v3h-3v-3zM23.5 252.5v3h-3v-3zM23.5 268.5v3h-3v-3zM23.5 284.5v3h-3v-3zM7.5 12.5v3h-3v-3zM7.5 28.5v3h-3v-3zM7.5 44.5v3h-3v-3zM7.5 60.5v3h-3v-3zM7.5 76.5v3h-3v-3zM7.5 92.5v3h-3v-3zM7.5 108.5v3h-3v-3zM7.5 124.5v3h-3v-3zM7.5 140.5v3h-3v-3zM7.5 156.5v3h-3v-3zM7.5 172.5v3h-3v-3zM7.5 188.5v3h-3v-3zM7.5 204.5v3h-3v-3zM7.5 220.5v3h-3v-3zM7.5 236.5v3h-3v-3zM7.5 252.5v3h-3v-3zM7.5 268.5v3h-3v-3zM7.5 284.5v3h-3v-3z"></path></g></g><defs><radialGradient id="dots_svg__b" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="rotate(-136.519 627.256 92.637) scale(929.292 677.674)"><stop stop-color="#D9D9D9"></stop><stop offset="0.574" stop-color="#D9D9D9" stop-opacity="0"></stop></radialGradient><clipPath id="dots_svg__a"><path fill="#fff" transform="rotate(90 306 306)" d="M0 0h297v612H0z"></path></clipPath></defs></svg></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"site":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ke4xTi5OD9gnn2YtRXyCV","type":"Entry","createdAt":"2022-10-14T08:31:23.161Z","updatedAt":"2023-06-06T20:01:24.167Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"site"}},"locale":"en-US"},"fields":{"title":"Anyscale","description":"From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab.","identifier":"site-master-new","footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"16eb9I0yhaIALNOPR8WKTY","type":"Entry","createdAt":"2020-09-27T16:03:59.581Z","updatedAt":"2022-10-27T01:20:31.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer","body":"Â© Anyscale, Inc 2022","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"C7AXPFHtleWPOZ9BuT73K","type":"Asset","createdAt":"2020-09-28T14:40:50.925Z","updatedAt":"2020-09-28T14:40:50.925Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"anyscale_logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/C7AXPFHtleWPOZ9BuT73K/927f5130df1357db041d7a7908ab572e/Graphic_only__transparent_background_.png","details":{"size":1415,"image":{"width":331,"height":331}},"fileName":"Graphic only (transparent background).png","contentType":"image/png"}}}],"subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1cDdKq2eUfklc6w52dU6dg","type":"Entry","createdAt":"2020-09-27T16:05:42.307Z","updatedAt":"2022-10-27T01:20:26.714Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-1","header":"Products","body":"- [Anyscale Compute Platform](/platform)\n- [Ray Open Source](/ray-open-source)\n"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5tpXhcAJwSdhX8Wqw9HYTt","type":"Entry","createdAt":"2020-09-27T16:06:37.409Z","updatedAt":"2022-10-27T01:20:21.887Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":10,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-2","header":"Events","body":"- [Webinars](/events)\n- [Meetups](/events)\n- [Summits](/events)\n"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3c3LEomScM93hIg3i2zItU","type":"Entry","createdAt":"2020-09-27T16:07:42.856Z","updatedAt":"2022-10-27T01:20:18.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-3","header":"Company","body":"- [About Us](/about)\n- [News](/press)\n- [Careers](/careers)\n- [Community](/community)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6du4SSXPTxDZKoIVNHVVnT","type":"Entry","createdAt":"2022-10-27T01:20:13.320Z","updatedAt":"2022-10-27T01:20:13.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-4","header":"Learn","body":"- [Blog](/blog)\n- [Demos \u0026 Webinars](/events)\n- [Anyscale Academy](/events)\n- [Anyscale Docs](https://docs.anyscale.com)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1hovCXhGcWlgJmrqNzJ79k","type":"Entry","createdAt":"2022-10-27T01:20:09.895Z","updatedAt":"2022-10-27T01:20:09.895Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-5","header":"Scalable AI and Python","body":"- [Data Ingestion](/data-ingestion)\n- [Reinforcement Learning](/reinforcement-learning)\n- [Ray AIR](/ray-air)\n- [Model Serving](/model-serving)\n- [Hyperparameter Tuning](/hyperparameter-tuning)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"YFy5fUqwOD5ShdMaYEvzB","type":"Entry","createdAt":"2022-10-27T01:20:06.631Z","updatedAt":"2022-10-27T01:20:06.631Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-6","header":"Use Cases","body":"- [Demand Forecasting/Pricing](/demand-forecasting)\n- [Industrial Automation](/industrial-automation)\n- [Scalable ML Platforms](/machine-learning)\n- [NLP](/natural-language-processing)\n- [Recommendation System](/recommendation-system)"}}]}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"KBAZ5OavkdZfNTVjdJkp3","type":"Entry","createdAt":"2022-10-14T10:47:29.682Z","updatedAt":"2022-10-14T10:47:29.682Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Products","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KNN1Kmzo7jxSlxX1k74XQ","type":"Entry","createdAt":"2022-10-07T13:14:07.892Z","updatedAt":"2022-10-07T13:14:07.892Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"52Mmru1brS6ZkmItPeIW86","type":"Entry","createdAt":"2022-10-07T13:13:22.198Z","updatedAt":"2022-11-03T01:33:41.860Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Anyscale Platform","link":"/platform"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6cN6ZwdiFNiPcqIhd7EiQY","type":"Entry","createdAt":"2022-10-07T13:13:49.673Z","updatedAt":"2022-10-07T13:13:49.673Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Open Source","link":"/ray-open-source"}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5R1OTMfuLnPqont0Fjn9Xb","type":"Entry","createdAt":"2022-10-07T13:20:19.578Z","updatedAt":"2022-10-14T10:43:21.550Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Solutions","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1mp3IF3ZJE5JAViu3L9E95","type":"Entry","createdAt":"2022-10-07T13:16:00.346Z","updatedAt":"2022-10-07T13:21:01.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Scalable AI and Python","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4VUyKorAkgQP2V295h8eW3","type":"Entry","createdAt":"2022-10-07T13:15:35.949Z","updatedAt":"2022-10-07T13:15:35.949Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Data Ingestion","link":"/data-ingestion"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"Ouf5WgAzYmLovCZFNWqYm","type":"Entry","createdAt":"2022-10-07T13:17:09.728Z","updatedAt":"2022-10-07T13:17:09.728Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Reinforcement Learning","link":"/reinforcement-learning"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5kVPKqUbXBbKmrkvsImcIi","type":"Entry","createdAt":"2022-10-07T13:17:50.307Z","updatedAt":"2022-10-07T13:17:50.307Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray AIR","link":"/ray-air"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5uUidEvWw1oqjWrRcskEW","type":"Entry","createdAt":"2022-10-07T13:18:21.246Z","updatedAt":"2022-10-07T13:18:21.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Model Serving","link":"/model-serving"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Rbw0CrBN1KecPafCPvF4H","type":"Entry","createdAt":"2022-10-07T13:18:45.601Z","updatedAt":"2022-10-07T13:18:45.601Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Hyperparameter Tuning","link":"/hyperparameter-tuning"}}],"card":"##### Scalable AI and Python\n\nLeverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more."}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2RgWCuIiZe2TI6Nx5Ns7sB","type":"Entry","createdAt":"2022-10-07T13:25:03.101Z","updatedAt":"2023-05-31T12:47:25.274Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Use Cases","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3XMbpU1mPdsU7gktltlAzX","type":"Entry","createdAt":"2023-05-31T12:43:54.011Z","updatedAt":"2023-05-31T12:43:54.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Large Language Models","link":"/large-language-models"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6MAYw6HTSmH5JHcOoYjCzC","type":"Entry","createdAt":"2022-10-07T13:22:29.165Z","updatedAt":"2022-10-07T13:22:29.165Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Demand Forecasting/Pricing","link":"/demand-forecasting"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"30BlKhv6h4neaZwjA2jlxO","type":"Entry","createdAt":"2022-10-07T13:23:09.471Z","updatedAt":"2022-10-07T13:23:09.471Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Industrial Automation","link":"/industrial-automation"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2gGwUVxDuBeUso1uQH2gEW","type":"Entry","createdAt":"2022-10-07T13:23:32.145Z","updatedAt":"2022-10-07T13:23:32.145Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Scalable ML Platforms","link":"/machine-learning"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1cbwbmIvP5ZkV9iNEraQGP","type":"Entry","createdAt":"2022-10-07T13:23:59.732Z","updatedAt":"2022-10-07T13:23:59.732Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"NLP","link":"/natural-language-processing"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2A7FMbckpN5bgW1wp5ZuSh","type":"Entry","createdAt":"2022-10-07T13:24:41.861Z","updatedAt":"2022-10-07T13:24:41.861Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Recommendation System","link":"/recommendation-system"}}],"card":"##### Use Cases\n\nLeverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more."}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QNdrsmqz82oiBJvFTQ26n","type":"Entry","createdAt":"2023-04-13T11:37:45.354Z","updatedAt":"2023-04-13T11:37:45.354Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ecosystem","link":"/ray-ecosystem"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i0nHjF3pmjzGAdEE4V5MG","type":"Entry","createdAt":"2021-12-05T02:08:18.719Z","updatedAt":"2022-03-03T02:01:08.894Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Success Stories","link":"/user-stories"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5mFZvzY56aQCY4xvlBDi9W","type":"Entry","createdAt":"2022-10-14T10:46:35.218Z","updatedAt":"2022-10-14T10:46:35.218Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Learn","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"32tQWjMRAI5qci97W3cuie","type":"Entry","createdAt":"2022-10-07T13:28:46.057Z","updatedAt":"2023-03-22T20:31:55.243Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JQbOCOkxHfH2ZPGNNQVbt","type":"Entry","createdAt":"2021-12-05T14:57:28.664Z","updatedAt":"2021-12-05T14:57:28.664Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Blog","link":"/blog"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4qEkJgYkXZmiDq3lsxg0xW","type":"Entry","createdAt":"2023-03-22T20:31:21.666Z","updatedAt":"2023-03-22T20:31:21.666Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Summit 2023","link":"https://raysummit.anyscale.com/?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=home_nav-pulldown"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48sdeWXptwt3UgEZzeTt3k","type":"Entry","createdAt":"2022-10-07T13:26:28.331Z","updatedAt":"2022-10-07T13:26:28.331Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Videos \u0026 Webinars","link":"/events"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6hMfRHFDZUXcJH1tIq0xhY","type":"Entry","createdAt":"2021-10-21T20:59:36.918Z","updatedAt":"2023-03-06T20:24:26.811Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Training","link":"https://github.com/ray-project/ray-educational-materials"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cVWy7hmqbAzJCW6d7cLUx","type":"Entry","createdAt":"2022-10-07T13:28:33.537Z","updatedAt":"2022-10-07T13:28:33.537Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Anyscale Docs","link":"https://docs.anyscale.com"}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Qjs7zNfhrinHTNnzMpVIO","type":"Entry","createdAt":"2022-10-14T10:44:42.403Z","updatedAt":"2022-10-14T10:44:42.403Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Company","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ZFDrG8YGgU0oyNgBOId7X","type":"Entry","createdAt":"2022-10-07T13:32:38.746Z","updatedAt":"2022-10-07T13:32:38.746Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2RD65wmpn4gmL5Bvxu5VOD","type":"Entry","createdAt":"2020-09-08T17:53:32.413Z","updatedAt":"2021-06-20T15:04:11.482Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"About us","link":"/about"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71EHE1t3QrjnQb8hD9O8Ww","type":"Entry","createdAt":"2022-10-07T13:32:07.593Z","updatedAt":"2022-10-07T13:32:07.593Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"News","link":"/press"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3TNUPaz7ViQ6hbiZciBOQV","type":"Entry","createdAt":"2020-09-28T23:19:39.838Z","updatedAt":"2022-05-12T19:18:18.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Careers","link":"/careers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ubc5oYY5moS7HexIp7AjO","type":"Entry","createdAt":"2022-07-22T20:40:53.600Z","updatedAt":"2022-07-22T20:40:53.600Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Community","link":"/community"}}]}}]}}],"bannerText":"Are you passionate about Ray, scalable AI and distributed computing? Join us at Ray Summit!","bannerLink":"https://raysummit.anyscale.com/?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=homepage_ticker","bannerCta":" Find out more! "}},"title":"Blog","slug":"blog","sections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6nWVrKik3UzKQc0m6QjMQ7","type":"Entry","createdAt":"2020-09-01T18:35:40.585Z","updatedAt":"2023-06-20T17:29:52.368Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":59,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"featured-posts","header":"Blog","subheader":"Featured Posts and News","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7xkHJWpQ3mRxrgAQw3oYFk","type":"Entry","createdAt":"2023-06-15T13:47:45.558Z","updatedAt":"2023-06-16T13:24:48.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Introducing the Anyscale Databricks Connector","seoTitle":"Integrating Anyscale Ray with Databricks","slug":"introducing-the-anyscale-databricks-connector","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3e6JquwKuIz6QvCWO0BAz","type":"Entry","createdAt":"2023-06-15T13:16:27.453Z","updatedAt":"2023-06-15T13:16:27.453Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Greene","slug":"eric-greene","link":"https://www.linkedin.com/in/greene/"}}],"publishedDate":"2023-06-15","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Introducing the Anyscale Databricks connector. Simple Data Transfer and Efficient AI Workflows with Anyscale and Databricks.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The Anyscale Databricks Connector is a new Ray Datasets capability that facilitates easy data transfer between Databricks clusters and Anyscale hosted Ray clusters. The connector makes it easy for Data Scientists using Anyscale to leverage their Databricks data lake during machine learning and discovery.Â  It also enables a simpler way for machine learning engineers to create end-to-end workloads by allowing the entire ML pipeline to be executed within a single Python script. By taking advantage of the highly-scalable nature of Ray and Ray Datasets, machine learning workloads such as training, tuning and batch serving jobs can be executed more quickly and at lower cost, all while taking advantage of the latest advancements in AI through other Ray integrations with Hugging Face, XGBoost, LightGBM and many AI frameworks and libraries.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Key Benefits","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simple access to data","nodeType":"text"},{"data":{},"marks":[],"value":" - Access Databricks data through SQL queries within Anyscale Workspaces Visual Code or JupyterHub environments, empowering data science discovery and development.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Improved data security and governance","nodeType":"text"},{"data":{},"marks":[],"value":" - Ensure data security and governance by directly copying data from Databricks data lakes to Ray clusters, eliminating the need for intermediate steps and maintaining separate controls over sensitive data. All data is encrypted in transit and at rest.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Highly-scalable data exchange","nodeType":"text"},{"data":{},"marks":[],"value":" - Take advantage of the parallel read and write capabilities of Ray Datasets, enabling the exchange of terabytes of data in minutes. This capability significantly speeds up data-intensive operations and enhances scalability while reducing job run times and overall costs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simplified workload development","nodeType":"text"},{"data":{},"marks":[],"value":" - Simplify machine learning workflows by consolidating all the necessary logic into a single script. This script can query features, train and tune models, and score and materialize results back into the data lake, streamlining the entire process.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Unlock the latest AI Innovations","nodeType":"text"},{"data":{},"marks":[],"value":" - Leverage the power of Databricks for querying and joining data, while benefiting from the scalability and simplicity of Ray AIR for machine learning and AI development. Ray AIR provides integration to the latest AI innovations such as pretrained Hugging Face language models.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Simple, Secure and Scalable Data Access","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing the Anyscale Databricks connector, large datasets can be queried from a Databricks SQL warehouse, and quickly transferred into a Ray Dataset distributed across the Ray cluster. The Databricks connector reads query results in chunks of data, in parallel across the Ray cluster. The size of the dataset and the speed at which it can be transferred scales based on the size of your Ray cluster and the number of simultaneous requests the underlying Databricks SQL warehouse supports.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6taTlZK9ffeivO5RCUJiQ4","type":"Asset","createdAt":"2023-06-15T13:27:43.116Z","updatedAt":"2023-06-15T13:27:43.116Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"1 - Anyscale Databricks Architecture","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6taTlZK9ffeivO5RCUJiQ4/c5fc0bfef768f0fb6e8785bf71ded1e2/1_-_Anyscale_Databricks_Arch.jpg","details":{"size":95989,"image":{"width":761,"height":478}},"fileName":"1 - Anyscale Databricks Arch.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Anyscale Databricks connectors, parallel reads and writes and overall speed will scale with the size of the Ray cluster. The benchmarks for reading and writing demonstrate how the cluster scales out to support larger data sets.  ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2KimYEe845rk6fb2oYaCmz","type":"Asset","createdAt":"2023-06-15T13:35:36.597Z","updatedAt":"2023-06-15T13:35:36.597Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2 - Anyscale Databricks Table Example","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2KimYEe845rk6fb2oYaCmz/8302017d6535b5d676d3dfb50dc12cbd/2_-_Anyscale_Databricks_Table_Example.jpg","details":{"size":218942,"image":{"width":2178,"height":1002}},"fileName":"2 - Anyscale Databricks Table Example.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Simplified ML Development","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing the Databricks connector from within an Anyscale Workspaces, Data Scientists and Machine Learning Engineers have a unified experience while developing workloads. The data query, feature engineering, training, tuning and inference can all be executed within a single Python script that scales across a Ray cluster. Ray integrates with the latest AI and Machine learning libraries, enabling the most advanced ML workloads that work with 3rd party libs  like Hugging Face, XGBoost, LightGBM, and most PyTorch and TensorFlow model architectures.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7c7IX8TpJKbaXdnLIwoD88","type":"Asset","createdAt":"2023-06-15T13:39:22.298Z","updatedAt":"2023-06-15T13:39:22.298Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"3 - Anyscale Databricks UI","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7c7IX8TpJKbaXdnLIwoD88/f17fb8c8884ce197c9ad35a8ce3f6b4e/3_-_Anyscale_Databricks_Product_UI.jpg","details":{"size":242920,"image":{"width":2226,"height":1230}},"fileName":"3 - Anyscale Databricks Product UI.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2muX02EO70X2MXN8wBRyUJ","type":"Asset","createdAt":"2023-06-15T13:41:08.580Z","updatedAt":"2023-06-15T13:41:08.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"4 - Anyscale Databricks Product UI 2","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2muX02EO70X2MXN8wBRyUJ/cc5301c192c66e632cb3141a9e5bde55/4_-_Anyscale_Databricks_Product_UI_2.jpg","details":{"size":383181,"image":{"width":1860,"height":1282}},"fileName":"4 - Anyscale Databricks Product UI 2.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlock new use cases with AI and ML ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nAnyscale and Ray integrate with most open source AI and ML libraries, enabling the latest innovation in AI to be applied to Databricks data. Ray makes working with Hugging face, XGBoost, LightGBM, TensorFlow and PyTorch and SciKit Learn a unified experience, with the added benefits of scaling with distributed training, tuning and serving.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical ML workload to train a LightGBM can be implemented in 20 lines of code.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6rUeSQ88fvbTd68jkhrii0","type":"Asset","createdAt":"2023-06-15T13:42:58.984Z","updatedAt":"2023-06-15T13:42:58.984Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"5 - Anyscale Databricks deployment","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6rUeSQ88fvbTd68jkhrii0/43de8dc64eaf8792ad73ac6faf8b4551/5_-_Anyscale_Databricks_deployment.jpg","details":{"size":272363,"image":{"width":1470,"height":1402}},"fileName":"5 - Anyscale Databricks deployment.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Want to see for yourself? ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/signup"},"content":[{"data":{},"marks":[],"value":"Request a Trial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" today!\n","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71IpYQvsxplh7okIL7RJVx","type":"Asset","createdAt":"2023-06-15T13:50:54.646Z","updatedAt":"2023-06-16T13:19:09.178Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"locale":"en-US"},"fields":{"title":"0 -Anyscale Databricks headline image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/71IpYQvsxplh7okIL7RJVx/b193e65fbc2d6500e3ccb2162d323080/0_-_Updated_Databricks_Anyscale_Image.png","details":{"size":90723,"image":{"width":1500,"height":1000}},"fileName":"0 - Updated Databricks Anyscale Image.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2GdxAQ2xXKoNlzKc7DXvLS","type":"Entry","createdAt":"2023-05-31T19:04:09.560Z","updatedAt":"2023-06-01T03:29:01.792Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Announcing Aviary: Open Source Multi-LLM Serving","slug":"announcing-aviary-open-source-multi-llm-serving-solution","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-05-31","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Exec Summary","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": Open source LLMs are getting better every day, so today weâre open sourcing Aviary to make testing, evaluating and deploying Open Source LLMs easier â we found it harder than we thought it should be so we used Ray Serve to fix it.Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Try it out with Aviary Explorer ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://aviary.anyscale.com/"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Github Repo ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/aviary"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Video demo and explanation ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.youtube.com/watch?v=WmqPfQOXJ-4"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Slack support ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Forum support ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/c/llms-generative-ai/27"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Want a managed version of Aviary? Sign up ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://bit.ly/run-aviary"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". \n\nWeâre excited to announce the release of Aviary: a new open source project that simplifies and enables easy self-hosted serving of multiple LLM models efficiently.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Weâre big fans of open source LLMs here at Anyscale. The rate of improvement of open source LLMs has been ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.semianalysis.com/p/google-we-have-no-moat-and-neither"},"content":[{"nodeType":"text","value":"nothing short of phenomenal","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". This has given the AI community many options beyond the big âclosedâ players like OpenAI, Anthropic, Cohere and more.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Why are companies exploring self hosted open source LLMs? There are a few reasons: ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Cost: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"LLMs are very expensive to operate. A single query can cost tens of cents. Using open source can be considerably cheaper.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Latency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"By colocating LLMs and business logic, sometimes even on the same machine, latency â one of the biggest issues with deploying LLMs â can be kept low.Â Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Transparency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Organizations want to understand exactly what is happening in their models. Open source allows them to understand whatâs happening inside them.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Deployment flexibility: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Open Source models can be deployed on premise, in the cloud using the userâs cloud resources, or as part of a SaaS offering.Â Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Data Control: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Data governance issues are much easier to guarantee if the data does not leave your control.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Customization:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" Customizing/fine-tuning on proprietary data to enable domain specific answers at high quality.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We were writing some tools for ourselves to really understand the answers to the questions above, as well as the advantages of closed models, mainly focused around quality. A funny thing happened on the way, however: we discovered that while the open source models have improved, the infrastructure for serving LLMs has not kept up. We started building our own library for loading the models, autoscaling them efficiently, etc on top of Ray Serve, dealing with variations between the models on things like stop tokens etc.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It was then that we realized, actually the rest of the LLM community might benefit from this library as well. So, today we are releasing Aviary as open source.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary helps leverage the advantages of open source models by building on the solid foundation of ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io"},"content":[{"nodeType":"text","value":"Ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", the popular framework for scalable AI. In particular, it takes advantage of ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"nodeType":"text","value":"Ray Serve","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", the highly flexible serving framework that is part of Ray.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary does this by:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Providing an extensive suite of pre-configured open source LLMs, with reasonable defaults that work out of the box.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Integrating acceleration approaches like DeepSpeed with the packaged LLMs.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Simplifying the deployment of multiple LLMs within a single unified framework.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Simplifying the addition of new LLMs to within minutes in most cases","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Offering unique autoscaling support, including scale-to-zero (a first in open source).Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Weâve also included a demo Gradio frontend that shows off whatâs possible with these capabilities, as well as some command line tools to address the evaluation questions that originally motivated this project.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"You can see these features in the video below:Â ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jx26kCBQB1GRKEulIDxoi","type":"Entry","createdAt":"2023-05-31T02:10:20.974Z","updatedAt":"2023-05-31T02:10:20.974Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Aviary Announcement Video","videoUrl":"https://www.youtube.com/watch?v=WmqPfQOXJ-4"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"While existing solutions have some of these features individually and we are grateful to build on their foundations (such as Hugging Faceâs text-generation-inference), none of the existing solutions brings these capabilities together in a way that is convenient for users. We are also planning to continue to expand the feature set of Aviary, adding support for features like streaming, continuous batching and others.Â Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary is also open to community contributions â especially for adding new LLMs. Weâll then redeploy these in production so the rest of the world can use those LLMs too. We will also be supporting Aviary through Slack, Discourse, GitHub issues and an LLM-based RayBot.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"At the same time we understand that not everyone wants to take on the additional challenges of maintaining their own aviary of models. For that reason, we will also be offering a hosted version of Aviary that builds on top of our Anyscale managed Ray platform and offers additional features around deployment including:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Using spot instances with on-demand fallback for large potential savings","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Zero downtime upgrades with no dropped requests","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Faster deployment and autoscaling","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Improved observability tools.Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We are also announcing that Aviary will be available at no additional charge for existing Anyscale customers via our workspaces solution, and weâre actively onboarding new Aviary customers now. If youâd like to deploy Aviary, please reach out to us ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://bit.ly/run-aviary"},"content":[{"nodeType":"text","value":"here","marks":[],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1c2aG5pfLWU6YenPBZIHvI","type":"Asset","createdAt":"2023-05-31T19:00:13.845Z","updatedAt":"2023-05-31T19:00:13.845Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Aviary Light Mode","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1c2aG5pfLWU6YenPBZIHvI/742f647b979b4eb4b682e37fc35c71a7/aviary-light-mode.png","details":{"size":712180,"image":{"width":3030,"height":1750}},"fileName":"aviary-light-mode.png","contentType":"image/png"}}},"mainImageFit":"contain","hideIntro":true,"showMainImage":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UsGSYssf1ebf8N5mRbNxT","type":"Entry","createdAt":"2023-05-17T17:26:50.771Z","updatedAt":"2023-05-24T20:17:43.464Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Numbers every LLM Developer should know","slug":"num-every-llm-developer-should-know","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-05-17","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"When I was at Google, there was a document put together by Jeff Dean, the legendary engineer, called ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://brenocon.com/dean_perf.html"},"content":[{"nodeType":"text","value":"Numbers every Engineer should know","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Itâs really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Prompts","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"40-90%: Amount saved by appending âBe Conciseâ to your prompt","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Itâs important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money [1]. This can be broadened beyond simply appending âbe conciseâ to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"1.3: Average tokens per word","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"LLMs operate on tokens. Tokens are words or sub-parts of words, so âeatingâ might be broken into two tokens âeatâ and âingâ.Â A 750 word document will be about 1000 tokens.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Knowing this ratio is important because most billing is done in tokens, and the LLMâs context window size is also defined in tokens.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Prices","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Prices [2] are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~50: Cost Ratio of GPT-4 to GPT-3.5 Turbo","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"What this means is that for many practical applications, itâs much better to use GPT-4 for things like generation and then use that data to fine tune a smaller model. It is roughly 50 times cheaper to use GPT-3.5-Turbo [3] than GPT-4 (the âroughlyâ is because GPT-4 charges differently for the prompt and the generated output)Â  â so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"5: Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embeddingÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. âWhat is the capital of Delaware?â when looked up in an neural information retrieval system costs about 5x less [4] than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x!Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"10: Cost Ratio of OpenAI embedding to Self-Hosted embeddingÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using HuggingFaceâs SentenceTransformers (which are pretty much as good as OpenAIâs embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees).Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"6: Cost Ratio of OpenAI base vs fine tuned model queries","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"1: Cost Ratio of Self-Hosted base vs fine-tuned model queriesÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If youâre self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Training and Fine Tuning\n","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://arxiv.org/abs/2302.13971"},"content":[{"nodeType":"text","value":"LLaMa paper","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. Thatâs not something most companies can do (shameless plug time: of course, we at Anyscale can â thatâs our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray"},"content":[{"nodeType":"text","value":"bread and butter","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"! Contact us if youâd like to learn more). The point is that training your own LLM is possible, but itâs not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"\u003c 0.001: Cost ratio of fine tuning vs training from scratch","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using"},"content":[{"nodeType":"text","value":"6B parameter model for about $7","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Even at OpenAIâs rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), youâre looking at $40. However, fine tuning is one thing and training from scratch is another [5].\n","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"GPU Memory","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If youâre self-hosting a model, itâs really important to understand GPU memory because LLMs push your GPUâs memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It may seem strange, but itâs important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"2x number of parameters: Typical GPU memory requirements of an LLM for serving","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. Thereâs usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy there is too much loss of resolution. Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but thatâs atypical.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~1GB: Typical GPU memory requirements of an embedding model","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/"},"content":[{"nodeType":"text","value":"sentence transformers","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". OpenAI also has its own embeddings that they provide commercially.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"You typically donât have to worry about how much memory embeddings take on the GPU, theyâre fairly small. Weâve even had the embedding and the LLM on the same GPU.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"\u003e10x: Throughput improvement from batching LLM requestsÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second.Â  The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~1 MB: GPU Memory required for 1 token of output with a 13B parameter model","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say â I have 24GB to spare, whatâs 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but itâs still a real issue.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"embedded-entry-inline","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1ACoDuIrrm71E0BWViBO4Y","type":"Entry","createdAt":"2023-05-17T17:26:34.593Z","updatedAt":"2023-05-17T17:26:34.593Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"inlineImage"}},"locale":"en-US"},"fields":{"name":"numbers-cheatsheet","image":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1AkaPcJlWoSpqixtqeKcD1","type":"Asset","createdAt":"2023-05-17T16:48:22.989Z","updatedAt":"2023-05-17T20:48:45.978Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"numbers-cheatsheet","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1AkaPcJlWoSpqixtqeKcD1/9505980d855c36120b4818980745fd00/Screenshot_2023-05-17_at_1.46.09_PM.png","details":{"size":550573,"image":{"width":2194,"height":1734}},"fileName":"Screenshot 2023-05-17 at 1.46.09 PM.png","contentType":"image/png"}}},"isRetina2x":false}}},"content":[]},{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Next Steps","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Review the up-to-date metrics referenced in this blog in the following ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/llm-numbers"},"content":[{"nodeType":"text","value":"Github repo","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". \n\nSee our earlier ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"nodeType":"text","value":"blog series on solving Generative AI infrastructure","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" ","marks":[{"type":"underline"}],"data":{}},{"nodeType":"text","value":"and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"nodeType":"text","value":"using LangChain with Ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in learning more about Ray, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io/"},"content":[{"nodeType":"text","value":"Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://docs.ray.io/"},"content":[{"nodeType":"text","value":"Docs.Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nTo connect with the Ray community join #LLM on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"Ray Slack","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" or our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"Discuss forum","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://www.anyscale.com/platform"},"content":[{"nodeType":"text","value":"Anyscale.com/Platform ","marks":[],"data":{}}]},{"nodeType":"text","value":"and click the 'Try it now' button","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Summit 2023:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"nodeType":"text","value":"Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"nodeType":"text","value":"practical training focused on LLMs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"hr","data":{},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[1] Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2022-05-14.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[2] Retrieved from ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://openai.com/pricing"},"content":[{"nodeType":"text","value":"http://openai.com/pricing","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on 2022-05-14.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[3] ","marks":[],"data":{}},{"nodeType":"text","value":"GPT-4","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). ","marks":[],"data":{}},{"nodeType":"text","value":"GPT-3.5 Turbo","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": 0.2c/1k tokens.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[4] This assumes the vector lookup is âfree.â Itâs not, but it uses CPUs (much cheaper) and is fairly fast.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[5] 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3ZnD84ZZAfH60U5ncBebaO","type":"Asset","createdAt":"2023-05-17T15:56:36.431Z","updatedAt":"2023-05-17T18:04:30.622Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"numbers-cover-image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3ZnD84ZZAfH60U5ncBebaO/227f68207ec7731e789f342e7ec320e8/Screenshot_2023-05-17_at_10.12.01_AM.png","details":{"size":445922,"image":{"width":2348,"height":1616}},"fileName":"Screenshot 2023-05-17 at 10.12.01 AM.png","contentType":"image/png"}}},"mainImageFit":"contain","hideIntro":true,"showMainImage":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3vUo7K4aHmhARGamZX7Ujj","type":"Entry","createdAt":"2023-05-11T15:17:24.618Z","updatedAt":"2023-06-16T21:13:49.520Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Streaming distributed execution across CPUs and GPUs","seoTitle":"Streaming distributed execution across CPUs and GPUs","slug":"streaming-distributed-execution-across-cpus-and-gpus","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2aJRcdEj9iAktAnqn0wucE","type":"Entry","createdAt":"2021-02-16T07:30:38.421Z","updatedAt":"2021-02-16T07:30:38.421Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Stephanie Wang","slug":"stephanie-wang","link":"https://github.com/stephanie-wang"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2edothjNBpyQEu2aX55KRl","type":"Entry","createdAt":"2023-05-10T15:30:16.699Z","updatedAt":"2023-05-10T15:30:16.699Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Cheng Su","slug":"cheng-su"}}],"publishedDate":"2023-05-11","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"In a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/offline-batch-inference-comparing-ray-apache-spark-and-sagemaker"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"previous blog,","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" we showed how Ray Data can provide speedups over SageMaker and frameworks like Apache Spark for large-scale batch inference workloads. This blog post delves further into how Ray Data streaming works and how to use it for your own ML pipelines.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Some of the most demanding machine learning (ML) use cases we have encountered involve pipelines that span both CPU and GPU devices in distributed environments. These situations arise in various workloads, including:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Batch inference, which involves a CPU-intensive preprocessing stage (e.g., video decoding or image resizing) before utilizing a GPU-intensive model to make predictions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed training, where similar CPU-heavy transformations are required to prepare or augment the dataset prior to GPU training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Interestingly, in many of these workloads, the preprocessing steps often prove to be the bottleneck. This can happen when preprocessing requires ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"parallelism across multiple nodes","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"significant memory to buffer results","nodeType":"text"},{"data":{},"marks":[],"value":" between the CPU and GPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For instance, consider the decoding of compressed images in memory. A typical JPEG decompression ratio is 10:1, which implies that the output memory size may be ten times that of the input, resulting in substantial memory pressure in addition to CPU load. The challenges become more complex with other data modalities, such as video; for example, H264 can decompress at a 2000:1 ratio, producing 200GB of frame outputs for a 100MB file input. This means that practitioners trying to offload CPU-heavy preprocessing onto multiple CPUs or machines have to deal with intermediate results that are far larger than their already sizable source datasets:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7ntuAY4zsUXtrtIxSofnyg","type":"Asset","createdAt":"2023-05-10T15:33:44.406Z","updatedAt":"2023-05-10T18:55:03.562Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"streaming_figure_1","description":"Figure 1. Decoding of compressed images with large images spilled to disk","file":{"url":"//images.ctfassets.net/xjan103pcp94/7ntuAY4zsUXtrtIxSofnyg/1fdb9631162d6b66777d205d1d3a7e1c/Screenshot_from_2023-05-10_11-00-43.png","details":{"size":64895,"image":{"width":1493,"height":436}},"fileName":"Screenshot from 2023-05-10 11-00-43.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Executed naively, these compute and memory-intensive workloads fail to fully utilize expensive GPU hardware. For example, we could buffer 200GB of frame outputs on disk, but that could add minutes of overheads. Even if this is overlapped with GPU computation, we can end up with long pauses on the GPU. This motivates fully streamed execution in the cluster setting, which avoids such delays by streaming intermediate data through cluster memory:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"40VpN8OKxEM7wHohV802gE","type":"Asset","createdAt":"2023-05-10T15:36:35.326Z","updatedAt":"2023-05-10T18:56:06.767Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_2","description":"Figure 2. Decoding intermediate data streamed through memory","file":{"url":"//images.ctfassets.net/xjan103pcp94/40VpN8OKxEM7wHohV802gE/c07086bfe677dc5e3109496f5a4ec083/Screenshot_from_2023-05-10_11-01-00.png","details":{"size":35974,"image":{"width":1199,"height":268}},"fileName":"Screenshot from 2023-05-10 11-01-00.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we guide you through our development of a versatile streaming backend for Ray Data, and show examples of how to implement the demanding use cases mentioned earlier. We discuss the performance benefits of implementing pipelined execution across CPUs and GPUs compared to bulk processing, and discuss its broader applicability to various workloads, including distributed training.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pipelined execution in a cluster","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To understand the necessity and advantages of pipelined (or streaming) execution across CPU and GPU devices, let's first examine how a ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Bulk_synchronous_parallel"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"bulk synchronous parallel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (BSP) framework might handle batch inference. Because of its simplicity and generality, BSP is a common way frameworks (e.g., MapReduce, Apache Spark) parallelize distributed computations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Note that pipelining within a single machine is already commonly used in data loading libraries such as ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Torch DataLoader","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://petastorm.readthedocs.io/en/latest/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Petastorm.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" Here, we discuss pipelining computations across an entire cluster of machines.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical batch inference job consists of the following operations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6mkS4HlXNovPW6Dl387jYe","type":"Asset","createdAt":"2023-05-10T15:40:10.724Z","updatedAt":"2023-05-10T19:28:57.091Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"locale":"en-US"},"fields":{"title":"streaming_figure_3","description":"Figure 3. A typical batch inference job sequential operations","file":{"url":"//images.ctfassets.net/xjan103pcp94/6mkS4HlXNovPW6Dl387jYe/1a707558e51ac00b776ae6f5d808b74e/Screenshot_from_2023-05-10_12-23-58.png","details":{"size":33844,"image":{"width":1526,"height":244}},"fileName":"Screenshot from 2023-05-10 12-23-58.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To implement such an inference job, you'd write the code using Ray Data (or another similar framework):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1kYVOacE6epR4czS5YoOHX","type":"Entry","createdAt":"2023-05-10T15:41:24.239Z","updatedAt":"2023-05-10T15:41:24.239Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_1","body":"import ray\n\n# Define model and preprocessor.\nmodel = ...\npreprocess_fn = ...\n\n# Define the inference function.\ndef model_fn(batch: Dict[str, np.ndarray]):\n    return {\"results\": model(batch)}\n\n# The following snippet implements the pipeline.\nray.data.read_parquet(...) \\           # 1. Load\n    .map_batches(preprocess_fn) \\      # 2. Preprocess\n    .map_batches(model_fn) \\           # 3. Inference\n    .write_parquet(...)                # 4. Save\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a BSP framework, each of these operations can be modeled as a 1:1 transformation, which can be fused together into a single stage. Each task within the stage will execute the mentioned operations locally in sequence:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4oQ40RlSK1AF7Y4ZnVGV8J","type":"Asset","createdAt":"2023-05-10T15:44:10.631Z","updatedAt":"2023-05-10T18:59:16.654Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_4","description":"Figure 4. BSP framework operations modeled as a 1:1 transformation","file":{"url":"//images.ctfassets.net/xjan103pcp94/4oQ40RlSK1AF7Y4ZnVGV8J/a0413e90a55ec69d7d0ba49554a7f2a0/Screenshot_from_2023-05-10_11-07-39.png","details":{"size":103326,"image":{"width":1511,"height":583}},"fileName":"Screenshot from 2023-05-10 11-07-39.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This execution strategy is ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"memory optimal","nodeType":"text"},{"data":{},"marks":[],"value":", as there is no intermediate data and execution is inherently pipelined (since there is only a single stage). However, challenges emerge in a heterogeneous setting, where certain pipeline steps may prefer to run on CPUs or GPUs independently. In Ray Data, such inference pipelines are expressed with transformations that launch actors scheduled onto GPUs ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"(num_gpus=1 and compute=ActorPoolStrategy)","nodeType":"text"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3nLf8c4wMq9KInsI4ca7HB","type":"Entry","createdAt":"2023-05-10T15:46:05.506Z","updatedAt":"2023-05-10T16:00:09.475Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_2","body":"import ray\nfrom ray.data import ActorPoolStrategy\n\nModel = ...\npreprocess_fn = ...\n\n# Define the model as a stateful class with cached setup.\nclass MyModelCallableCls:\n    def __init__(self):\n        self.model = Model(...)\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e\n        return {\"results\": self.model(batch)}\n\n# Modify the pipeline to use GPU actors.\nray.data.read_parquet(...) \\\n    .map_batches(preprocess_fn) \\\n    .map_batches(\n          MyModelCallableCls,\n          num_gpus=1,\n          compute=ActorPoolStrategy(size=N)) \\\n    .write_parquet(...)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the above code snippet, the load and preprocessing steps run on CPU (Stage 1), inference runs on GPUs (Stage 2), and then the result saving runs on CPU again (Stage 3). This configuration leads to spilling of data to remote storage when intermediate results (e.g., decoded video frames) exceed cluster memory sizes. Hence, we see that ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"BSP is not memory optimal for heterogeneous workloads","nodeType":"text"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"66FIu8nzl2kZRB1Ht5xZFL","type":"Asset","createdAt":"2023-05-10T15:49:26.155Z","updatedAt":"2023-05-10T19:00:24.815Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_5","description":"Figure 5. BSP is not memory optimal for heterogeneous workloads","file":{"url":"//images.ctfassets.net/xjan103pcp94/66FIu8nzl2kZRB1Ht5xZFL/295ab9176fab5d04ee47ac53c5e1b418/Screenshot_from_2023-05-10_11-02-18.png","details":{"size":162611,"image":{"width":1750,"height":730}},"fileName":"Screenshot from 2023-05-10 11-02-18.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"These unnecessary overheads can be avoided with end-to-end pipelining (i.e., streaming) across the cluster, which we present in the next section.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data streaming","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In Ray 2.4, we've changed the default execution strategy of Ray Data to streaming from bulk synchronous. These streaming Datasets are fully backwards compatible with the existing API, i.e., they can be transformed lazily with map operations, support shuffle operations, and also caching / materialization in memory:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5CSJWRWgN6pQwxcFMBhMSr","type":"Entry","createdAt":"2023-05-10T15:50:53.654Z","updatedAt":"2023-05-10T15:59:44.870Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_3","body":"# Create a dataset over parquet files\nds: ray.data.Dataset = ray.data.read_parquet(...)\n\n# Transform the dataset\nds = ds.map_batches(my_preprocess_fn)\nds = ds.map_batches(my_model_fn)\n\n# Iterate over dataset batches in streaming fashion\nfor batch in ds.iter_batches():\n   print(batch)\n\n# Materialize all contents of the dataset in memory\nds = ds.materialize()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In other words, the Ray Data API now leverages streaming execution for improved performance on large datasets, with the same simple transformation API as in previous Ray versions.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Video processing example","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To more fully understand the capabilities of this API, let's walk through what this looks like in an example video processing pipeline. In the pipeline, we'll first decode video frames, annotate each frame with an ML model, apply a classification model to the annotated frames, and then save the results to JSON:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"s1kmF3kQraHomGuDsvOQt","type":"Asset","createdAt":"2023-05-10T15:55:25.682Z","updatedAt":"2023-05-10T19:30:26.377Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"streaming_figure_6","description":"Figure 6. A pipeline decoding video frames, annotating frames, applying classification, and saving results to JSON.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/s1kmF3kQraHomGuDsvOQt/9385b71522afe76eebc6342f54387ffa/Screenshot_from_2023-05-10_12-24-36.png","details":{"size":38452,"image":{"width":1478,"height":230}},"fileName":"Screenshot from 2023-05-10 12-24-36.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's start by declaring the components of our pipeline. For brevity, we'll just provide stub classes here, but you can find the ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/ericl/ac086f0a9bdf41b6101dca665045dfec"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"full skeleton here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (and the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference docs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") that can be adapted to your own workloads. Data between these steps is kept in dicts of numpy arrays:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3DoR7DQ01JPe4k0MjPdubR","type":"Entry","createdAt":"2023-05-10T15:57:40.210Z","updatedAt":"2023-05-10T15:59:24.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_5","body":"# Declare a function that decodes frames.\ndef decode_frames(batch: Dict[str, np.ndarray]) -\u003e Dict:\n    video_data = batch[\"bytes\"]\n    ...\n    return {\"frames\": decoded_frames}\n\n# Declare a model that annotates decoded frame data.\nclass FrameAnnotator:\n    def __init__(self):\n        self.model = ...\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e Dict:\n        frames = batch[\"frames\"]\n        return {\"annotated_frames\": self.model(frames)}\n\n# Declare a model that classifies annotated frame data.\nclass FrameClassifier:\n    def __init__(self):\n        self.model = ...\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e Dict:\n        frames = batch[\"annotated_frames\"]\n        return {\"results\": self.model(frames)}","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next, we'll use Ray Data to connect these steps together. This takes just a few lines of code, and we can customize the resource requirements per step to configure CPUs and GPUs:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1U2wSU4PEADDsQpVQqJJAa","type":"Entry","createdAt":"2023-05-10T15:59:06.340Z","updatedAt":"2023-05-10T15:59:06.340Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_6","body":"# Create a dataset from video binary data.\nds = ray.data.read_binary(...)\n\n# Apply the decode step. We can customize the resources per\n# task. Here each decode task requests 4 CPUs from Ray.\nds = ds.map_batches(decode_frames, num_cpus=4)\n\n# Apply the annotation step, using an actor pool of size 5.\n# Each actor runs on a CPU.\nds = ds.map_batches(\n    FrameAnnotator,\n    compute=ActorPoolStrategy(size=5))\n\n# Apply the classification step, using a pool of 2 GPU actors,\n# and a fixed data batch size of 64 for the actor calls.\nds = ds.map_batches(\n    FrameClassifier,\n    num_gpus=1,\n    batch_size=64,\n    compute=ActorPoolStrategy(size=2))\n\n# Trigger execution and write outputs to json.\nds.write_json(\"/tmp/output\")\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The configured streaming topology from above looks like this logically. Since each step has a unique resource requirement / task vs actor strategy, here every step becomes its own stage (Ray Data fuses steps with equivalent resource requirements):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2XaeSiV04QKLILBLAlD0t5","type":"Asset","createdAt":"2023-05-11T15:44:03.608Z","updatedAt":"2023-05-11T15:44:03.608Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"streaming_figure_7","description":"Figure 7: Streaming execution across CPUs and GPUs","file":{"url":"//images.ctfassets.net/xjan103pcp94/2XaeSiV04QKLILBLAlD0t5/d75fceb54323c7587312028b40a4b37c/Screenshot_from_2023-05-10_11-02-51.png","details":{"size":68124,"image":{"width":1445,"height":299}},"fileName":"Screenshot from 2023-05-10 11-02-51.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We run the above script (","nodeType":"text"},{"data":{"uri":"https://gist.github.com/ericl/ac086f0a9bdf41b6101dca665045dfec"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GitHub gist","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") in an Anyscale workspace with 2 GPU nodes and 5 CPU nodes (you can use the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/getting-started.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray cluster launcher","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to also launch a cluster for this):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3hsRFIiW2RGkFM5Igr2jVT","type":"Entry","createdAt":"2023-05-10T16:04:25.585Z","updatedAt":"2023-05-10T16:06:19.918Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_7","body":"$ python workload.py\n\n2023-05-02 15:10:01,105 INFO streaming_executor.py:91 -- Executing DAG\nInputDataBuffer[Input] -\u003e TaskPoolMapOperator[MapBatches(decode_frames)] -\u003e \nActorPoolMapOperator[MapBatches(FrameAnnotator)] -\u003e\nActorPoolMapOperator[MapBatches(FrameClassifier)] -\u003e \nTaskPoolMapOperator[Write]\n2023-05-02 15:10:01,128 INFO actor_pool_map_operator.py:114 -- MapBatches(FrameAnnotator): Waiting for 5 pool actors to start...\n2023-05-02 15:10:02,691 INFO actor_pool_map_operator.py:114 -- MapBatches(FrameClassifier): Waiting for 2 pool actors to start...\n\nRunning: 25.0/112.0 CPU, 2.0/2.0 GPU, 33.19 GiB/32.27 GiB object_store_memory:  28%|ââââ        | 285/1000 [01:40\u003c03:12,  3.71it/s]","language":"Shell"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"While the workload is running, we can view the observability graphs in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-core/ray-dashboard.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Dashboard","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". We can see from the actor metrics that our 7 worker actors were active for the entire run (RUNNING_TASK), and that Ray Data kept them busy with MapBatches(FrameClassifier) as well as MapBatches(FrameAnnotator) tasks. Because of the streaming execution strategy, tasks for all stages run concurrently:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5MDgPW8470uVoy6MFBqowq","type":"Asset","createdAt":"2023-05-10T16:09:18.740Z","updatedAt":"2023-05-10T16:09:27.512Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_8","description":"Figure 8. View active tasks and actors for this job in the Ray Dashboard","file":{"url":"//images.ctfassets.net/xjan103pcp94/5MDgPW8470uVoy6MFBqowq/39f6300387d895145313c0caee8149cf/image3.png","details":{"size":77415,"image":{"width":1216,"height":516}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can also inspect hardware utilization in the Ray dashboard. We'll focus on the network activity. We can see two nodes receiving ~500MiB/s (these are the two GPU nodes hosting the FrameClassifier actors), and a number of other nodes driving significant traffic from 100-200MiB/s, likely sending data to the GPU nodes or between the decode and FrameAnnotator steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6QEOgHncDDrSl3SKl9YzWF","type":"Asset","createdAt":"2023-05-10T16:12:30.030Z","updatedAt":"2023-05-10T19:54:24.559Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"streaming_figure_9","description":"Figure 9. Ray Dashboard showing network activity","file":{"url":"//images.ctfassets.net/xjan103pcp94/6QEOgHncDDrSl3SKl9YzWF/bae6e33fce2749f2ade8afd73681e16b/Screenshot_from_2023-05-10_12-42-27.png","details":{"size":107840,"image":{"width":1635,"height":707}},"fileName":"Screenshot from 2023-05-10 12-42-27.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The overall pipeline finishes in ~5 minutes, successfully processing ~350GB of video frame data total on a heterogeneous cluster of CPUs and GPUs. We see that Ray Data was able to pipeline the entire execution across the cluster to make the best use of memory.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Discussion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Optimizations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"In order to seamlessly execute such streaming topologies, Ray Data provides a number of optimizations under the hood. This includes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Memory stability","nodeType":"text"},{"data":{},"marks":[],"value":": Ray Data relies on the underlying Ray scheduler to schedule tasks and actors, but still needs to manage back-pressure across the streaming topology to bound memory usage and avoid object store spilling. It does this by only scheduling new tasks if it would keep the streaming execution under ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/data-internals.html#streaming-execution"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"configured resources limits","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Intuitively, enforcing a cap on intermediate result memory usage is needed to avoid degrading to bulk execution.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data locality","nodeType":"text"},{"data":{},"marks":[],"value":": While Ray will already place tasks on nodes where their input arguments are local, Ray Data's streaming backend extends this to optimize the scheduling of actor tasks. For example, in the above example, a lot of network traffic is avoided between the `decode_frames` and `FrameAnnotator` steps by routing decoded frames to actors that are on the same node.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Fault tolerance","nodeType":"text"},{"data":{},"marks":[],"value":": Ray Data leverages Ray's built-in ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-core/fault-tolerance.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"fault tolerance","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in order to handle object loss in large jobs. When objects are lost, they are recomputed based on their task lineage tracked by Ray. If actors were needed to produce these objects, Ray restarts these actors prior to re-submitting the task.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scalability","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"To give an idea of the upper scalability envelope of Ray Data streaming, we ran a number of synthetic benchmarks that stressed Ray's object manager and Ray Data's ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/execution/streaming_executor.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"streaming executor","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" processing a 20TiB array dataset. Under the hood, Ray Data is using ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.remote() / ray.wait() ","nodeType":"text"},{"data":{},"marks":[],"value":"to orchestrate the streaming topology, which means it is similarly scalable as Ray itself:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2vKUP0VZYl0yPNspQmfU05","type":"Asset","createdAt":"2023-05-10T16:17:37.701Z","updatedAt":"2023-05-10T16:17:37.701Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"streaming_figure_10","description":"Figure 10. Synthetic benchmarks stressing Ray's object manager and Ray Data's streaming executor processing a 20TiB array dataset","file":{"url":"//images.ctfassets.net/xjan103pcp94/2vKUP0VZYl0yPNspQmfU05/0566c92beb5ea743d9ecc3404a551705/image9.png","details":{"size":44857,"image":{"width":1636,"height":640}},"fileName":"image9.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"These benchmarks were run on a cluster of 500 machines, and test pipelines with between one to four stages similar to the video streaming example. We see that for the simplest pipeline on this cluster, Ray Data can process input data at a rate exceeding 1TiB/s. For more complex multi-stage pipelines, Ray Data can still sustain about 100-200GiB/s of end-to-end throughput on a large cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed Training","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"A significant portion of ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" users today are also using Ray Data as a distributed data backend for ML training. Ray Data streaming allows these users to efficiently work with datasets that are larger than cluster memory for training, leveraging both CPU and GPU nodes to speed up their jobs. We are working on enhancing Ray Train's API to natively work with Ray Data streaming.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What's next?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Today, early users are taking advantage of the Ray Data streaming backend to create efficient large-scale inference pipelines over unstructured data, including video and audio data. To learn more about how to build your own pipeline, check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference tutorial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in our docs. We'll also be following up with a blog on more unstructured application examples. To learn more generally about streaming in Ray Data, check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/data.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"library docs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and let us know what ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/issues"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"improvements","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" you'd like to see to Ray Data.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Also, we will be presenting at the ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Register now","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â â early bird registration is open until","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Â June 30, 2023.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5qTVayH4YnKM7pGL8CiSwc","type":"Asset","createdAt":"2023-05-11T15:46:46.691Z","updatedAt":"2023-05-11T15:46:46.691Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_streaming_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5qTVayH4YnKM7pGL8CiSwc/99806986301b595308c6c89a8bad6249/images_and_diagrams__13_.png","details":{"size":83586,"image":{"width":960,"height":540}},"fileName":"images and diagrams (13).png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1n9cmtmuJ3wQPGW1TtXZ4t","type":"Entry","createdAt":"2023-05-08T15:57:32.674Z","updatedAt":"2023-05-09T00:38:09.625Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Building a Self Hosted Question Answering Service using LangChain + Ray in 20 minutes","slug":"building-a-self-hosted-question-answering-service-using-langchain-ray","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-05-08","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 3 of a blog series. In this blog, weâll show you how to build an LLM question and answering service. In future parts, we will optimize the code and measure performance: cost, latency and throughput.","body":{"nodeType":"document","data":{},"content":[{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4laoMTyctlD4QnuM9KzTI6","type":"Entry","createdAt":"2023-05-08T20:47:41.452Z","updatedAt":"2023-05-08T20:47:41.452Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Building a Question Answering Chatbot","videoUrl":"https://youtu.be/Sy-Xp-sdlh0"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\nThis blog post builds on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"nodeType":"text","value":"Part 1","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" of our LangChain series, where we built a semantic search service in about 100 lines. Still, search is so â¦ 2022. What if we wanted to build a question answering service?Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"One option is we could just ask the LLM directly without any background at all. Unfortunately one of the biggest problems with LLMs is not just ignorance (âI donât knowâ) but hallucination (âI think I know but I actually donât ","marks":[],"data":{}},{"nodeType":"text","value":"at all","marks":[{"type":"italic"}],"data":{}},{"nodeType":"text","value":".â) This is perhaps the biggest issue facing LLMs at the current time. The way we overcome that is by combining factual information from our search engine and the capabilities of an LLM together.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Again, as we demonstrated before, there is a powerful combination in Ray + LangChain. LangChain provides a chain that is well suited to this (Retrieval QA). To give a fuller picture of how the pieces come together, weâre going to implement some parts that could usually just as easily be wrapped.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The Question Answering Service we will build will query the results from our Search Engine, and then use an LLM to summarize the results of the search.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Previously we had shown this diagram for how to serve semantic search results: ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1yspRkXhEt1xIRdiuztbyh","type":"Asset","createdAt":"2023-05-08T15:43:10.951Z","updatedAt":"2023-05-08T15:43:10.951Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"qna-save-search-queries","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1yspRkXhEt1xIRdiuztbyh/f5a50d1046b085b95cd18742e51d5393/qna-save-search-queries.png","details":{"size":243832,"image":{"width":1600,"height":794}},"fileName":"qna-save-search-queries.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We are going to augment that now by creating a chain that consists of the above stage, then generating a prompt, and feeding that to an LLM to generate the answer.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Hence, the resulting system we are trying to build looks like this:Â ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mD1pRX9Do1SDRCUor1Yex","type":"Asset","createdAt":"2023-05-08T15:44:48.379Z","updatedAt":"2023-05-08T15:44:48.379Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"qna-answering-questions-with","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2mD1pRX9Do1SDRCUor1Yex/d4041f6d0d296c73edd77de26e076a17/qna-answering-questions-with.png","details":{"size":170666,"image":{"width":1556,"height":1024}},"fileName":"qna-answering-questions-with.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In other words, we take the vector search results, we take a prompt template, generate the prompt and then pass that to the LLM. Today we will use ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/Stability-AI/StableLM"},"content":[{"nodeType":"text","value":"StableLM","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" but you can easily swap in whatever model you want to.Â \n\nBefore we get started, itâs worth noting that you can find the source code to this project in our LangChain Ray examples repo at: ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/langchain-ray"},"content":[{"nodeType":"text","value":"https://github.com/ray-project/langchain-ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 1: The Prompt Template","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The prompt template is derived from the suggested one from StableLM, but modified for our use case.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In serve.py, we declare the following template:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2MvraLhg4SD47Hq41r5Jd7","type":"Entry","createdAt":"2023-05-08T15:46:38.818Z","updatedAt":"2023-05-08T15:46:38.818Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-1","body":"TEMPLATE = \"\"\"\n\u003c|SYSTEM|\u003e# StableLM Tuned (Alpha version)\n- You are a helpful, polite, fact-based agent for answering questions about Ray. \n- Your answers include enough detail for someone to follow through on your suggestions. \n\u003c|USER|\u003e\n\nPlease answer the following question using the context provided. If you don't know the answer, just say that you don't know. Base your answer on the context below. Say \"I don't know\" if the answer does not appear to be in the context below. \n\nQUESTION: {question} \nCONTEXT: \n{context}\n\nANSWER: \u003c|ASSISTANT|\u003e\n\"\"\"","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Letâs go through the template. The first part is the â\u003c|SYSTEM|\u003eâ tag. You can think of this as setting the âpersonalityâ of the LLM. LLMs are trained to treat the system tag differently. Not all LLMs support this, but OpenAI and StableLM do.Â  The second part is the â\u003c|USER|\u003eâ tag which is the question we want to ask. Note that the question and context are âtemplatizedâ â we will provide them from another source. Finally, since LLMs generate outputs by continuing on from the input, we say to the LLM âOK, hereâs where you take over.âÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The question will come from the userâs query. The context will use what we built last time: the search results from our semantic search.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 2: Setting up the embeddings and the LLM","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now letâs have a look at the __init__ method for our deployment. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3A4WRFfgvE3dNjwHACGYRB","type":"Entry","createdAt":"2023-05-08T15:47:56.437Z","updatedAt":"2023-05-08T15:47:56.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-2","body":"def __init__(self):\n       #... the same code from Part 1 .. \n       self.llm = StableLMPipeline.from_model_id(model_id=\"stabilityai/stablelm-tuned-alpha-7b\",\n                                                    task=\"text-generation\", model_kwargs=\n                                                    {\"device_map\":\"auto\", \"torch_dtype\": torch.float16})\n       WandbTracer.init({\"project\": \"wandb_prompts_2\"})\n       self.chain = load_qa_chain(\n           llm=self.llm,\n           chain_type=\"stuff\",\n           prompt=PROMPT)","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"As you can see, weâve added just 3 lines.Â \n\nThe first line creates a new StableLM LLM. In this case we had to write a little bit of glue code because we wanted to specify using ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://en.wikipedia.org/wiki/Half-precision_floating-point_format"},"content":[{"nodeType":"text","value":"float16","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" (halving the memory consumption of the model). We are working with the authors of Langchain to make this unnecessary. The key line from that file is this one: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"lthNpTGxWTEV31PX65mYb","type":"Entry","createdAt":"2023-05-08T15:50:17.006Z","updatedAt":"2023-05-08T15:50:17.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-3","body":" response = self.pipeline(prompt, temperature=0.1, max_new_tokens=256, do_sample=True)\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Here we specify the maximum number of tokens, and that we want it to pretty much answer the question the same way every time, and that we want to do one word at a time.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The second line sets up our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.wandb.ai/ref/python/integrations/wandbtracer"},"content":[{"nodeType":"text","value":"tracing with Weights and Biases","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". This is completely optional, but will allow us to visualize the input. You can find out more about Weights and Biases ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://wandb.ai"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The third thing we do is create a new chain that is specifically designed for answering questions. We specify the LLM to use, the prompt to use and finally the âchain typeâ â for now we set this to âstuffâ but there are other options like âmap_reduceâ, and also pass in the prompt.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 3: Respond to questions","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Weâve now got our Langchain ready, now all we have to do is write the code that uses the chain to answer questions!Â ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4Z72OU7xq4tPFzV8zLks6","type":"Entry","createdAt":"2023-05-08T15:51:41.475Z","updatedAt":"2023-05-08T15:51:41.475Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-4","body":"   def answer_question(self, query):\n       search_results = self.db.similarity_search(query)\n       print(f'Results from db are: {search_results}')\n       result = self.chain({\"input_documents\": search_results, \"question\":query})\n\n       print(f'Result is: {result}')\n       return result[\"output_text\"]","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Youâll notice the first line is identical to our previous version. Now we execute the chain with both our search results and the question being fed into the template.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 4: Go!","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now letâs get this started. If youâre using Weights and Biases, donât forget to log in using wandb login. To start, letâs do serve run serve:deployment.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now itâs started, letâs use a simple query script to test it.","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4XXeXIabNrMpB1kEND5KBK","type":"Entry","createdAt":"2023-05-08T15:53:45.904Z","updatedAt":"2023-05-08T15:53:45.904Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-5","body":"$ python query.py 'What are placement groups?'\nPlacement groups are a way for users to group resources together and schedule tasks or actors on those resources. They allow users to reserve resources across multiple nodes and can be used for gang-scheduling actors or for spreading resources apart. Placement groups are represented by a list of bundles, which are used to group resources together and schedule tasks or actors. After the placement group is created, tasks or actors can be scheduled according to the placement group and even on individual bundles.\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Yay! It works (well, mostly, that part at the end is a bit weird)! Letâs also check that it doesnât make stuff up: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12iqa2l4xNWuK18GLTu9Ly","type":"Entry","createdAt":"2023-05-08T15:54:22.876Z","updatedAt":"2023-05-08T15:54:22.876Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-6","body":"$ python query.py 'How do I make fried rice?'\nI don't know.\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Letâs just check the prompt that was sent to StableLM. This is where Weights and Biases comes in. Pulling up our interface we can find the prompt that was sent:Â ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"64oHWc7Rnz0fgCIBOR8gEF","type":"Entry","createdAt":"2023-05-08T15:55:06.135Z","updatedAt":"2023-05-08T15:55:06.135Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-7","body":"\u003c|SYSTEM|\u003e# StableLM Tuned (Alpha version)\n- You are a helpful, polite, fact-based agent for answering questions about Ray. \n- Your answers include enough detail for someone to follow through on your suggestions. \n\u003c|USER|\u003e\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\n\nPlease answer the following question using the context provided. \nQUESTION: What are placement groups? \nCONTEXT: \nPlacement Groups#\nPlacement groups allow users to atomically reserve groups of resources across multiple nodes (i.e., gang scheduling).\nThey can be then used to schedule Ray tasks and actors packed as close as possible for locality (PACK), or spread apart\n(SPREAD). Placement groups are generally used for gang-scheduling actors, but also support tasks.\nHere are some real-world use cases:\n\nray list placement-groups provides the metadata and the scheduling state of the placement group.\nray list placement-groups --detail provides statistics and scheduling state in a greater detail.\nNote\nState API is only available when you install Ray is with pip install \"ray[default]\"\nInspect Placement Group Scheduling State#\nWith the above tools, you can see the state of the placement group. The definition of states are specified in the following files:\nHigh level state\nDetails\n\nPlacement groups are represented by a list of bundles. For example, {\"CPU\": 1} * 4 means youâd like to reserve 4 bundles of 1 CPU (i.e., it reserves 4 CPUs).\nBundles are then placed according to the placement strategies across nodes on the cluster.\nAfter the placement group is created, tasks or actors can be then scheduled according to the placement group and even on individual bundles.\nCreate a Placement Group (Reserve Resources)#\n\nSee the User Guide for Objects.\nPlacement Groups#\nPlacement groups allow users to atomically reserve groups of resources across multiple nodes (i.e., gang scheduling). They can be then used to schedule Ray tasks and actors packed as close as possible for locality (PACK), or spread apart (SPREAD). Placement groups are generally used for gang-scheduling actors, but also support tasks.\nSee the User Guide for Placement Groups.\nEnvironment Dependencies#\n\nANSWER: \u003c|ASSISTANT|\u003e\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We can see the search results that we made in are being included. StableLM is then using this to synthesize its answer.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Conclusion","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this blog post we showed how we could build on the simple search engine we built in the previous blog in this series and make a retrieval-based question answering service. It didnât need us to do much: we needed to bring up a new LLM (StableLM),Â  we needed to generate a prompt with the search results in it, and then feed that result to the LLM asking it to derive an answer from it. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Next Steps","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Review the code and data used in this blog in the following ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/langchain-ray/tree/main/open_source_LLM_retrieval_qa"},"content":[{"nodeType":"text","value":"Github repo","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". \n\nIf you are interested in learning more about Ray, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io/"},"content":[{"nodeType":"text","value":"Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://docs.ray.io/"},"content":[{"nodeType":"text","value":"Docs.Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\n","marks":[],"data":{}},{"nodeType":"text","value":"See our earlierÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"nodeType":"text","value":"blog series on solving Generative AI infrastructure","marks":[],"data":{}}]},{"nodeType":"text","value":"Â with Ray.","marks":[],"data":{}},{"nodeType":"text","value":"\n\nTo connect with the Ray community join #LLM on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"Ray Slack","marks":[],"data":{}}]},{"nodeType":"text","value":" or our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"Discuss forum","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://www.anyscale.com/platform"},"content":[{"nodeType":"text","value":"Anyscale.com/Platform ","marks":[],"data":{}}]},{"nodeType":"text","value":"and click the 'Try it now' button","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Summit 2023:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"nodeType":"text","value":"Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"nodeType":"text","value":"practical training focused on LLMs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mD1pRX9Do1SDRCUor1Yex","type":"Asset","createdAt":"2023-05-08T15:44:48.379Z","updatedAt":"2023-05-08T15:44:48.379Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"qna-answering-questions-with","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2mD1pRX9Do1SDRCUor1Yex/d4041f6d0d296c73edd77de26e076a17/qna-answering-questions-with.png","details":{"size":170666,"image":{"width":1556,"height":1024}},"fileName":"qna-answering-questions-with.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2C7QtMnASHfVdag9xYamXh","type":"Entry","createdAt":"2023-05-04T15:03:36.052Z","updatedAt":"2023-06-06T14:20:09.992Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker","seoTitle":"Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker","slug":"offline-batch-inference-comparing-ray-apache-spark-and-sagemaker","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DwU0wh6AmJoGv8JOy6n0Q","type":"Entry","createdAt":"2020-11-11T23:34:15.165Z","updatedAt":"2021-02-05T22:22:08.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","slug":"amog-kamsetty","link":"https://www.linkedin.com/in/amogkamsetty/","bio":"Amog is a software engineer at Anyscale. He helps maintain and develop two machine learning libraries: Ray Tune for distributed hyperparameter tuning and RaySGD for distributed deep learning training.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J134jX2ybZRLPfcAIGLTI","type":"Asset","createdAt":"2020-11-13T00:05:05.533Z","updatedAt":"2020-11-13T00:05:05.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J134jX2ybZRLPfcAIGLTI/12589bb0c6792051050ebc6e1e83af25/Amog_Kamsetty_.jpg","details":{"size":57660,"image":{"width":480,"height":640}},"fileName":"Amog Kamsetty .jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}}],"publishedDate":"2023-05-04","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Update 5/10: ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"Thanks to the Databricks Apache Spark developers for pointing out the prefetching flag ","nodeType":"text"},{"data":{},"marks":[{"type":"code"},{"type":"italic"}],"value":"spark.databricks.execution.pandasUDF.prefetch.maxBatches ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"available in Databricks Runtime that can be used with the Iterator API. It is also possible to implement this manually in Spark open source with background threads. With prefetching set to 4 batches, Spark reaches ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"},{"type":"italic"}],"value":"159.86 images/sec","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":". The code can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/blob/main/spark/code/torch-batch-inference-s3-10G-standard-iterator-databricks-prefetch.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"italic"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"TL;DR","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As more companies use large scale machine learning (ML) models for training and evaluation, offline batch inference becomes an essential workload. A number of challenges come with it: managing compute infrastructure; optimizing use of all heterogeneous resources; and transferring data from storage to hardware accelerators. Addressing these challenges, Ray performs significantly better as it can coordinate clusters of diverse resources, allowing for better utilization of the specific resource requirements of the workload.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we conduct a comparison of three different solutions for offline batch inference: AWS ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker Batch Transform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Our experiments demonstrate that Ray Data achieves speeds up to ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"17x ","nodeType":"text"},{"data":{},"marks":[],"value":"faster than SageMaker Batch Transform and","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" 2x","nodeType":"text"},{"data":{},"marks":[],"value":" faster than Spark for offline image classification. Ray Data also scales effectively to terabyte-sized datasets. The code for these benchmarks is publicly available and can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introduction","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46bNG3QTa1atN02rsemfe8","type":"Asset","createdAt":"2023-05-03T22:20:44.214Z","updatedAt":"2023-05-03T22:20:44.214Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_1_batch_offline_inference","description":"Figure 1. General parallel batch processing architecture\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/46bNG3QTa1atN02rsemfe8/120bf3aac26ae94257b41ac674f42e94/image3.png","details":{"size":48540,"image":{"width":960,"height":540}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Offline batch inference is a critical workload for many AI products, especially with the growing usage of pre-trained foundation models. At its core, it seems like a simple problem: given a trained model and a dataset, get model predictions for each data point. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, there are many challenges to doing this at scale:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 1: ","nodeType":"text"},{"data":{},"marks":[],"value":"Managing compute infrastructure and cloud clusters, especially when needing to use ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"heterogeneous clusters","nodeType":"text"},{"data":{},"marks":[],"value":", consisting of different instance types to maximize throughput.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 2: ","nodeType":"text"},{"data":{},"marks":[],"value":"Parallelizing data processing and utilizing all resources (CPUs \u0026 GPUs) in the cluster at any given point in time.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 3: ","nodeType":"text"},{"data":{},"marks":[],"value":"Efficiently transferring data between cloud storage, CPU RAM for preprocessing, and GPU RAM for model inference, especially for unstructured data like images and text. This has ramifications for both performance and developer velocity.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 4:","nodeType":"text"},{"data":{},"marks":[],"value":" A user experience that makes it easy to iterate and develop while working at scale.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What does the industry recommend for offline batch inference?","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The industry hasnât converged on a standard solution. ","nodeType":"text"},{"data":{"uri":"https://twitter.com/benhamner/status/1605730034450169857"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"It seems like every ML practitioner has their own favorite approach.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5hRMwuEGUS4nJ7eV5riIK9","type":"Asset","createdAt":"2023-05-03T22:23:14.590Z","updatedAt":"2023-05-03T22:23:14.590Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"tweet_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5hRMwuEGUS4nJ7eV5riIK9/05b76abcbe4ea58ae3839fd319858594/image4.png","details":{"size":177304,"image":{"width":1798,"height":588}},"fileName":"image4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We see that there are three categories of solutions that attempt to address the above challenges. However, only category three addresses all the challenges.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Batch Services: ","nodeType":"text"},{"data":{},"marks":[],"value":"Cloud providers such as AWS, GCP, and Azure provide batch services to manage compute infrastructure for you. Some newer products like ","nodeType":"text"},{"data":{"uri":"https://modal.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Modal Labs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provide a user experience layer around cloud providers to abstract away even more complexity. Regardless of which service you choose, the process is the same: you provide your code, and the service runs your code on each node in a cluster. However, while infrastructure management is necessary (Challenge #1), it is not enough. These services have limitations, such as a lack of software libraries to address Challenges #2, #3, and #4 which makes them suitable only for experienced users who can write their own optimized batch inference code.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Online Inference solutions: ","nodeType":"text"},{"data":{},"marks":[],"value":"Â Solutions like ","nodeType":"text"},{"data":{"uri":"https://www.bentoml.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Bento ML","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provide APIs to make it easy to write performant inference code and can abstract away infrastructure complexities. But they are designed for online inference rather than offline batch inferenceâtwo different problems with different sets of requirements. These solutions often donât perform well in the offline case, leading inference service providers like ","nodeType":"text"},{"data":{"uri":"https://modelserving.com/blog/unifying-real-time-and-batch-inference-with-bentoml-and-spark"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Bento ML to integrating with Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for offline inference.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed data systems:","nodeType":"text"},{"data":{},"marks":[],"value":" These solutions are designed to handle large amounts of data and are better suited to handle all three challenges listed above. They break large datasets into reasonable batches for processing and can map a given function (i.e., the model) over the dataset efficiently, effectively handling the challenges of offline batch inference at scale. Examples include ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ultimately, we have observed that Ray Data is the best practical solution for offline batch inference for modern deep learning applications, both in terms of performance and user experience.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We ran the following comparisons and experiments to corroborate the above observations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing AWS SageMaker Batch Transform, Apache Spark, and Ray Data performance and UX on an image classification use case","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling up Ray Data batch inference to 10 TB, showing 90%+ GPU utilization","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Image Classification: SageMaker Batch Transform vs. Apache Spark vs. Ray Data","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We ran an image classification task from the ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1911.02549"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLPerf Inference Benchmark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" suite in the offline setting. This benchmark uses images from the ","nodeType":"text"},{"data":{"uri":"https://image-net.org/challenges/LSVRC/2012/2012-downloads.php#Images"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"ImageNet 2012 Dataset","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and a pre-trained ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch ResNet50 model.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We timed a full end-to-end batch inference workload. With images stored in the parquet format in a public S3 bucket, the workload involved the following three steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Reading images from S3","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Simple CPU preprocessing (resizing, cropping, normalization)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Model inference on GPU","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"While all Ray experiments were run on Anyscale, Spark experiments were run on Databricks runtime v12.0, with Apache Spark 3.3.1. More details on the full configurations can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"in the repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In particular for Spark, we used two different setups:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Single-cluster","nodeType":"text"},{"data":{},"marks":[],"value":": Both the CPU preprocessing and GPU inference were done on the same cluster. We tried both versions of the PySpark Pandas UDF API: ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.pandas_udf.html#pyspark.sql.functions.pandas_udf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Series-based and Iterator-based","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Multi-cluster","nodeType":"text"},{"data":{},"marks":[],"value":": CPU preprocessing is done on one cluster, the preprocessed data is saved to a shared file system, and GPU inference is done on a different cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"All the source code for the experiments conducted above is available ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", including additional details on various other configurations that we attempted.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Â Results of throughput from experiments","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"When running with a 10 GB dataset on a single g4dn.12xlarge instance (48 CPUs, 4 GPUs), we observe the following throughput (images/sec) results for ResNet-50 batch inference:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Urc9cqOTvtI8d4w9DjiEm","type":"Asset","createdAt":"2023-05-04T00:09:19.405Z","updatedAt":"2023-05-04T00:09:19.405Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_5_table","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Urc9cqOTvtI8d4w9DjiEm/42718e584dcc4d0d16592749b0782b4f/Screen_Shot_2023-05-03_at_5.01.12_PM.png","details":{"size":91393,"image":{"width":1211,"height":541}},"fileName":"Screen Shot 2023-05-03 at 5.01.12 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The above throughput differences translate directly into cost savings as the batch inference job with Ray can run much fasterâso the faster it finishes the fewer resources are consumed and the cheaper it gets. Additionally, Ray is fully open source.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2dNPqjBTFiDZn2gcXkdir9","type":"Asset","createdAt":"2023-05-03T22:50:03.518Z","updatedAt":"2023-05-03T22:50:03.518Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 1","description":"Figure 1. Throughput difference among three different solutions for 10GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/2dNPqjBTFiDZn2gcXkdir9/f07a675dcb4f124e29aeb2f6618dbf15/image1.png","details":{"size":163062,"image":{"width":1334,"height":948}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"SageMaker Batch Transform for batch inference","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As we can see, SageMaker Batch Transform is not well suited for batch inference, both for throughput and cost.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While SageMaker handles","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" Challenge #1 ","nodeType":"text"},{"data":{},"marks":[],"value":"to some degree and abstracts away compute infrastructure management, it doesnât fully address the remaining challenges","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The low-throughput results are primarily because while SageMaker Batch Transform claims to be a ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"solution for batch inference on large datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", under the hood, it uses the same architecture as for online serving systems. It starts an HTTP server, and deploys your model as an endpoint. And then for each image, a separate request is sent to the server.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"SageMaker also does not provide support for batching across ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"multiple files","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and even if you batched your data beforehand, the max payload per request is 100 MB, nowhere near enough to fully saturate your GPUs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In terms of developer velocity, when running these benchmarks, we ran into a few additional challenges with SageMaker. In particular, the need to spin up a new cluster every time we ran a script and the difficult to parse stack traces slowed down developer iterations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While online inference solutions have their place, the additional overhead they have, as well as the inability to fully utilize cluster resources and saturate GPUs by maximizing batch sizes and effectively pipelining data make them highly inefficient for offline batch inference.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing Spark and Ray for batch inference","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Clearly, as we observed above, shoehorning online inference solutions for offline use cases is not the right approach. Letâs compare the two distributed data system approaches: Spark and Ray Data.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We see that Ray Data has","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" 2x ","nodeType":"text"},{"data":{},"marks":[],"value":"faster throughput than Spark. Letâs break down how Ray Data and Spark fare for the challenges mentioned earlier:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #2: Hybrid CPU and GPU workload","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deep learning workloads involve ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"both CPU and GPU computation","nodeType":"text"},{"data":{},"marks":[],"value":". For optimal parallelization, all the CPUs and GPUs in the cluster should be fully utilized at any given time. However, this is a limitation of Spark scheduling as all the stages are fused together, regardless of their resource requirements. This means that in the single cluster setup, the total parallelism is limited by the number of GPUs, leading to underutilization of CPUs. This also means that GPUs are completely idle during the reading and preprocessing steps.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"When using a separate CPU and GPU cluster, data needs to be persisted and orchestrated between stages. Overall, for these CPU+GPU workloads, Spark does not fully address ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"challenge #2","nodeType":"text"},{"data":{},"marks":[],"value":" and cannot utilize all resources in the cluster concurrently. We attempted to use Sparkâs stage level scheduling feature to address this issue, but were ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/tree/main/spark#stage-level-scheduling"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"unsuccessful","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6m5c6zQiMVCithTLEIazb8","type":"Asset","createdAt":"2023-05-03T23:02:01.263Z","updatedAt":"2023-05-03T23:02:01.263Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 2","description":"Figure 2. How Ray data efficiently pipelines and optimizes hardware: CPUs/GPUs","file":{"url":"//images.ctfassets.net/xjan103pcp94/6m5c6zQiMVCithTLEIazb8/1be9788a7aa54707a42f38023597ae2f/5mb.gif","details":{"size":4991804,"image":{"width":3260,"height":1182}},"fileName":"5mb.gif","contentType":"image/gif"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data, on the other hand, can independently scale the CPU preprocessing and GPU inference steps. It also streams data ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"through these stages","nodeType":"text"},{"data":{},"marks":[],"value":", increasing CPUs and GPUs utilization and reducing out-of-memory errors (OOMs), and fine-grained resource allocation for ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html#scaling-and-resource-allocation"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"autoscaling and management","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #3: Large, multi-dimensional tensors","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deep learning workloads often involve dealing with complex data types like images, audio, or video, which are represented by multi-dimensional ","nodeType":"text"},{"data":{"uri":"https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"arrays","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Having first-class support for these data types is necessary for better performance and developer experience. Ray Data has Numpy batch format as a first-class citizen, and does not require Pandas, which is suited for deep learning and leads to higher performance and more memory efficiency, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-2-2-improved-developer-experience-performance-and-stability"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"as shown in previous benchmarks","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #4: Ray is Python native","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another fundamental difference is that Ray is Python native whereas Spark is not, which also impacts performance. We ran a microbenchmark that applies a ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"time.sleep(1)","nodeType":"text"},{"data":{},"marks":[],"value":" UDF. Surprisingly, running the UDF takes ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/tree/main/spark#microbenchmark"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"80 seconds","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"; we believe due to JVM\u003c\u003ePyarrow overhead! On Ray Data, the same microbenchmark takes a little ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/blob/main/ray/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"under 5 seconds","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Not to mention, this makes Spark development more difficult. In particular when debugging, the recommended way is to include debug information in the ","nodeType":"text"},{"data":{"uri":"https://stackoverflow.com/a/57175768"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"output pandas dataframe rather than using print statements","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Even as we scale to a larger 300 GB dataset using 4 nodes and 16 GPUs, we see that these performance differences still hold:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5wDKGXZNHB2Hlt6JEMMKOt","type":"Asset","createdAt":"2023-05-03T23:04:40.956Z","updatedAt":"2023-05-03T23:04:40.956Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_3_batch_inference","description":"Figure 3. Throughput difference between Spark and Ray solution for 300 GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/5wDKGXZNHB2Hlt6JEMMKOt/da27ee6ea7e5773abef42eb291dd1a97/image5.png","details":{"size":92846,"image":{"width":1102,"height":830}},"fileName":"image5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"While big data processing systems are the best solutions for offline inference, for deep learning workloads, Ray Data outperforms Spark.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling up Ray Data and maximizing performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we test the scalability of Ray Data and attempt to maximize performance by using a ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"heterogeneous cluster","nodeType":"text"},{"data":{},"marks":[],"value":" consisting of a mix of GPU nodes and CPU-only nodes. Deep learning workloads involving complex data types are often memory-bound, so by using additional CPU-only nodes, we add more total memory to our cluster to better maximize GPU utilization.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlike SageMaker or Spark, Ray works natively with heterogeneous clusters consisting of different instance types, fully addressing ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"challenge #1.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We run the same benchmark, except with 10 TB worth of images, using a single cluster of 10 g4dn.12xlarge instances and 10 m5.16xlarge instances.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Throughput: 11,580.958 img/sec","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"As we can see below, Ray Data achieves over 90%+ GPU utilization throughout the workload run:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"16XJHcynegHxIdae1hupAF","type":"Asset","createdAt":"2023-05-03T23:07:22.875Z","updatedAt":"2023-05-03T23:07:22.875Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"batch_inference_figure_4","description":"Figure 4. GPU utilization of all 40 GPUs in the cluster. We are roughly at 90%+ GPU utilization during the entire duration of the workload.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/16XJHcynegHxIdae1hupAF/8eab9567ac78a8d558bcd04218f7e1ca/image2.png","details":{"size":466601,"image":{"width":1999,"height":621}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nThe ability to scale to many GPUs and large data sizes without needing to rewrite code and not sacrificing on throughput is what sets Ray Data apart compared to other solutions for deep learning workloads.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog, we identified four technical challenges for doing batch inference at scale and investigated three commonly cited industry solutions for offline batch inferences. Using the cited approaches, we compared Ray, Spark, and Amazon SageMaker Batch Transform.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We concluded that Ray Data best meets all four challenges:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data abstracts away compute infrastructure management, and can maximize performance for memory-bound workloads as it natively supports heterogeneous clusters.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"streams","nodeType":"text"},{"data":{},"marks":[],"value":" data from cloud storage -\u003e CPU -\u003e GPU, ensuring that all resources in the cluster are utilized at any given point in time. This improves throughput and reduces overall cost.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data has native support for multi-dimensional tensors, which is vital for deep learning workloads.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data is Python native and programmatic, making it easy to develop and debug.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Overall, Ray Data is the best option for deep learning offline batch inference. In our image classification benchmarks, as shown in the figures above, Ray Data significantly outperforms SageMaker Batch Transform (by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"17x)","nodeType":"text"},{"data":{},"marks":[],"value":" and Spark (by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"2x ","nodeType":"text"},{"data":{},"marks":[],"value":"and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"3x","nodeType":"text"},{"data":{},"marks":[],"value":") while linearly scaling to TB level data sizes. All code for these benchmarks can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Lastly, as a result of higher throughput and Ray as an open source solution, Ray lowers the overall costs on top of the base EC2 cost.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Whatâs Next","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stay tuned for a follow up post where we dive into Ray Data streaming's design and how to use it for your own ML pipelines. Meanwhile, you can get started with Ray Data for batch inference ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Alternatively, ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfC6rmBeb9pLPSKRf1e7VCy7BtkjSGu20gh0xjQhdo6rcUflQ/viewform"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"fill out this form","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â to connect with us if you are interested in working on a proof of concept (POC) for your offline batch inference workload with Ray.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have our ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit 2023 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"early-bird registration open. Secure your spot, and save some money.\n\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n\n\n\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2dNPqjBTFiDZn2gcXkdir9","type":"Asset","createdAt":"2023-05-03T22:50:03.518Z","updatedAt":"2023-05-03T22:50:03.518Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 1","description":"Figure 1. Throughput difference among three different solutions for 10GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/2dNPqjBTFiDZn2gcXkdir9/f07a675dcb4f124e29aeb2f6618dbf15/image1.png","details":{"size":163062,"image":{"width":1334,"height":948}},"fileName":"image1.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"51oIQOHymWExFWIRYQRXge","type":"Entry","createdAt":"2023-05-02T17:47:40.421Z","updatedAt":"2023-05-12T07:23:26.789Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Turbocharge LangChain: guide to 20x faster embedding","slug":"turbocharge-langchain-now-guide-to-20x-faster-embedding","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DwU0wh6AmJoGv8JOy6n0Q","type":"Entry","createdAt":"2020-11-11T23:34:15.165Z","updatedAt":"2021-02-05T22:22:08.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","slug":"amog-kamsetty","link":"https://www.linkedin.com/in/amogkamsetty/","bio":"Amog is a software engineer at Anyscale. He helps maintain and develop two machine learning libraries: Ray Tune for distributed hyperparameter tuning and RaySGD for distributed deep learning training.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J134jX2ybZRLPfcAIGLTI","type":"Asset","createdAt":"2020-11-13T00:05:05.533Z","updatedAt":"2020-11-13T00:05:05.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J134jX2ybZRLPfcAIGLTI/12589bb0c6792051050ebc6e1e83af25/Amog_Kamsetty_.jpg","details":{"size":57660,"image":{"width":480,"height":640}},"fileName":"Amog Kamsetty .jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6G9xTnF76ZUKKvgcVOqhtm","type":"Entry","createdAt":"2022-08-23T02:58:28.957Z","updatedAt":"2023-05-02T04:18:43.884Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Philipp Moritz","slug":"philipp-moritz","link":"https://www.linkedin.com/in/philipp-moritz-61419682/","bio":"Co-founder and CTO at Anyscale"}}],"publishedDate":"2023-05-03","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 2([part 1 here](https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray)) of a blog series. In this blog, weâll show you how to turbocharge embeddings. In future parts, we will show you how to combine a vector database and an LLM to create a fact-based question answering service. Additionally, we will optimize the code and measure performance: cost, latency and throughput.","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7nxbgQNlTcYZECfJ0vAabS","type":"Entry","createdAt":"2023-05-02T17:16:29.953Z","updatedAt":"2023-05-02T17:16:29.953Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"LangChain + Ray Tutorial: How to Generate Embeddings For 33,000 Pages for $1","videoUrl":"https://youtu.be/hGnZajytlac"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Generating embeddings from documents is a critical step for LLM workflows. Many LLM apps are being built today through retrieval based similarity search:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Documents are embedded and stored in a vector database.Â Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Incoming queries are used to pull semantically relevant passages from the vector database, and these passages are used as context for LLMs to answer the query.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"previous blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we showed how to use ","nodeType":"text"},{"data":{"uri":"https://github.com/hwchase17/langchain"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to do step 1, and we also showed how to parallelize this step by using Ray tasks for faster embedding creation.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we take it one step further, scaling out to many more documents. Continue reading to see how to use ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a distributed data processing system thatâs a part of the Ray framework, to generate and store embeddings for 2,000 PDF documents from cloud storage, parallelizing across 20 GPUs, all in under 4 minutes and in less than 100 lines of code.\n\nWhile in this walkthrough we use ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to read PDF files from S3 cloud storage, it also supports a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/api/input_output.html#input-output"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"wide number of other data formats","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" like text data, parquet, images, and can read data from a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/api/input_output.html#input-output"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"variety of sources","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" like MongoDB and SQL Databases.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Why do I need to parallelize this?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2CHfkngLbr34XsDZ7IKBAW","type":"Asset","createdAt":"2023-05-02T02:38:14.040Z","updatedAt":"2023-05-02T02:38:14.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Why do I need to parallelize this?","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2CHfkngLbr34XsDZ7IKBAW/af3491b43c8afcdb307890881b2adf5c/embedding-why-do-I-need-to-parallelize-this.png","details":{"size":153788,"image":{"width":1600,"height":1004}},"fileName":"embedding-why-do-I-need-to-parallelize-this.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"LangChain provides all the tools and the integrations for building LLM applications, including loading, embedding, and storing documents. While LangChain works great for quickly getting started with a handful of documents, when you want to scale your corpus up to thousands or more documents, this can quickly become unwieldy.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Naively using a for loop to do this for each document within a corpus of a 2,000 documents takes 75 minutes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4eHSySPomCQud1Y6m1yiyB","type":"Entry","createdAt":"2023-05-02T02:45:13.859Z","updatedAt":"2023-05-02T04:15:57.187Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Embedded-Code-Snippet-1","body":"import os\nfrom tqdm import tqdm\n\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=100,\n    length_function=len,\n)\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nhf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n\n# Put your directory containing PDFs here\ndirectory = '/tmp/data/'\npdf_documents = [os.path.join(directory, filename) for filename in os.listdir(directory)]\n\nlangchain_documents = []\nfor document in tqdm(pdf_documents):\n    try:\n        loader = PyPDFLoader(document)\n        data = loader.load()\n        langchain_documents.extend(data)\n    except Exception:\n        continue\n\nprint(\"Num pages: \", len(langchain_documents))\nprint(\"Splitting all documents\")\nsplit_docs = text_splitter.split_documents(langchain_documents)\n\nprint(\"Embed and create vector index\")\ndb = FAISS.from_documents(split_docs, embedding=hf)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Clearly, if you want to iterate quickly and try out different multiple document corpuses, splitting techniques, chunk sizes, or embedding models, just doing this in a for loop wonât cut it.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Instead, for faster development, you need to horizontally scale, and for this you need a framework to make this parallelization very easy.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"By using Ray Data, we can define our embedding generation pipeline and execute it in a few lines of code, and it will automatically scale out, leveraging the compute capabilities of all the CPUs and GPUs in our cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stages of our Data Pipeline","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this example, we want to generate embeddings for our document corpus consisting of the top 2,000 arxiv papers on âlarge language modelsâ. There are over 30,000 pages in all these documents. The code for generating this dataset can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/blob/main/embedding_pdf_documents/arxiv_dataset_generation.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J2BxxlXHAsmjVdrjCqLdn","type":"Asset","createdAt":"2023-05-02T02:52:04.314Z","updatedAt":"2023-05-02T02:52:04.314Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Stages of our Data Pipeline","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J2BxxlXHAsmjVdrjCqLdn/2ee02bfb6d2d96c7be6ab848c757d45e/embedded-stages-of-our-pipeline.png","details":{"size":127047,"image":{"width":1512,"height":556}},"fileName":"embedded-stages-of-our-pipeline.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs take a look at the stages of our data pipeline:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Load the PDF documents from our S3 bucket as raw bytes","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use PyPDF to convert those bytes into string text","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use LangChainâs text splitter to split the text into chunks","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use a pre-trained sentence-transformers model to embed each chunk","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Store the embeddings and the original text into a FAISS vector store","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The full data pipeline was run on 5 g4dn.12xlarge instances on AWS EC2, consisting of 20 GPUs in total. The code for the full data pipeline can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/blob/main/embedding_pdf_documents/embedding_ray.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Starting the Ray Cluster","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Follow the steps ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/blob/main/embedding_pdf_documents/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to set up a multi-node Ray cluster on AWS.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray clusters can also be started on GCP, Azure, or other cloud providers. See the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/getting-started.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Cluster documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for full info.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Alternatively, you can use ","nodeType":"text"},{"data":{"uri":"https://docs.anyscale.com/develop/workspaces/get-started"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Workspaces on Anyscale","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to manage your Ray clusters.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now that we have the cluster setup, letâs go through the steps in our script.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Installing Dependencies","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"First, we need to install the necessary dependencies on all the nodes in our Ray cluster. We can do this via Rayâs ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#id1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"runtime environment","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" feature.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4EbdJbi2f8hfsYWczaTcqZ","type":"Entry","createdAt":"2023-05-02T03:46:04.461Z","updatedAt":"2023-05-02T03:46:04.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Embedding-code-snippet-2","body":"import ray\n\nray.init(runtime_env={\"pip\": [\"langchain\", \"pypdf\", \"sentence_transformers\", \"transformers\"]})","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 1","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Load the 2,143 documents from our S3 bucket as raw bytes.The S3 bucket contains unmodified PDF files that have been downloaded from arxiv.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can easily do this via Ray Dataâs read APIs, which creates a Ray Dataset object. Ray Datasets are lazy. Further operations can be chained and the stages are run only when execution is triggered.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Cuzf28zxzpUhECAe0PxyR","type":"Entry","createdAt":"2023-05-02T03:47:03.011Z","updatedAt":"2023-05-02T17:11:29.904Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-3","body":"from ray.data.datasource import FileExtensionFilter\n\n# Filter out non-PDF files.\nds = ray.data.read_binary_files(\"s3://ray-llm-batch-inference/\", partition_filter=FileExtensionFilter(\"pdf\"))","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 2","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Use PyPDF to load in the raw bytes and parse them as string text. We also skip over any documents or pages that are unparseable. Even after skipping these, we still have over 33,642 pages in our dataset.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We use the pypdf library directly to read PDFs directly from bytes rather than file paths. Once ","nodeType":"text"},{"data":{"uri":"https://github.com/hwchase17/langchain/pull/3915"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://github.com/hwchase17/langchain/pull/3915","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is merged, LangChainâs PyPdfLoader can be used directly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"60msecWkdwI269c9fYmRih","type":"Entry","createdAt":"2023-05-02T03:49:06.119Z","updatedAt":"2023-05-02T04:15:18.317Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-4","body":"def convert_to_text(pdf_bytes: bytes):\n    pdf_bytes_io = io.BytesIO(pdf_bytes)\n\n    try:\n        pdf_doc = PdfReader(pdf_bytes_io)\n    except pypdf.errors.PdfStreamError:\n        # Skip pdfs that are not readable.\n        # We still have over 30,000 pages after skipping these.\n        return []\n\n    text = []\n    for page in pdf_doc.pages:\n        try:\n            text.append(page.extract_text())\n        except binascii.Error:\n            # Skip all pages that are not parseable due to malformed characters.\n            print(\"parsing failed\")\n    return text\n\n# We use `flat_map` as `convert_to_text` has a 1-\u003eN relationship.\n# It produces N strings for each PDF (one string per page).\n# Use `map` for 1-\u003e1 relationship.\nds = ds.flat_map(convert_to_text)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 3","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Split the text into chunks using LangChainâs TextSplitter abstraction. After applying this transformation, the 33,642 pages are split into 144,411 chunks. Each chunk will then be encoded into an embedding in Step 4.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3S1Tcod9rihoVKoyg9RMMz","type":"Entry","createdAt":"2023-05-02T03:50:32.629Z","updatedAt":"2023-05-02T04:14:39.402Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-5","body":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ndef split_text(page_text: str):\n    # Use chunk_size of 1000.\n    # We felt that the answer we would be looking for would be \n    # around 200 words, or around 1000 characters.\n    # This parameter can be modified based on your documents and use case.\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000, chunk_overlap=100, length_function=len\n    )\n    split_text: List[str] = text_splitter.split_text(page_text)\n\n    split_text = [text.replace(\"\\n\", \" \") for text in split_text]\n    return split_text\n\n# We use `flat_map` as `split_text` has a 1-\u003eN relationship.\n# It produces N output chunks for each input string.\n# Use `map` for 1-\u003e1 relationship.\nds = ds.flat_map(split_text)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 4","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Then, we can embed each of our chunks using a pre-trained sentence transformer model on GPUs. Here, we leverage Ray Actors for stateful computation, allowing us to initialize a model only once per GPU, rather than for every single batch.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"At the end of this stage, we have 144,411 encodings by running 20 model replicas across 20 GPUs, each processing a batch of 100 chunks at a time to maximize GPU utilization.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Dp9uXgJznvx0VgoNwZt0e","type":"Entry","createdAt":"2023-05-02T03:53:27.990Z","updatedAt":"2023-05-02T04:13:54.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-6","body":"class Embed:\n    def __init__(self):\n        # Specify \"cuda\" to move the model to GPU.\n        self.transformer = SentenceTransformer(model_name, device=\"cuda\")\n\n    def __call__(self, text_batch: List[str]):\n        # We manually encode using sentence_transformer since LangChain\n        # HuggingfaceEmbeddings does not support specifying a batch size yet.\n        embeddings = self.transformer.encode(\n            text_batch,\n            batch_size=100,  # Large batch size to maximize GPU utilization.\n            device=\"cuda\",\n        ).tolist()\n\n        return list(zip(text_batch, embeddings))\n\n# Use `map_batches` since we want to specify a batch size to maximize GPU utilization.\nds = ds.map_batches(\n    Embed,\n    # Large batch size to maximize GPU utilization.\n    # Too large a batch size may result in GPU running out of memory.\n    # If the chunk size is increased, then decrease batch size.\n    # If the chunk size is decreased, then increase batch size.\n    batch_size=100,  # Large batch size to maximize GPU utilization.\n    compute=ray.data.ActorPoolStrategy(min_size=20, max_size=20),  # I have 20 GPUs in my cluster\n    num_gpus=1,  # 1 GPU for each actor.\n)\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We use the `sentence_transformers` library directly so that we can provide a specific batch size. Once ","nodeType":"text"},{"data":{"uri":"https://github.com/hwchase17/langchain/pull/3914"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://github.com/hwchase17/langchain/pull/3914","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is merged, LangChainâs `HuggingfaceEmbeddings` can be used instead.\n\n","nodeType":"text"},{"data":{},"marks":[{"type":"underline"}],"value":"Stage 5","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we can execute this Data Pipeline by iterating through it, and we store the results in a persisted FAISS vector database for future querying.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3WstnFWdDedO9wxSKYDGR8","type":"Entry","createdAt":"2023-05-02T03:54:47.846Z","updatedAt":"2023-05-02T04:17:51.737Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-7","body":"from langchain import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\ntext_and_embeddings = []\nfor output in ds.iter_rows():\n    text_and_embeddings.append(output)\n\nvectore_store = FAISS.from_embeddings(\n    text_and_embeddings,\n    # Provide the embedding model to embed the query.\n    # The documents are already embedded.\n    embedding=HuggingFaceEmbeddings(model_name=model_name)\n)\n\n# Persist the vector store.\nvectore_store.save_local(\"faiss_index\")","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Execution","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Executing this code, we see that all 20 GPUs are utilized at near 100% utilization. And what would normally take over an hour to run, can now be done in under 4 minutes! If you use AWS spot instances, this would only cost $0.95 total.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5rAh6B2lrImLTTXvpBMeHG","type":"Asset","createdAt":"2023-05-02T04:00:42.271Z","updatedAt":"2023-05-02T04:00:42.271Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Execution-1","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5rAh6B2lrImLTTXvpBMeHG/70baf2935bfad28c70d5fb310fe15ef1/embedding-execution-1.png","details":{"size":9146,"image":{"width":352,"height":130}},"fileName":"embedding-execution-1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1yk9oF8K2IIuK88xWBTfGr","type":"Asset","createdAt":"2023-05-02T04:01:19.132Z","updatedAt":"2023-05-02T04:43:16.984Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"locale":"en-US"},"fields":{"title":"Embedding - Execution 2","file":{"url":"//images.ctfassets.net/xjan103pcp94/1yk9oF8K2IIuK88xWBTfGr/ee185a15ff08445d14f3e47ed7f0f2b9/embedding-execution-2.jpg","details":{"size":127225,"image":{"width":1097,"height":780}},"fileName":"embedding-execution-2.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Querying the Vector Database","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can now load in our persisted FAISS database, and query it for similarity search. Letâs see the top document thatâs most relevant to the âprompt engineeringâ query:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3DulKBBh4pLynITCBlacTf","type":"Entry","createdAt":"2023-05-02T04:04:27.586Z","updatedAt":"2023-05-02T04:04:27.586Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-8","body":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain import FAISS\n\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nquery_embedding = HuggingFaceEmbeddings(model_name=model_name)\ndb = FAISS.load_local(\"faiss_index\", query_embedding)\ndocuments = db.similarity_search(query=\"prompt engineering\", k=1)\n[doc.page_content for doc in documents]","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"SMSzMi5hzq9x7y2pUs2Ja","type":"Entry","createdAt":"2023-05-02T04:05:20.246Z","updatedAt":"2023-05-02T04:05:35.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-9","body":"['Prompt Engineering for Job Classiï¬cation 7 5 LLMs \u0026 Prompt Engineering Table '\n '3. Overview of the various prompt modiï¬cations explored in thi s study. '\n 'Short name Description Baseline Provide a a job posting and asking if it is '\n 'ï¬t for a graduate. CoT Give a few examples of accurate classiï¬cation before '\n 'queryi ng. Zero-CoT Ask the model to reason step-by-step before providing '\n 'its an swer. rawinst Give instructions about its role and the task by adding '\n 'to the user msg. sysinst Give instructions about its role and the task as a '\n 'system msg. bothinst Split instructions with role as a system msg and task '\n 'as a user msg. mock Give task instructions by mocking a discussion where it '\n 'ackn owledges them. reit Reinforce key elements in the instructions by '\n 'repeating the m. strict Ask the model to answer by strictly following a '\n 'given templat e. loose Ask for just the ï¬nal answer to be given following a '\n 'given temp late. right Asking the model to reach the right conclusion.']\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next Steps","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"See part 3 ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/building-a-self-hosted-question-answering-service-using-langchain-ray"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Review the code and data used in this blog in the following ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/tree/main/embedding_pdf_documents"},"content":[{"data":{},"marks":[],"value":"Github repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\n","nodeType":"text"},{"data":{},"marks":[],"value":"See our earlierÂ ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[],"value":"blog series on solving Generative AI infrastructure","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â with Ray.","nodeType":"text"},{"data":{},"marks":[],"value":"\n\nIf you are interested in learning more about Ray, see ","nodeType":"text"},{"data":{"uri":"http://ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"http://docs.ray.io/"},"content":[{"data":{},"marks":[],"value":"Docs.Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nTo connect with the Ray community join #LLM on the ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"data":{},"marks":[],"value":"Ray Slack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or our ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[],"value":"Discuss forum","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","nodeType":"text"},{"data":{"uri":"http://www.anyscale.com/platform"},"content":[{"data":{},"marks":[],"value":"Anyscale.com/Platform ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"and click the 'Try it now' button","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray Summit 2023:","nodeType":"text"},{"data":{},"marks":[],"value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"practical training focused on LLMs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J2BxxlXHAsmjVdrjCqLdn","type":"Asset","createdAt":"2023-05-02T02:52:04.314Z","updatedAt":"2023-05-02T02:52:04.314Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Stages of our Data Pipeline","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J2BxxlXHAsmjVdrjCqLdn/2ee02bfb6d2d96c7be6ab848c757d45e/embedded-stages-of-our-pipeline.png","details":{"size":127047,"image":{"width":1512,"height":556}},"fileName":"embedded-stages-of-our-pipeline.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2rfQivy2LoE7OTc9bGVGvm","type":"Entry","createdAt":"2023-04-27T13:59:53.933Z","updatedAt":"2023-06-06T14:28:19.779Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Announcing Ray 2.4.0: Infrastructure for LLM training, tuning, inference, and serving","seoTitle":"Announcing Ray 2.4.0: Infrastructure for LLM training, tuning, inference, and serving","slug":"announcing-ray-2-4-0-infrastructure-for-llm-training-tuning-inference-and","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6TqWiyUibvC9ETfyYljP8b","type":"Entry","createdAt":"2022-02-11T15:42:52.165Z","updatedAt":"2022-02-11T15:42:52.165Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jiajun Yao","slug":"jiajun-yao"}}],"publishedDate":"2023-04-27","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.4.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2.4 release ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"features exciting improvements across the Ray ecosystem. In this blog post, we highlight new features and examples for Generative AI workloads and share four overall additions:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements to Ray data for ease of use, stability, and observability","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Improved Serve observability","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Introduce RLlibâs module for custom reinforcement learning","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Improved Ray scalability for large clusters","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Generative AI model examples and new features using Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Over the past few months, we have seen a flurry of innovative activities around ","nodeType":"text"},{"data":{"uri":"https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-generative-ai"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"generative AI models","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Large_language_model"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"large language models (LLMs)","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". To continue our steadfast commitment to make Ray a pivotal compute substrate for ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"generative AI workloads","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and address the challenges (as explained in our ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"blog series","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"), we have added significant enhancements in this release to ensure that these open source LLM models and workloads are accessible to the open source community and performant with Ray.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QDe7dun9hKhLCoGFSXKh9","type":"Asset","createdAt":"2023-04-26T19:23:25.596Z","updatedAt":"2023-04-26T19:23:25.596Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_image","file":{"url":"//images.ctfassets.net/xjan103pcp94/1QDe7dun9hKhLCoGFSXKh9/d9f93233f6ee72608e34735eac545a74/image1.png","details":{"size":154908,"image":{"width":960,"height":540}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Generative AI Workloads:","nodeType":"text"},{"data":{},"marks":[],"value":" First, with 2.4 we are releasing a set of new working examples showing how to use Ray with ","nodeType":"text"},{"data":{"uri":"https://stablediffusionweb.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stable Diffusion","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and LLMs like ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/transformers/model_doc/gptj"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GPT-J","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Below we tabulate the workload of each of these models and their respective links. More are underway with respect to Rayâs integration with ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"49AVYyPR2gbRD9P3NOH7My","type":"Entry","createdAt":"2023-04-26T19:44:22.407Z","updatedAt":"2023-04-26T22:30:40.568Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"generative_ai_features_and_examples","header":"Table 1. New LLM features, examples, and tutorials","body":"| Generative AI Workload| Description| Links|\n| ---------- | ---------- | ---------- |\n|Fine-tuning GPT-J|In this example, we showcase how to use [Ray Train](https://docs.ray.io/en/releases-2.4.0/train/train.html) for fine-tuning [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) using [DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed) and Hugging Face. GPT-J is a GPT-2-like causal language mode, with 6B parameters, trained on the PILE dataset.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html)|\n|Fine-tuning Dreambooth|This example shows how to fine-tune a [DreamBooth](https://dreambooth.github.io/) model using [Ray Train](https://docs.ray.io/en/releases-2.4.0/train/train.html). Dreambooth is a technique to fine-tune diffusion models (like Stable Diffusion) by injecting a custom subject to the model.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/dreambooth_finetuning.html)|\n|Scalable Batch Inference  with GPT-J|This example showcases how to use[ Ray Data](https://docs.ray.io/en/releases-2.4.0/data/dataset.html) for scalable batch inference with  GPT-J.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_batch_prediction.html)| Scalable Batch Inference  with Stable Diffusion|In this example, showcase how to use [Ray Data](https://docs.ray.io/en/releases-2.4.0/data/dataset.html) for scalable batch inference with [Stable Diffusion](https://stablediffusionweb.com/). Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from CompVis, Stability AI and LAION.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/stablediffusion_batch_prediction.html)|\n|Serving GPT-J|In this example, we showcase how to use [Ray Serve](https://docs.ray.io/en/releases-2.4.0/serve/index.html) for serving GPT-J.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_serving.html)|\n|Serving Stable Diffusion|In this example, we showcase how to use [Ray Serve](https://docs.ray.io/en/releases-2.4.0/serve/index.html) for serving [Stable Diffusion.](https://stablediffusionweb.com/)|[Source and documentation](https://docs.ray.io/en/master/serve/tutorials/stable-diffusion.html)|\n|Ray integration with LangChain|In this tutorial, we showcase how to build an open source search engine with Ray and [LangChain](https://python.langchain.com/en/latest/index.html)|[Source and blog](https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray)|"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray AIR new trainers:","nodeType":"text"},{"data":{},"marks":[],"value":" Second, to further enable additional large model workloads on Ray, weâre also releasing an ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"AccelerateTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", allowing you to run ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/accelerate"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"HuggingFace Accelerate","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/accelerate/usage_guides/deepspeed"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"DeepSpeed","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on Ray with minimal code changes. This Trainer integrates with the rest of the Ray ecosystemâincluding the ability to run distributed ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"hyperparameter tuning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with each trial dispatched as a distributed training job.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"See how ","nodeType":"text"},{"data":{"uri":"https://github.com/CarperAI/trlx/blob/9bc08369ca9ec83342c4d7755205dab1a7723006/trlx/sweep.py#L325-L340"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"CarperAI/trlx","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" uses this for RLHF hyperparameter sweeps, and take a peek at the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html#ray-train-huggingface-accelerate-acceleratetrainer"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" too.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4CHwg0j8NDYgH399cu2hGC","type":"Entry","createdAt":"2023-04-26T21:23:30.012Z","updatedAt":"2023-04-26T21:23:30.012Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"accelerate_trainer_code_snippet","body":"from ray.train.huggingface.accelerate import AccelerateTrainer\n\ndef train_func():\n    ...\n\ntrainer = AccelerateTrainer(\n    train_func,\n    accelerate_config={}  # you can pass in a config filepath here as well\n    scaling_config=ScalingConfig(\n        trainer_resources={\"CPU\": 0},\n        num_workers=4,\n        use_gpu=True,\n        resources_per_worker={\"CPU\": 2},\n    ),\n)\n\nresult = trainer.fit()\n\n# Integration with Ray Tune\ntuner = tune.Tuner(\n    trainer,\n    param_space={\"lr\": tune.grid_search([0.1, 0.2, 0.3]),\n)\n\nresults = tuner.fit()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can see the full example for AccelerateTrainer ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html#ray-train-huggingface-accelerate-acceleratetrainer"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Updated LightningTrainer:","nodeType":"text"},{"data":{},"marks":[],"value":" Third, in the broader deep learning space, weâre introducing the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.lightning.LightningTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LightningTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", allowing you to scale your ","nodeType":"text"},{"data":{"uri":"https://lightning.ai/docs/pytorch/stable//index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch Lightning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on Ray. As part of our continued effort for seamless integration and ease of use, we have enhanced and replaced our existing ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray_lightning","nodeType":"text"},{"data":{},"marks":[],"value":" integration, which was widely adopted, with the latest changes to PyTorch Lighting.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The new ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"LightningTrainer","nodeType":"text"},{"data":{},"marks":[],"value":" provides better compatibility with the other Ray libraries: Ray Tune or Ray Data or Ray Serve directly with Ray connecting APIs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Check out ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.lightning.LightningTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"the documentation for this component","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or take a look ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/examples/lightning/lightning_mnist_example.html#lightning-mnist-example"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"at an example","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1p8EMLSFTFDI0RuBceMrPe","type":"Entry","createdAt":"2023-04-26T21:27:50.764Z","updatedAt":"2023-04-26T21:27:50.764Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"updated_lightning_trainder_code_snippet","body":"from pytorch_lightning.loggers.csv_logs import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\nfrom ray.train.lightning import (\n    LightningTrainer,\n    LightningConfigBuilder,\n    LightningCheckpoint,\n)\n\nclass MNISTClassifier:\n\n    â¦\n\nclass MNISTDataModule:\n    â¦\n\nlightning_config = (\n    LightningConfigBuilder()\n    .module(MNISTClassifier, lr=1e-3, feature_dim=128)\n    .trainer(\n        max_epochs=10,\n        accelerator=\"cpu\",\n        log_every_n_steps=100,\n        logger=CSVLogger(\"logs\"),\n    )\n    .fit_params(datamodule=datamodule)\n    .checkpointing(monitor=\"val_accuracy\", mode=\"max\", save_top_k=3)\n    .build()\n)\nscaling_config = ScalingConfig(\n    num_workers=4, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n)\n\nrun_config = RunConfig(\n    name=\"ptl-mnist-example\",\n    local_dir=\"/tmp/ray_results\",\n    checkpoint_config=CheckpointConfig(\n        num_to_keep=3,\n        checkpoint_score_attribute=\"val_accuracy\",\n        checkpoint_score_order=\"max\",\n    ),\n)\ntrainer = LightningTrainer(\n    lightning_config=lightning_config,\n    scaling_config=scaling_config,\n    run_config=run_config,\n)\n\nresult = trainer.fit()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"See the full source code ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/examples/lightning/lightning_mnist_example.html#lightning-mnist-example"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements in Ray DataÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The enhancements and features in the release cut across a few dimensions: stability, ease of use, and observability.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data and ease of use: ","nodeType":"text"},{"data":{},"marks":[],"value":"Data ingestion and preprocessing for machine learning (ML) at scale are central to any end-to-end ML.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To enable ease of use, the default backend execution model for Ray Data is streaming-based under the hood. As such developers need not worry about instructing Ray Data to use the streaming execution backend; itâs the default execution mode. Second, the default streaming backend lends itself well to large scale workloads such as setting up distributed training data streaming and batch inference or preprocessing. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4fRyCVJVYH8NAjkcFhTgDX","type":"Entry","createdAt":"2023-04-26T21:30:40.030Z","updatedAt":"2023-04-26T21:30:40.030Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_data_code_snippet","body":"import ray\nfrom ray.data import ActorPoolStrategy\n\nimport time\n\ndef preprocess_func(x):\n   # Do some work with the data x\n   x = ...\n   time.sleep(0.1)\n   return x\n\nfor _ in (\n   ray.data.range_tensor(5000, shape=(80, 80, 3), parallelism=200)\n   .map_batches(preprocess_func, compute=ActorPoolStrategy(size=4))\n   .map_batches(preprocess_func, compute=ActorPoolStrategy(size=3))\n   .map_batches(preprocess_func, num_cpus=1)\n   .iter_batches()\n\n):\n\n   pass","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This launches a simple 4-stage pipeline. We use different compute args for each stage, which forces them to be run as separate operators instead of getting fused together. You should see a log message indicating streaming execution is being used.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While the above example is using dummy tasks, these sort of operator pipelines are a common pattern Ray users desire to scale:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6T6VTlZroKCZ6eyyII0HC1","type":"Asset","createdAt":"2023-04-26T21:34:22.865Z","updatedAt":"2023-04-26T21:34:22.865Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_data_distributed_image","description":"Figure 1. Ray data streaming data with ActorPool ","file":{"url":"//images.ctfassets.net/xjan103pcp94/6T6VTlZroKCZ6eyyII0HC1/d7202c977c1a0c25eeb4563ab5c50319/image3.png","details":{"size":106561,"image":{"width":1249,"height":515}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray data and SQL","nodeType":"text"},{"data":{},"marks":[],"value":": Additionally, we have extended Ray Dataâs ability to fetch data from a common ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/creating-datastreams.html#reading-from-sql-databases"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SQL data source","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", with ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"read_sql(...)","nodeType":"text"},{"data":{},"marks":[],"value":", enabling you to connect to a common SQL data store.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3imkshawnZw7DglAUBNTN3","type":"Entry","createdAt":"2023-04-26T21:36:16.889Z","updatedAt":"2023-04-26T21:36:16.889Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_data_sql_connector_code_snippet","body":"import mysql.connector\n\nimport ray\n\ndef create_connection():\n    return mysql.connector.connect(\n        user=\"admin\",\n        password=...,\nhost=\"example-mysql-database.c2c2k1yfll7o.us-west-2.rds.amazonaws.com\",\n      connection_timeout=30,\n      database=\"example\",\n    )\n\n# Get all movies\ndatastream = ray.data.read_sql(\"SELECT * FROM movie\", create_connection)\n# Get movies after the year 1980\ndatastream = ray.data.read_sql(\n    \"SELECT title, score FROM movie WHERE year \u003e= 1980\", create_connection\n)\n# Get the number of movies per year\ndatastream = ray.data.read_sql(\n    \"SELECT year, COUNT(*) FROM movie GROUP BY year\", create_connection\n)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements to Serve observability","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In 2.4, we are adding new functionality for observability and monitoring for Ray Serve applications. You can use the Ray dashboard to get a high-level overview of your Ray cluster and Ray Serve applicationâs states. This includes details such as:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"the number of deployment replicas currently running","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"logs for your Serve controller, deployment replicas, and HTTP proxies","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"the Ray nodes (i.e., machines) running in your Ray cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can access the Ray dashboard at port 8265 at your clusterâs URI. For example, if youâre running Ray Serve locally, use ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"http://localhost:8265","nodeType":"text"},{"data":{},"marks":[],"value":" in your browser.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3CtBAZRebvQVRJsbp63rD2","type":"Asset","createdAt":"2023-04-26T21:39:28.459Z","updatedAt":"2023-04-26T21:39:28.459Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_serve_observebility","description":"Figure 1. Serve observesability metrics in the Ray Dashboard.","file":{"url":"//images.ctfassets.net/xjan103pcp94/3CtBAZRebvQVRJsbp63rD2/0951a7ad1f4d4ec1ae2a3ba60a3af218/image2.png","details":{"size":146211,"image":{"width":1999,"height":1040}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"For more information on the dashboard, read the documentation for the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/ray-core/ray-dashboard.html#dash-serve-view"},"content":[{"data":{},"marks":[],"value":"Serve page","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"RLlibâs new module for custom reinforcement","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In RLlib, we are introducing a new ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/rllib-rlmodule.html#rl-modules-alpha"},"content":[{"data":{},"marks":[],"value":"RLModule","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" abstraction (alpha). RLModule API provides a unified way to define custom reinforcement learning models in RLlib. This API enables you to design and implement your own models to suit specific needs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To maintain consistency and usability, RLlib offers a standardized approach for defining module objects for both single-agent and multi-agent reinforcement learning environments through the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/package_ref/rl_modules.html"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"SingleAgentRLModuleSpec","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/package_ref/doc/ray.rllib.core.rl_module.marl_module.MultiAgentRLModuleSpec.html#ray.rllib.core.rl_module.marl_module.MultiAgentRLModuleSpec"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"MultiAgentRLModuleSpec","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" classes. The built-in RLModules in RLlib follow this consistent design pattern, making it easier for you to understand and utilize these modules.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1w6lK0sBUAVl4BmOGBQcQ9","type":"Entry","createdAt":"2023-04-26T21:48:25.122Z","updatedAt":"2023-04-26T21:48:25.122Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"rllib_multiagent_code_snippet","body":"import gymnasium as gym\nfrom ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec\nfrom ray.rllib.core.testing.torch.bc_module import DiscreteBCTorchModule\n\nenv = gym.make(\"CartPole-v1\")\n\nspec = SingleAgentRLModuleSpec(\n    module_class=DiscreteBCTorchModule,\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    model_config_dict={\"fcnet_hiddens\": [64]},\n)\n\nmodule = spec.build()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is an experimental module that serves as a general replacement for ModelV2, and is subject to change. It will eventually match the functionality of the previous stack. If you only use high-level RLlib APIs such as ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html#ray.rllib.algorithms.algorithm.Algorithm"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Algorithm","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", you should not experience significant changes, except for a few new parameters to the configuration object. Read more about it in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/rllib/rllib-rlmodule.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Core scaling to 2000 nodes","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Weâve also invested time in making sure Ray can support larger scale workloads. With this release, we are announcing official support for Ray clusters to up to 2000 nodes. See","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/release/benchmarks/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":" scalability envelope","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for more details.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"First and foremost, we want to thank all contributors for their valuable contributions to this new release of ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.4.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2.4","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Your enduring support continues to foster the wider use of Ray adoption.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This release is a milestone in our efforts to offer Ray as a performant compute infrastructure for LLM training, inference, and serving. We shared a number of examples that speak to these efforts.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Additionally, we enhanced Ray Data, improved Serve observability, introduced a new RLlib customization module, and scaled Ray core to support large workloads, up to 2000 nodes. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Have a go at the latest release with pip install âray[default]â and let us know of your feedback. Weâre always interested to hear from you â feel free to reach out to us on ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Github","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discuss","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Whatâs Next?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stay tuned for more Generative AI how-to blog series as we continue our efforts to make Ray the platform choice and compute substrate for all your Generative AI workloads. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you missed our four-part blog series on Generative AI, in which lay out our vision, position, and solution on how Ray addresses the challenges associated with OSS Generative AI workloads, peruse the links below. Much of the above Generative AI examples and features stem from this series.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-ray-solves-common-production-challenges-for-generative-ai"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"How Ray solves common production challenges for generative AI infrastructure","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Training 175B Parameter Language Models at 1000 GPU scale with Alpa and Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/faster-stable-diffusion-fine-tuning-with-ray-air"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Faster stable diffusion fine-tuning with Ray AIR","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"How to fine tune and serve LLMs simply, quickly and cost effectively using Ray + DeepSpeed + HuggingFace","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Building an LLM open source search engine in 100 lines using LangChain and Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Join our ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/community"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Community","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the ","nodeType":"text"},{"data":{"uri":"https://forms.gle/9TSdDYUgxYs8SA9e8"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray #LLM slack channel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have our ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit 2023 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"early-bird registration open. Secure your spot, and save some money.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QDe7dun9hKhLCoGFSXKh9","type":"Asset","createdAt":"2023-04-26T19:23:25.596Z","updatedAt":"2023-04-26T19:23:25.596Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_image","file":{"url":"//images.ctfassets.net/xjan103pcp94/1QDe7dun9hKhLCoGFSXKh9/d9f93233f6ee72608e34735eac545a74/image1.png","details":{"size":154908,"image":{"width":960,"height":540}},"fileName":"image1.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"574o0Bh7HzHlCEc6AXphfF","type":"Entry","createdAt":"2023-04-19T16:00:08.661Z","updatedAt":"2023-05-31T18:34:15.880Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":16,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Building an LLM open source search engine in 100 lines using LangChain and Ray","seoTitle":"Building an LLM Open-Source Search Engine in 100 Lines","slug":"llm-open-source-search-engine-langchain-ray","description":"In part 1 of a new blog series, we show how to build a search engine in 100 lines using LLM embeddings and a vector database.","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-04-18","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 1 of a blog series. In this blog, weâll introduce you to LangChain and Ray Serve and how to use them to build a search engine using LLM embeddings and a vector database. In future parts, we will show you how to turbocharge embeddings and how to combine a vector database and an LLM to create a fact-based question answering service. Additionally, we will optimize the code and measure performance: cost, latency and throughput.\n\n\u003cscript type=\"application/ld+json\"\u003e{\n\"@context\": \"http://schema.org\",\n\"@type\": \"VideoObject\",\n\"name\": \"Open Source LLM Search Engine with LangChain on Ray\",\n\"description\": \"Waleed, Head of Engineering at Anyscale, explains how to use LangChain and Ray Serve to build a search engine using LLM embeddings and a vector database. Blog: https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray\",\n\"thumbnailUrl\": \"https://i.ytimg.com/vi/v7a8SR-sZpI/default.jpg\",\n\"uploadDate\": \"2023-04-19T16:00:41Z\",\n\"duration\": \"PT7M36S\",\n\"embedUrl\": \"https://www.youtube.com/embed/v7a8SR-sZpI\",\n\"interactionCount\": \"4540\"\n}\u003c/script\u003e","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"b9V0HkkRoYEqCnIrMO9kC","type":"Entry","createdAt":"2023-04-19T17:16:15.326Z","updatedAt":"2023-04-19T17:16:15.326Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"LLM Search Engine with Langchain","videoUrl":"https://www.youtube.com/watch?v=v7a8SR-sZpI"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog, we'll cover:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"An introduction to ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and show why itâs awesome.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"An explanation of how Ray complements ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" by:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Showing how with a few minor changes, we can speed parts of the process up by a factor of 4x or more","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Making ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"âs capabilities available in the cloud using Ray Serve","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Using self-hosted models by running ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the model all in the same Ray cluster without having to worry about maintaining individual machines.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introduction","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a very powerful framework for ML orchestration, but with great power comes voluminous documentation. 120 megabytes in fact. How can we make that documentation more accessible? ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Answer: make it searchable! It used to be that creating your own high quality search results was hard. But by using ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we can build it in about 100 lines of code.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is where ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" comes in. ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provides an amazing suite of tools for everything around LLMs. Itâs kind of like ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/"},"content":[{"data":{},"marks":[],"value":"HuggingFace","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" but specialized for LLMs. There are tools (chains) for prompting, indexing, generating and summarizing text. While an amazing tool, using Ray with it can make ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" even more powerful. In particular, it can:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Simply and quickly help you deploy a ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" service.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Rather than relying on remote API calls, allow Chains to run co-located and auto-scalable with the LLMs itself. This brings all the advantages we ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"discussed in a previous blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": lower cost, lower latency, and control over your data.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Building the index","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4OzISThpksdKgjZ0gVJUiB","type":"Asset","createdAt":"2023-04-19T16:59:49.253Z","updatedAt":"2023-05-01T09:40:14.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"doc-index-starter-langchain","description":"Build a document index easily with Ray and Langchain","file":{"url":"//images.ctfassets.net/xjan103pcp94/4OzISThpksdKgjZ0gVJUiB/85bb7fccdfef1df3d061c57e9af1062a/index-langchain.jpg","details":{"size":28918,"image":{"width":900,"height":381}},"fileName":"index-langchain.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"First we will build the index via the following steps.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Download the content we want to index locally.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Read the content and cut it into tiny little pieces (about a sentence each). This is because it is easier to match queries against pieces of a page rather than the whole page.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use the ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/sentence-transformers"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Sentence Transformers","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" library from HuggingFace to generate a vector representation of each sentence.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Embed those vectors in a Vector database (we use ","nodeType":"text"},{"data":{"uri":"https://github.com/facebookresearch/faiss"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FAISS","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", but you could use whatever you like).Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The amazing thing about this code is how simple it is - ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/d06097768abbea54d59e5d3ed4f045f3"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"See Here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". As you will see, thanks to LangChain, all the heavy lifting is done for us. Letâs pick a few excerpts.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Assuming weâve downloaded the Ray docs, this is all we have to do to read all the docs in: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5XcvTPG6LASLRu3arklXJ0","type":"Entry","createdAt":"2023-04-17T08:22:33.719Z","updatedAt":"2023-04-17T08:22:33.719Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-1","body":"loader = ReadTheDocsLoader(\"docs.ray.io/en/master/\")\ndocs = loader.load() ","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The next step is to break each document into little chunks. LangChain uses splitters to do this. So all we have to do is this: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"72vZr3kpoioEza3WTS1vfT","type":"Entry","createdAt":"2023-04-17T08:23:19.379Z","updatedAt":"2023-04-17T08:23:19.379Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-2","body":"chunks = text_splitter.create_documents(\n    [doc.page_content for doc in docs], \n    metadatas=[doc.metadata for doc in docs])","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We want to preserve the metadata of what the original URL was, so we make sure to retain the metadata along with these documents.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now we have the chunks we can embed them as vectors. LLM providers do offer APIs for doing this remotely (and this is how most people use LangChain). But, with just a little bit of glue we can download Sentence Transformers from HuggingFace and run them locally (inspired by LangChainâs support for llama.cpp). ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/aea1d312d68c9431949442cc562d5f2c"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Hereâs the glue code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"By doing so, we reduce latency, stay on open source technologies, and donât need a HuggingFace key or to pay for API usage.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have the embeddings, now we can use a vector database â in this case FAISS â to store the embeddings. Vector databases are optimized for doing quick searches in high dimensional spaces. Again, LangChain makes this effortless. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4YljOvEjiKrfhFRmpA1PDG","type":"Entry","createdAt":"2023-04-17T08:23:35.223Z","updatedAt":"2023-04-17T08:23:35.223Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-3","body":"from langchain.vectorstores import FAISS\n\ndb = FAISS.from_documents(chunks, embeddings)\ndb.save_local(FAISS_INDEX_PATH)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"And thatâs it. The code for this is ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/d0915e52cbe56dff328f5c00ded21107"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Now we can build the store.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"39FzcaaXnaSmciROgXSD0x","type":"Entry","createdAt":"2023-04-17T08:29:45.440Z","updatedAt":"2023-04-17T08:29:45.440Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-python-build-snippet","body":"% python build_vector_store.py"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This takes about 8 minutes to execute. Most of that time is spent doing the embeddings. Of course, itâs not a big deal in this case, but imagine if you were indexing hundreds of gigabytes instead of hundreds of megabytes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Accelerating indexing using Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"[Note: This is a slightly more advanced topic and can be skipped on first reading. It just shows how we can do it more quickly â 4x to 8x times more quickly]","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How can we speed up indexing? The great thing is that embedding is easy to parallelize. What if we:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Sliced the list of chunks into 8 shards.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Embedded each of the 8 shards separately.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Merge the shards.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7tDpD5Q7nxtRyX9lgDvbkI","type":"Asset","createdAt":"2023-04-17T08:04:57.120Z","updatedAt":"2023-04-17T08:04:57.120Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"langchain-ray-accelerated-indexing","description":"Build a document index 4-8x faster with Ray","file":{"url":"//images.ctfassets.net/xjan103pcp94/7tDpD5Q7nxtRyX9lgDvbkI/6209fbd875c5cd379c2289ef6f6554f0/Screen_Shot_2023-04-16_at_6.20.10_PM.png","details":{"size":277144,"image":{"width":2284,"height":936}},"fileName":"Screen Shot 2023-04-16 at 6.20.10 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"One key thing to realize is that embedding is GPU accelerated, so if we want to do this, we need 8 GPUs. Thanks to Ray, those 8 GPUS donât have to be on the same machine. But even on a single machine, there are significant advantages to using Ray. And one does not have to go to the complexity of setting up a Ray cluster, all you need to do is ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"pip install ray[default]","nodeType":"text"},{"data":{},"marks":[],"value":" and then ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"import ray","nodeType":"text"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This requires some minor surgery to the code. Hereâs what we have to do.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"First, create a task that creates the embedding and then uses it to index a shard. Note the Ray annotation and us telling us each task will need a whole GPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3NmJuC8SstZJpoHLrzrLgg","type":"Entry","createdAt":"2023-04-17T08:23:48.410Z","updatedAt":"2023-04-17T08:23:48.410Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-4","body":"@ray.remote(num_gpus=1)\ndef process_shard(shard): \n    embeddings = LocalHuggingFaceEmbeddings('multi-qa-mpnet-base-dot-v1')\n    result = FAISS.from_documents(shard, embeddings)\n    return result\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next, split the workload in the shards. NumPy to the rescue! This is a single line:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7lotIjqpm0Sm03BPHE4t2n","type":"Entry","createdAt":"2023-04-17T08:24:01.572Z","updatedAt":"2023-04-17T08:24:01.572Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-5","body":"shards = np.array_split(chunks, db_shards)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Then, create one task for each shard and wait forÂ the results.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7m8iFk9DvJOBZc3r4maPTY","type":"Entry","createdAt":"2023-04-17T08:24:13.248Z","updatedAt":"2023-04-17T08:24:13.248Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-6","body":"futures = [process_shard.remote(shards[i]) for i in range(db_shards)]\nresults = ray.get(futures)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, letâs merge the shards together. We do this using simple linear merging.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"21KuagZ35WVzVVJ7Xq4qHy","type":"Entry","createdAt":"2023-04-17T08:24:27.839Z","updatedAt":"2023-04-17T08:29:23.908Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-7","body":"db = results[0]\nfor i in range(1,db_shards):\n    db.merge_from(results[i])","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/4c41f3ee66040f57d34c6a40e42b5969"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Hereâs what the sped up code looks like.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"You might be wondering, does this actually work? We ran some tests on a g4dn.metal instance with 8 GPUs. The original code took 313 seconds to create the embeddings, the new code took 70 seconds, thatâs about a 4.5x improvement. Thereâs still some one-time overheads to creating tasks, setting up the GPUs etc. This reduces as the data increases. For example, we did a simple test with 4 times the data, and it was around 80% of the theoretical maximum performance (ie. 6.5x faster vs theoretical maximum of 8x faster from the 8 GPUs).Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can use the Ray Dashboard to see how hard those GPUs are working. Sure enough theyâre all close to 100% running the process_shard method we just wrote. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"14IoasmHxwEeQdAEGJqlyO","type":"Asset","createdAt":"2023-04-17T08:10:19.509Z","updatedAt":"2023-04-17T21:38:58.348Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"accelerated-index-langchain-dashboard","description":"Dashboard shows that GPU utilization is maxed out across all instances","file":{"url":"//images.ctfassets.net/xjan103pcp94/14IoasmHxwEeQdAEGJqlyO/5ae6e6739e258252a78c889ca7959683/raydash.png","details":{"size":278628,"image":{"width":2442,"height":842}},"fileName":"raydash.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"It turns out merging vector databasesÂ  is pretty fast, taking only 0.3 seconds for all 8 shards to be merged.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Serving","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1g6zBePU72Rmz5MBH2reaB","type":"Asset","createdAt":"2023-04-17T08:08:50.696Z","updatedAt":"2023-04-17T08:08:50.696Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Serving-Queries-Ray-Langchain","description":"Serve search queries with Ray and Langchain","file":{"url":"//images.ctfassets.net/xjan103pcp94/1g6zBePU72Rmz5MBH2reaB/db400e9bbbc445d7214d45658f81992f/Screen_Shot_2023-04-16_at_9.42.46_PM.png","details":{"size":380753,"image":{"width":2718,"height":1348}},"fileName":"Screen Shot 2023-04-16 at 9.42.46 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Serving is another area where the combination of LangChain and Ray Serve shows its power. This is just scratching the surface: weâll explore amazing capabilities like independent auto scaling and request batching in our next blog post in the series. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The steps required to do this are: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Load the FAISS database we created and the instantiate the embedding","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Start using FAISS to do similarity searches. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve makes this magically easy. Ray uses a âdeploymentâ to wrap a simple python class. The __init__ method does the loading and then __call__ is what actually does the work. Ray takes care of connecting it to the internet, bringing up a service, http and so on. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hereâs a simplified version of the code: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7GwywaSkEi9NnwUhk3wMMD","type":"Entry","createdAt":"2023-04-17T08:24:42.996Z","updatedAt":"2023-04-17T08:24:42.996Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-8","body":"@serve.deployment\nclass VectorSearchDeployment:\n    def __init__(self):\n        self.embeddings = â¦ \n        self.db = FAISS.load_local(FAISS_INDEX_PATH, self.embeddings)\n\n    def search(self,query): \n        results = self.db.max_marginal_relevance_search(query)\n        retval = \u003csome string processing of the results\u003e\n        return retval\n\n    async def __call__(self, request: Request) -\u003e List[str]:\n        return self.search(request.query_params[\"query\"])\n\ndeployment = VectorSearchDeployment.bind()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Thatâs it! ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs now start this service with the command line (of course Serve has more deployment options than this, but this is an easy way):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"o0cCaJMv4yeC2ikcQuJhf","type":"Entry","createdAt":"2023-04-17T08:29:00.307Z","updatedAt":"2023-04-17T08:29:00.307Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-serve-snippet","body":"% serve run serve_vector_store:deployment"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now we can write a simple python script to query the service to get relevant vectors(itâs just a web server running on port 8000). ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3E0la6cbL8n7UPpvve36nJ","type":"Entry","createdAt":"2023-04-17T08:24:56.873Z","updatedAt":"2023-04-17T08:24:56.873Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-9","body":"import requests\nimport sys\nquery = sys.argv[1]\nresponse = requests.post(f'http://localhost:8000/?query={query}')\nprint(response.content.decode())","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"And now letâs try it out: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SL7auZuJzHEyaq8fVIifv","type":"Entry","createdAt":"2023-04-17T08:25:16.327Z","updatedAt":"2023-04-17T21:14:41.421Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-10","body":"$ python query.py 'Does Ray Serve support batching?'\nFrom http://docs.ray.io/en/master/serve/performance.html\n\nYou can check out our microbenchmark instructions\nto benchmark Ray Serve on your hardware.\nRequest Batching#\n====\n\nFrom http://docs.ray.io/en/master/serve/performance.html\n\nYou can enable batching by using the ray.serve.batch decorator. Letâs take a look at a simple example by modifying the MyModel class to accept a batch.\nfrom ray import serve\nimport ray\n@serve.deployment\nclass Model:\n    def __call__(self, single_sample: int) -\u003e int:\n        return single_sample * 2\n====\n\nFrom http://docs.ray.io/en/master/ray-air/api/doc/ray.train.lightgbm.LightGBMPredictor.preferred_batch_format.html\n\nnative batch format.\nDeveloperAPI: This API may change across minor Ray releases.\n====\n\nFrom http://docs.ray.io/en/master/serve/performance.html\n\nMachine Learning (ML) frameworks such as Tensorflow, PyTorch, and Scikit-Learn support evaluating multiple samples at the same time.\nRay Serve allows you to take advantage of this feature via dynamic request batching.\n===="}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We showed in the above code just how easy it is to build key components of an LLM-based search engine and serve its responses to the entire world by combining the power of LangChain and Ray Serve. And we didnât have to deal with a single pesky API key!Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Tune in for Part 2, where we will show how to turn this into a chatgpt-like answering system. Weâll use open source LLMs like Dolly 2.0 to do that.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"And finally weâll share Part 3 where weâll talk about scalability and cost. The above is fine for a few hundred queries per second, but what if you need to scale to a lot more? And are the claims about latency correct? ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next Steps","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"See part 2 ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/turbocharge-langchain-now-guide-to-20x-faster-embedding"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nReview the code and data used in this blog in the following ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/tree/main/open_source_LLM_search_engine"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Github repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". \n\nSee our earlier ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[],"value":"blog series on solving Generative AI infrastructure","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with Ray.\n\nIf you are interested in learning more about Ray, see ","nodeType":"text"},{"data":{"uri":"http://ray.io"},"content":[{"data":{},"marks":[],"value":"Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"http://docs.ray.io"},"content":[{"data":{},"marks":[],"value":"Docs.Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To connect with the Ray community join #LLM on the ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"data":{},"marks":[],"value":"Ray Slack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or our ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[],"value":"Discuss forum","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","nodeType":"text"},{"data":{"uri":"http://www.anyscale.com/platform"},"content":[{"data":{},"marks":[],"value":"Anyscale.com/Platform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and click the 'Try it now' button","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray Summit 2023:","nodeType":"text"},{"data":{},"marks":[],"value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"practical training focused on LLMs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4OzISThpksdKgjZ0gVJUiB","type":"Asset","createdAt":"2023-04-19T16:59:49.253Z","updatedAt":"2023-05-01T09:40:14.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"doc-index-starter-langchain","description":"Build a document index easily with Ray and Langchain","file":{"url":"//images.ctfassets.net/xjan103pcp94/4OzISThpksdKgjZ0gVJUiB/85bb7fccdfef1df3d061c57e9af1062a/index-langchain.jpg","details":{"size":28918,"image":{"width":900,"height":381}},"fileName":"index-langchain.jpg","contentType":"image/jpeg"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7bptc6mEv7bhFZvq5AOXqc","type":"Entry","createdAt":"2023-04-18T20:36:43.249Z","updatedAt":"2023-04-19T16:00:08.646Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Why I Joined Anyscale: Solving Cutting-Edge Problems in a Time of Enormous Change","seoTitle":"why i joined anyscale by sidney rabsatt","slug":"why-i-joined-anyscale-solving-cutting-edge-problems-in-a-time-of-enormous","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"461P1q4TML68SzYxOG9sxm","type":"Entry","createdAt":"2023-04-18T20:05:20.525Z","updatedAt":"2023-04-18T20:05:20.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Sidney Rabsatt","slug":"sidney-rabsatt","link":"https://www.linkedin.com/in/sidney-rabsatt/"}}],"publishedDate":"2023-04-19","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42F9iHs5a05rRrOuqEQwvB","type":"Entry","createdAt":"2022-06-15T01:30:47.322Z","updatedAt":"2022-06-15T01:30:47.322Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Culture","slug":"culture","tintColor":"#39C8E5"}},"intro":"Why Sidney Rabsatt joined Anyscale as Head of Product.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Throughout my career, I've had the opportunity to work on cutting-edge problems across some of the most impactful technological transitions of our time. From the early days of the World Wide Web to the rise of Mobile and Cloud computing, Iâve worked on numerous commercial products across Networking, Observability, and App/Cloud Infrastructure and on some of the most widely-adopted open-source projects including Wireshark, Nginx, and now Ray. I've been deeply involved in resolving the complexities and, at times, unexpected infrastructure obstacles that technologists encounter, while empowering companies to fully benefit from these advancements.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7uPFFDyMQg5GZ5FU5kM8rw","type":"Asset","createdAt":"2023-04-18T20:34:23.318Z","updatedAt":"2023-04-18T20:34:23.318Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Sidney Rabsatt Headshot","description":"Picture of Sidney Rabsatt, Anyscale Head of Product","file":{"url":"//images.ctfassets.net/xjan103pcp94/7uPFFDyMQg5GZ5FU5kM8rw/225783b86665705f9c198768754f8722/Sidney_pic.jpeg","details":{"size":501555,"image":{"width":3540,"height":2832}},"fileName":"Sidney pic.jpeg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"AI","nodeType":"text"},{"data":{},"marks":[],"value":" promises to be the most complex transition across the most dimensions that affect our lives. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"I joined Anyscale to embrace and be part of solving those challenges. I joined to make AI available to all organizations, to give people better tools, and do it responsibly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray and Anyscale work at the core of the best known and most advanced AI applications such as Generative AI with OpenAIâs ChatGPT and Prediction for Uber rides / ETAs, not to mention what Spotify, Instacart and others are doing.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The team is one of the strongest in the industry, comprised of AI/ML PhDâs and experienced AI experts from top schools like UC Berkeley and companies like Uber, Google, Amazon, Meta, and more!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Our technology is proven and continues to evolve rapidly with strong community involvement. And the opportunity is vast to define how best to develop and run AI.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"AI will touch and shape our lives in many ways and joining Anyscale to lead Product is how Iâm getting involved.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7uPFFDyMQg5GZ5FU5kM8rw","type":"Asset","createdAt":"2023-04-18T20:34:23.318Z","updatedAt":"2023-04-18T20:34:23.318Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Sidney Rabsatt Headshot","description":"Picture of Sidney Rabsatt, Anyscale Head of Product","file":{"url":"//images.ctfassets.net/xjan103pcp94/7uPFFDyMQg5GZ5FU5kM8rw/225783b86665705f9c198768754f8722/Sidney_pic.jpeg","details":{"size":501555,"image":{"width":3540,"height":2832}},"fileName":"Sidney pic.jpeg","contentType":"image/jpeg"}}},"mainImageFit":"contain","hideIntro":true,"showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7e5A9XUzg0NfneWobPOKxf","type":"Entry","createdAt":"2023-04-10T23:01:28.472Z","updatedAt":"2023-05-31T18:31:32.938Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":12,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"How to fine tune and serve LLMs simply, quickly and cost effectively using Ray + DeepSpeed + HuggingFace","seoTitle":"How to Fine-Tune a 6 Billion Parameter LLM for Less Than $7","slug":"how-to-fine-tune-and-serve-llms","description":"In part 4 of our Generative AI series, we share how to build a system for fine-tuning \u0026 serving LLMs in 40 minutes or less.","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2yKvtARlwNkkIYOYLFgkhB","type":"Entry","createdAt":"2022-06-22T19:54:47.912Z","updatedAt":"2022-06-22T19:54:47.912Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jun Gong","slug":"jun-gong"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BGRfsYFKUgIpGjaXTUYml","type":"Entry","createdAt":"2021-08-09T15:38:43.390Z","updatedAt":"2021-08-09T15:38:43.390Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Antoni Baum","slug":"antoni-baum","link":"https://www.linkedin.com/in/yard1/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}}],"publishedDate":"2023-04-10","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 4 of our blog series on Generative AI. In the previous blog posts we explained:\n1.[Why Ray is a sound platform for Generative AI](https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure)\n2.[we showed how it can push the performance limits](https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray)\n3.[how you can use Ray for stable diffusion](https://www.anyscale.com/blog/faster-stable-diffusion-fine-tuning-with-ray-air). \n","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this blog, we share aÂ  practical approach on how you can use the combination of HuggingFace, DeepSpeed, and Ray to build a system for fine-tuning and serving LLMs, in 40 minutes for less than $7 for a 6 billion parameter model. In particular, we illustrate the following:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Using these three components, you can simply and quickly put together an open-source LLM fine-tuning and serving system.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"By taking advantage of Rayâs distributed capabilities, we show how this can be both more cost-effective and faster than using a single large (and often ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://aiascendant.substack.com/p/taiwan-is-pandora-gpus-are-unobtainium"},"content":[{"nodeType":"text","value":"unobtainable","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":") machine.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Hereâs what weâll be doing:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Discussing why you might want to run your own LLM instead of using one of the new API providers.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing you the evolving tech stack we are seeing for cost-effective LLM fine-tuning and serving, combining HuggingFace, DeepSpeed, Pytorch, and Ray.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing you 40 lines of Python code that can enable you to serve a 6 billion parameter GPT-J model.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing you, for less than $7, how you can fine-tune the model to sound more medieval using the works of Shakespeare by doing it in a distributed fashion on low-cost machines, which is considerably more cost-effective than using a single large powerful machine.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing how you can serve the fine-tuned 6B LLM compiled model binary.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing how the fine-tuned model compares to a prompt engineering approach with large systems. ","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Why would I want to run my own LLM?Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"There are ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://openai.com"},"content":[{"nodeType":"text","value":"many","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://anthropic.com/"},"content":[{"nodeType":"text","value":"many","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://cohere.ai/"},"content":[{"nodeType":"text","value":"providers","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" of LLM APIs online. Why would you want to run your own? There are a few reasons:Â ","marks":[],"data":{}}]},{"nodeType":"ordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Cost, especially for fine-tuned inference","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": For example, OpenAI charges 12c per 1000 tokens (about 700 words) for a fine-tuned model on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://openai.com/pricing"},"content":[{"nodeType":"text","value":"Davinci","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Itâs important to remember that many user interactions require multiple backend calls (e.g. one to help with the prompt generation, post-generation moderation, etc), so itâs very possible that a single interaction with an end user could cost a few dollars. For many applications, this is cost prohibitive.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Latency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"using these LLMs is especially slow. A GPT-3.5 query for example can take up to 30 seconds. Combine a few round trips from your data center to theirs and it is possible for a query to take minutes. Again, this makes many applications impossible. Bringing the processing in-house allows you to optimize the stack for your application, e.g. by using low-resolution models, tightly packing queries to GPUs, and so on. We have heard from users that optimizing their workflow has often resulted in a 5x or more latency improvement.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Data Security \u0026 Privacy: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"In order to get the response from these APIs, you have to send them a lot of data for many applications (e.g. send a few snippets of internal documents and ask the system to summarize them). Many of the API providers reserve the right to use those instances for retraining. Given the sensitivity of organizational data and also frequent legal constraints like data residency, this is especially limiting. One, particularly concerning recent development, is the ability to ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://arxiv.org/pdf/2301.13188.pdf"},"content":[{"nodeType":"text","value":"regenerate training data from learned models","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", and people ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.techradar.com/news/samsung-workers-leaked-company-secrets-by-using-chatgpt"},"content":[{"nodeType":"text","value":"unintentionally disclosing secret information","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"How to create and run your own LLMÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The LLM space is an incredibly fast-moving space, and it is currently evolving very rapidly. What we are seeing is a particular technology stack that combines multiple technologies: ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70GhsrIVrh0Qz0fNDUp4A8","type":"Asset","createdAt":"2023-04-07T14:17:45.223Z","updatedAt":"2023-04-07T14:17:45.223Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"llm-stack","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/70GhsrIVrh0Qz0fNDUp4A8/458e076a02c4745369c683851c378536/Screenshot_2023-04-07_at_10.17.23_AM.png","details":{"size":144021,"image":{"width":1073,"height":663}},"fileName":"Screenshot 2023-04-07 at 10.17.23 AM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"What weâve also seen is a reluctance to go beyond a single machine for training. In part, because there is a perception that moving to multiple machines is seen as complicated. The good news is this is where ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io"},"content":[{"nodeType":"text","value":"Ray.io","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" shines (ba-dum-tish). It simplifies cross-machine coordination and orchestration aspects using not much more than Python and Ray decorators, but also is a great framework for composing this entire stack together.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\nRecent results on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html"},"content":[{"nodeType":"text","value":"Dolly","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/lm-sys/FastChat"},"content":[{"nodeType":"text","value":"Vicuna","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" (both trained on Ray or trained on models built with Ray like ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://huggingface.co/EleutherAI/gpt-j-6b"},"content":[{"nodeType":"text","value":"GPT-J","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":") are small LLMs (relatively speaking â say the open source model GPT-J-6B with 6 billion parameters) that can be incredibly powerful when fine-tuned on the right data. The key is fine-tuning and the right data parts. So you do not always need to use the latest and greatest model with 150 billion-plus parameters to get useful results. Letâs get started! \n","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Serving a pre-existing model with Ray for text generation","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The detailed steps on how to serve the GPT-J model with Ray can be found ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/gptj_serving.html"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", so letâs highlight some of the aspects of how we do that. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"56imF50uP87DrWXWfjxNqV","type":"Entry","createdAt":"2023-04-07T18:37:36.999Z","updatedAt":"2023-04-07T18:47:21.234Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"serve-llm","body":"Â Â Â @serve.deployment(ray_actor_options={\"num_gpus\":1})\nÂ Â Â classPredictDeployment:\nÂ Â Â Â Â def__init__(self, model_id:str, revision:str=None):\nÂ Â Â Â Â Â Â Â from transformers import AutoModelForCausalLM, AutoTokenizer\nÂ Â Â Â Â Â Â Â import torch\nÂ Â Â Â Â Â Â Â self.model = AutoModelForCausalLM.from_pretrained(\nÂ Â Â Â Â Â Â Â Â Â Â Â \"EleutherAI/gpt-j-6B\",\nÂ Â Â Â Â Â Â Â Â Â Â Â revision=revision,\nÂ Â Â Â Â Â Â Â Â Â Â Â torch_dtype=torch.float16,\nÂ Â Â Â Â Â Â Â Â Â Â Â low_cpu_mem_usage=True,\nÂ Â Â Â Â Â Â Â Â Â Â Â device_map=\"auto\",Â  # automatically makes use of all GPUs available to the Actor\nÂ Â Â Â Â Â Â Â )\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Serving in Ray happens in actors, in this case, one called PredictDeployment. This code shows the __init__ method of the action that downloads the model from Hugging Face. To launch the model on the current node, we simply do: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7jOn5zhnrYvHQZg3fqmwjE","type":"Entry","createdAt":"2023-04-07T18:39:26.501Z","updatedAt":"2023-04-07T18:47:32.494Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"serve-cmd","body":"deployment = PredictDeployment.bind(model_id=model_id, revision=revision)\nserve.run(deployment)\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"That starts a service on port 8000 of the local machine.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We can now query that service using a few lines of Python","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6HQg2dtvaXrNsKuUvSIX3S","type":"Entry","createdAt":"2023-04-07T18:41:24.773Z","updatedAt":"2023-04-07T18:47:42.594Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"serve-query","body":"import requests\nprompt = (\n    âOnce upon a time, there was a horse. â\n)\nsample_input = {\"text\": prompt}\noutput = requests.post(\"http://localhost:8000/\", json=[sample_input]).json()\nprint(output)","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"And sure enough, this prints out a continuation of the above opening. Each time it runs, there is something slightly different.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\"Once upon a time, there was a horse.","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"But this particular horse was too big to be put into a normal stall. Instead, the animal was moved into an indoor pasture, where it could take a few hours at a time out of the stall. The problem was that this pasture was so roomy that the horse would often get a little bored being stuck inside. The pasture also didnât have a roof, and so it was a good place for snow to accumulate.\"","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is certainly an interesting direction and story â¦ but now we want to set it in the medieval era. What can we do? ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Fine Tuning Your LLM","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now that weâve shown how to serve a model, how do we fine-tune it to be more medieval? What about if we train it on 2500 lines from Shakespeare?Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is where ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.deepspeed.ai/"},"content":[{"nodeType":"text","value":"DeepSpeed","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" comes in. DeepSpeed is a set of optimized algorithms for training and fine-tuning networks. The problem is that DeepSpeed doesnât have an orchestration layer. This is not so much of a problem on a single machine, but if you want to use multiple machines, this typically involves a bunch of bespoke ssh commands, complex managed keys, and so on.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is where Ray can help.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html"},"content":[{"nodeType":"text","value":"page","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" in the Ray documentation discusses how to fine-tune it to sound more like something from the 15th century with a bit of flair.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Letâs go through the key parts. First, we load the data from hugging face","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12gknw4CuwTUwzyVwR5MEH","type":"Entry","createdAt":"2023-04-07T18:42:15.242Z","updatedAt":"2023-04-07T18:47:51.606Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hf-load-data","body":"from datasets import load_dataset\nprint(\"Loading tiny_shakespeare dataset\")\ncurrent_dataset = load_dataset(\"tiny_shakespeare\")\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Skipping the tokenization code, hereâs the heart of the code that we will run for each worker.Â ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5633wVi5PVmd43ciiJRLrv","type":"Entry","createdAt":"2023-04-07T18:43:13.431Z","updatedAt":"2023-04-07T18:48:00.199Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hf-trainer","body":"def trainer_init_per_worker(train_dataset, eval_dataset=None,**config):\nÂ Â Â Â # Use the actual number of CPUs assigned by Ray\nÂ Â Â Â model = GPTJForCausalLM.from_pretrained(model_name, use_cache=False)\nÂ Â Â Â model.resize_token_embeddings(len(tokenizer))\nÂ Â Â Â enable_progress_bar()\nÂ Â Â Â metric = evaluate.load(\"accuracy\")\nÂ Â Â Â trainer = Trainer(\nÂ Â Â Â Â Â Â Â model=model,\nÂ Â Â Â Â Â Â Â args=training_args,\nÂ Â Â Â Â Â Â Â train_dataset=train_dataset,\nÂ Â Â Â Â Â Â Â eval_dataset=eval_dataset,\nÂ Â Â Â Â Â Â Â compute_metrics=compute_metrics,\nÂ Â Â Â Â Â Â Â tokenizer=tokenizer,\nÂ Â Â Â Â Â Â Â data_collator=default_data_collator,\nÂ Â Â Â )\nÂ Â Â Â return trainer\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\nAnd now we create a Ray AIR HuggingFaceTrainer that orchestrates the distributed run and wraps around multiple copies of the training loop above: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"Qo7c20UkHRTIAgy2zvQ4G","type":"Entry","createdAt":"2023-04-07T18:43:58.073Z","updatedAt":"2023-04-07T18:48:09.302Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hf-trainer2","body":"trainer = HuggingFaceTrainer(\nÂ Â Â Â trainer_init_per_worker=trainer_init_per_worker,\nÂ Â Â Â trainer_init_config={\nÂ Â Â Â Â Â Â Â \"batch_size\":16,Â  # per device\nÂ Â Â Â Â Â Â Â \"epochs\":1,\nÂ Â Â Â },\nÂ Â Â Â scaling_config=ScalingConfig(\nÂ Â Â Â Â Â Â Â num_workers=num_workers,\nÂ Â Â Â Â Â Â Â use_gpu=use_gpu,\nÂ Â Â Â Â Â Â Â resources_per_worker={\"GPU\":1,\"CPU\": cpus_per_worker},\nÂ Â Â Â ),\nÂ Â Â Â datasets={\"train\": ray_datasets[\"train\"],\"evaluation\": ray_datasets[\"validation\"]},\nÂ Â Â Â preprocessor=Chain(splitter, tokenizer),\n)\nresults = trainer.fit()\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"While there is some complexity here, it is not much more complex than the code to get it to run on a simple machine. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Fine-tuning and Performance","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"One of the most important topics related to LLMs is the question of cost. In this particular case, the costs are small (in part because we ran only one epoch of fine-tuning, depending on the problem 1-10 epochs of fine-tuning are used, and also in part because this dataset is not so large). But running the tests on different configurations shows us that understanding performance is not always easy. The below shows some benchmarking results with different configurations of machines on AWS. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3k8l58AKMayxIzIhrO7Btr","type":"Entry","createdAt":"2023-04-07T14:39:44.461Z","updatedAt":"2023-04-07T14:39:44.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"fine-tune-performance","body":"\n| Configuration| #instances| Time (mins)| Total Cost (on-demand)|Total Cost (spot)| Cost Ratio|\n| ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n| 16 x g4dn.4xlarge (1 x T4 16GB GPU)|16|48|$15.41|__$6.17__|100%|\n| 32 x g4dn.4xlarge (1 x T4 16GB GPU)|32|__30__|$19.26|$7.71|125%|\n| 1 x p3.16xlarge (8 x V100 16GB GPU)|1|44|$17.95|$9.27|150%|\n| 1 x g5.48xlarge (8 x A10G 24GB GPU)|1|84|$22.81|$10.98|178%|\n"}}},"content":[]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3C7MbWRZ8oYJoE00IW4mIi","type":"Asset","createdAt":"2023-04-11T16:45:53.578Z","updatedAt":"2023-04-11T16:45:53.578Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"llm-graph","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3C7MbWRZ8oYJoE00IW4mIi/93f686b238a84d2ef64abe3aa7670791/Screenshot_2023-04-11_at_12.44.46_PM.png","details":{"size":76511,"image":{"width":1047,"height":644}},"fileName":"Screenshot 2023-04-11 at 12.44.46 PM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Note:","marks":[{"type":"underline"}],"data":{}},{"nodeType":"text","value":" we tried to run the same test with A100s, but we were unable to obtain the p4d machines to do so.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Looking at these numbers, we see some surprises:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Perhaps the most obvious machine to use â the g5.48xlarge â the machine with the highest on-paper performance â is both the most expensive and the slowest at almost twice the price when using spot instances.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The p3.16xlarge with its use of NVLink between the GPUs is a considerably better option.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Most surprising of all, using multiple machines is both the ","marks":[],"data":{}},{"nodeType":"text","value":"cheapest ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"and the ","marks":[],"data":{}},{"nodeType":"text","value":"fastest ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"option.Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The exact same code is running on all the machines, and aside from tweaking the number of GPU workers, nothing else was changed. Using multiple machines gave us the cheapest (16 machines) and the fastest (32 machines) option of the ones we benchmarked.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is the beauty and power of Ray. The code itself was simple enough, and in fact, was able to use a standard library âÂ  DeepSpeed â with no modifications. So it was no more complex in this case than a single machine. Simultaneously, it gave more options and flexibility to optimize to be both cheaper and faster than a single machine.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Closing the loop: Serving the fine-tuned model","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now that we have a fine-tuned model, letâs try to serve it. The only change we need to make is to (a) copy the model to s3 from the fine-tuning process and (b) load it from there. In other words, the only change from the previous code we started with originally is: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"202WfToEkr4LafSdvnynbx","type":"Entry","createdAt":"2023-04-07T18:45:09.318Z","updatedAt":"2023-04-07T18:48:20.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"load-model","body":"        checkpoint = Checkpoint.from_uri(\n             \"s3://demo-pretrained-model/gpt-j-6b-shakespeare\"\n        )\n        with checkpoint.as_directory() as dir:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                dir,\n                torch_dtype=torch.float16,\n                low_cpu_mem_usage=True,\n                device_map=\"auto\")\n            self.tokenizer = AutoTokenizer.from_pretrained(dir)\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"And now letâs try querying it again:Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Once upon a time there was a horse. This horse was in my youth, a little unruly, but yet the best of all. I have, sir; I know every horse in the field, and the best that I have known is the dead. And now I thank the gods, and take my leave.","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"As you can see, it definitely has more of a Shakespearean flavor.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Conclusion","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We have shown a new tech stack that combines Ray, HuggingFace, DeepSpeed, and PyTorch to make a system that:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Makes it simple and quick to deploy as a service.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Can be used to cost-effectively fine-tune and is actually most cost-effective when using multiple machines without the complexity.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"How fine-tuning â even a single epoch â can change the output of a trained model.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Deploying a fine-tuned model is only marginally harder than deploying a standard one.Â ","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Next Steps","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If you want to use LangChain + Ray to serve LLM's, see our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"nodeType":"text","value":"LangChain blog series","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in learning more about Ray, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io"},"content":[{"nodeType":"text","value":"Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://docs.ray.io"},"content":[{"nodeType":"text","value":"Docs.Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"To connect with the Ray community join #LLM on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"Ray Slack","marks":[],"data":{}}]},{"nodeType":"text","value":" or our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"Discuss forum","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in our Ray hosted service for ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/model-serving"},"content":[{"nodeType":"text","value":"ML Training and Serving","marks":[],"data":{}}]},{"nodeType":"text","value":", see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://www.anyscale.com/platform"},"content":[{"nodeType":"text","value":"Anyscale.com/Platform","marks":[],"data":{}}]},{"nodeType":"text","value":" and click the 'Try it now' button","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Summit 2023:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"nodeType":"text","value":"Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"nodeType":"text","value":"practical training focused on LLMs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"19zr72hDLSKFt8vQMz3hb6","type":"Asset","createdAt":"2023-04-11T00:38:37.097Z","updatedAt":"2023-04-11T00:38:37.097Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"fine-tune-stack","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/19zr72hDLSKFt8vQMz3hb6/fd9f6b83a9fe5b66456ae54ecf9bb04d/fine-tune-stack.png","details":{"size":344489,"image":{"width":1716,"height":1180}},"fileName":"fine-tune-stack.png","contentType":"image/png"}}}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70ZthWUkgA42DqmZ1GVmuM","type":"Entry","createdAt":"2022-06-15T16:41:59.066Z","updatedAt":"2022-06-15T16:43:47.038Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"blog-types-tags","body":"This section is used to order the \"Types\" and \"Tags\" that show up for filters on the Blog Index","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"56lORqEsxxZXgpuGzAhJBC","type":"Entry","createdAt":"2022-06-15T16:42:23.797Z","updatedAt":"2022-06-15T16:44:24.730Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"list"}},"locale":"en-US"},"fields":{"title":"Types","identifier":"blog-type-filters","items":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42F9iHs5a05rRrOuqEQwvB","type":"Entry","createdAt":"2022-06-15T01:30:47.322Z","updatedAt":"2022-06-15T01:30:47.322Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Culture","slug":"culture","tintColor":"#39C8E5"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KeH0JzrIEpuheazhMAhKf","type":"Entry","createdAt":"2022-06-15T01:31:23.950Z","updatedAt":"2022-06-15T01:31:23.950Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"User Story","slug":"user-story","tintColor":"#6D59C6"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1fDHWgr5HgjPURy6aaDlnB","type":"Entry","createdAt":"2022-06-15T16:42:41.243Z","updatedAt":"2022-06-22T15:37:31.744Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"list"}},"locale":"en-US"},"fields":{"title":"Products / Libraries","identifier":"blog-tag-filters","items":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i6fp6Gfdy5wFeDDOaKcfh","type":"Entry","createdAt":"2021-12-03T22:33:42.248Z","updatedAt":"2021-12-03T22:33:42.248Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Anyscale","identifier":"anyscale"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}}]}}]}}],"recommendations":[],"articles":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7cYVGynV1klxQ4jJ6zu4wh","type":"Entry","createdAt":"2022-05-18T18:49:29.847Z","updatedAt":"2023-03-31T23:44:22.550Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Multi-model composition with Ray Serve deployment graphs","slug":"multi-model-composition-with-ray-serve-deployment-graphs","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3pIMa06o2RZXbtCwhydJM8","type":"Entry","createdAt":"2022-05-18T16:01:41.609Z","updatedAt":"2022-05-18T16:01:41.609Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jiao Dong","slug":"jiao-dong"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"WL3JYnrIlGGwE7Sl0dg9Q","type":"Entry","createdAt":"2022-05-18T18:38:58.432Z","updatedAt":"2022-05-18T18:38:58.432Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Shreyas Krishnaswamy","slug":"shreyas-krishnaswamy"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"kQhxB9LxeWeB3noKuPXiJ","type":"Entry","createdAt":"2020-09-20T16:36:33.304Z","updatedAt":"2020-09-20T16:36:33.304Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Simon Mo","slug":"simon-mo","bio":"Simon Mo is a software engineer at Anyscale. Before Anyscale, he was a student at UC Berkeley participating in research at the RISELab. He focuses on studying and building systems for machine learning, in particular, how to make ML model serving systems more efficient, ergonomic, and scalable. He works on Ray Serve at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"fy7QSQIwnpMZWvM1ndmhx","type":"Asset","createdAt":"2020-09-20T16:36:28.219Z","updatedAt":"2020-09-20T16:36:28.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Simon Mo headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/fy7QSQIwnpMZWvM1ndmhx/cdef7ccbc81d0ea4903ed73336e42b48/Simon-Mo-headshot.png","details":{"size":365363,"image":{"width":550,"height":550}},"fileName":"Simon-Mo-headshot.png","contentType":"image/png"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7cbdj2z67h6KL6mmQH7yNJ","type":"Entry","createdAt":"2021-09-30T22:00:29.490Z","updatedAt":"2021-09-30T22:00:29.490Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Edward Oakes","slug":"edward-oakes","link":"https://www.linkedin.com/in/edward-oakes-8bb3b0a6/"}}],"publishedDate":"2022-05-18","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Learn more about the Ray Serve Deployment Graph API, now in alpha, which allows developers to build scalable and flexible inference serving pipelines as directed acyclic graphs (DAGs) that take advantage of Ray's compute for scaling.","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Machine learning serving pipelines are getting longer, wider, and more dynamic. They often consist of many models to make a single prediction. To support complex workloads that require composing many different models together, machine learning applications in production typically follow patterns such as model chaining, fanout and ensemble, or dynamic selection and dispatch.","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7ecTzCoQoMJnQ8o9CZBMZe","type":"Asset","createdAt":"2022-05-18T16:07:55.153Z","updatedAt":"2022-05-18T18:31:40.706Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"blog-deployment-graph-api-figure-1-model-chaining","description":"Figure 1: Chaining multiple models in sequence. This is common in applications like image processing, where each image is passed through a series of transformations.","file":{"url":"//images.ctfassets.net/xjan103pcp94/7ecTzCoQoMJnQ8o9CZBMZe/078124b513cc9057aa5d05cf5f786865/Figure_1_-_Model_chaining.png","details":{"size":21288,"image":{"width":1499,"height":159}},"fileName":"Figure 1 - Model chaining.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3DJ7vVRxYIvcFO7JmIUMCx","type":"Asset","createdAt":"2022-05-18T16:09:40.404Z","updatedAt":"2022-05-18T18:31:54.617Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"blog-deployment-graph-api-figure-2-fanout-and-ensemble","description":"Figure 2: Ensembling multiple models. Often, business logic is used to select the best output or filter the outputs for a given request.","file":{"url":"//images.ctfassets.net/xjan103pcp94/3DJ7vVRxYIvcFO7JmIUMCx/77a45caa275ffa46f5135f4d6726dd4f/Figure_2_-_Fanout_and_ensemble.png","details":{"size":52587,"image":{"width":1498,"height":615}},"fileName":"Figure 2 - Fanout and ensemble.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1sc6GTqmJi259uVibPsDrY","type":"Asset","createdAt":"2022-05-18T16:10:51.501Z","updatedAt":"2022-05-18T18:32:10.649Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"blog-deployment-graph-api-figure-3-dynamic-selection-and-dispatch","description":"Figure 3: Dynamically selecting and dispatching to models. Many use cases donât require every available model for every input request. This pattern allows only querying the necessary models.","file":{"url":"//images.ctfassets.net/xjan103pcp94/1sc6GTqmJi259uVibPsDrY/0d8a4442488f34dd26cbf1872b59409e/Figure_3_-_Dynamic_selection_and_dispatch.png","details":{"size":62515,"image":{"width":1498,"height":625}},"fileName":"Figure 3 - Dynamic selection and dispatch.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Serve was built to support these patterns and is both easy to develop and production ready. But after listening to Ray Serve users, we've identified ways to improve the process of composing and orchestrating complicated deployment graphs.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Say hello to the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/releases-2.2.0/serve/model_composition.html#deployment-graph-api"},"content":[{"nodeType":"text","value":"Ray Serve Deployment Graph API","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", now in alpha, which allows developers to build scalable and flexible inference serving pipelines as directed acyclic graphs (DAGs) that take advantage of Ray's compute for scaling.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this blog post, we'll explain how the Deployment Graph API enables fast local development to production deployment and is scalable with a unified DAG API across Ray libraries. We'll also walk through a real-world example of how to build, iterate, and deploy a deployment graph with the API.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Strengths of Ray Serve","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Serve has unique strengths suited to multi-step, multi-model inference pipelines: flexible scheduling, efficient communication, fractional resource allocation, and shared memory. The Deployment Graph API enables users to develop complex machine learning pipelines locally and then deploy them to production with dynamic scaling and lightweight updates (e.g., for model weights).","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Compared to other serving frameworks' inference graphs APIs, which are less pythonic and in which authoring is dominated by hand-writing YAML, Ray Serve offers easy, composable, and pythonic APIs, adhering to Ray's philosophy of simple API abstractions.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Deploying a single model","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"With our current ","marks":[],"data":{}},{"nodeType":"text","value":"@serve.deployment","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" decorator, users can easily add the decorator to a class or function to make it a serve ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/serve/key-concepts.html#serve-key-concepts-deployment"},"content":[{"nodeType":"text","value":"Deployment","marks":[{"type":"underline"},{"type":"code"}],"data":{}}]},{"nodeType":"text","value":", and get two exposed channels to invoke it either via HTTP with FastAPI integration, or Python deployment handle.","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1vwyHIklm2AaU1qi8sCcce","type":"Asset","createdAt":"2022-05-18T16:15:05.813Z","updatedAt":"2022-05-18T17:28:51.077Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-deployment-graph-api-figure-4-serve-deployment-API","description":"Figure 4: Serve Deployment API for single deployment","file":{"url":"//images.ctfassets.net/xjan103pcp94/1vwyHIklm2AaU1qi8sCcce/ba69b650707514d281b25e4e522f8bbb/Figure_4_-__serve.deployment_API.png","details":{"size":43700,"image":{"width":1500,"height":500}},"fileName":"Figure 4 - @serve.deployment API.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The Python deployment handle is what people primarily use to compose and orchestrate complicated deployment graphs. While this deployment decorator can take a single Python function or class and convert it into a singular model deployment, there could be business use cases where multiple models and many steps produce the final inference. One solution is to use Python handles to build the inference graph dynamically, which we will explore next.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Building a complex deployment graph based on Python handleÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"After collecting feedback from existing Ray Serve users, we found that composing graph DAGs with the existing Python handle could be improved. For example, consider the following diagram that shows a content understanding pipeline with graph structure and implementation presented side by side.","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"reHS1wi37ikaH9uLmzpZW","type":"Asset","createdAt":"2022-05-18T16:16:55.699Z","updatedAt":"2022-05-18T17:32:33.191Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-deployment-graph-api-figure-5-content-understanding-pipeline","description":"Figure 5: Content understanding pipeline with deployment handle","file":{"url":"//images.ctfassets.net/xjan103pcp94/reHS1wi37ikaH9uLmzpZW/ed75eef15f886587eab985ea8354cba7/Figure_5_-_Content_understanding_pipeline.png","details":{"size":120913,"image":{"width":1500,"height":500}},"fileName":"Figure 5 - Content understanding pipeline.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"On the left, reading from top to bottom:Â ","marks":[],"data":{}}]},{"nodeType":"ordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Send an image to preprocessing node","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Forward to three downstream models in parallelÂ ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Combine outputs of each model","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Send to final node to do post processing","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In the sample code on the right, we define a wrapper deployment that gets deployment handles to a number of other models. Then when a request comes in, we use the handles to broadcast and evaluate each of the models. Finally, we have some post processing for the result.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This implementation does what we set out for, but it has some problems that need to be addressed:","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The graph structure is hidden in the logic of the entire codebase. ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"This means our users need to read the entire codebase and track down each handle in order to see how the graph is composed, without a static definition of the topology. In order to test the graph, users need to manually write a deployment script that deploys each deployment in the correct topological order","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Itâs hard to operationalize the deployment graph for production.Â ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Given the observation above, itâs also difficult to take some operational actions of the deployment graph without diving into the codebase, such as adjusting parameters like ","marks":[],"data":{}},{"nodeType":"text","value":"num_replicas","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":", update link to latest model weights, etc.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It can be hard to optimize the graph. ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"If you look carefully at the code snippet above, it called ","marks":[],"data":{}},{"nodeType":"text","value":"await","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" in a loop as an anti-pattern, since we will wait for each result to return one by one instead of parallelizing them. With a static graph definition, we can avoid these performance bugs and provide advanced support for optimizations such as fusing or co-locating nodes on the same host.","marks":[],"data":{}}]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"Our solution: Ray Serve Deployment Graph API","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We designed and implemented the Deployment Graph API with the following features:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Python native","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":". Instead of writing a YAML file, we make Python the first-class authoring experience with syntax that aligns with current Ray APIs, where chaining, parallelization, ensemble, and dynamic dispatch patterns can be easily expressed with plain Python code.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Fast local development to production deployment","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":". We take local development experience seriously and provide APIs to make iteration as pleasant as possible while paving a clear road from local development to large-scale production deployments.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Independently scalable","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":". We discovered a common pattern where a single node acted as the bottleneck, affecting the entire graphâs latency or throughput, so we ensure each node in the deployment graph is independently scalable by design.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Unified DAG API","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" across the Ray ecosystem. In Ray 2.0, we will have a unified DAG API that is consistent across Ray Core, Ray Serve, Ray Datasets, and Ray Workflows.","marks":[],"data":{}}]}]}]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"Example: Scalable image processing pipeline for content understanding","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"For the rest of this blog, we will use Figure 6 below as the reference architecture, and will show you how to build it step-by-step using the Deployment Graph API with the following properties:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Each ânodeâ in the deployment graph is a Ray Serve deployment whereÂ ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The deployment consists of multiple tasks or actors in a group.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Each task or actor can require fractional CPU or GPU resources.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The deployment spans across multiple hosts in the cluster.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The deployment is independently scalable and configurable with rolling updates.","marks":[],"data":{}}]}]}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"End-to-end graph consists of the patterns we mentioned at the beginning: longer (chaining), wider (parallel fanout with ensemble), and dynamic dispatch. All with simple Python code.","marks":[],"data":{}}]}]}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"37KNeAMsZRPHQ8YFBXYiT6","type":"Asset","createdAt":"2022-05-18T16:20:21.748Z","updatedAt":"2022-05-18T17:39:01.477Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-deployment-graph-api-figure-6-scalable-image-processing-pipeline-for-content-understanding","description":"Figure 6: Scalable image processing pipeline for content understanding","file":{"url":"//images.ctfassets.net/xjan103pcp94/37KNeAMsZRPHQ8YFBXYiT6/beae6f05739925c60ebea9e993427664/Figure_6_-_Scalable_image_processing_pipeline_for_content_understanding.png","details":{"size":147593,"image":{"width":1500,"height":1000}},"fileName":"Figure 6 - Scalable image processing pipeline for content understanding.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now letâs get to the first step from the user input on the far left, which in this case is an image URL, to a deployment that is dedicated to downloading images.","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"Step 1: User input to downloader node","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The code we need to construct a simple user input to downloader connection as shown in Figure 7 below is simple:","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"69CBARrSKVst3vMCbuMGTv","type":"Asset","createdAt":"2022-05-18T16:23:59.449Z","updatedAt":"2022-05-18T17:45:46.121Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"blog-deployment-graph-api-figure-7-step-1","description":"Figure 7: Step 1 - Add user input to downloader","file":{"url":"//images.ctfassets.net/xjan103pcp94/69CBARrSKVst3vMCbuMGTv/9d3924d8b4dc8a10f1a40a7ef9666ead/Figure_7_-_Step_1__Add_user_input_to_downloader.png","details":{"size":34029,"image":{"width":1500,"height":600}},"fileName":"Figure 7 - Step 1_ Add user input to downloader.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KNG2iyEXymaiFNIodASMG","type":"Entry","createdAt":"2022-05-18T16:24:42.372Z","updatedAt":"2022-05-18T16:24:42.372Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Deployment Graph API blog, code example 1","body":"@serve.deployment\nclass Downloader:\n   ...\n   def run(self):\n       pass\n\n# Create a downloader node by binding on its constructor\ndownloader = Downloader.bind() \n\nwith InputNode() as image_url:\n   # Bind image_url to class method \"run\" of downloader node\n   dag = downloader.run.bind(image_url)","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The ","marks":[],"data":{}},{"nodeType":"text","value":"InputNode","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" is a special node in the graph that represents the user input for the graph, which will be resolved to a Python object value at runtime.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":".bind()","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" is our graph building API that can be called on a decorated function, class, or class method. Its syntax is very similar to Rayâs ","marks":[],"data":{}},{"nodeType":"text","value":".remote()","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" except upon each call it creates a node that acts as the building block of the DAG.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this step, weâre using ","marks":[],"data":{}},{"nodeType":"text","value":".bind()","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" to send the user input value created by ","marks":[],"data":{}},{"nodeType":"text","value":"InputNode()","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" to the downloader node.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Note that at deployment graph authoring time, we only need to instantiate and bind the Downloader as if itâs a regular Python class, and use its class method. The user will automatically get what a Ray Serve deployment offers, such as scaling and fine-grained fractional resource allocation with custom configs, as shown in the diagram.","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"Step 2: Extending the chainÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Next, letâs continue building the chain by connecting the ","marks":[],"data":{}},{"nodeType":"text","value":"preprocessor","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" node with previous components. For simplicity, we will skip the class definition of Processor â letâs say it has a class method called âprocessâ. ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3jmAMfNCsszQJDJzjkscx9","type":"Asset","createdAt":"2022-05-18T16:28:11.405Z","updatedAt":"2022-05-18T17:47:54.689Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-deployment-graph-api-figure-8-step-2","description":"Figure 8: Step 2 - Send downloaded image to preprocessor","file":{"url":"//images.ctfassets.net/xjan103pcp94/3jmAMfNCsszQJDJzjkscx9/ef7a32f0e20b69db614d28598068b5d4/Figure_8_-_Step_2__Send_downloaded_image_to_preprocessor__1_.png","details":{"size":50145,"image":{"width":1500,"height":600}},"fileName":"Figure 8 - Step 2_ Send downloaded image to preprocessor (1).png","contentType":"image/png"}}}},"content":[]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"13JvszL7haImClPH1rmdkw","type":"Entry","createdAt":"2022-05-18T16:28:49.959Z","updatedAt":"2022-05-18T16:28:49.959Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Deployment Graph API blog, code example 2","body":"# Create a downloader node by binding on its constructor\ndownloader = Downloader.bind()\n# Create a preprocessor node by binding on its constructor\npreprocessor = Preprocessor.bind()\n\nwith InputNode() as image_url:\n   # Bind image_url to class method \"run\" of downloader node\n   downloaded_image = downloader.run.bind(image_url)\n   # Bind intermediate downloaded image to preprocessors' class method\n   dag = preprocessor.process.bind(downloaded_image)","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We can see extending a new node in the chain is as simple as binding a new instance, and passing the previous stepâs output ","marks":[],"data":{}},{"nodeType":"text","value":"downloaded_image","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" variable to the processorâs class method ","marks":[],"data":{}},{"nodeType":"text","value":"process","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":". The value of ","marks":[],"data":{}},{"nodeType":"text","value":"downloaded_image","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" will be resolved to a Python object at runtime, and fed as input to ","marks":[],"data":{}},{"nodeType":"text","value":"process","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"Step 3: Add parallel callsÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now that we have the processed image, letâs proceed with sending it to two separate downstream nodes in parallel. Note that they can be co-located on the same host where each of them can take fractional GPU, which is supported natively by Ray Serve.","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"748STO6evUPWQzoAG6v708","type":"Asset","createdAt":"2022-05-18T16:30:56.634Z","updatedAt":"2022-05-18T17:49:50.158Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-deployment-graph-api-figure-9-step-3","description":"Figure 9: Step 3 - Send image to downstream models in parallel","file":{"url":"//images.ctfassets.net/xjan103pcp94/748STO6evUPWQzoAG6v708/99889bf4531c42d617b709dc9fb238a0/Figure_9_-_Step_3__Send_image_to_downstream_models_in_parallel.png","details":{"size":68220,"image":{"width":1500,"height":600}},"fileName":"Figure 9 - Step 3_ Send image to downstream models in parallel.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6nyuQRXDqdZbunIcE50955","type":"Entry","createdAt":"2022-05-18T16:31:53.892Z","updatedAt":"2022-05-18T16:31:53.892Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Deployment Graph API blog, code example 3","body":"# Create a downloader node by binding on its constructor\ndownloader = Downloader.bind()\n# Create a preprocessor node by binding on its constructor\npreprocessor = Preprocessor.bind()\n\nsegmentation_model = Segmentation.bind(weights=\"s3://bucket/file\")\nintegrity_model = Integrity.bind(weights=\"s3://bucket/file\")\n\nwith InputNode() as image_url:\n   # Bind image_url to class method \"run\" of downloader node\n   downloaded_image = downloader.run.bind(image_url)\n   # Bind intermediate downloaded image to preprocessors' class method\n   processed_image = preprocessor.process.bind(downloaded_image)\n\n   segmentation_output = segmentation_model.forward.bind(processed_image)\n   integrity_model = integrity_model.forward.bind(processed_image)","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Expressing parallel calls is very trivial: just use the same variable. The same variable name (backed by an IR node behind the scene) will be resolved to the same value in separate nodes.","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"Step 4: Add dynamic dispatchÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now letâs add another edge to the graph where dynamic dispatch happens. It will receive the same image as two other nodes, but at runtime it will dynamically choose only one path, depending on the metadata of the image based on its category.","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"79UmkDZaJN4EifTqXrhYnm","type":"Asset","createdAt":"2022-05-18T16:33:23.821Z","updatedAt":"2022-05-18T17:52:30.705Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"blog-deployment-graph-api-figure-10-step-4","description":"Figure 10: Step 4 - Add dynamic dispatch","file":{"url":"//images.ctfassets.net/xjan103pcp94/79UmkDZaJN4EifTqXrhYnm/3bf55546f77a56438e98eeddc2efb56b/Figure_10_-_Step_4__Add_dynamic_dispatch.png","details":{"size":132805,"image":{"width":1500,"height":1000}},"fileName":"Figure 10 - Step 4_ Add dynamic dispatch.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"625JEQuxJe1mmqb22HyBKw","type":"Entry","createdAt":"2022-05-18T16:33:55.376Z","updatedAt":"2022-05-31T17:42:07.212Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Deployment Graph API blog, code example 4","body":"@serve.deployment\nclass Dispatcher:\n   def __init__(self, fruit_model, car_model, plant_model, general_model):\n       # Upstream nodes can be passed into constructor and used in class\n       # method calls\n       self.fruit_model = fruit_model\n       self.car_model = car_model\n       self.plant_model = plant_model\n       self.general_model = general_model\n\n   async def run(self, image, metadata):\n       # Dynamic dispatch can be simply expressed in python with Ray API calls\n       if metadata == \"fruit\":\n           return await self.fruit_model.remote(image)\n       elif metadata == \"car\":\n           return await self.car_model.remote(image)\n       elif metadata == \"plant\":\n           return await self.plant_model.remote(image)\n       else:\n           return await self.general_model.remote(image)\n\nfruit_model = Fruit_Classification.bind(weights=\"s3://bucket/file\")\ncar_model = Car_Classification.bind(weights=\"s3://bucket/file\")\nplant_model = Plant_Classification.bind(weights=\"s3://bucket/file\")\ngeneral_model = General_Classification.bind(weights=\"s3://bucket/file\")\ndispatcher = Dispatcher.bind(fruit_model, car_model, plant_model, general_model)\n\nwith InputNode() as image_url:\n   # Bind image_url to class method \"run\" of downloader node\n   downloaded_image = downloader.run.bind(image_url)\n   # Bind intermediate downloaded image to preprocessors' class method\n   processed_image = preprocessor.process.bind(downloaded_image)\n\n   ...\n\n   dag = dispatcher.run.bind(processed_image, \"plant\")","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Creating individual model nodes is the same as previous steps. There are a few new pieces regarding building dynamic dispatch that illustrate some properties of our API:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Nodes can be passed into another nodeâs constructor and assigned as an instance variable.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"At runtime, control flow and dynamic dispatch can be easily expressed with plain Python code.","marks":[],"data":{}}]}]}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"Step 5: Add ensemble aggregator","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now weâre at our last step of combining outputs from multiple nodes in an aggregator. We can easily express this as a plain function node in the graph:","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"yq30OG5lKklmA1Z1BX1ev","type":"Asset","createdAt":"2022-05-18T16:35:47.751Z","updatedAt":"2022-05-18T17:41:41.488Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-deployment-graph-api-figure-11-step-5","description":"Figure 11: Step 5 - Add ensemble","file":{"url":"//images.ctfassets.net/xjan103pcp94/yq30OG5lKklmA1Z1BX1ev/74f0b652a2db17a6fa5794264342e3b4/Figure_6_-_Scalable_image_processing_pipeline_for_content_understanding.png","details":{"size":147593,"image":{"width":1500,"height":1000}},"fileName":"Figure 6 - Scalable image processing pipeline for content understanding.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2soiFZwZST24a9EeZEGLL1","type":"Entry","createdAt":"2022-05-18T16:36:14.402Z","updatedAt":"2022-06-02T13:34:51.685Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Deployment Graph API blog, code example 5","body":"@serve.deployment\ndef aggregate_fn(segmentation_output, integrity_output, dispatched_output):\n   return segmentation_output + integrity_output + dispatched_output\n\nwith InputNode() as image_url:\n   # Bind image_url to class method \"run\" of downloader node\n   downloaded_image = downloader.run.bind(image_url)\n   # Bind intermediate downloaded image to preprocessors' class method\n   processed_image = preprocessor.process.bind(downloaded_image)\n\n   segmentation_output = segmentation_model.forward.bind(processed_image)\n   integrity_output = integrity_model.forward.bind(processed_image)\n   dispatched_output = dispatcher.run.bind(processed_image, \"plant\")\n\n   dag = aggregate_fn.bind(\n       segmentation_output, integrity_output, dispatched_output\n   )","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"At this step you can see more attributes of the Deployment Graph API:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"A function can be used as a node in a graph.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We can pass values as well as other nodes as input to the function node.","marks":[],"data":{}}]}]}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"Step 6: Defining HTTP input","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"At this stage weâve built the body of the DAG with Python APIs that facilitate local execution with Ray. The next step is to configure the serving aspect, such as HTTP endpoint, HTTP input adapter, and Python handle.","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"66r9kwQX25IRB7gsJ42cK7","type":"Asset","createdAt":"2022-05-18T16:37:34.886Z","updatedAt":"2022-05-18T16:37:34.886Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-deployment-graph-api-figure-12-step-6","description":"Figure 12: Step 6 - Add DAGDriver deployment for HTTP","file":{"url":"//images.ctfassets.net/xjan103pcp94/66r9kwQX25IRB7gsJ42cK7/91d4554e87ef180241d0d9adbdde508d/blog-deployment-graph-api-figure-12-step-6.png","details":{"size":147426,"image":{"width":1414,"height":816}},"fileName":"blog-deployment-graph-api-figure-12-step-6.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3zjXji4ypc2EOvF3PFtC8m","type":"Entry","createdAt":"2022-05-18T16:38:02.578Z","updatedAt":"2022-05-18T16:38:02.578Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Deployment Graph API blog, code example 6","body":"from ray.serve.drivers import DAGDriver\nfrom ray.serve.http_adapters import json_request\n\nwith InputNode() as image_url:\n   ...\n\n   dag = aggregate_fn.bind(\n       segmentation_output, integrity_output, dispatched_output\n   )\n\n   serve_dag = DAGDriver  \\\n       .options(num_replicas=3, route_prefix=\"/my_graph\")  \\\n       .bind(dag, http_adapter=json_request)","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"There are two new concepts for this:","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"DAGDriver","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": The ingress component of your deployment graph that holds the graph as a DAG Python object. Itâs a regular Ray Serve deployment object that can be configured and scaled independently.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"http_adapter","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": Facilitates conversion of HTTP raw input into a Python object schema.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"DAG Driver is a regular Ray Serve deployment that can be independently scaled and configured with custom HTTP options. It exposes the same entries like current deployment: HTTP and Python handle, with a few built-in components to facilitate HTTP request -\u003e Python object adaption. You can learn more about it in our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/serve/http-guide.html#http-adapters"},"content":[{"nodeType":"text","value":"documentation","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Also check out our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/2022_04_13_ray_serve_meetup_demo"},"content":[{"nodeType":"text","value":"working demo of building a content understanding graph using our API","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"Step 7: Running the deployment graph","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1IcDgG6dTf81lpa45t3gxS","type":"Asset","createdAt":"2022-05-18T16:39:48.765Z","updatedAt":"2022-05-18T17:55:26.501Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"blog-deployment-graph-api-figure-13-step-7","description":"Figure 13: Step 7 - Deployment Graph iteration and deployment cycle","file":{"url":"//images.ctfassets.net/xjan103pcp94/1IcDgG6dTf81lpa45t3gxS/c762b195f774fb308d1343247291e870/Figure_13_-_Step_7__Deployment_Graph_iteration_and_deployment_cycle.png","details":{"size":55402,"image":{"width":1500,"height":500}},"fileName":"Figure 13 - Step 7_ Deployment Graph iteration and deployment cycle.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This flow chart shows our designed local development and production deployment flow of a deployment graph, from the Python incremental build to iteration of the body of your DAG to adding Ray Serve attributes such as HTTP endpoint and adapter.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The green blocks and ","marks":[],"data":{}},{"nodeType":"text","value":"serve run","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" in Figure 13 are available in Ray 1.12.1, and the purple blocks such as ","marks":[],"data":{}},{"nodeType":"text","value":"serve build","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"text","value":"serve deploy","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" are in active development.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"You might notice we only covered the green blocks in the diagram from the authoring and development side. We have more work in progress that further enhances the operational aspects of deployment graphs such as REST API/CLI to build your deployment graph into YAML files to facilitate declarative and idempotent operations. Stay tuned for future blog posts!","marks":[],"data":{}}]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"Future improvementsÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We plan to continue to improve the Deployment Graph API in future releases. Current items on our roadmap include:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Operationalizing deployment graphs to pave the way from local development to remote clusters in production","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Performance optimizations based on given static graph topology","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"ModelMesh layer on Ray","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Model multiplexing on the same host","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Scale-to-zero with lazily invoked models","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Better UX and visualization support","marks":[],"data":{}}]}]}]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"Bonus: Unified Ray DAG API","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"DAG is a very common abstraction that we see across many computation tasks. In our design of the DAG API, we put in a lot of effort to ensure it has the right layering under one set of unified APIs for authoring and execution, but also exposes the right abstractions for library-specific executor and options.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/document/d/16wRobgsfCvOXnsaQq9lwQMDnnFWzXaYicIQ6e65W5W0/edit#heading=h.e2gody4jxurd"},"content":[{"nodeType":"text","value":"You can learn more about this in our Ray 2.0 design doc","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2zF6wKySt8fcVIsySlun4O","type":"Asset","createdAt":"2022-05-18T16:41:58.433Z","updatedAt":"2022-05-18T17:56:22.953Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-deployment-graph-api-figure-14-unified-ray-dag-api","description":"Figure 14: Unified Ray DAG API","file":{"url":"//images.ctfassets.net/xjan103pcp94/2zF6wKySt8fcVIsySlun4O/9d487f027a1f117b53d6e67da80a69f4/Figure_14_-_Unified_Ray_DAG_API.png","details":{"size":52900,"image":{"width":1500,"height":500}},"fileName":"Figure 14 - Unified Ray DAG API.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"ConclusionÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Weâve reviewed how our users currently use deployment handles to compose complex deployment graphs and challenges weâve observed for this increasingly important workload: no static graph definition, which leads to challenges of operationalizing deployment graphs without codebase context, and the missed opportunity to optimize performance of deployment graphs.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We then showed how the new Deployment Graph API in Ray Serve addresses these challenges and walked through a real-world example of how to build, iterate, and deploy a deployment graph with the API. The Ray Serve Deployment Graph API is","marks":[],"data":{}},{"nodeType":"text","value":" Python-native","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":", enables ","marks":[],"data":{}},{"nodeType":"text","value":"fast local development to production deployment","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":", and is ","marks":[],"data":{}},{"nodeType":"text","value":"scalable","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" with a ","marks":[],"data":{}},{"nodeType":"text","value":"unified DAG API ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"across Ray libraries.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We value input from the Ray community!Â  If youâre ready to get started with Ray Serve or the Deployment Graph API, check out the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/index.html"},"content":[{"nodeType":"text","value":"detailed tutorial in our documentation","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". If you have suggestions on how the Deployment Graph API can work better for your use case, head over to our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/c/ray-serve/6"},"content":[{"nodeType":"text","value":"user forums","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" or file an issue or feature request on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray"},"content":[{"nodeType":"text","value":"GitHub","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". We canât wait to partner with you to adopt Ray Serve in your project. ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/1l8HT35jXMPtxVUtQPeGoe09VGp5jcvSv0TqPgyz6lGU"},"content":[{"nodeType":"text","value":"Letâs get in touch","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" as well!","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If youâre interested in our mission to simplify distributed computing, ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://jobs.lever.co/anyscale"},"content":[{"nodeType":"text","value":"weâre actively hiring across the company","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"Resources","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/events/2022/04/14/productionizing-ml-at-scale-with-ray-serve"},"content":[{"nodeType":"text","value":"Ray Meetup: Productionizing ML at scale with Ray Serve","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/serve/model_composition.html#deployment-graph-api"},"content":[{"nodeType":"text","value":"Ray docs: Deployment Graph","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/serve/http-guide.html#http-adapters"},"content":[{"nodeType":"text","value":"Ray docs: HTTP adapters","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7MpB03sWhcnSgH7vjuC7lC","type":"Asset","createdAt":"2022-05-18T18:47:18.853Z","updatedAt":"2022-05-18T18:47:18.853Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-deployment-graph-api-thumb","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7MpB03sWhcnSgH7vjuC7lC/29e37382b2aaa3162a53a2892200841e/blog-deployment-graph-api-thumb.png","details":{"size":1365909,"image":{"width":1500,"height":1000}},"fileName":"blog-deployment-graph-api-thumb.png","contentType":"image/png"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Zk7jixFrLqHf3IJLzrO2B","type":"Entry","createdAt":"2022-05-03T18:47:31.422Z","updatedAt":"2022-05-03T18:47:31.422Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Ray Summit: Aug 23-24 in SF!","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5LTWfPAScfUtzXj2KOUALz","type":"Entry","createdAt":"2022-04-12T15:29:42.674Z","updatedAt":"2022-10-04T22:06:07.976Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":17,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"summit"}},"locale":"en-US"},"fields":{"title":"Ray Summit 2022","slug":"ray-summit-2022","description":"Centered on Ray, the open-source distributed computing framework, Ray Summit brings together ML practitioners, data scientists, and more to discuss the growing demand for distributed computing.","sections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xIJZXdAqhOEizUCUuQaiV","type":"Entry","createdAt":"2022-03-31T16:04:49.746Z","updatedAt":"2022-09-12T20:58:36.991Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":20,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-hero","header":"The Place for _Everything Ray_","body":"Missed us in San Francisco for Ray Summit 2022? Don't fret, you can watch your favorite sessions on demand! ","ctaText":"View Sessions","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"9F9S7NLaIdPL4ThYwVlFW","type":"Asset","createdAt":"2022-09-06T14:49:38.936Z","updatedAt":"2022-09-06T14:53:02.350Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Presented by Anyscale","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/9F9S7NLaIdPL4ThYwVlFW/8ecd09b334942298f0b8c2d5567ec032/presented-by-anyscale.svg","details":{"size":22813,"image":{"width":289,"height":32}},"fileName":"presented-by-anyscale.svg","contentType":"image/svg+xml"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4A0d4TUfp0TCqUA60CXopR","type":"Asset","createdAt":"2022-09-06T14:53:48.720Z","updatedAt":"2022-09-06T14:53:48.720Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4A0d4TUfp0TCqUA60CXopR/2e4240c268d074d840f7a00d069a768a/rs-collage_2x.jpg","details":{"size":568547,"image":{"width":2400,"height":1376}},"fileName":"rs-collage@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6FGUP0cXunTbgs2UemeAU","type":"Asset","createdAt":"2022-09-06T14:54:09.975Z","updatedAt":"2022-09-06T14:54:09.975Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event, stacked vertically","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6FGUP0cXunTbgs2UemeAU/16ed175024bc9cc0a2fae3cb13538dff/rs-collage-mobile_2x.jpg","details":{"size":243282,"image":{"width":750,"height":1420}},"fileName":"rs-collage-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4RKa7qVhWoIaoNSa1ZH2SK"}}],"type":"2022 Summit Hero","styling":["transparent-header"]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6GL7ZzuSLadWdVFKHj7yIH","type":"Entry","createdAt":"2022-04-20T18:00:12.964Z","updatedAt":"2022-08-08T22:50:29.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-keynote-speakers","header":"Keynote Speakers","ctaText":"See all speakers","ctaLink":"/ray-summit-2022/speakers","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6VhewtcICU9SCXjwd5qhHJ"}}],"type":"2022 Summit Speakers","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JTi5fsyo0Q3YrF3SAQglg","type":"Entry","createdAt":"2022-08-08T22:49:55.163Z","updatedAt":"2022-08-09T07:03:38.615Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-ray-summit-selected-speakers","header":"Highlighted Talks","ctaText":"Full Agenda","ctaLink":"/ray-summit-2022/agenda","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"50eS4cKXBNE4GHezdZ8Rws"}},{"sys":{"type":"Link","linkType":"Entry","id":"5iioKfOgEqVzgQ1DMmA1Wy"}},{"sys":{"type":"Link","linkType":"Entry","id":"YCBUZOXJGB0qcJF8TKjfI"}}],"type":"2022 Summit Agenda Preview"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5cgHuoJ713DewhVY6iUYrG","type":"Entry","createdAt":"2022-05-03T21:57:45.147Z","updatedAt":"2022-05-09T13:41:03.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-trainings","header":"Exclusive Ray Training Sessions","body":"Ray Summit is a great opportunity to level up your Ray skills through hands-on training sessions with the builders and maintainers of Ray. Weâre offering several pre-conference training sessions on August 22. Space is limited â [reserve your spot now](https://www.myeventi.events/raysummit22/attendee/)!","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3sYQTUmD5X5ncvGRFyIRR6","type":"Asset","createdAt":"2022-05-03T04:40:25.040Z","updatedAt":"2022-05-03T04:40:25.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image desktop","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3sYQTUmD5X5ncvGRFyIRR6/a77c675b07d63e73dd79ada9d182498a/training-image_2x.jpg","details":{"size":313509,"image":{"width":1136,"height":1480}},"fileName":"training-image@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7rsC56a4YCgENLyxsHyeIb","type":"Asset","createdAt":"2022-05-03T04:40:54.089Z","updatedAt":"2022-05-03T04:40:54.089Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image mobile","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7rsC56a4YCgENLyxsHyeIb/e2592c9b4424048b3c916fbbbc313065/training-image-mobile_2x.jpg","details":{"size":153801,"image":{"width":830,"height":874}},"fileName":"training-image-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4Hb3J9SrCFTDXqIhIdSIwa"}},{"sys":{"type":"Link","linkType":"Entry","id":"hZ4Tl1wvj7CwF00slwpLg"}},{"sys":{"type":"Link","linkType":"Entry","id":"YJ5KWyjg4NGf29NrfHcJX"}}],"type":"2022 Summit Trainings"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mRJQtuDbIhAuEbwLT2V1K","type":"Entry","createdAt":"2022-03-31T21:42:56.117Z","updatedAt":"2022-04-11T23:46:07.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-reasons-to-attend","header":"Why Attend?","body":"As the annual user conference for the [open source Ray community](https://www.ray.io/community), Ray Summit is a unique opportunity to enhance your Ray skills, build your professional network, and learn what's on the Ray roadmap.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"5f6gxk4a4ujE2y6FZ8ATQ5"}},{"sys":{"type":"Link","linkType":"Entry","id":"20QwyiOmkHCEOkMvp0Jhtk"}},{"sys":{"type":"Link","linkType":"Entry","id":"6QxivsuhhLoVSloRQs9ZlQ"}}],"type":"2022 Summit Content Tiles"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"15k0SGqB2ZKOUfAsM3JnyV","type":"Entry","createdAt":"2022-03-31T16:05:55.739Z","updatedAt":"2022-08-05T00:25:20.225Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":16,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-about","header":"Celebrate the Ray community","body":"Get ready for our epic community bash at the Art of Tinkering exhibit at the San Francisco Exploratorium on August 23 from 5:30 to 9:30 p.m. Come explore, play, and celebrate with the Ray Community. This is an exclusive opportunity to experience this amazing interactive exhibition before it closes in September.","ctaText":"Check out the agenda","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UcZT4h1P7uFDz6cn6JH8l","type":"Asset","createdAt":"2022-06-02T20:37:22.299Z","updatedAt":"2022-06-02T20:37:22.299Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Man taking a photo on his phone","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2UcZT4h1P7uFDz6cn6JH8l/5040a9909f18ab015c27b220077f916e/taking-picture.jpg","details":{"size":107846,"image":{"width":980,"height":772}},"fileName":"taking-picture.jpg","contentType":"image/jpeg"}}}],"type":"2022 Summit Split Image","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4P8Jn0q7nBU05iD6DmcQCC","type":"Entry","createdAt":"2022-04-01T19:15:31.735Z","updatedAt":"2022-04-11T16:36:59.573Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-social-tiles","header":"Join the Conversation","body":"Ready to get involved in the Ray community before the conference? Ask a question in the forums. Open a pull request. Or share why youâre excited with the hashtag __#RaySummit__ on Twitter.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6OMTlXkJ1OHXNkEjyPrXTH"}},{"sys":{"type":"Link","linkType":"Entry","id":"7EE28ee1ATQtodoLloT6mg"}},{"sys":{"type":"Link","linkType":"Entry","id":"1CRAzFxndL37wiNg4x8WKV"}},{"sys":{"type":"Link","linkType":"Entry","id":"2qrec5rR8kabZxCHaPS7NH"}}],"type":"2022 Summit Social Tiles"}}],"headerLogo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70DIdxwT8XY96aw7vMUHOu","type":"Asset","createdAt":"2022-06-01T15:21:46.804Z","updatedAt":"2022-06-01T15:21:46.804Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo Dark Blue","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/70DIdxwT8XY96aw7vMUHOu/5ca0a8b7533c8df70bc0e032b889862a/rs-logo-nav.svg","details":{"size":9954,"image":{"width":108,"height":48}},"fileName":"rs-logo-nav.svg","contentType":"image/svg+xml"}}},"headerLogoWhite":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3qyLXKGgGfWhv4oPqYl0SE","type":"Asset","createdAt":"2022-04-20T21:50:58.774Z","updatedAt":"2022-04-20T21:50:58.774Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3qyLXKGgGfWhv4oPqYl0SE/c4dee9e7a344c39db11fefafbf255b5b/rs-logo-horizontal.svg","details":{"size":9975,"image":{"width":108,"height":48}},"fileName":"rs-logo-horizontal.svg","contentType":"image/svg+xml"}}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KUtRQbdqJo1RrWMrVyfEm","type":"Entry","createdAt":"2022-03-31T16:21:13.402Z","updatedAt":"2022-04-08T17:25:33.228Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Home","link":"/ray-summit-2022"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4MvQcRSkHLMqIJoO9O2cQe","type":"Entry","createdAt":"2022-03-31T16:17:27.336Z","updatedAt":"2022-09-12T16:59:51.221Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"On-Demand","link":"/ray-summit-2022/agenda","subLinks":[{"sys":{"type":"Link","linkType":"Entry","id":"3N9kRDz36tieTI4YGVWKBJ"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46B5diwEDDr3kB2XDoGfDP","type":"Entry","createdAt":"2022-05-11T17:13:42.174Z","updatedAt":"2022-05-11T17:13:42.174Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Speakers","link":"/ray-summit-2022/speakers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Dt5fy6A7rBpXdG7HeKMGd","type":"Entry","createdAt":"2022-03-31T16:17:43.592Z","updatedAt":"2022-03-31T16:17:43.592Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Sponsors","link":"/ray-summit-2022/sponsors"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SvRE9pnyq3BXUPGKRWx6W","type":"Entry","createdAt":"2022-06-03T19:29:31.009Z","updatedAt":"2022-06-03T19:29:31.009Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Who Attends","link":"/ray-summit-2022/who-attends"}}],"mainSponsors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"xMQIOU9n7gVS4mkX2Nlag","type":"Entry","createdAt":"2021-01-31T16:33:06.495Z","updatedAt":"2021-05-11T13:28:31.312Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Ray","url":"https://ray.io","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jLroqm98RCMLGASthyuPv","type":"Asset","createdAt":"2021-05-11T13:28:26.930Z","updatedAt":"2021-05-11T13:28:26.930Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/jLroqm98RCMLGASthyuPv/f76cc04827316de5d3f8b529f6a3f02c/Ray_BM.svg","details":{"size":4742,"image":{"width":93,"height":36}},"fileName":"Ray_BM.svg","contentType":"image/svg+xml"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11syIHhtX8JajSlXHSHE1O","type":"Entry","createdAt":"2021-01-31T16:31:24.509Z","updatedAt":"2021-05-11T13:28:00.309Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Anyscale","url":"https://www.anyscale.com","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UkDmTzmZHIOG7yxFP4sVL","type":"Asset","createdAt":"2021-05-11T13:27:55.219Z","updatedAt":"2021-05-11T13:27:55.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Anyscale","file":{"url":"//images.ctfassets.net/xjan103pcp94/5UkDmTzmZHIOG7yxFP4sVL/db10aebda9a9aad10903470781a6f2e6/ANSC_BM.svg","details":{"size":7431,"image":{"width":158,"height":36}},"fileName":"ANSC_BM.svg","contentType":"image/svg+xml"}}}}}],"footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cfwECWrOqJ5Fegr3YpReS","type":"Entry","createdAt":"2022-04-01T21:22:12.001Z","updatedAt":"2022-04-13T17:35:29.511Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-footer","body":"Â©Anyscale, Inc 2022\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/privacy-policy\"\u003ePrivacy Policy\u003c/a\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/code-of-conduct\"\u003eCode of Conduct\u003c/a\u003e"}},"recommendations":[]}},"ctaText":"Register now","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5nTlh9josVs5zpZqU3vTbn","type":"Asset","createdAt":"2022-05-03T18:47:25.125Z","updatedAt":"2022-05-03T18:47:25.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-ray-summit-2022","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5nTlh9josVs5zpZqU3vTbn/e66043f62ed4a4b53d7e182e50289ec3/blog-recommended-content-ray-summit-2022.png","details":{"size":17234,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-ray-summit-2022.png","contentType":"image/png"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"79MO0POTrTySavkSOhGbjT","type":"Entry","createdAt":"2022-03-24T22:49:50.185Z","updatedAt":"2022-03-24T22:56:17.734Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Serving ML models in production: Common patterns","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2HvnSZK5ssuHHocJvhm04l","type":"Entry","createdAt":"2021-10-01T16:29:03.786Z","updatedAt":"2022-06-22T16:21:03.872Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Serving ML Models in Production: Common Patterns","slug":"serving-ml-models-in-production-common-patterns","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"kQhxB9LxeWeB3noKuPXiJ","type":"Entry","createdAt":"2020-09-20T16:36:33.304Z","updatedAt":"2020-09-20T16:36:33.304Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Simon Mo","slug":"simon-mo","bio":"Simon Mo is a software engineer at Anyscale. Before Anyscale, he was a student at UC Berkeley participating in research at the RISELab. He focuses on studying and building systems for machine learning, in particular, how to make ML model serving systems more efficient, ergonomic, and scalable. He works on Ray Serve at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"fy7QSQIwnpMZWvM1ndmhx","type":"Asset","createdAt":"2020-09-20T16:36:28.219Z","updatedAt":"2020-09-20T16:36:28.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Simon Mo headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/fy7QSQIwnpMZWvM1ndmhx/cdef7ccbc81d0ea4903ed73336e42b48/Simon-Mo-headshot.png","details":{"size":365363,"image":{"width":550,"height":550}},"fileName":"Simon-Mo-headshot.png","contentType":"image/png"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7cbdj2z67h6KL6mmQH7yNJ","type":"Entry","createdAt":"2021-09-30T22:00:29.490Z","updatedAt":"2021-09-30T22:00:29.490Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Edward Oakes","slug":"edward-oakes","link":"https://www.linkedin.com/in/edward-oakes-8bb3b0a6/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-10-01","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Over the past couple years, we've listened to ML practitioners across many different industries to learn and improve the tooling around ML production use cases. Through this, we've seen 4 common patterns of machine learning in production: pipeline, ensemble, business logic, and online learning. In the ML serving space, implementing these patterns typically involves a tradeoff between ease of development and production readiness. Ray Serve was built to support these patterns by being both easy to develop and production ready.","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4xzqeZGkvoF6cBdEauda1I","type":"Entry","createdAt":"2021-09-30T22:02:40.713Z","updatedAt":"2021-09-30T22:02:40.713Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Serving ML Models in Production: Common Patterns","videoUrl":" https://www.youtube.com/watch?v=mM4hJLelzSw","caption":"This post is based on Simon Moâs âPatterns of Machine Learning in Productionâ [talk](https://www.youtube.com/watch?v=mM4hJLelzSw \"Serving ML Models in Production: Common Patterns\") from Ray Summit 2021. "}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Over the past couple years, we've listened to ML practitioners across many different industries to learn and improve the tooling around ML production use cases. Through this, we've seen 4 common patterns of machine learning in production: pipeline, ensemble, business logic, and online learning. In the ML serving space, implementing these patterns typically involves a tradeoff between ease of development and production readiness. ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" was built to support these patterns by being both easy to develop and production ready. It is a scalable and programmable serving framework built on top of ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to help you scale your microservices and ML models in production.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This post goes over:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"What is Ray Serve","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Where Ray Serve fits in the ML Serving Space","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Some common patterns of ML in production ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"How to implement these patterns using Ray Serve","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What is Ray Serve?\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1mw8vMERbD818wGhan6npt","type":"Asset","createdAt":"2021-09-30T22:13:54.942Z","updatedAt":"2021-09-30T23:58:30.276Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"Ray Ecosystem Serving ML Models in Production: Common Patterns","description":"Ray Serve is built on top of the Ray distributed computing platform, allowing it to easily scale to many machines, both in your datacenter and in the cloud. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/1mw8vMERbD818wGhan6npt/36b4a626338057b92520f43d85547154/RayEcosystem.png","details":{"size":494592,"image":{"width":2162,"height":918}},"fileName":"RayEcosystem.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve is an easy-to-use scalable model serving library built on Ray. Some advantages of the library include:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scalability: Horizontally scale across hundreds of processes or machines, while keeping the overhead in single-digit milliseconds","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Multi-model composition: Easily compose multiple models, mix model serving with business logic, and independently scale components, without complex microservices.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/batch.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Batching","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": Native support for batching requests to better utilize hardware and improve throughput.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/http-servehandle.html#serve-fastapi-http"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FastAPI Integration","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":":Â Scale an existing FastAPI server easily or define an HTTP interface for your model using its simple, elegant API.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Framework-agnostic: Use a single toolkit to serve everything from deep learning models built with frameworks like ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/serve/tutorials/pytorch.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Tensorflow and Keras","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", to ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/sklearn.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Scikit-Learn models","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-ikigai-labs-serves-interactive-ai-workflows-at-scale-using-ray-serve"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"arbitrary Python business logic","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can get started with Ray Serve by checking out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve Quickstart.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Where Ray Serve fits in the ML Serving Space","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6IcTIir1U1WBJdSbdygQ08","type":"Asset","createdAt":"2021-09-30T22:18:13.805Z","updatedAt":"2021-09-30T22:18:13.805Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"WhereRayServeFitsIn","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6IcTIir1U1WBJdSbdygQ08/70ceeb0e4f5c8b72b7007c61cb19eed8/WhereRayServeFitsIn.png","details":{"size":92883,"image":{"width":768,"height":444}},"fileName":"WhereRayServeFitsIn.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The image above shows that In the ML serving space, there is typically a tradeoff between ease of development and production readiness.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Web Frameworks","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To deploy a ML service, people typically start with the simplest systems out of the box like Flask or FastAPI. However, even though they can deliver a single prediction well and work well in proofs of concept, they cannot achieve high performance and scaling up is often costly.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Custom Tooling","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If web frameworks fail, teams typically transition to some sort of custom tooling by gluing together several tools to make the system ready for production. However, these custom toolings are typically hard to develop, deploy, and manage.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Specialized Systems","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There is a group of specialized systems for deploying and managing ML models in production. While these systems are great at managing and serving ML models, they often have less flexibility than web frameworks and often have a high learning curve.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve is a web framework specialized for ML model serving. It aspires to be easy to use, easy to deploy, and production ready.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What makes Ray Serve Different?","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"fZRYjZC4BoGvXCytyN3CQ","type":"Asset","createdAt":"2021-09-30T22:25:08.320Z","updatedAt":"2021-09-30T22:25:08.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Many Tools Run 1 Model Well","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/fZRYjZC4BoGvXCytyN3CQ/6e8fc978f76159db3638e3f98b30b0bf/ManyToolsRun1ModelWell.png","details":{"size":131094,"image":{"width":1004,"height":416}},"fileName":"ManyToolsRun1ModelWell.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are so many tools for training and serving one model. These tools help you run and deploy one model very well. The problem is that machine learning in real life is usually not that simple. In a production setting, you can encounter problems like:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Wrangling with infrastructure to scale beyond one copy of a model.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Having to work through complex YAML configuration files, learn custom tooling, and develop MLOps expertise.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Hit scalability or performance issues, unable to deliver business SLA objectives.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Many tools are very costly and can often lead to underutilization of resources. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling out a single model is hard enough. For many ML in production use cases, we observed that complex workloads require composing many different models together. Ray Serve is natively built for this kind of use case involving many models spanning multiple nodes. You can check out ","nodeType":"text"},{"data":{"uri":"https://youtu.be/mM4hJLelzSw?t=651"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"this part of the talk","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" where we go in depth about Ray Serveâs architectural components.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Patterns of ML Models in Production ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4RxLx15blgVAW8gQwQx8w9","type":"Asset","createdAt":"2021-09-30T22:27:41.224Z","updatedAt":"2021-09-30T22:27:41.224Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"13PatternsMLProduction","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4RxLx15blgVAW8gQwQx8w9/831258e5b2d865cba9bba4c00518768a/13PatternsMLProduction.png","details":{"size":64775,"image":{"width":882,"height":426}},"fileName":"13PatternsMLProduction.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A significant portion of ML applications in production follow 4 model patterns:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"pipeline","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"ensemble","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"business logic","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"online learningÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"This section will describe each of these patterns, show how they are used, go over how existing tools typically implement them, and show how Ray Serve can solve these challenges.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pipeline Pattern\n","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4JZHXITWwYcUhGidslJ14A","type":"Asset","createdAt":"2021-09-30T22:30:20.128Z","updatedAt":"2021-09-30T22:30:20.128Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ATypicalComputerVisionPipeline","description":"A typical computer vision pipeline","file":{"url":"//images.ctfassets.net/xjan103pcp94/4JZHXITWwYcUhGidslJ14A/a4bf91ad39086863ee3f31bbe2f9e038/ATypicalComputerVisionPipeline.png","details":{"size":451936,"image":{"width":1476,"height":452}},"fileName":"ATypicalComputerVisionPipeline.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The image above shows a typical computer vision pipeline that uses multiple deep learning models to caption the object in the picture. This pipeline consists of the following steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"1) The raw image goes through common preprocessing like image decoding, augmentation and clipping.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"2) A detection classifier model is used to identify the bounding box and the category. It's a cat.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"3) The image is passed into a keypoint detection model to identify the posture of the object. For the cat image, the model could identify key points like paws, neck, and head.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"4) Lastly, an NLP synthesis model generates a category of what the picture shows. In this case, a standing cat.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical pipeline rarely consists of just one model. To tackle real-life issues, ML applications often use many different models to perform even simple tasks. In general, pipelines break a specific task into many steps, where each step is conquered by a machine learning algorithm or some procedure. Letâs now go over a couple pipelines you might already be familiar with. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Scikit-Learn Pipeline","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"Pipeline([(âscalerâ, StandardScaler()), (âsvcâ, SVC())])","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"scikit-learnâs pipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" can be used to combine multiple âmodelsâ and âprocessing objectsâ together. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Recommendation Systems","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"[EmbeddingLookup(), FeatureInteraction(), NearestNeighbors(), Ranking()] ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are common pipeline patterns in recommendation systems. Item and video recommendations like those that you might see at ","nodeType":"text"},{"data":{"uri":"https://www.amazon.science/publications/two-decades-of-recommender-systems-at-amazon-com"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Amazon","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://research.google/pubs/pub45530/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"YouTube","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", respectively,Â  typically go through multiple stages like embedding lookup, feature interaction, nearest neighbor models, and ranking models.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Common Preprocessing","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"[HeavyWeightMLMegaModel(), DecisionTree()/BoostingModel()]","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are some very common use cases where some massive ML models are used to take care of common processing for text or images. For example, at Facebook, groups of ML researchers at ","nodeType":"text"},{"data":{"uri":"https://ai.facebook.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FAIR","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" create state of the art heavyweight models for vision and text. Then different product groups create downstream models to tackle their business use case (e.g. ","nodeType":"text"},{"data":{"uri":"https://ai.facebook.com/blog/under-the-hood-suicide-prevention-tools-powered-by-ai/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"suicide prevention","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") by implementing smaller models using random forest. The shared common preprocessing step oftentimes are materialized into a ","nodeType":"text"},{"data":{"uri":"https://www.tecton.ai/blog/what-is-a-feature-store/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"feature store pipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"General Pipeline Implementation Options","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2h69wovKC65iUOj3CSs7cu","type":"Asset","createdAt":"2021-09-30T22:35:27.708Z","updatedAt":"2021-09-30T22:35:27.708Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"17PipelineImplementation","description":"Before Ray Serve, implementing pipelines generally meant you had to choose between wrapping your models in a web server or using many specialized microservices. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/2h69wovKC65iUOj3CSs7cu/8462545f5593c27d61c7151fd069a56b/17PipelineImplementation.png","details":{"size":106807,"image":{"width":840,"height":414}},"fileName":"17PipelineImplementation.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In general, there are two approaches to implement a pipeline: wrap your models in a web server or use many specialized microservices.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Wrap Models in a Web Server","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The left side of the image above shows models that get run in a for loop during the web handling path. Whenever a request comes in, models get loaded (they can also be cached) and run through the pipeline. While this is simple and easy to implement, a major flaw is that this is hard to scale and not performant because each request gets handled sequentially.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Many Specialized Microservices","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The right side of the image above shows many specialized microservices where you essentially build and deploy one microservice per model. These microservices can be native ML platforms, ","nodeType":"text"},{"data":{"uri":"https://www.kubeflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Kubeflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", or even hosted services like AWS ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/sagemaker/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". However, as the number of models grow, the complexity and operational cost drastically increases. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Implementing Pipelines in Ray Serve","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11KQrEzA1s2n4S3SUKuH28","type":"Entry","createdAt":"2021-09-30T22:37:57.811Z","updatedAt":"2021-10-01T00:09:45.131Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"implementingPipelinesInRayServe","body":"@serve.deployment\nclass Featurizer: â¦\n\n@serve.deployment\nclass Predictor: â¦\n\n@serve.deployment\nclass Orchestrator\n   def __init__(self):\n      self.featurizer = Featurizer.get_handle()\n      self.predictor = Predictor.get_handle()\n\n   async def __call__(self, inp):\n      feat = await self.featurizer.remote(inp)\n      predicted = await self.predictor.remote(feat)\n      return predicted\n\nif __name__ == â__main__â:\n    Featurizer.deploy()\n    Predictor.deploy()\n    Orchestrator.deploy()","language":"python","caption":"Pseudocode showing how Ray Serve allows deployments to call other deployments"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In Ray Serve, you can directly call other deployments within your deployment.Â  In this code above, there are three deployments. ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Featurizer","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Predictor","nodeType":"text"},{"data":{},"marks":[],"value":" are just regular deployments containing the models. The ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Orchestrator","nodeType":"text"},{"data":{},"marks":[],"value":" receives the web input, passes it to the featurizer process via the featurizer handle, and then passes the computed feature to the predictor process. The interface is just Python and you donât need to learn any new framework or domain-specific language.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve achieves this with a mechanism called ServeHandle which gives you a similar flexibility to embed everything in the web server, without sacrificing performance or scalability. It allows you to directly call other deployments that live in other processes on other nodes. This allows you to scale out each deployment individually and load balance calls across the replicas.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you would like to get a deeper understanding of how this works, ","nodeType":"text"},{"data":{"uri":"https://youtu.be/mM4hJLelzSw?t=650"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"check out this section of Simon Moâs talk","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to learn about Ray Serveâs architecture. If you would like an example of a computer vision pipeline in production, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures#:~:text=A%20Tool%20for%20High%20Performance%20ML%20Systems"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"check out how Robovision used 5 ML models for vehicle detection","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ensemble Pattern","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3XfSBLbvH4TgteNQhSV3jC","type":"Asset","createdAt":"2021-09-30T22:39:02.077Z","updatedAt":"2021-09-30T22:39:02.077Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ensemble Pattern","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3XfSBLbvH4TgteNQhSV3jC/9754f37495ab90cbf5943494e2e13e7a/25Ensemble.png","details":{"size":32706,"image":{"width":730,"height":352}},"fileName":"25Ensemble.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a lot of production use cases, a pipeline is appropriate. However, one limitation of pipelines is that there can often be many upstream models for a given downstream model. This is where ensembles are useful. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"41I4eZnNsSRrneS0QO59Ss","type":"Asset","createdAt":"2021-09-30T22:41:17.502Z","updatedAt":"2021-09-30T22:41:17.502Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Serve Ensemble Usecase","description":"Ensemble Use Cases","file":{"url":"//images.ctfassets.net/xjan103pcp94/41I4eZnNsSRrneS0QO59Ss/aa86f27d79ef6a431743f8a8349a0343/26RayServeEnsembleUsecase.png","details":{"size":139926,"image":{"width":1364,"height":350}},"fileName":"26RayServeEnsembleUsecase.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ensemble patterns involve mixing output from one or more models. They are also called model stacking in some cases. Below are three use cases of ensemble patterns.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Model Update","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"New models are developed and trained over time. This means there will always be new versions of the model in production. The question becomes, how do you make sure the new models are valid and performant in live online traffic scenarios? One way to do this is by putting some portion of the traffic through the new model. You still select the output from the known good model, but you are also collecting live output from the newer version of the models in order to validate it.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Aggregation","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The most widely known use case is for aggregation. For regression models, outputs from multiple models are averaged. For classification models, the output will be a voted version of multiple modelsâ output. For example, if two models vote for cat and one model votes for dog, then the aggregated output will be cat. Aggregation helps combat inaccuracy in individual models and generally makes the output more accurate and âsaferâ.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dynamic Selection","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another use case for ensemble models is to dynamically perform model selection given input attributes. For example, if the input contains a cat, model A will be used because it is specialized for cats.Â  If the input contains a dog, model B will be used because it is specialized for dogs. Note that this dynamic selection doesnât necessarily mean the pipeline itself has to be static. It could also be selecting models given user feedback. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"General Ensemble Implementation Options","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"58Z7G6vwRfpDBSIepG0fQZ","type":"Asset","createdAt":"2021-09-30T23:40:47.037Z","updatedAt":"2021-09-30T23:41:09.505Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"General Ensemble Implementation","description":"Before Ray Serve, implementing ensembles generally meant you had to choose between wrapping your models in a web server or using many specialized microservices. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/58Z7G6vwRfpDBSIepG0fQZ/7e666548b9cef471ae4194991784f455/27EnsembleDeployment.png","details":{"size":167575,"image":{"width":1332,"height":510}},"fileName":"27EnsembleDeployment.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ensemble implementations suffer the same sort of issues as pipelines. It is simple to wrap models in a web server, but it is not performant. When you use specialized microservices, you end up having a lot of operational overhead as the number of microservices scale with the number of models. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71qMXFLqM4EeyAIHCMnVzY","type":"Asset","createdAt":"2021-09-30T23:44:12.374Z","updatedAt":"2021-09-30T23:44:12.374Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2020 Anyscale demo","description":"Ensemble example from the [2020 Anyscale demo](https://youtu.be/8GTd8Y_JGTQ)","file":{"url":"//images.ctfassets.net/xjan103pcp94/71qMXFLqM4EeyAIHCMnVzY/a76ac67159a965c670a53a944404b39a/28RayServeEnsembleRaySummit.png","details":{"size":371197,"image":{"width":1341,"height":600}},"fileName":"28RayServeEnsembleRaySummit.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray Serve, the kind of pattern is incredibly simple. You can look at the ","nodeType":"text"},{"data":{"uri":"https://youtu.be/8GTd8Y_JGTQ"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"2020 Anyscale demo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to see how to utilize Ray Serveâs handle mechanism to perform dynamic model selection.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another example of using Ray Serve for ensembling is Wildlife Studios combining output of many classifiers for a single prediction. You can check out how they were able to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/wildlife-studios-serves-in-game-offers-3x-faster-at-1-10th-the-cost-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"serve in-game offers 3x faster with Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Business Logic Pattern","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Productionizing machine learning will always involve business logic. No models can stand-alone and serve requests by themselves. Business logic patterns involve everything thatâs involved in a common ML task that is not ML model inference. This includes:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Database lookups for relational records","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Web API calls for external services","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Feature store lookup for pre-compute feature vectors","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Feature transformations like data validation, encoding, and decoding.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"General Business Logic Implementation Options","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ixwTFsSoaWNza9Mp3OPjj","type":"Asset","createdAt":"2021-09-30T23:45:47.219Z","updatedAt":"2021-09-30T23:45:47.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"General Business Logic Implementation Options","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6ixwTFsSoaWNza9Mp3OPjj/8144cd2bea5d42a6333ad1d1f55270c0/31BusinessLogicInAction.png","details":{"size":111620,"image":{"width":984,"height":462}},"fileName":"31BusinessLogicInAction.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The pseudocode for the web handler above does the following things:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"It loads the model (letâs say from S3)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Validates the input from the database","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Looks up some pre-computed features from the feature store.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Only after the web handler completes these business logic steps are the inputs passed through to ML models. The problem is that the requirements of model inference and business logic lead to the server being ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"both network bounded and compute bounded","nodeType":"text"},{"data":{},"marks":[],"value":". This is due to the model loading step, database lookup, and feature store lookups being network bounded and I/O heavy as well as the model inference being compute bound and memory hungry. The combination of these factors lead to an inefficient utilization of resources. Scaling will be expensive.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2tzQhijrmJQ1i1dtp8SeF","type":"Asset","createdAt":"2021-09-30T23:47:03.551Z","updatedAt":"2021-09-30T23:47:03.551Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Web handler approach (left) and microservices approach (right)","description":"Web handler approach (left) and microservices approach (right)","file":{"url":"//images.ctfassets.net/xjan103pcp94/2tzQhijrmJQ1i1dtp8SeF/de61106c82ef5dd503d6200ecdaebd30/32WheretoRunBusinessLogic.png","details":{"size":80148,"image":{"width":1337,"height":464}},"fileName":"32WheretoRunBusinessLogic.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A common way to increase utilization is to split models out into model servers or microservices.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The web app is purely network bounded while the model servers are compute bounded. However, a common problem is the interface between the two. If you put too much business logic into the model server, then the model servers become a mix of network bounded and compute bounded calls.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you let the model servers be pure model servers, then you have the â","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"tensor-in, tensor-out","nodeType":"text"},{"data":{},"marks":[],"value":"â interface problem. The input types for model servers are typically very constrained to just tensors or some alternate form of it. This makes it hard to keep the pre-processing, post-processing, and business logic in sync with the model itself.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"It becomes hard to reason about the interaction between the processing logic and the model itself because during training, the processing logic and models are tightly coupled, but when serving, they are split across two servers and two implementations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Neither the web handler approach nor the microservices approach is satisfactory.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Implementing Business Logic in Ray Serve","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4WEB4OkdjgTl7azrJctx4A","type":"Asset","createdAt":"2021-09-30T23:48:49.625Z","updatedAt":"2021-09-30T23:48:49.625Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Business Logic in Ray Serve","description":"Business Logic in Ray Serve","file":{"url":"//images.ctfassets.net/xjan103pcp94/4WEB4OkdjgTl7azrJctx4A/703ef612cc6e50aaa48433deef6ba985/33BusinessLogicInRayServe.png","details":{"size":114370,"image":{"width":856,"height":398}},"fileName":"33BusinessLogicInRayServe.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray Serve, you just have to make some simple changes to the old web server to alleviate the issues described above. Instead of loading the model directly, you can retrieve a ServeHandle that wraps the model, and offload the computation to another deployment. All the data types are preserved and there is no need to write âtensor-in, tensor-outâ API calls--you can just pass in regular Python types. Additionally, the model deployment class can stay in the same file, and be deployed together with the prediction handler. This makes it easy to understand and debug the code.Â ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"model.remote","nodeType":"text"},{"data":{},"marks":[],"value":" looks like just a function and you can easily trace it to the model deployment class.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this way, Ray Serve helps you split up the business logic and inference into two separation components, one I/O heavy and the other compute heavy. This allows you to scale each piece individually, without losing the ease of deployment. Additionally, because ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"model.remote","nodeType":"text"},{"data":{},"marks":[],"value":" is just a function call, itâs a lot easier to test and debug than separate external services.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve FastAPI Integration","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"HStDeyRVrI7LM2WdUP2s6","type":"Asset","createdAt":"2021-09-30T23:50:19.687Z","updatedAt":"2021-09-30T23:50:19.687Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Serve: Ingress with FastAPI","description":"Ray Serve: Ingress with FastAPI","file":{"url":"//images.ctfassets.net/xjan103pcp94/HStDeyRVrI7LM2WdUP2s6/4ff62672c24788b1d07d59736b85817b/34RayServeIngress.png","details":{"size":297542,"image":{"width":1656,"height":838}},"fileName":"34RayServeIngress.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"An important part of implementing business logic and other patterns is authentication and input validation. ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/http-servehandle.html#serve-fastapi-http"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve natively integrates with FastAPI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which is a type safe and ergonomic web framework. ","nodeType":"text"},{"data":{"uri":"https://fastapi.tiangolo.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FastAPI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" has features like automatic dependency injection, type checking and validation, and OpenAPI doc generation.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray Serve, you can directly pass the FastAPI app object into it with ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"@serve.ingress","nodeType":"text"},{"data":{},"marks":[],"value":". This decorator makes sure that all existing FastAPI routes still work and that you can attach new routes with the deployment class so states like loaded models, and networked database connections can easily be managed. Architecturally, we just made sure that your FastAPI app is correctly embedded into the replica actor and the FastAPI app can scale out across many Ray nodes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Online LearningÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Online learning is an emerging pattern thatâs become more and more widely used. It refers to a model running in production that is constantly being updated, trained, validated and deployed. Below are three use cases of online learning patterns.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dynamically Learn Model Weights","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are use cases for dynamically learning model weights online. As users interact with your services, these updated model weights can contribute to a personalized model for each user or group. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6J7nynhCYa2vV5Ud57cn0m","type":"Asset","createdAt":"2021-09-30T23:53:34.472Z","updatedAt":"2021-09-30T23:53:34.472Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Online Learning example at Ant Group (image courtesy of Ant Group) ","description":"[Online Learning example at Ant Group](https://www.anyscale.com/blog/online-resource-allocation-with-ray-at-ant-group) (image courtesy of Ant Group) ","file":{"url":"//images.ctfassets.net/xjan103pcp94/6J7nynhCYa2vV5Ud57cn0m/0362b38997ac5c4e8c1db756cf4a1dcd/38OnlineLearningAntGroup.png","details":{"size":311903,"image":{"width":1208,"height":533}},"fileName":"38OnlineLearningAntGroup.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"One case study of online learning consists of an online resource allocation business solution at Ant Group. The model is trained from offline data, then combined with real time streaming data source, and then served live traffic. One thing to note is that online learning systems are drastically more complex than their static serving counterparts. In this case, putting models in the web server, or even splitting it up into multiple microservices, would not help with the implementation. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dynamically Learn Parameters to Orchestrate Models","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are also use cases for learning parameters to orchestrate or compose models, for example, ","nodeType":"text"},{"data":{"uri":"https://dl.acm.org/doi/fullHtml/10.1145/3383313.3411550"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"learning which model a user prefers","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". This manifests often in model selection scenarios or contextual bandit algorithms.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Reinforcement Learning","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Reinforcement learning is the branch of machine learning that trains agents to interact with the environment. The environment can be the physical world or a simulated environment. You can learn about reinforcement learning ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and see how you can deploy a RL model using Ray Serve ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/rllib.html#serve-rllib-tutorial"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â  ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ConclusionÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Zmo0bB3BCXWqX3bfhtxlg","type":"Asset","createdAt":"2021-09-30T23:55:14.299Z","updatedAt":"2021-09-30T23:55:14.299Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"WhereRayServeFitsIn","description":"Ray Serve is easy to develop and production ready.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Zmo0bB3BCXWqX3bfhtxlg/f6a8378f2cbb372c458904651e7bd3c1/WhereRayServeFitsIn.png","details":{"size":92883,"image":{"width":768,"height":444}},"fileName":"WhereRayServeFitsIn.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This post went over 4 main patterns of machine learning in production, how Ray Serve can help you natively scale and work with complex architectures, and how ML in production often means many models in production. Ray Serve is built with all of this in mind on top of the distributed runtime Ray. If youâre interested in learning more about Ray, you can check out the ","nodeType":"text"},{"data":{"uri":"https://ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", join us on ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discourse","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and check out the ","nodeType":"text"},{"data":{"uri":"http://tinyurl.com/ray-white-paper"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"whitepaper","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"! If you're interested in working with us to make it easier to leverage Ray, we're ","nodeType":"text"},{"data":{"uri":"https://jobs.lever.co/anyscale"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"hiring","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"!","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"BBDpqdGlEcQZHPoQxytXf","type":"Asset","createdAt":"2021-10-01T00:19:41.125Z","updatedAt":"2021-10-01T00:19:41.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Where Ray Serve Fits In","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/BBDpqdGlEcQZHPoQxytXf/7dff7d7f06b7a5d55cfd255cc3cd88e6/WhereRayServeFitsIn.png","details":{"size":92883,"image":{"width":768,"height":444}},"fileName":"WhereRayServeFitsIn.png","contentType":"image/png"}}},"recommendations":[],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BzSmyQ2ojATOq6qOpD4PX","type":"Entry","createdAt":"2021-10-02T19:52:20.790Z","updatedAt":"2021-10-02T19:52:20.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Technology","identifier":"technology"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}}],"hideIntro":true}},"thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"FaPihBS7H8opDps1E8dXE","type":"Asset","createdAt":"2022-03-21T15:11:27.810Z","updatedAt":"2022-03-21T15:28:52.364Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"blog-recommended-content-gears-dark","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/FaPihBS7H8opDps1E8dXE/20dca1470a6453ffd33c172009cc488a/blog-recommended-content-gears-dark.jpg","details":{"size":40281,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-gears-dark.jpg","contentType":"image/jpeg"}}},"recommendations":null}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}}],"hideIntro":true,"showMainImage":false,"thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"qyo3m0dc8CvMNCfhxDy5e","type":"Asset","createdAt":"2022-03-24T21:30:36.648Z","updatedAt":"2022-03-24T21:30:36.648Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-code-dark","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/qyo3m0dc8CvMNCfhxDy5e/409ed6f79aea5822b10aaa365318a005/blog-recommended-content-code-dark.jpg","details":{"size":39745,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-code-dark.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2av3oXhIPMs6KsZndnxGUQ","type":"Entry","createdAt":"2022-03-02T16:24:57.961Z","updatedAt":"2022-06-22T15:53:52.465Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Deploying XGBoost models with Ray Serve","slug":"deploying-xgboost-models-with-ray-serve","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"kQhxB9LxeWeB3noKuPXiJ","type":"Entry","createdAt":"2020-09-20T16:36:33.304Z","updatedAt":"2020-09-20T16:36:33.304Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Simon Mo","slug":"simon-mo","bio":"Simon Mo is a software engineer at Anyscale. Before Anyscale, he was a student at UC Berkeley participating in research at the RISELab. He focuses on studying and building systems for machine learning, in particular, how to make ML model serving systems more efficient, ergonomic, and scalable. He works on Ray Serve at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"fy7QSQIwnpMZWvM1ndmhx","type":"Asset","createdAt":"2020-09-20T16:36:28.219Z","updatedAt":"2020-09-20T16:36:28.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Simon Mo headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/fy7QSQIwnpMZWvM1ndmhx/cdef7ccbc81d0ea4903ed73336e42b48/Simon-Mo-headshot.png","details":{"size":365363,"image":{"width":550,"height":550}},"fileName":"Simon-Mo-headshot.png","contentType":"image/png"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4W4CsBpJcrALb90x0HXkRQ","type":"Entry","createdAt":"2022-02-07T15:00:35.988Z","updatedAt":"2022-02-07T15:00:35.988Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Chandler Gibbons","slug":"chandler-gibbons"}}],"publishedDate":"2022-03-02","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"In this article, weâll cover how to deploy XGBoost with two frameworks: Flask and Ray Serve. Weâll also highlight the advantages of Ray Serve over other serving solutions when comparing models in production.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"XGBoost is an optimized distributed gradient boosting library and algorithm that implements machine learning algorithms under the gradient boosting framework. This library is designed to be highly efficient and flexible, using parallel tree boosting to provide fast and efficient solutions for several data science and machine learning problems. In a previous blog post, we explored ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/three-ways-to-speed-up-xgboost-model-training"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"three ways to speed up XGBoost model training","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"XGBoost has quickly become the state-of-the-art machine learning algorithm for solving tasks with structured data. This is mainly due to its high speed and exceptional performance. It is faster than other ensemble classifiers and its core algorithm is parallelizable, meaning that it can run on multi-core and GPU computers.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Options for serving machine learning and XGBoost models include cloud-hosted platforms such as ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/pm/sagemaker/?trk=ps_a134p000007BxdXAAS\u0026trkCampaign=acq_paid_search_brand\u0026sc_channel=PS\u0026sc_campaign=acquisition_EEM\u0026sc_publisher=Google\u0026sc_category=Machine%20Learning\u0026sc_country=EEM\u0026sc_geo=EMEA\u0026sc_outcome=acq\u0026sc_detail=amazon%20sagemaker\u0026sc_content=Sagemaker_e\u0026sc_matchtype=e\u0026sc_segment=532493333428\u0026sc_medium=ACQ-P%7CPS-GO%7CBrand%7CDesktop%7CSU%7CMachine%20Learning%7CSagemaker%7CEEM%7CEN%7CText%7Cxx%7CNon-EU\u0026s_kwcid=AL!4422!3!532493333428!e!!g!!amazon%20sagemaker\u0026ef_id=CjwKCAjwz5iMBhAEEiwAMEAwGJVvxz6gY_Gh9Jt8v4Mp4YLThNrk21PDdMgAErngGy2cuuNwfE6j2BoCUNQQAvD_BwE:G:s\u0026s_kwcid=AL!4422!3!532493333428!e!!g!!amazon%20sagemaker"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Amazon SageMaker","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.kubeflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"KubeFlow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", Google Cloud ","nodeType":"text"},{"data":{"uri":"https://cloud.google.com/ai-platform"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"AI Platform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and Microsoftâs ","nodeType":"text"},{"data":{"uri":"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Azure ML SDK","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". These are powerful serving tools provided by some of the largest tech companies, but they can be very expensive to use. In addition, these tools only work with their own ecosystems.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Manually taking machine learning models from concept to production is typically complex and time-consuming, so there are several frameworks for deploying XGBoost in production.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this article, weâll cover how to deploy XGBoost with two frameworks: Flask and Ray Serve. Weâll also highlight the advantages of Ray Serve over other serving solutions when comparing models in production.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deploying XGBoost with Flask","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Flask is the most common Python-based microframework used for deploying XGBoost, as it has no dependencies on external libraries. Flask is considered an exceptional deployment framework for XGBoost because it is easy to set up and is an efficient tool with REST endpoints. Plus, unlike XGBoost Server, Flask is framework-agnostic and has an HTTP request handling function. Flask is also a free deployment framework, unlike SageMaker and other cloud-hosted solutions. These are only a handful of features that make the Flask an optimal solution for deploying XGBoost into production.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this section, weâll train, test, and deploy an XGBoost model with Flask. This XGBoost model will be trained to predict the onset of diabetes using the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"pima-indians-diabetes","nodeType":"text"},{"data":{},"marks":[],"value":" dataset from the ","nodeType":"text"},{"data":{"uri":"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"UCI Machine Learning Repository website","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". This small dataset contains several numerical medical variables of eight different features related to diabetes, in addition to one target variable â ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Outcome","nodeType":"text"},{"data":{},"marks":[],"value":". So, weâll use XGBoost to model and solve a simple prediction problem.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Building the XGBoost model","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"First, we will load some dependencies in addition to the data. Then the training starts:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"24naGARFASgidTVaGCNzsB","type":"Entry","createdAt":"2022-02-24T18:13:31.957Z","updatedAt":"2022-02-24T18:13:31.957Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Deploying XGBoost models with Ray Serve, code example 1","body":"from numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# load data\ndataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n# split data into X and y\nX = dataset[:,0:8]\nY = dataset[:,8]\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The second step is to create the XGBoost model and fit it to the numerical data we have:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1psQXEhgNvhUqIppiNFotj","type":"Entry","createdAt":"2022-02-24T18:14:13.170Z","updatedAt":"2022-02-24T18:14:13.170Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Deploying XGBoost models with Ray Serve, code example 2","body":"model = XGBClassifier()\nmodel.fit(X_train, y_train)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Once the model is trained, we test it using our testing set, and then calculate some metrics for evaluation purposes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"295eH7ujFHKhmOJTHk8wIB","type":"Entry","createdAt":"2022-02-24T18:14:46.278Z","updatedAt":"2022-02-24T18:14:46.278Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Deploying XGBoost models with Ray Serve, code example 3","body":"y_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deploying XGBoost with Flask","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"This step contains several stages:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Pickling (serialization)","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"Once the model is trained and tested, we can save it for future inferences using the pickle serialization module.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6TuP4eNY4l4siX8OoeQ4sE","type":"Entry","createdAt":"2022-02-24T18:15:26.609Z","updatedAt":"2022-02-24T18:15:26.609Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Deploying XGBoost models with Ray Serve, code example 4","body":"import pickle\n\n# saving the model\nwith open('model.pkl','wb') as f:\n    pickle.dump(model, f)\n\n# loading the model\nwith open(\"model.pkl\", \"rb\") as f:\n    model = pickle.load(f)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Creating a Flask app to serve the model","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"To deploy our XGBoost model, we'll use ","nodeType":"text"},{"data":{"uri":"https://flask.palletsprojects.com/en/2.0.x/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Flask","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". In order to create our Flask web app that can predict the onset of diabetes, we will need a prediction route to make inferences from our XGBoost.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ce1wzIn10EDj1sw6P3iDV","type":"Entry","createdAt":"2022-02-24T18:16:00.810Z","updatedAt":"2022-02-24T18:16:00.810Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Deploying XGBoost models with Ray Serve, code example 5","body":"import pickle\nimport numpy as np\nfrom flask import Flask, request, jsonify, render_template\n\napp = Flask(__name__)\nwith open(\"model.pk\", \"rb\") as f:\n    model = pickle.load(f)\n\n@app.route(\"/predict\", methods=[\"POST\"])\ndef predict():\n    data = request.get_json(force=True)\n    prediction = model.predict([np.array(list(data.values()))])\n    output = prediction[0]\n    return jsonify(output)\n\nif __name__ == \"__main__\":\n    app.run(debug=True)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Querying the predict API using requests","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this step, we create the request.py file. This file displays the predicted value by calling the APIs defined in the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"app.py","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6z2yLZiAi6uiSRui4SoKn","type":"Entry","createdAt":"2022-02-24T18:16:43.196Z","updatedAt":"2022-02-24T18:16:43.196Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Deploying XGBoost models with Ray Serve, code example 6","body":"import requests\n\nurl = \"http://localhost:5000/predict\"\nr = requests.post(\n    url,\n    json={\n        \"Pregnancies\": 6,\n        \"Glucose\": 148,\n        \"BloodPressure\": 72,\n        \"SkinThickness\": 35,\n        \"Insulin\": 0,\n        \"BMI\": 33.6,\n        \"DiabetesPedigree\": 0.625,\n        \"Age\": 50,\n        \"Outcome\": 1,\n    },\n)\nprint(r.json())","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deploying XGBoost with Ray Serve","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Despite the usefulness of Flask in deploying machine learning models, it still has some drawbacks. For instance, it is unsuitable for large applications and lacks login and authentication capabilities.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Beyond these, Flaskâs main drawback for machine learning model serving is the challenges it presents with scaling. With Flask, scaling every component requires you to run many parallel instances, and you must decide how to do it. Will you use virtual machines, physical machines, or perhaps a Kubernetes cluster? Whichever method you choose, you will be on the hook for spinning up instances of your app and load balancing. Deploying an XGBoost model with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is one solution to this problem, since it provides you with a simple web server that leverages the complex routing, scaling, and testing logic necessary for production deployments.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray Serve, itâs easier to scale out your model on a multi-node Ray cluster, as you can take full advantage of its serving ability to dynamically update running deployments. In addition, Ray Serve is framework-agnostic, so it can serve different machine learning frameworks such as ","nodeType":"text"},{"data":{"uri":"https://www.tensorflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"TensorFlow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and ","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Scikit-learn","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Altogether, it allows for high-efficiency, high-performance production.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now, letâs use the XGBoost model that we created before and deploy it with Ray Serve.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"First, letâs install Ray Serve:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"pip install \"ray[serve]\"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Then, we start Ray Serve, which runs on top of several Ray clusters.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"ray start --head","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next, we run the following Python script to import Ray Serve, start it up, and connect to the local running Ray cluster:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1uFcw0bDJgEhQT1koY0HfS","type":"Entry","createdAt":"2022-02-24T18:17:51.805Z","updatedAt":"2022-02-24T18:17:51.805Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Deploying XGBoost models with Ray Serve, code example 7","body":"import ray\nfrom ray import serve\nray.init(address='auto', namespace=\"serve\") # Connect to the local running Ray cluster.\nserve.start(detached=True) # Start the Ray Serve processes within the Ray cluster.","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Note that the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"serve.start","nodeType":"text"},{"data":{},"marks":[],"value":" method is used to start up a few ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/actors.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray actors","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which are used by Ray Serve to route HTTP requests to the appropriate models.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Also note that weâre only running Ray locally to test our code. This already gives us an advantage over Flask, because, by default, Ray uses all available CPU cores on our machine, while the Flask app we created previously only uses a single core. And this is only a small taste of the benefits Ray can provide â because we can just as easily deploy our model to a Ray cluster with dozens or even hundreds of nodes to serve our XGBoost model at scale without changing the code at all.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now that Ray Serve is ready, it is time to create the model and deploy it. Since our XGBoost model is already created and trained, we just need to load and read it as a class, and then start the deployment process using Ray Serve.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs jump right into it:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7aDrAaoIEcyvVwdTFJnsC4","type":"Entry","createdAt":"2022-02-24T18:18:47.538Z","updatedAt":"2022-02-24T18:18:47.538Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Deploying XGBoost models with Ray Serve, code example 8","body":"import pickle\nimport json\nimport ray\nfrom ray import serve\n\n@serve.deployment(num_replicas=2, route_prefix=\"/regressor\")\nclass XGB:\n    def __init__(self):\n        with open(\"model.pkl\", \"rb\") as f:\n            self.model = pickle.load(f)\n\n    async def __call__(self, starlette_request):\n        payload = await starlette_request.json()\n        print(\"Worker: received starlette request with data\", payload)\n\n        input_vector = [\n            payload[\"Pregnancies\"],\n            payload[\"Glucose\"],\n            payload[\"Blood Pressure\"],\n            payload[\"Skin Thickness\"],\n            payload[\"Insulin\"],\n            payload[\"BMI\"],\n            payload[\"DiabetesPedigree\"],\n            payload[\"Age\"],\n        ]\n        prediction = self.model.predict([input_vector])[0]\n        return {\"result\": prediction}","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now, hereâs where the magic happens. The following few lines of code will deploy our XGBoost model to our running Ray Serve instance. We simply do this by running Ray Serve API calls in the Python framework.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Do6qubSHk4CQbCVPTQGHb","type":"Entry","createdAt":"2022-02-24T18:19:20.742Z","updatedAt":"2022-02-24T18:19:20.742Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Deploying XGBoost models with Ray Serve, code example 9","body":"# now we initialize /connect to the Ray service\nserve.start(detached=True)\n# Deploy the model.\nXGB.deploy()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"And there we go! Our XGBoost model is now deployed successfully on a Ray Serve application by simply calling deploy on the class we have defined. In fact, there are two copies of the model running at the same time and handling responses. It is even easier to scale it out by changing the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"num_replicas","nodeType":"text"},{"data":{},"marks":[],"value":" parameters.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can now query the endpoint of our deployed model by sending a request to it. Note that our HTTP runs at ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"localhost:8000","nodeType":"text"},{"data":{},"marks":[],"value":" by default.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2zCGbNgxSP3X0qORFenGUp","type":"Entry","createdAt":"2022-02-24T18:20:12.702Z","updatedAt":"2022-02-24T18:20:12.702Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Deploying XGBoost models with Ray Serve, code example 10","body":"import requests\n\nsample_request_input = {\n    \"Pregnancies\": 6,\n    \"Glucose\": 148,\n    \"BloodPressure\": 72,\n    \"SkinThickness\": 35,\n    \"Insulin\": 0,\n    \"BMI\": 33.6,\n    \"DiabetesPedigree\": 0.625,\n    \"Age\": 50,\n}\nresponse = requests.get(\"http://localhost:8000/regressor\", json=sample_request_input)\nprint(response.text)\n# Response:\n#  \"result\": \"1\"","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"As seen above, the final result will be 1 for a diabetes onset that is predicted as âpossibleâ or âimminent,â or 0 for when a diabetes onset is not predicted.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"And that's all there is to it! You now know how to serve an XGBoost model using Flask and scale it easily using Ray Serve.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To learn more about Ray Serve, the official documentation is a great place to start. You can begin by ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"learning the basics","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and then dive into the details on ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/ml-models.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"serving up machine learning models","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Or, register for our upcoming meetup, where we'll discuss ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/events/2022/03/31/productionizing-ml-at-scale-with-ray-serve"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"productionizing ML at scale with Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3dqOXYWzlbeecbJzvZ5k3J","type":"Asset","createdAt":"2022-02-24T18:29:27.491Z","updatedAt":"2022-02-24T20:58:03.257Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-ray-serve-flask-thumb","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3dqOXYWzlbeecbJzvZ5k3J/3be8e88afaff5b96b28c0b471946858e/blog-ray-serve-flask-thumb.png","details":{"size":405502,"image":{"width":1500,"height":1000}},"fileName":"blog-ray-serve-flask-thumb.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}}],"hideIntro":true,"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1LcZPnPrUOUvS2lSBVHIck","type":"Entry","createdAt":"2022-02-23T15:39:50.582Z","updatedAt":"2022-06-22T15:56:00.650Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Serving PyTorch models with FastAPI and Ray Serve","slug":"serving-pytorch-models-with-fastapi-and-ray-serve","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"kQhxB9LxeWeB3noKuPXiJ","type":"Entry","createdAt":"2020-09-20T16:36:33.304Z","updatedAt":"2020-09-20T16:36:33.304Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Simon Mo","slug":"simon-mo","bio":"Simon Mo is a software engineer at Anyscale. Before Anyscale, he was a student at UC Berkeley participating in research at the RISELab. He focuses on studying and building systems for machine learning, in particular, how to make ML model serving systems more efficient, ergonomic, and scalable. He works on Ray Serve at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"fy7QSQIwnpMZWvM1ndmhx","type":"Asset","createdAt":"2020-09-20T16:36:28.219Z","updatedAt":"2020-09-20T16:36:28.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Simon Mo headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/fy7QSQIwnpMZWvM1ndmhx/cdef7ccbc81d0ea4903ed73336e42b48/Simon-Mo-headshot.png","details":{"size":365363,"image":{"width":550,"height":550}},"fileName":"Simon-Mo-headshot.png","contentType":"image/png"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4W4CsBpJcrALb90x0HXkRQ","type":"Entry","createdAt":"2022-02-07T15:00:35.988Z","updatedAt":"2022-02-07T15:00:35.988Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Chandler Gibbons","slug":"chandler-gibbons"}}],"publishedDate":"2022-02-23","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"In this article, we will highlight the options available for serving a PyTorch model into production and deploying it with several frameworks, such as TorchServe, Flask, and FastAPI.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Deploying a machine learning model can be challenging, depending on where the model will be deployed and which tools are used to serve it into production. Every deployment platform has its own pros and cons, as do the serving tools used. So, it is crucial to select the proper tools and the best platform to most effectively serve your model into production and deploy it.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this article, we will highlight the options available for serving a ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" model into production and deploying it with several frameworks, such as ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/serve/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"TorchServe","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://flask.palletsprojects.com/en/2.0.x/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Flask","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and ","nodeType":"text"},{"data":{"uri":"https://fastapi.tiangolo.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FastAPI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". We will also examine how the integration of FastAPI and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" can help with scaling our PyTorch model serving API across a Ray cluster.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"PyTorch model serving frameworks","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The different frameworks available to serve PyTorch models can be divided into three categories:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Customized tools such as TorchServe","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Cloud-hosted solutions such as Amazon SageMaker","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Web-based frameworks such as Flask, FastAPI, and Ray Serve","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"TorchServe was developed by PyTorch as a flexible and easy-to-use tool for serving PyTorch and Torch-scripted models. It can be deployed locally since it comes with a convenient CLI, and it is easy to scale out using ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/sagemaker/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Amazon SageMaker","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/eks/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Amazon EKS","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, TorchServe has several drawbacks. TorchServe is experimental and open source, so there are frequent changes, patches, and updates. Plus, this tool only works with PyTorch and Torch-scripted models, which means it is not framework-agnostic and it is Java-dependent. Moreover, these customized tools are typically hard to develop, deploy, and manage.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Other options for serving machine learning and PyTorch models in particular are cloud-hosted platforms such as","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/pm/sagemaker/?trk=ps_a134p000007BxdXAAS\u0026trkCampaign=acq_paid_search_brand\u0026sc_channel=PS\u0026sc_campaign=acquisition_EEM\u0026sc_publisher=Google\u0026sc_category=Machine%20Learning\u0026sc_country=EEM\u0026sc_geo=EMEA\u0026sc_outcome=acq\u0026sc_detail=amazon%20sagemaker\u0026sc_content=Sagemaker_e\u0026sc_matchtype=e\u0026sc_segment=532493333428\u0026sc_medium=ACQ-P%7CPS-GO%7CBrand%7CDesktop%7CSU%7CMachine%20Learning%7CSagemaker%7CEEM%7CEN%7CText%7Cxx%7CNon-EU\u0026s_kwcid=AL!4422!3!532493333428!e!!g!!amazon%20sagemaker\u0026ef_id=CjwKCAjwz5iMBhAEEiwAMEAwGJVvxz6gY_Gh9Jt8v4Mp4YLThNrk21PDdMgAErngGy2cuuNwfE6j2BoCUNQQAvD_BwE:G:s\u0026s_kwcid=AL!4422!3!532493333428!e!!g!!amazon%20sagemaker"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":" Amazon SageMaker","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.kubeflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"KubeFlow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", Google Cloud ","nodeType":"text"},{"data":{"uri":"https://cloud.google.com/ai-platform"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"AI Platform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and Microsoftâs","nodeType":"text"},{"data":{"uri":"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":" Azure ML SDK","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". These are powerful serving tools provided by some of the largest tech companies, but they can be very expensive to use. In addition these tools only work with their own ecosystems.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Web-based serving tools such as Flask can be preferable solutions for some of these problems. Flask is a web framework that is efficient, easy to set up, and framework-agnostic. However, as with other web-based serving tools, it can present challenges with scaling.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What is Ray Serve?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a library for serving machine learning models that runs on top of the Ray","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/whats-new-in-the-ray-distributed-library-ecosystem"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":" Distributed Library Ecosystem","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". It is a simple web server that leverages the complex routing, scaling, and testing logic necessary for production deployments. It is also framework-agnostic and Python-first, so models can be configured and served declaratively in pure Python without YAML or JSON configuration files.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve can also be an efficient tool for deploying PyTorch models because it is easy to scale, whether in your data center or in the cloud.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"FastAPI and Ray Serve for serving PyTorch models","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve provides a solid solution to scalability, management, and other previously discussed issues by providing end-to-end control over the request lifecycle, while allowing each model to scale independently. Moreover, the integration of Ray Serve and FastAPI for serving the PyTorch model can improve this whole process. The idea is that you create your FastAPI model and then scale it up with Ray Serve, which helps in serving the model from one CPU to 100+ CPU clusters. This will lead to a huge improvement in the number of requests served per second.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The Ray Serve docs ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/pytorch.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"show how to serve a PyTorch model","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" directly. This approach is excellent if you only want to serve a single model. However, we may want to build out a more comprehensive API, and it's quick and easy to do so with FastAPI.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Fortunately, Ray Serve has built-in FastAPI support, and we can adapt the code from the Ray Serve docs to work in a FastAPI application. Here's everything needed to build a FastAPI app that serves up a pre-trained ResNet model trained on ImageNet data:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2sNVOmEGoTUoQ6qvTRBEy","type":"Entry","createdAt":"2022-02-18T18:02:50.303Z","updatedAt":"2022-03-09T20:36:28.629Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Serving PyTorch models blog, code example 1","body":"import ray\nfrom ray import serve\nfrom fastapi import FastAPI, UploadFile, File\n\nimport torch\nfrom torchvision import transforms\nfrom torchvision.models import resnet18\nfrom PIL import Image\n\nfrom io import BytesIO\n\napp = FastAPI()\nray.init(address=\"auto\")\nserve.start(detached=True)\n\n@serve.deployment\n@serve.ingress(app)\nclass ModelServer:\n  def __init__(self):\n    self.count = 0\n    self.model = resnet18(pretrained=True).eval()\n    self.preprocessor = transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Lambda(lambda t: t[:3, ...]),  # remove the alpha channel\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n  def classify(self, image_payload_bytes):\n    pil_image = Image.open(BytesIO(image_payload_bytes))\n\n    pil_images = [pil_image]  #batch size is one\n    input_tensor = torch.cat(\n        [self.preprocessor(i).unsqueeze(0) for i in pil_images])\n\n    with torch.no_grad():\n        output_tensor = self.model(input_tensor)\n    return {\"class_index\": int(torch.argmax(output_tensor[0]))}\n\n  @app.get(\"/\")\n  def get(self):\n    return \"Welcome to the PyTorch model server.\"\n\n  @app.post(\"/classify_image\")\n  async def classify_image(self, file: UploadFile = File(...)):\n    image_bytes = await file.read()\n    return self.classify(image_bytes)\n\nModelServer.deploy()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We start by creating a FastAPI application, connecting to a Ray cluster, and starting Ray Serve. If you want to connect to a Ray cluster running on remote machines, you'll need to specify the address of one of the cluster nodes when you call ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.init()","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Then, we create a class-based FastAPI application that loads the model in the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"__init__","nodeType":"text"},{"data":{},"marks":[],"value":" method and classifies images in the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"classify","nodeType":"text"},{"data":{},"marks":[],"value":" method.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"When an API user sends a POST request to the asynchronous ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"classify_image","nodeType":"text"},{"data":{},"marks":[],"value":" endpoint and sends an image in the request body, FastAPI automatically includes the image in the file parameter, from which we read the raw bytes of the image and send them on to the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"classify","nodeType":"text"},{"data":{},"marks":[],"value":" function.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"could","nodeType":"text"},{"data":{},"marks":[],"value":" run this FastAPI app on its own, without Ray Serve. But then we'd be on the hook for manually scaling it out. Fortunately, Ray Serve makes it easy to automatically turn our FastAPI app into a distributed application that scales automatically. All we had to do to make this possible was add two decorators to our FastAPI class: ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"@serve.deployment","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"@serve.ingress(app)","nodeType":"text"},{"data":{},"marks":[],"value":". Then, we add a call to ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ModelServer.deploy()","nodeType":"text"},{"data":{},"marks":[],"value":" to the end of our code and that's it. Our FastAPI application is ready to run in a Ray cluster.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Once the app is up and running, we can test it by using Postman to send a request to ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"http://127.0.0.1:8000/ModelServer/classify_image","nodeType":"text"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7MTQrRMy8XSlebxcR0USto","type":"Asset","createdAt":"2022-02-18T18:05:38.745Z","updatedAt":"2022-02-18T18:05:38.745Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-serving-pytorch-models-1","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7MTQrRMy8XSlebxcR0USto/3fc76e7b0aea6a37c0afe64ffeb3fa6d/blog-serving-pytorch-models-1.png","details":{"size":74141,"image":{"width":1257,"height":712}},"fileName":"blog-serving-pytorch-models-1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this case, we sent it an ","nodeType":"text"},{"data":{"uri":"https://unsplash.com/photos/OzAeZPNsLXk"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"image of a cat","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and it returned:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"{","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"Â Â \"class_index\": 285","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"}Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"blockquote"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now, this isn't very descriptive. We ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"could","nodeType":"text"},{"data":{},"marks":[],"value":" add a friendly description from ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"a list of ImageNet class identifiers","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Whether we want to depends on whom we expect to consume our API. It may very well be more useful to return the ImageNet class index and let the API consumer decide what to do with it. For example, the consumer may already have a list of class descriptions custom-translated into 50 different languages, so our API returning an English label wouldn't be very helpful.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this case, class index 285 maps to âEgyptian cat,â which is a decent enough description of the image we uploaded.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you'd like to try the app for yourself, you can find a copy of it in ","nodeType":"text"},{"data":{"uri":"https://github.com/contentlab-io/ray_fastapi/tree/main"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"this GitHub repository","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Testing","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"To measure the performance of the serving process, we will use our laptop as a head node. We will only use two cores by setting our number of replicas to 2 in the code above, and we will run the server by running the above sample code exactly as before.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now, if we saturate our back end by sending many requests, we will notice that the number of queries per second increases from 0.67 to 0.81. In order to increase that number even more, we can utilize more cores. For argumentâs sake, letâs add 4. This can improve the number of queries per second to 0.84, which leads to a reduction in latency, making our model able to reply to many requests in a shorter period of time.Â Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Testing on a cluster","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"For the purposes of parallel computation and testing our model on a Ray cluster, we can use the batching features provided by Ray Serve. To do this, we should test on a cluster (a group of nodes) instead of only one node. We can initiate this using ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Anyscale","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", in which a cloud-based Ray cluster is created.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Consequently, this will increase the number of queries per second and enable Ray Serve to simplify the use of these features and scale our models. We can get an even greater speed increase using more cores and replicas.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deploying and serving a scalable machine model into production can be a challenging task. To serve a model, you can choose from a variety of available machine learning tools to best meet your needs. In this article, weâve seen that the integration of FastAPI with Ray Serve can be one of the best solutions for deploying a PyTorch model, and that using Ray Serve can be one of the greatest solutions for scaling it up.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For more on Ray and PyTorch, check out our blog post on ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-hutom-io-uses-ray-and-pytorch-to-scale-surgical-video-analysis-and"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"how Hutom.io uses Ray and PyTorch to scale surgical video analysis and review","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", or catch our ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/events/2022/03/03/ray-train-pytorch-torchx-and-distributed-deep-learning"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"upcoming Meetup, where weâll cover Ray Train, PyTorch, TorchX, and distributed deep learning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Interested in learning more about Ray Serve? ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/events/2022/03/31/productionizing-ml-at-scale-with-ray-serve"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Register for our upcoming Meetup","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", where we'll discuss productionizing ML at scale with Ray Serve.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7a3oiXbYbzNqODqB5TEGcb","type":"Asset","createdAt":"2022-02-18T18:14:52.228Z","updatedAt":"2022-02-18T18:14:52.228Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-serving-pytorch-models-thumb","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7a3oiXbYbzNqODqB5TEGcb/222936192b85d984697fa38edc89ed49/blog-serving-pytorch-models-thumb.jpg","details":{"size":272397,"image":{"width":1500,"height":1000}},"fileName":"blog-serving-pytorch-models-thumb.jpg","contentType":"image/jpeg"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}}],"hideIntro":true,"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4XPH3uNZ4JtaKjwTf0BeYs","type":"Entry","createdAt":"2021-10-18T17:04:20.513Z","updatedAt":"2022-06-22T16:18:01.729Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Cheaper and 3X Faster Parallel Model Inference with Ray Serve","slug":"cheaper-and-3x-faster-parallel-model-inference-with-ray-serve","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6QpKcnK8HVSNmP06bj64ca","type":"Entry","createdAt":"2021-10-18T17:03:25.677Z","updatedAt":"2021-10-18T17:03:25.677Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Leonnardo Rabello","slug":"leonnardo-rabello"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1GJLXI77ZjN3RZDvIiKkXv","type":"Entry","createdAt":"2021-10-18T17:03:50.323Z","updatedAt":"2021-10-18T17:03:50.323Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Lucas Machado","slug":"lucas-machado"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"kQhxB9LxeWeB3noKuPXiJ","type":"Entry","createdAt":"2020-09-20T16:36:33.304Z","updatedAt":"2020-09-20T16:36:33.304Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Simon Mo","slug":"simon-mo","bio":"Simon Mo is a software engineer at Anyscale. Before Anyscale, he was a student at UC Berkeley participating in research at the RISELab. He focuses on studying and building systems for machine learning, in particular, how to make ML model serving systems more efficient, ergonomic, and scalable. He works on Ray Serve at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"fy7QSQIwnpMZWvM1ndmhx","type":"Asset","createdAt":"2020-09-20T16:36:28.219Z","updatedAt":"2020-09-20T16:36:28.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Simon Mo headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/fy7QSQIwnpMZWvM1ndmhx/cdef7ccbc81d0ea4903ed73336e42b48/Simon-Mo-headshot.png","details":{"size":365363,"image":{"width":550,"height":550}},"fileName":"Simon-Mo-headshot.png","contentType":"image/png"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7cbdj2z67h6KL6mmQH7yNJ","type":"Entry","createdAt":"2021-09-30T22:00:29.490Z","updatedAt":"2021-09-30T22:00:29.490Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Edward Oakes","slug":"edward-oakes","link":"https://www.linkedin.com/in/edward-oakes-8bb3b0a6/"}}],"publishedDate":"2021-10-19","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KeH0JzrIEpuheazhMAhKf","type":"Entry","createdAt":"2022-06-15T01:31:23.950Z","updatedAt":"2022-06-15T01:31:23.950Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"User Story","slug":"user-story","tintColor":"#6D59C6"}},"intro":"Wildlife Studiosâs ML team was deploying sets of ensemble models using Flask. It quickly became too hard and too expensive to scale. By using Ray Serve, Wildlife Studios was able to improve the latency and throughput while reducing the cost. Ray Serve not only made it easy to parallelize model inference, but also enabled âmodel disaggregation and model compositionâ: splitting the models into separate components and running those in parallel. This enabled the scaling of each part of the request pipeline separately.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"TL;DR","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Wildlife Studiosâ ML team was deploying sets of ensemble models using Flask. It quickly became too hard and too expensive to scale. By using Ray Serve, Wildlife Studios was able to improve the latency and throughput while reducing the cost. Ray Serve not only made it easy to parallelize model inference, but also enabled âmodel disaggregation and model compositionâ: splitting the models into separate components and running those in parallel. This enabled the scaling of each part of the request pipeline separately. The primary reason for saving resources while also keeping latency down was actually due to a number of other benefits ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provided like:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"(1) better cache efficiency because there were fewer models residing in memory.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"(2) pipelining inference allowed both preprocessing and models to be busy at the same time.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"(3) autoscaling which resulted in less resources being used when traffic was low, instead of always provisioning for the peak.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Business use case overview","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/wildlife-studios-serves-in-game-offers-3x-faster-at-1-10th-the-cost-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"A previous business case study","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" covered how the Dynamic Offers team at Wildlife Studios focuses on serving highly relevant offers to improve gaming experience and maximize both the dollars spent per player and overall playtime. Every day, when users log into one of the games, the game server pings the ML service to create a dynamic offer for in-game purchase items. For example, it will create a âfor limited time, you can get 50% off for item Xâ offer considering the userâs attributes and purchase history. Wildlifeâs data scientists train many models for each game and different user groups to optimize the purchase rate of these offers.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The team manages the model deployments on internal K8s cluster and is responsible for delivering dynamic offers response within 2s of P95 latency SLA. The is important for two reasons: (1) many of Wildlife Studiosâ top revenue-generating games have a tight latency constraint, fallbacking to non-personalized offers that leads to a non-optimal experience for players, and (2) slow-loading offers can result in a loading screen, prompting players to leave the game. Currently there are about 4 games in Wildlifeâs portfolio, each game has about 2-3 different versions of the models trained on different historical data. Each version, which is a single unit of deployment, contains 12+ models trained using the same algorithm but different data.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"System design","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"When a request comes in, the Dynamic Offers system uses multiple models to deliver a single response. This is using a common technique called âmodel ensemblingâ. Ensembling is similar to gathering recommendations from a group of experts, each expert has their expertise and experiences, aggregating them to boost the final decisionâs accuracy score.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5cScqWXEyXMgZpmD8pO4o3","type":"Asset","createdAt":"2021-10-18T23:27:55.206Z","updatedAt":"2021-10-18T23:27:55.206Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"1_Wildlife Studiosâ legacy system architecture","description":"Wildlife Studiosâ legacy system architecture for dynamic offer recommendations.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5cScqWXEyXMgZpmD8pO4o3/3afac1e9a81a001f7217adc71ab808cf/1.png","details":{"size":263998,"image":{"width":1794,"height":908}},"fileName":"1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Architecturally, existing model servers follow a pattern of â","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"wrap your models in the web server","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"â. For each model version, there is a single K8s Deployment that contains some number of Pods (the amount varies by the popularity of each game, between 10 and 200). Each Pod runs a container running a Python Flask server.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the Flask server, all the models belonging to the same version are loaded at startup time. Upon each request, the Flask server will retrieve user features from DynamoDB and then ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"run through the models sequentially","nodeType":"text"},{"data":{},"marks":[],"value":". Finally, it will aggregate all the outputs and return the final output to the requester.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Challenges","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are two major challenges of the current setup:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"High latency per request","nodeType":"text"},{"data":{},"marks":[],"value":": the end to end response time can take 500-600ms at P95 measurement. This is considered too high a response time and puts them at risk to deliver \u003c2s SLA goal.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Low utilization","nodeType":"text"},{"data":{},"marks":[],"value":": in order to handle traffic spikes, the web servers are replicated to many copies (sometimes up to 500 copies). Each web server loads all the models within a version. This means at a given time, many models are unused and the compute resources are underutilized.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Solution: Parallelize Model Inference with Ray Serve","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4Yp3m0wnP3L62cWxFglMx5","type":"Asset","createdAt":"2021-10-18T23:28:54.126Z","updatedAt":"2021-10-18T23:28:54.126Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2_Ray enables running model inference in parallel and reduces inference latency by 3x.","description":"Ray enables running model inference in parallel and reduces inference latency by 3x.","file":{"url":"//images.ctfassets.net/xjan103pcp94/4Yp3m0wnP3L62cWxFglMx5/dc3f9a56347e4a812cc48bbd075678b5/2.png","details":{"size":219771,"image":{"width":1852,"height":776}},"fileName":"2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray is a distributed runtime making it easy to parallelize Python and Java code across a cluster. With Ray, the sequential model inference for each request can be parallelized easily. Instead of running through 15 models one at a time, Ray can help run all of them in parallel. This yields lower end to end latency. A microbenchmark showed that with Ray the latency has been reduced by more than 3x: instead of 500ms at p95, the parallel inference now takes 150ms at p95 latency.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Naive parallelization leads to high resource usage and low utilization","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"However, just paralleling the inference models is not enough.","nodeType":"text"},{"data":{},"marks":[],"value":" Doing so naively inside the web server can indeed reduce end to end latency, however, ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"it will drastically increase the resource usage","nodeType":"text"},{"data":{},"marks":[],"value":" because each server needs to host many models in parallel in a threadpool. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KU4Wcfgm4IHz7rq86B2Vx","type":"Asset","createdAt":"2021-10-19T05:01:33.789Z","updatedAt":"2021-10-19T05:01:33.789Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"3_Ray Serve scales each component of the model inference pipeline into separate components, and scales them seamlessly on a multi-node cluster.","description":"Ray Serve scales each component of the model inference pipeline into separate components, and scales them seamlessly on a multi-node cluster.","file":{"url":"//images.ctfassets.net/xjan103pcp94/4KU4Wcfgm4IHz7rq86B2Vx/920e79c460a7ecb42cfb64f9aeb06869/3.png","details":{"size":288451,"image":{"width":1856,"height":670}},"fileName":"3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is where Ray Serve comes in. Ray Serve is a specialized model serving library built on top of Ray. Ray Serve helps to aggregate the components into âDeploymentâ units and scale each âDeploymentâ independently. It also gives the ability for each deployment to call each other in a flexible and user-friendly way.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Wildlife implemented the new system on top of Ray Serve by splitting each model into their deployment, and creating a âSupervisorâ deployment that gets the user feature from DynamoDB. Once the user feature is retrieved, the supervisor calls 15 different model deployments for predictions in parallel, and aggregates them together.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The key advantage here is that instead of binding to 1 web server + 15 models per pod in the previous Flask app setting, the engineers and devops can configure each replication factor of each deployment separately. For example, the supervisor deployment is very lightweight, it just needs to receive the request, proxy it to the model deployments and aggregate the output to form a response. So the supervisor deployment has a replication factor of 2, each replica can handle at least a few hundred queries per second. In contrast, the model deployments are heavy weight so they are replicated to 6 replicas each, each replica can handle about tens of queries per second.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While this certainly helped, the reason whyÂ  Ray Serve was able to achieve low resource utilization while also keeping the latency down was primarily due to good cache","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" efficiency and latency hiding with pipelining.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Serve loads fewer model in memory and keep them busy: good for cache efficiency","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5WVkSYC26ISZEejfkyPKsW","type":"Asset","createdAt":"2021-10-19T05:02:16.016Z","updatedAt":"2021-10-19T05:02:16.016Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"4_Keeping less models in memory, and scaling them independently helps to increase CPU cache hit rate.","description":"Keeping less models in memory, and scaling them independently helps to increase CPU cache hit rate.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5WVkSYC26ISZEejfkyPKsW/b71a1869efd10a8e5f768d3770b183ee/4.png","details":{"size":214055,"image":{"width":1560,"height":590}},"fileName":"4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Using Ray Serve has resulted in good cache efficiency due to deployments requiring less models in memory compared to the web server approach. This is due to two factors:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"1) Model deployments in Ray Serve are more frequently used because they are shared among many requests","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"2) Ray Serve, unlike the web server approach, doesn't require you to load all the models into memory for each web server pod.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"These two factors combined lead to more efficient utilization of memory which results in a higher CPU cache hit rate for each model. Using a back of envelope calculation, we can estimate the Ray Serve deployment model reduces memory usage by at least 3x. With dynamic batching, we anticipate saving even more memory because it should increase the throughput of a single replica. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Serve can saturate the pipeline with requests, no component is idle","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"41lwJ7UgSFvQqK30D3nQMV","type":"Asset","createdAt":"2021-10-19T05:03:06.717Z","updatedAt":"2021-10-19T05:03:06.717Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"5_Ray Serve ensures no component is idle when serving requests.","description":"Ray Serve ensures no component is idle when serving requests.","file":{"url":"//images.ctfassets.net/xjan103pcp94/41lwJ7UgSFvQqK30D3nQMV/c565d966b4ab506512924e8121f13865/5.png","details":{"size":507455,"image":{"width":1550,"height":1002}},"fileName":"5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.cs.bu.edu/~best/crs/cs551/lectures/lecture-20.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Latency hiding","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" comes from the fact that each web server can only handle one request at a time. while the server is getting user features from DynamoDB, the models must be idle; while the models are running through computation, new requests must wait. With Ray Serve, each step of the pipeline can be kept busy. Each step of the pipeline can process requests concurrently; while some components are busy with a group of requests, other components can be busy too. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Serve loads fewer model in memory and keep them busy: good for cache efficiency","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"KxTGBO3IhBYV4HnbmrQPq","type":"Asset","createdAt":"2021-10-19T05:06:20.616Z","updatedAt":"2021-10-19T05:06:20.616Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"6_The live traffic pattern is variable and predictable. Serveâs replica autoscaling drives the cost down further during idle time.","description":"The live traffic pattern is variable and predictable. Serveâs replica autoscaling drives the cost down further during idle time.","file":{"url":"//images.ctfassets.net/xjan103pcp94/KxTGBO3IhBYV4HnbmrQPq/451e8d1db5f60332ac92cb2e67e8258f/6.png","details":{"size":456033,"image":{"width":1526,"height":730}},"fileName":"6.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Lastly, the requested traffic is not constant as it goes up and down because users log in and out of the games. Ray Serve provides mechanisms for autoscaling to shrink the size of deployment and save resource usage when the traffic is low.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2p16B3QCebVsKsin4z68Ff","type":"Asset","createdAt":"2021-10-19T05:06:59.636Z","updatedAt":"2021-10-19T05:06:59.636Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"7_Comparing tail latency of the new Ray Serve system (blue) to the existing system (red) with live production traffic, Ray Serve is 3x faster than the existing system. ","description":"Comparing tail latency of the new Ray Serve system (blue) to the existing system (red) with live production traffic, Ray Serve is 3x faster than the existing system. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/2p16B3QCebVsKsin4z68Ff/450c7a3b06fcb6400f63cf60e8c1dfc1/7.png","details":{"size":311401,"image":{"width":1598,"height":1090}},"fileName":"7.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The Ray Serve version of the model ensemble services was deployed to serve production traffic and it showed visible drop in p95 latency. Because of this, the Wildlife team has been moving more models into Ray Serve for production.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"From a technical perspective,Â  the key takeaway from this post is that Ray Serve makes it easy to doÂ  âmodel disaggregation and model compositionâ. That is splitting models into separate components and running those in parallel. Ray Serve enables scaling at each part of the request pipeline separately. The reasons behind saving resources while keep latency down was due to","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"(1) better cache efficiency because there were fewer models residing in memory.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"(2) pipelining inference allowed both preprocessing and models to be busy at the same time.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"(3) autoscaling which resulted in less resources being used when traffic was low, instead of always provisioning for the peak.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3USZJ31O6NYlFYxgeUT1yC","type":"Asset","createdAt":"2021-10-19T15:45:52.565Z","updatedAt":"2021-10-19T15:45:52.565Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"The live traffic pattern is variable and predictable. Serveâs replica autoscaling drives the cost down further during idle time.","description":"The live traffic pattern is variable and predictable. Serveâs replica autoscaling drives the cost down further during idle time.","file":{"url":"//images.ctfassets.net/xjan103pcp94/3USZJ31O6NYlFYxgeUT1yC/d04b6035b7131b87694c4459d3c1a99c/6.png","details":{"size":456033,"image":{"width":1526,"height":730}},"fileName":"6.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}}],"hideIntro":true,"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2HvnSZK5ssuHHocJvhm04l","type":"Entry","createdAt":"2021-10-01T16:29:03.786Z","updatedAt":"2022-06-22T16:21:03.872Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Serving ML Models in Production: Common Patterns","slug":"serving-ml-models-in-production-common-patterns","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"kQhxB9LxeWeB3noKuPXiJ","type":"Entry","createdAt":"2020-09-20T16:36:33.304Z","updatedAt":"2020-09-20T16:36:33.304Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Simon Mo","slug":"simon-mo","bio":"Simon Mo is a software engineer at Anyscale. Before Anyscale, he was a student at UC Berkeley participating in research at the RISELab. He focuses on studying and building systems for machine learning, in particular, how to make ML model serving systems more efficient, ergonomic, and scalable. He works on Ray Serve at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"fy7QSQIwnpMZWvM1ndmhx","type":"Asset","createdAt":"2020-09-20T16:36:28.219Z","updatedAt":"2020-09-20T16:36:28.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Simon Mo headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/fy7QSQIwnpMZWvM1ndmhx/cdef7ccbc81d0ea4903ed73336e42b48/Simon-Mo-headshot.png","details":{"size":365363,"image":{"width":550,"height":550}},"fileName":"Simon-Mo-headshot.png","contentType":"image/png"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7cbdj2z67h6KL6mmQH7yNJ","type":"Entry","createdAt":"2021-09-30T22:00:29.490Z","updatedAt":"2021-09-30T22:00:29.490Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Edward Oakes","slug":"edward-oakes","link":"https://www.linkedin.com/in/edward-oakes-8bb3b0a6/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-10-01","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Over the past couple years, we've listened to ML practitioners across many different industries to learn and improve the tooling around ML production use cases. Through this, we've seen 4 common patterns of machine learning in production: pipeline, ensemble, business logic, and online learning. In the ML serving space, implementing these patterns typically involves a tradeoff between ease of development and production readiness. Ray Serve was built to support these patterns by being both easy to develop and production ready.","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4xzqeZGkvoF6cBdEauda1I","type":"Entry","createdAt":"2021-09-30T22:02:40.713Z","updatedAt":"2021-09-30T22:02:40.713Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Serving ML Models in Production: Common Patterns","videoUrl":" https://www.youtube.com/watch?v=mM4hJLelzSw","caption":"This post is based on Simon Moâs âPatterns of Machine Learning in Productionâ [talk](https://www.youtube.com/watch?v=mM4hJLelzSw \"Serving ML Models in Production: Common Patterns\") from Ray Summit 2021. "}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Over the past couple years, we've listened to ML practitioners across many different industries to learn and improve the tooling around ML production use cases. Through this, we've seen 4 common patterns of machine learning in production: pipeline, ensemble, business logic, and online learning. In the ML serving space, implementing these patterns typically involves a tradeoff between ease of development and production readiness. ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" was built to support these patterns by being both easy to develop and production ready. It is a scalable and programmable serving framework built on top of ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to help you scale your microservices and ML models in production.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This post goes over:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"What is Ray Serve","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Where Ray Serve fits in the ML Serving Space","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Some common patterns of ML in production ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"How to implement these patterns using Ray Serve","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What is Ray Serve?\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1mw8vMERbD818wGhan6npt","type":"Asset","createdAt":"2021-09-30T22:13:54.942Z","updatedAt":"2021-09-30T23:58:30.276Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"Ray Ecosystem Serving ML Models in Production: Common Patterns","description":"Ray Serve is built on top of the Ray distributed computing platform, allowing it to easily scale to many machines, both in your datacenter and in the cloud. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/1mw8vMERbD818wGhan6npt/36b4a626338057b92520f43d85547154/RayEcosystem.png","details":{"size":494592,"image":{"width":2162,"height":918}},"fileName":"RayEcosystem.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve is an easy-to-use scalable model serving library built on Ray. Some advantages of the library include:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scalability: Horizontally scale across hundreds of processes or machines, while keeping the overhead in single-digit milliseconds","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Multi-model composition: Easily compose multiple models, mix model serving with business logic, and independently scale components, without complex microservices.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/batch.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Batching","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": Native support for batching requests to better utilize hardware and improve throughput.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/http-servehandle.html#serve-fastapi-http"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FastAPI Integration","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":":Â Scale an existing FastAPI server easily or define an HTTP interface for your model using its simple, elegant API.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Framework-agnostic: Use a single toolkit to serve everything from deep learning models built with frameworks like ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/serve/tutorials/pytorch.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Tensorflow and Keras","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", to ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/sklearn.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Scikit-Learn models","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-ikigai-labs-serves-interactive-ai-workflows-at-scale-using-ray-serve"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"arbitrary Python business logic","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can get started with Ray Serve by checking out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve Quickstart.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Where Ray Serve fits in the ML Serving Space","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6IcTIir1U1WBJdSbdygQ08","type":"Asset","createdAt":"2021-09-30T22:18:13.805Z","updatedAt":"2021-09-30T22:18:13.805Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"WhereRayServeFitsIn","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6IcTIir1U1WBJdSbdygQ08/70ceeb0e4f5c8b72b7007c61cb19eed8/WhereRayServeFitsIn.png","details":{"size":92883,"image":{"width":768,"height":444}},"fileName":"WhereRayServeFitsIn.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The image above shows that In the ML serving space, there is typically a tradeoff between ease of development and production readiness.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Web Frameworks","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To deploy a ML service, people typically start with the simplest systems out of the box like Flask or FastAPI. However, even though they can deliver a single prediction well and work well in proofs of concept, they cannot achieve high performance and scaling up is often costly.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Custom Tooling","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If web frameworks fail, teams typically transition to some sort of custom tooling by gluing together several tools to make the system ready for production. However, these custom toolings are typically hard to develop, deploy, and manage.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Specialized Systems","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There is a group of specialized systems for deploying and managing ML models in production. While these systems are great at managing and serving ML models, they often have less flexibility than web frameworks and often have a high learning curve.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve is a web framework specialized for ML model serving. It aspires to be easy to use, easy to deploy, and production ready.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What makes Ray Serve Different?","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"fZRYjZC4BoGvXCytyN3CQ","type":"Asset","createdAt":"2021-09-30T22:25:08.320Z","updatedAt":"2021-09-30T22:25:08.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Many Tools Run 1 Model Well","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/fZRYjZC4BoGvXCytyN3CQ/6e8fc978f76159db3638e3f98b30b0bf/ManyToolsRun1ModelWell.png","details":{"size":131094,"image":{"width":1004,"height":416}},"fileName":"ManyToolsRun1ModelWell.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are so many tools for training and serving one model. These tools help you run and deploy one model very well. The problem is that machine learning in real life is usually not that simple. In a production setting, you can encounter problems like:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Wrangling with infrastructure to scale beyond one copy of a model.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Having to work through complex YAML configuration files, learn custom tooling, and develop MLOps expertise.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Hit scalability or performance issues, unable to deliver business SLA objectives.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Many tools are very costly and can often lead to underutilization of resources. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling out a single model is hard enough. For many ML in production use cases, we observed that complex workloads require composing many different models together. Ray Serve is natively built for this kind of use case involving many models spanning multiple nodes. You can check out ","nodeType":"text"},{"data":{"uri":"https://youtu.be/mM4hJLelzSw?t=651"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"this part of the talk","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" where we go in depth about Ray Serveâs architectural components.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Patterns of ML Models in Production ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4RxLx15blgVAW8gQwQx8w9","type":"Asset","createdAt":"2021-09-30T22:27:41.224Z","updatedAt":"2021-09-30T22:27:41.224Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"13PatternsMLProduction","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4RxLx15blgVAW8gQwQx8w9/831258e5b2d865cba9bba4c00518768a/13PatternsMLProduction.png","details":{"size":64775,"image":{"width":882,"height":426}},"fileName":"13PatternsMLProduction.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A significant portion of ML applications in production follow 4 model patterns:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"pipeline","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"ensemble","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"business logic","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"online learningÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"This section will describe each of these patterns, show how they are used, go over how existing tools typically implement them, and show how Ray Serve can solve these challenges.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pipeline Pattern\n","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4JZHXITWwYcUhGidslJ14A","type":"Asset","createdAt":"2021-09-30T22:30:20.128Z","updatedAt":"2021-09-30T22:30:20.128Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ATypicalComputerVisionPipeline","description":"A typical computer vision pipeline","file":{"url":"//images.ctfassets.net/xjan103pcp94/4JZHXITWwYcUhGidslJ14A/a4bf91ad39086863ee3f31bbe2f9e038/ATypicalComputerVisionPipeline.png","details":{"size":451936,"image":{"width":1476,"height":452}},"fileName":"ATypicalComputerVisionPipeline.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The image above shows a typical computer vision pipeline that uses multiple deep learning models to caption the object in the picture. This pipeline consists of the following steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"1) The raw image goes through common preprocessing like image decoding, augmentation and clipping.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"2) A detection classifier model is used to identify the bounding box and the category. It's a cat.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"3) The image is passed into a keypoint detection model to identify the posture of the object. For the cat image, the model could identify key points like paws, neck, and head.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"4) Lastly, an NLP synthesis model generates a category of what the picture shows. In this case, a standing cat.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical pipeline rarely consists of just one model. To tackle real-life issues, ML applications often use many different models to perform even simple tasks. In general, pipelines break a specific task into many steps, where each step is conquered by a machine learning algorithm or some procedure. Letâs now go over a couple pipelines you might already be familiar with. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Scikit-Learn Pipeline","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"Pipeline([(âscalerâ, StandardScaler()), (âsvcâ, SVC())])","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"scikit-learnâs pipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" can be used to combine multiple âmodelsâ and âprocessing objectsâ together. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Recommendation Systems","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"[EmbeddingLookup(), FeatureInteraction(), NearestNeighbors(), Ranking()] ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are common pipeline patterns in recommendation systems. Item and video recommendations like those that you might see at ","nodeType":"text"},{"data":{"uri":"https://www.amazon.science/publications/two-decades-of-recommender-systems-at-amazon-com"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Amazon","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://research.google/pubs/pub45530/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"YouTube","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", respectively,Â  typically go through multiple stages like embedding lookup, feature interaction, nearest neighbor models, and ranking models.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Common Preprocessing","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"[HeavyWeightMLMegaModel(), DecisionTree()/BoostingModel()]","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are some very common use cases where some massive ML models are used to take care of common processing for text or images. For example, at Facebook, groups of ML researchers at ","nodeType":"text"},{"data":{"uri":"https://ai.facebook.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FAIR","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" create state of the art heavyweight models for vision and text. Then different product groups create downstream models to tackle their business use case (e.g. ","nodeType":"text"},{"data":{"uri":"https://ai.facebook.com/blog/under-the-hood-suicide-prevention-tools-powered-by-ai/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"suicide prevention","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") by implementing smaller models using random forest. The shared common preprocessing step oftentimes are materialized into a ","nodeType":"text"},{"data":{"uri":"https://www.tecton.ai/blog/what-is-a-feature-store/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"feature store pipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"General Pipeline Implementation Options","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2h69wovKC65iUOj3CSs7cu","type":"Asset","createdAt":"2021-09-30T22:35:27.708Z","updatedAt":"2021-09-30T22:35:27.708Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"17PipelineImplementation","description":"Before Ray Serve, implementing pipelines generally meant you had to choose between wrapping your models in a web server or using many specialized microservices. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/2h69wovKC65iUOj3CSs7cu/8462545f5593c27d61c7151fd069a56b/17PipelineImplementation.png","details":{"size":106807,"image":{"width":840,"height":414}},"fileName":"17PipelineImplementation.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In general, there are two approaches to implement a pipeline: wrap your models in a web server or use many specialized microservices.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Wrap Models in a Web Server","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The left side of the image above shows models that get run in a for loop during the web handling path. Whenever a request comes in, models get loaded (they can also be cached) and run through the pipeline. While this is simple and easy to implement, a major flaw is that this is hard to scale and not performant because each request gets handled sequentially.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Many Specialized Microservices","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The right side of the image above shows many specialized microservices where you essentially build and deploy one microservice per model. These microservices can be native ML platforms, ","nodeType":"text"},{"data":{"uri":"https://www.kubeflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Kubeflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", or even hosted services like AWS ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/sagemaker/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". However, as the number of models grow, the complexity and operational cost drastically increases. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Implementing Pipelines in Ray Serve","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11KQrEzA1s2n4S3SUKuH28","type":"Entry","createdAt":"2021-09-30T22:37:57.811Z","updatedAt":"2021-10-01T00:09:45.131Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"implementingPipelinesInRayServe","body":"@serve.deployment\nclass Featurizer: â¦\n\n@serve.deployment\nclass Predictor: â¦\n\n@serve.deployment\nclass Orchestrator\n   def __init__(self):\n      self.featurizer = Featurizer.get_handle()\n      self.predictor = Predictor.get_handle()\n\n   async def __call__(self, inp):\n      feat = await self.featurizer.remote(inp)\n      predicted = await self.predictor.remote(feat)\n      return predicted\n\nif __name__ == â__main__â:\n    Featurizer.deploy()\n    Predictor.deploy()\n    Orchestrator.deploy()","language":"python","caption":"Pseudocode showing how Ray Serve allows deployments to call other deployments"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In Ray Serve, you can directly call other deployments within your deployment.Â  In this code above, there are three deployments. ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Featurizer","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Predictor","nodeType":"text"},{"data":{},"marks":[],"value":" are just regular deployments containing the models. The ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Orchestrator","nodeType":"text"},{"data":{},"marks":[],"value":" receives the web input, passes it to the featurizer process via the featurizer handle, and then passes the computed feature to the predictor process. The interface is just Python and you donât need to learn any new framework or domain-specific language.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve achieves this with a mechanism called ServeHandle which gives you a similar flexibility to embed everything in the web server, without sacrificing performance or scalability. It allows you to directly call other deployments that live in other processes on other nodes. This allows you to scale out each deployment individually and load balance calls across the replicas.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you would like to get a deeper understanding of how this works, ","nodeType":"text"},{"data":{"uri":"https://youtu.be/mM4hJLelzSw?t=650"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"check out this section of Simon Moâs talk","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to learn about Ray Serveâs architecture. If you would like an example of a computer vision pipeline in production, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures#:~:text=A%20Tool%20for%20High%20Performance%20ML%20Systems"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"check out how Robovision used 5 ML models for vehicle detection","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ensemble Pattern","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3XfSBLbvH4TgteNQhSV3jC","type":"Asset","createdAt":"2021-09-30T22:39:02.077Z","updatedAt":"2021-09-30T22:39:02.077Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ensemble Pattern","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3XfSBLbvH4TgteNQhSV3jC/9754f37495ab90cbf5943494e2e13e7a/25Ensemble.png","details":{"size":32706,"image":{"width":730,"height":352}},"fileName":"25Ensemble.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a lot of production use cases, a pipeline is appropriate. However, one limitation of pipelines is that there can often be many upstream models for a given downstream model. This is where ensembles are useful. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"41I4eZnNsSRrneS0QO59Ss","type":"Asset","createdAt":"2021-09-30T22:41:17.502Z","updatedAt":"2021-09-30T22:41:17.502Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Serve Ensemble Usecase","description":"Ensemble Use Cases","file":{"url":"//images.ctfassets.net/xjan103pcp94/41I4eZnNsSRrneS0QO59Ss/aa86f27d79ef6a431743f8a8349a0343/26RayServeEnsembleUsecase.png","details":{"size":139926,"image":{"width":1364,"height":350}},"fileName":"26RayServeEnsembleUsecase.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ensemble patterns involve mixing output from one or more models. They are also called model stacking in some cases. Below are three use cases of ensemble patterns.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Model Update","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"New models are developed and trained over time. This means there will always be new versions of the model in production. The question becomes, how do you make sure the new models are valid and performant in live online traffic scenarios? One way to do this is by putting some portion of the traffic through the new model. You still select the output from the known good model, but you are also collecting live output from the newer version of the models in order to validate it.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Aggregation","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The most widely known use case is for aggregation. For regression models, outputs from multiple models are averaged. For classification models, the output will be a voted version of multiple modelsâ output. For example, if two models vote for cat and one model votes for dog, then the aggregated output will be cat. Aggregation helps combat inaccuracy in individual models and generally makes the output more accurate and âsaferâ.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dynamic Selection","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another use case for ensemble models is to dynamically perform model selection given input attributes. For example, if the input contains a cat, model A will be used because it is specialized for cats.Â  If the input contains a dog, model B will be used because it is specialized for dogs. Note that this dynamic selection doesnât necessarily mean the pipeline itself has to be static. It could also be selecting models given user feedback. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"General Ensemble Implementation Options","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"58Z7G6vwRfpDBSIepG0fQZ","type":"Asset","createdAt":"2021-09-30T23:40:47.037Z","updatedAt":"2021-09-30T23:41:09.505Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"General Ensemble Implementation","description":"Before Ray Serve, implementing ensembles generally meant you had to choose between wrapping your models in a web server or using many specialized microservices. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/58Z7G6vwRfpDBSIepG0fQZ/7e666548b9cef471ae4194991784f455/27EnsembleDeployment.png","details":{"size":167575,"image":{"width":1332,"height":510}},"fileName":"27EnsembleDeployment.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ensemble implementations suffer the same sort of issues as pipelines. It is simple to wrap models in a web server, but it is not performant. When you use specialized microservices, you end up having a lot of operational overhead as the number of microservices scale with the number of models. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71qMXFLqM4EeyAIHCMnVzY","type":"Asset","createdAt":"2021-09-30T23:44:12.374Z","updatedAt":"2021-09-30T23:44:12.374Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2020 Anyscale demo","description":"Ensemble example from the [2020 Anyscale demo](https://youtu.be/8GTd8Y_JGTQ)","file":{"url":"//images.ctfassets.net/xjan103pcp94/71qMXFLqM4EeyAIHCMnVzY/a76ac67159a965c670a53a944404b39a/28RayServeEnsembleRaySummit.png","details":{"size":371197,"image":{"width":1341,"height":600}},"fileName":"28RayServeEnsembleRaySummit.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray Serve, the kind of pattern is incredibly simple. You can look at the ","nodeType":"text"},{"data":{"uri":"https://youtu.be/8GTd8Y_JGTQ"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"2020 Anyscale demo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to see how to utilize Ray Serveâs handle mechanism to perform dynamic model selection.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another example of using Ray Serve for ensembling is Wildlife Studios combining output of many classifiers for a single prediction. You can check out how they were able to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/wildlife-studios-serves-in-game-offers-3x-faster-at-1-10th-the-cost-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"serve in-game offers 3x faster with Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Business Logic Pattern","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Productionizing machine learning will always involve business logic. No models can stand-alone and serve requests by themselves. Business logic patterns involve everything thatâs involved in a common ML task that is not ML model inference. This includes:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Database lookups for relational records","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Web API calls for external services","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Feature store lookup for pre-compute feature vectors","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Feature transformations like data validation, encoding, and decoding.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"General Business Logic Implementation Options","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ixwTFsSoaWNza9Mp3OPjj","type":"Asset","createdAt":"2021-09-30T23:45:47.219Z","updatedAt":"2021-09-30T23:45:47.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"General Business Logic Implementation Options","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6ixwTFsSoaWNza9Mp3OPjj/8144cd2bea5d42a6333ad1d1f55270c0/31BusinessLogicInAction.png","details":{"size":111620,"image":{"width":984,"height":462}},"fileName":"31BusinessLogicInAction.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The pseudocode for the web handler above does the following things:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"It loads the model (letâs say from S3)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Validates the input from the database","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Looks up some pre-computed features from the feature store.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Only after the web handler completes these business logic steps are the inputs passed through to ML models. The problem is that the requirements of model inference and business logic lead to the server being ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"both network bounded and compute bounded","nodeType":"text"},{"data":{},"marks":[],"value":". This is due to the model loading step, database lookup, and feature store lookups being network bounded and I/O heavy as well as the model inference being compute bound and memory hungry. The combination of these factors lead to an inefficient utilization of resources. Scaling will be expensive.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2tzQhijrmJQ1i1dtp8SeF","type":"Asset","createdAt":"2021-09-30T23:47:03.551Z","updatedAt":"2021-09-30T23:47:03.551Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Web handler approach (left) and microservices approach (right)","description":"Web handler approach (left) and microservices approach (right)","file":{"url":"//images.ctfassets.net/xjan103pcp94/2tzQhijrmJQ1i1dtp8SeF/de61106c82ef5dd503d6200ecdaebd30/32WheretoRunBusinessLogic.png","details":{"size":80148,"image":{"width":1337,"height":464}},"fileName":"32WheretoRunBusinessLogic.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A common way to increase utilization is to split models out into model servers or microservices.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The web app is purely network bounded while the model servers are compute bounded. However, a common problem is the interface between the two. If you put too much business logic into the model server, then the model servers become a mix of network bounded and compute bounded calls.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you let the model servers be pure model servers, then you have the â","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"tensor-in, tensor-out","nodeType":"text"},{"data":{},"marks":[],"value":"â interface problem. The input types for model servers are typically very constrained to just tensors or some alternate form of it. This makes it hard to keep the pre-processing, post-processing, and business logic in sync with the model itself.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"It becomes hard to reason about the interaction between the processing logic and the model itself because during training, the processing logic and models are tightly coupled, but when serving, they are split across two servers and two implementations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Neither the web handler approach nor the microservices approach is satisfactory.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Implementing Business Logic in Ray Serve","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4WEB4OkdjgTl7azrJctx4A","type":"Asset","createdAt":"2021-09-30T23:48:49.625Z","updatedAt":"2021-09-30T23:48:49.625Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Business Logic in Ray Serve","description":"Business Logic in Ray Serve","file":{"url":"//images.ctfassets.net/xjan103pcp94/4WEB4OkdjgTl7azrJctx4A/703ef612cc6e50aaa48433deef6ba985/33BusinessLogicInRayServe.png","details":{"size":114370,"image":{"width":856,"height":398}},"fileName":"33BusinessLogicInRayServe.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray Serve, you just have to make some simple changes to the old web server to alleviate the issues described above. Instead of loading the model directly, you can retrieve a ServeHandle that wraps the model, and offload the computation to another deployment. All the data types are preserved and there is no need to write âtensor-in, tensor-outâ API calls--you can just pass in regular Python types. Additionally, the model deployment class can stay in the same file, and be deployed together with the prediction handler. This makes it easy to understand and debug the code.Â ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"model.remote","nodeType":"text"},{"data":{},"marks":[],"value":" looks like just a function and you can easily trace it to the model deployment class.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this way, Ray Serve helps you split up the business logic and inference into two separation components, one I/O heavy and the other compute heavy. This allows you to scale each piece individually, without losing the ease of deployment. Additionally, because ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"model.remote","nodeType":"text"},{"data":{},"marks":[],"value":" is just a function call, itâs a lot easier to test and debug than separate external services.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve FastAPI Integration","nodeType":"text"}],"nodeType":"heading-3"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"HStDeyRVrI7LM2WdUP2s6","type":"Asset","createdAt":"2021-09-30T23:50:19.687Z","updatedAt":"2021-09-30T23:50:19.687Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Serve: Ingress with FastAPI","description":"Ray Serve: Ingress with FastAPI","file":{"url":"//images.ctfassets.net/xjan103pcp94/HStDeyRVrI7LM2WdUP2s6/4ff62672c24788b1d07d59736b85817b/34RayServeIngress.png","details":{"size":297542,"image":{"width":1656,"height":838}},"fileName":"34RayServeIngress.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"An important part of implementing business logic and other patterns is authentication and input validation. ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/http-servehandle.html#serve-fastapi-http"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve natively integrates with FastAPI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which is a type safe and ergonomic web framework. ","nodeType":"text"},{"data":{"uri":"https://fastapi.tiangolo.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FastAPI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" has features like automatic dependency injection, type checking and validation, and OpenAPI doc generation.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray Serve, you can directly pass the FastAPI app object into it with ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"@serve.ingress","nodeType":"text"},{"data":{},"marks":[],"value":". This decorator makes sure that all existing FastAPI routes still work and that you can attach new routes with the deployment class so states like loaded models, and networked database connections can easily be managed. Architecturally, we just made sure that your FastAPI app is correctly embedded into the replica actor and the FastAPI app can scale out across many Ray nodes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Online LearningÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Online learning is an emerging pattern thatâs become more and more widely used. It refers to a model running in production that is constantly being updated, trained, validated and deployed. Below are three use cases of online learning patterns.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dynamically Learn Model Weights","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are use cases for dynamically learning model weights online. As users interact with your services, these updated model weights can contribute to a personalized model for each user or group. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6J7nynhCYa2vV5Ud57cn0m","type":"Asset","createdAt":"2021-09-30T23:53:34.472Z","updatedAt":"2021-09-30T23:53:34.472Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Online Learning example at Ant Group (image courtesy of Ant Group) ","description":"[Online Learning example at Ant Group](https://www.anyscale.com/blog/online-resource-allocation-with-ray-at-ant-group) (image courtesy of Ant Group) ","file":{"url":"//images.ctfassets.net/xjan103pcp94/6J7nynhCYa2vV5Ud57cn0m/0362b38997ac5c4e8c1db756cf4a1dcd/38OnlineLearningAntGroup.png","details":{"size":311903,"image":{"width":1208,"height":533}},"fileName":"38OnlineLearningAntGroup.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"One case study of online learning consists of an online resource allocation business solution at Ant Group. The model is trained from offline data, then combined with real time streaming data source, and then served live traffic. One thing to note is that online learning systems are drastically more complex than their static serving counterparts. In this case, putting models in the web server, or even splitting it up into multiple microservices, would not help with the implementation. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Dynamically Learn Parameters to Orchestrate Models","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are also use cases for learning parameters to orchestrate or compose models, for example, ","nodeType":"text"},{"data":{"uri":"https://dl.acm.org/doi/fullHtml/10.1145/3383313.3411550"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"learning which model a user prefers","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". This manifests often in model selection scenarios or contextual bandit algorithms.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Reinforcement Learning","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Reinforcement learning is the branch of machine learning that trains agents to interact with the environment. The environment can be the physical world or a simulated environment. You can learn about reinforcement learning ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and see how you can deploy a RL model using Ray Serve ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/tutorials/rllib.html#serve-rllib-tutorial"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â  ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ConclusionÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Zmo0bB3BCXWqX3bfhtxlg","type":"Asset","createdAt":"2021-09-30T23:55:14.299Z","updatedAt":"2021-09-30T23:55:14.299Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"WhereRayServeFitsIn","description":"Ray Serve is easy to develop and production ready.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Zmo0bB3BCXWqX3bfhtxlg/f6a8378f2cbb372c458904651e7bd3c1/WhereRayServeFitsIn.png","details":{"size":92883,"image":{"width":768,"height":444}},"fileName":"WhereRayServeFitsIn.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This post went over 4 main patterns of machine learning in production, how Ray Serve can help you natively scale and work with complex architectures, and how ML in production often means many models in production. Ray Serve is built with all of this in mind on top of the distributed runtime Ray. If youâre interested in learning more about Ray, you can check out the ","nodeType":"text"},{"data":{"uri":"https://ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", join us on ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discourse","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and check out the ","nodeType":"text"},{"data":{"uri":"http://tinyurl.com/ray-white-paper"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"whitepaper","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"! If you're interested in working with us to make it easier to leverage Ray, we're ","nodeType":"text"},{"data":{"uri":"https://jobs.lever.co/anyscale"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"hiring","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"!","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"BBDpqdGlEcQZHPoQxytXf","type":"Asset","createdAt":"2021-10-01T00:19:41.125Z","updatedAt":"2021-10-01T00:19:41.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Where Ray Serve Fits In","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/BBDpqdGlEcQZHPoQxytXf/7dff7d7f06b7a5d55cfd255cc3cd88e6/WhereRayServeFitsIn.png","details":{"size":92883,"image":{"width":768,"height":444}},"fileName":"WhereRayServeFitsIn.png","contentType":"image/png"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Zk7jixFrLqHf3IJLzrO2B","type":"Entry","createdAt":"2022-05-03T18:47:31.422Z","updatedAt":"2022-05-03T18:47:31.422Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Ray Summit: Aug 23-24 in SF!","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5LTWfPAScfUtzXj2KOUALz","type":"Entry","createdAt":"2022-04-12T15:29:42.674Z","updatedAt":"2022-10-04T22:06:07.976Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":17,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"summit"}},"locale":"en-US"},"fields":{"title":"Ray Summit 2022","slug":"ray-summit-2022","description":"Centered on Ray, the open-source distributed computing framework, Ray Summit brings together ML practitioners, data scientists, and more to discuss the growing demand for distributed computing.","sections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xIJZXdAqhOEizUCUuQaiV","type":"Entry","createdAt":"2022-03-31T16:04:49.746Z","updatedAt":"2022-09-12T20:58:36.991Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":20,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-hero","header":"The Place for _Everything Ray_","body":"Missed us in San Francisco for Ray Summit 2022? Don't fret, you can watch your favorite sessions on demand! ","ctaText":"View Sessions","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"9F9S7NLaIdPL4ThYwVlFW","type":"Asset","createdAt":"2022-09-06T14:49:38.936Z","updatedAt":"2022-09-06T14:53:02.350Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Presented by Anyscale","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/9F9S7NLaIdPL4ThYwVlFW/8ecd09b334942298f0b8c2d5567ec032/presented-by-anyscale.svg","details":{"size":22813,"image":{"width":289,"height":32}},"fileName":"presented-by-anyscale.svg","contentType":"image/svg+xml"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4A0d4TUfp0TCqUA60CXopR","type":"Asset","createdAt":"2022-09-06T14:53:48.720Z","updatedAt":"2022-09-06T14:53:48.720Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4A0d4TUfp0TCqUA60CXopR/2e4240c268d074d840f7a00d069a768a/rs-collage_2x.jpg","details":{"size":568547,"image":{"width":2400,"height":1376}},"fileName":"rs-collage@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6FGUP0cXunTbgs2UemeAU","type":"Asset","createdAt":"2022-09-06T14:54:09.975Z","updatedAt":"2022-09-06T14:54:09.975Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit collage of speakers from event, stacked vertically","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6FGUP0cXunTbgs2UemeAU/16ed175024bc9cc0a2fae3cb13538dff/rs-collage-mobile_2x.jpg","details":{"size":243282,"image":{"width":750,"height":1420}},"fileName":"rs-collage-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4RKa7qVhWoIaoNSa1ZH2SK"}}],"type":"2022 Summit Hero","styling":["transparent-header"]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6GL7ZzuSLadWdVFKHj7yIH","type":"Entry","createdAt":"2022-04-20T18:00:12.964Z","updatedAt":"2022-08-08T22:50:29.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-keynote-speakers","header":"Keynote Speakers","ctaText":"See all speakers","ctaLink":"/ray-summit-2022/speakers","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6VhewtcICU9SCXjwd5qhHJ"}}],"type":"2022 Summit Speakers","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JTi5fsyo0Q3YrF3SAQglg","type":"Entry","createdAt":"2022-08-08T22:49:55.163Z","updatedAt":"2022-08-09T07:03:38.615Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-ray-summit-selected-speakers","header":"Highlighted Talks","ctaText":"Full Agenda","ctaLink":"/ray-summit-2022/agenda","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"50eS4cKXBNE4GHezdZ8Rws"}},{"sys":{"type":"Link","linkType":"Entry","id":"5iioKfOgEqVzgQ1DMmA1Wy"}},{"sys":{"type":"Link","linkType":"Entry","id":"YCBUZOXJGB0qcJF8TKjfI"}}],"type":"2022 Summit Agenda Preview"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5cgHuoJ713DewhVY6iUYrG","type":"Entry","createdAt":"2022-05-03T21:57:45.147Z","updatedAt":"2022-05-09T13:41:03.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-trainings","header":"Exclusive Ray Training Sessions","body":"Ray Summit is a great opportunity to level up your Ray skills through hands-on training sessions with the builders and maintainers of Ray. Weâre offering several pre-conference training sessions on August 22. Space is limited â [reserve your spot now](https://www.myeventi.events/raysummit22/attendee/)!","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3sYQTUmD5X5ncvGRFyIRR6","type":"Asset","createdAt":"2022-05-03T04:40:25.040Z","updatedAt":"2022-05-03T04:40:25.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image desktop","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3sYQTUmD5X5ncvGRFyIRR6/a77c675b07d63e73dd79ada9d182498a/training-image_2x.jpg","details":{"size":313509,"image":{"width":1136,"height":1480}},"fileName":"training-image@2x.jpg","contentType":"image/jpeg"}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7rsC56a4YCgENLyxsHyeIb","type":"Asset","createdAt":"2022-05-03T04:40:54.089Z","updatedAt":"2022-05-03T04:40:54.089Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Training image mobile","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7rsC56a4YCgENLyxsHyeIb/e2592c9b4424048b3c916fbbbc313065/training-image-mobile_2x.jpg","details":{"size":153801,"image":{"width":830,"height":874}},"fileName":"training-image-mobile@2x.jpg","contentType":"image/jpeg"}}}],"subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"4Hb3J9SrCFTDXqIhIdSIwa"}},{"sys":{"type":"Link","linkType":"Entry","id":"hZ4Tl1wvj7CwF00slwpLg"}},{"sys":{"type":"Link","linkType":"Entry","id":"YJ5KWyjg4NGf29NrfHcJX"}}],"type":"2022 Summit Trainings"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mRJQtuDbIhAuEbwLT2V1K","type":"Entry","createdAt":"2022-03-31T21:42:56.117Z","updatedAt":"2022-04-11T23:46:07.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-reasons-to-attend","header":"Why Attend?","body":"As the annual user conference for the [open source Ray community](https://www.ray.io/community), Ray Summit is a unique opportunity to enhance your Ray skills, build your professional network, and learn what's on the Ray roadmap.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"5f6gxk4a4ujE2y6FZ8ATQ5"}},{"sys":{"type":"Link","linkType":"Entry","id":"20QwyiOmkHCEOkMvp0Jhtk"}},{"sys":{"type":"Link","linkType":"Entry","id":"6QxivsuhhLoVSloRQs9ZlQ"}}],"type":"2022 Summit Content Tiles"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"15k0SGqB2ZKOUfAsM3JnyV","type":"Entry","createdAt":"2022-03-31T16:05:55.739Z","updatedAt":"2022-08-05T00:25:20.225Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":16,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-about","header":"Celebrate the Ray community","body":"Get ready for our epic community bash at the Art of Tinkering exhibit at the San Francisco Exploratorium on August 23 from 5:30 to 9:30 p.m. Come explore, play, and celebrate with the Ray Community. This is an exclusive opportunity to experience this amazing interactive exhibition before it closes in September.","ctaText":"Check out the agenda","ctaLink":"/ray-summit-2022/agenda","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UcZT4h1P7uFDz6cn6JH8l","type":"Asset","createdAt":"2022-06-02T20:37:22.299Z","updatedAt":"2022-06-02T20:37:22.299Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Man taking a photo on his phone","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2UcZT4h1P7uFDz6cn6JH8l/5040a9909f18ab015c27b220077f916e/taking-picture.jpg","details":{"size":107846,"image":{"width":980,"height":772}},"fileName":"taking-picture.jpg","contentType":"image/jpeg"}}}],"type":"2022 Summit Split Image","classNames":"gray_background"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4P8Jn0q7nBU05iD6DmcQCC","type":"Entry","createdAt":"2022-04-01T19:15:31.735Z","updatedAt":"2022-04-11T16:36:59.573Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-social-tiles","header":"Join the Conversation","body":"Ready to get involved in the Ray community before the conference? Ask a question in the forums. Open a pull request. Or share why youâre excited with the hashtag __#RaySummit__ on Twitter.","subsections":[{"sys":{"type":"Link","linkType":"Entry","id":"6OMTlXkJ1OHXNkEjyPrXTH"}},{"sys":{"type":"Link","linkType":"Entry","id":"7EE28ee1ATQtodoLloT6mg"}},{"sys":{"type":"Link","linkType":"Entry","id":"1CRAzFxndL37wiNg4x8WKV"}},{"sys":{"type":"Link","linkType":"Entry","id":"2qrec5rR8kabZxCHaPS7NH"}}],"type":"2022 Summit Social Tiles"}}],"headerLogo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70DIdxwT8XY96aw7vMUHOu","type":"Asset","createdAt":"2022-06-01T15:21:46.804Z","updatedAt":"2022-06-01T15:21:46.804Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo Dark Blue","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/70DIdxwT8XY96aw7vMUHOu/5ca0a8b7533c8df70bc0e032b889862a/rs-logo-nav.svg","details":{"size":9954,"image":{"width":108,"height":48}},"fileName":"rs-logo-nav.svg","contentType":"image/svg+xml"}}},"headerLogoWhite":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3qyLXKGgGfWhv4oPqYl0SE","type":"Asset","createdAt":"2022-04-20T21:50:58.774Z","updatedAt":"2022-04-20T21:50:58.774Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Summit 2022 Logo","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3qyLXKGgGfWhv4oPqYl0SE/c4dee9e7a344c39db11fefafbf255b5b/rs-logo-horizontal.svg","details":{"size":9975,"image":{"width":108,"height":48}},"fileName":"rs-logo-horizontal.svg","contentType":"image/svg+xml"}}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KUtRQbdqJo1RrWMrVyfEm","type":"Entry","createdAt":"2022-03-31T16:21:13.402Z","updatedAt":"2022-04-08T17:25:33.228Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Home","link":"/ray-summit-2022"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4MvQcRSkHLMqIJoO9O2cQe","type":"Entry","createdAt":"2022-03-31T16:17:27.336Z","updatedAt":"2022-09-12T16:59:51.221Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"On-Demand","link":"/ray-summit-2022/agenda","subLinks":[{"sys":{"type":"Link","linkType":"Entry","id":"3N9kRDz36tieTI4YGVWKBJ"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46B5diwEDDr3kB2XDoGfDP","type":"Entry","createdAt":"2022-05-11T17:13:42.174Z","updatedAt":"2022-05-11T17:13:42.174Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Speakers","link":"/ray-summit-2022/speakers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Dt5fy6A7rBpXdG7HeKMGd","type":"Entry","createdAt":"2022-03-31T16:17:43.592Z","updatedAt":"2022-03-31T16:17:43.592Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Sponsors","link":"/ray-summit-2022/sponsors"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SvRE9pnyq3BXUPGKRWx6W","type":"Entry","createdAt":"2022-06-03T19:29:31.009Z","updatedAt":"2022-06-03T19:29:31.009Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Who Attends","link":"/ray-summit-2022/who-attends"}}],"mainSponsors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"xMQIOU9n7gVS4mkX2Nlag","type":"Entry","createdAt":"2021-01-31T16:33:06.495Z","updatedAt":"2021-05-11T13:28:31.312Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Ray","url":"https://ray.io","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jLroqm98RCMLGASthyuPv","type":"Asset","createdAt":"2021-05-11T13:28:26.930Z","updatedAt":"2021-05-11T13:28:26.930Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/jLroqm98RCMLGASthyuPv/f76cc04827316de5d3f8b529f6a3f02c/Ray_BM.svg","details":{"size":4742,"image":{"width":93,"height":36}},"fileName":"Ray_BM.svg","contentType":"image/svg+xml"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"11syIHhtX8JajSlXHSHE1O","type":"Entry","createdAt":"2021-01-31T16:31:24.509Z","updatedAt":"2021-05-11T13:28:00.309Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"sponsor"}},"locale":"en-US"},"fields":{"name":"Anyscale","url":"https://www.anyscale.com","logo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UkDmTzmZHIOG7yxFP4sVL","type":"Asset","createdAt":"2021-05-11T13:27:55.219Z","updatedAt":"2021-05-11T13:27:55.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Anyscale","file":{"url":"//images.ctfassets.net/xjan103pcp94/5UkDmTzmZHIOG7yxFP4sVL/db10aebda9a9aad10903470781a6f2e6/ANSC_BM.svg","details":{"size":7431,"image":{"width":158,"height":36}},"fileName":"ANSC_BM.svg","contentType":"image/svg+xml"}}}}}],"footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cfwECWrOqJ5Fegr3YpReS","type":"Entry","createdAt":"2022-04-01T21:22:12.001Z","updatedAt":"2022-04-13T17:35:29.511Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"2022-summit-footer","body":"Â©Anyscale, Inc 2022\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/privacy-policy\"\u003ePrivacy Policy\u003c/a\u003e\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003ca target=\"_blank\" href=\"/code-of-conduct\"\u003eCode of Conduct\u003c/a\u003e"}},"recommendations":[]}},"ctaText":"Register now","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5nTlh9josVs5zpZqU3vTbn","type":"Asset","createdAt":"2022-05-03T18:47:25.125Z","updatedAt":"2022-05-03T18:47:25.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-ray-summit-2022","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5nTlh9josVs5zpZqU3vTbn/e66043f62ed4a4b53d7e182e50289ec3/blog-recommended-content-ray-summit-2022.png","details":{"size":17234,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-ray-summit-2022.png","contentType":"image/png"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"po86u7oYjYZvGsh15b2oO","type":"Entry","createdAt":"2022-03-24T22:42:36.065Z","updatedAt":"2022-03-24T22:42:36.065Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Considerations for deploying ML models in production","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4Shvq24HmMiYP6cT0dROLY","type":"Entry","createdAt":"2021-11-16T17:10:12.838Z","updatedAt":"2022-06-22T16:12:16.152Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Considerations for Deploying Machine Learning Models in Production","slug":"considerations-for-deploying-machine-learning-models-in-production","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}}],"publishedDate":"2021-11-16","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"A common grumble among data science or machine learning researchers or practitioners is that putting a model in production is difficult. As a result, some claim that a large percentage, 87%, of models never see the light of the day in production. \n\nâI have a model, I spent considerable time developing it on my laptop. Whatâs next? How do I get it into our production environment? What should I consider for my ML stack and tooling?â","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"A common grumble among data science or machine learning researchers or practitioners is that putting a model in production is difficult. As a result, some claim that a large percentage, 87%, of models never see the","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/why-90-percent-of-all-machine-learning-models-never-make-it-into-production-ce7e250d5a4a"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":" light of the day in production","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"âI have a model, I spent considerable time developing it on my laptop. Whatâs next? How do I get it into our production environment? What should I consider for my ML stack and tooling?âÂ ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"These questions frequently emerge at meetups or conferences, after talks on machine learning operations (MLOps). There is no singular panacea or silver bullet for this nascent field of ","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/mlops-practices-for-data-scientists-dbb01be45dd8"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLOps Best Practices","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" that attempts to address and remedy this crucial problem.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, there are some acceptable and common technical considerations and pitfalls to keep in mind when considering your ML stack and tools. In this first part of a series on putting ML models in production, weâll discuss some common considerations and common pitfalls for tooling and best practices and ML model serving patterns that are an essential part of your journey from model development to deployment in production.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Developing with Ease","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Consider your development environment first. Most data scientists or ML engineers invariably use their laptops for development, testing or debugging code. Because of simplicity, easy to access and install the latest ML libraries, practitioners overwhelmingly prefer laptops over clusters for development. We are spoiled by IDEs and syntax-highlighted editors for good reason.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Python developers like to customize their environments to match their staging environment, with library dependencies using conda or Python virtual environments. Ideally, as a best practice, if the same code developed on their laptop can run with minimal changes on a staging or production environment on the cluster, it immensely improves the end-to-end developer productivity.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Consider your laptop as a preferred choice of development environment, with the possibility of extending or syncing your code to the cluster environment in the cloud.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"0i5dwwrx5iEKmB1i1lCNY","type":"Asset","createdAt":"2021-11-05T17:52:21.850Z","updatedAt":"2021-11-10T03:26:31.917Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"LaptopDeploy","description":"[image source](https://www.kctv5.com/jobs/us-wages-jump-by-the-most-in-records-dating-back-20-years/article_b07e82f7-4d92-5739-876b-5a1fd9063263.html)","file":{"url":"//images.ctfassets.net/xjan103pcp94/0i5dwwrx5iEKmB1i1lCNY/d27552d814334ed1c5dc651a3479f8ad/laptop.png","details":{"size":592457,"image":{"width":1156,"height":690}},"fileName":"laptop.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Consideration Number #1","nodeType":"text"},{"data":{},"marks":[],"value":": ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"Use your laptop for development as a best practice","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Training at Scale and Tracking Model Experiments","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlike the traditional software development cycle, the model development cycle paradigm is different. A number of factors influence an ML modelâs success in production. First, the outcome of a model is measured by its metrics, such as an acceptable accuracy.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Second, achieving an accuracy that satisfies the business goal means experimentation with not only one model or ML library but many models and many ML libraries while tracking each experiment runs: metrics, parameters, artifacts, etc. As vital as accuracy is, so is a developerâs choice of ML libraries to experiment with.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7oiARsfqccrDvMD12jlWJe","type":"Asset","createdAt":"2021-11-05T18:13:20.173Z","updatedAt":"2021-11-05T18:13:20.173Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Traditional Software vs Machine Learning","description":"[image source](https://github.com/dmatrix/mlflow-workshop-part-1/blob/master/slides/mlflow-workshop-series-part-1.pdf)","file":{"url":"//images.ctfassets.net/xjan103pcp94/7oiARsfqccrDvMD12jlWJe/daf859d2f1ad06bf931f5259bbc36255/TraditionalSoftware.png","details":{"size":396774,"image":{"width":1626,"height":566}},"fileName":"TraditionalSoftware.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Third, accuracy is directly linked to the quality of acquired data: bad data results in a bad model. As the diagram below shows data preparationâfeature extractions, feature selection, standardized or normalized features, data imputations and encodingâare all imperative steps before the cleansed data lands into a feature store, accessible to your model training and testing phase or inference in deployment.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6usr388mdxS5gFgfxoAMLm","type":"Asset","createdAt":"2021-11-05T18:17:52.891Z","updatedAt":"2021-11-16T17:11:39.120Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"Modern Data Lake","description":"Phases of the Model Development Cycle","file":{"url":"//images.ctfassets.net/xjan103pcp94/6usr388mdxS5gFgfxoAMLm/b6a4f57518af3904c77a90362271ddad/ModernDataLake.png","details":{"size":217302,"image":{"width":1404,"height":766}},"fileName":"ModernDataLake.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Fourth, a choice of programming language that is not only familiar to your data teamâdata analysts, data scientists, and ML engineersâbut also supported by many ML libraries employed during model experimentation and training phases. Python seems to be the ","nodeType":"text"},{"data":{"uri":"https://insights.stackoverflow.com/survey/2020#technology-most-loved-dreaded-and-wanted-languages-loved"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"de facto choice","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Alongside a choice of a programming language is the choice of an ML framework for taming compute-intensive ML workloads: deep learning, distributed training, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/what-is-hyperparameter-tuning"},"content":[{"data":{},"marks":[],"value":"hyperparameter optimization","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (HPO), and inferenceâall at horizontal scaleâfrom your laptop, single node multiple cores to multiple nodes, with multiple cores.Â Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"And finally, the ability to easily deploy models in diverse environments at scale: part of web applications, inside mobile devices, as a web service in the cloud. etc","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Consideration Number #2: Consider using model life cycle development and management platforms like ","nodeType":"text"},{"data":{"uri":"https://mlflow.org/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"MLflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://dvc.org/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"DVC","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://wandb.ai/site"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Weights \u0026 Biases","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", or ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/sagemaker/studio/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"SageMaker Studio","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":". And ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":",","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/raysgd/raysgd.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":" Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" (formerly Ray SGD), ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/tutorials/beginner/dist_overview.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" and ","nodeType":"text"},{"data":{"uri":"https://www.tensorflow.org/guide/distributed_training"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"TensorFlow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" for distributed, compute-intensive and deep learning ML workloads.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Managing Machine Learning Features","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.featurestore.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Feature stores","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" are emerging pivotal components in the modern machine learning development cycle. As more data scientists and engineers work together to successfully put models in production, having a singular store to persist cleaned and featurized data is becoming an increasing necessity as part of the model development cycle shown.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3ICMHtw1H093lIiCaZkuxG","type":"Asset","createdAt":"2021-11-10T03:31:55.753Z","updatedAt":"2021-11-10T03:39:59.411Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Feature Store Managing ML Features","description":"Feature Store for Managing ML Features","file":{"url":"//images.ctfassets.net/xjan103pcp94/3ICMHtw1H093lIiCaZkuxG/f25617cb1641fc2c4f5ec9cb079fc017/FeatureStore_ManagingMLFeatures.png","details":{"size":147609,"image":{"width":1078,"height":644}},"fileName":"FeatureStore ManagingMLFeatures.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Feature stores address operational challenges. They provide a consistent set of data between training and inference. They avoid any data skew or inadvertent data leakage. They offer both customized capability of writing feature transformations, both on batch and streaming data, during the feature extraction process while training. And they allow request augmentation with historical data at inference, which is common in large fraud and anomaly detection deployed models or recommendation systems.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Aside from challenges and considerations of putting models in production, operationalizing ML data is equally important. Model accuracy depends on good data, and feature stores help manage precomputed and cleansed features for your model training and production inference during model serving.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Consideration Number #3: Consider feature stores as part of your model development process. Look to ","nodeType":"text"},{"data":{"uri":"https://feast.dev/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Feast","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.tecton.ai/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Tecton","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_featurestore.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"SageMaker","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", and ","nodeType":"text"},{"data":{"uri":"https://databricks.com/product/feature-store"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Databricks","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" for feature stores.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deploying, Serving and Inferencing Models at Scale","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Once the model is trained and tested, with confidence that it met the business requirements for model accuracy, seven crucial requirements for scalable model serving frameworks to consider are:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Framework agnostic","nodeType":"text"},{"data":{},"marks":[],"value":": A model serving-elected framework should be ML framework agnostic. That is, it can deploy any common model built with common ML frameworks. For example, PyTorch, TensorFlow, XGBoost, or Scikit-learn, each with its own algorithms and model architectures.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Business Logic:","nodeType":"text"},{"data":{},"marks":[],"value":"Â  Model prediction often requires preprocessing, post processing or ability to augment request data by connecting to a feature store or any other data store for validation. Model serving should allow this as part of its inference.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Model Replication: ","nodeType":"text"},{"data":{},"marks":[],"value":"Some models are compute-intensive or network-bound. As such the elected framework can fan out requests over to model replicas, load balancing among replicas to support parallel request handling during peak traffic.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Request Batching: ","nodeType":"text"},{"data":{},"marks":[],"value":"Not all models in production are employed for real-time serving. Often, models are scored in large batches of requests. For example, for deep learning models, parallelizing these image requests to multiple cores, taking advantage of hardware accelerators, to expedite batch scoring and utilize hardware resources is worthy of consideration.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"High Concurrency and Low Latency: ","nodeType":"text"},{"data":{},"marks":[],"value":"Models in production require real-time inference with low latency while handling bursts of heavy traffic of requests. The consideration is crucial for best user experience to receive millisecond responses on prediction requests.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Model Deployment CLI and APIs: ","nodeType":"text"},{"data":{},"marks":[],"value":"An ML engineer responsible to deploy a model should be able to use model serverâs deployment APIs or command line interfaces (CLI) simply to deploy model artifacts into production. This allows model deployment from within an existing CI/CD pipeline or workflow.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Patterns of Models in Production","nodeType":"text"},{"data":{},"marks":[],"value":": As ML applications are increasingly becoming pervasive in all sectors of industry, models trained for these ML applications are complex and composite. They range from computer vision to natural language processing to recommendation systems and reinforcement learning.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"That is, they donât exist in isolation. Nor do they predict results singularly. Instead they operate jointly and often in ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"four model ML patterns","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": pipeline, ensemble, business logic, and online learning. Each pattern has its purpose and merit.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1xy0sgeTJzs9wGLt2C5rfE","type":"Asset","createdAt":"2021-11-05T18:30:46.397Z","updatedAt":"2021-11-05T18:30:46.397Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ML Patterns in Production","description":"ML Model Patterns in Production ([image source](https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns))","file":{"url":"//images.ctfassets.net/xjan103pcp94/1xy0sgeTJzs9wGLt2C5rfE/4715269a6f5f0a049f958a3b213325e4/MLPatternsProduction.png","details":{"size":203430,"image":{"width":1482,"height":696}},"fileName":"MLPatternsProduction.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Machine Learning engineers adopt ","nodeType":"text"},{"data":{"uri":"https://youtu.be/gV4YS4e1CXg?t=272"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"two common approaches ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"to deploy these patterns of models in production. One is to embed models into a web server, the other is to offload to an external service. Each approach has its own pros and cons, with respect to the above seven considerations.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Consideration Number #4: Look to ","nodeType":"text"},{"data":{"uri":"https://www.seldon.io/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Seldon","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.kubeflow.org/docs/components/kfserving/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"KFServing","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", or ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" for all these seven requirements.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Observing and Monitoring Model in Production","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Model monitoring, often an overlooked stage as part of model development lifecycle, is critical to modelâs viability in the post deployment production stage. It is often an afterthought, at an ML engineerâs peril.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Models have an afterlife of viability. That viable life in production needs a constant watchful or sentinel eye. In fact, monitoring as a phase is simply a continuation of the model serving, as depicted in the diagram below.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1B0lehLJcFXrfMsC6Rdu6f","type":"Asset","createdAt":"2021-11-05T18:33:35.806Z","updatedAt":"2021-11-05T18:33:35.806Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ML Model Monitoring in Production","description":"ML Model Monitoring in Production ([image source](https://evidentlyai.com/blog/machine-learning-monitoring-what-it-is-and-how-it-differs))","file":{"url":"//images.ctfassets.net/xjan103pcp94/1B0lehLJcFXrfMsC6Rdu6f/e0f90578ed433933118a36b2aeea5c39/ML_Model_Monitoring_in_Production.png","details":{"size":192050,"image":{"width":1040,"height":416}},"fileName":"ML Model Monitoring in Production.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Why consider model monitoring? For a number of practical reasons, this stage is pivotal. Let's briefly discuss them.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data drifts over time: ","nodeType":"text"},{"data":{},"marks":[],"value":"As we mentioned above, our quality and accuracy of the model depends on the quality of the data. Data is complex and never static, meaning what the original model was trained with the extracted features may not be as important over time. For example, a geo location for a credit application model may not be as important, as demographics evolve. Some new features may emerge that need to be taken into account. For example, seasonal data changes. Such ","nodeType":"text"},{"data":{"uri":"https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"features drifts","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in data require retraining and redeploying the model, because the distribution of the variables is no longer relevant.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Model concept changes over time: ","nodeType":"text"},{"data":{},"marks":[],"value":"Many practitioners refer to this as model ","nodeType":"text"},{"data":{"uri":"https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"decay or model staleness","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". When the patterns of trained models no longer hold with the drifting data, the model is no longer valid because the relationships of its input features may not necessarily produce the model's expected prediction. Hence, its accuracy degrades.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Â ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Models fail over time: ","nodeType":"text"},{"data":{},"marks":[],"value":"Models fail for inexplicable reasons: a system failure or bad network connection; an overloaded system; a bad input or corrupted request. Detecting these failuresâ root causes early or its frequency mitigates user bad experience or deters mistrust in the service if the user receives wrong or bogus outcomes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Systems degrade over load: ","nodeType":"text"},{"data":{},"marks":[],"value":"Constantly being vigilant of the health of your dedicated model servers or services deployed is just as important as monitoring the health of your data pipelines that transform data or your entire data infrastructureâs key components: data stores, web servers, routers, cluster nodesâ system health, etc.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Collectively, these aforementioned monitoring model concepts are called ","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/what-is-ml-observability-29e85e701688"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"model observability","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". This step is now an acceptable imperative in ","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/mlops-practices-for-data-scientists-dbb01be45dd8"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLOps best practices","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Monitoring the health of your data and models should never be an afterthought. Rather, it ought to be part and parcel of your model development cycle.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Consideration Number #5: For model observability look to ","nodeType":"text"},{"data":{"uri":"https://github.com/evidentlyai/evidently"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Evidently.ai","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://arize.com/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Arize.ai","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.arthur.ai/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Arthur.ai","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.fiddler.ai/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Fiddler.ai","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://valohai.com/model-monitoring/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"Valohai.com","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":",","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":" or ","nodeType":"text"},{"data":{"uri":"https://whylabs.ai/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"whylabs.ai","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs recap. To avoid the common grumble of models not making it to production or having your model see the light of the day in production, take into account all the above considerations at heart if you want your models to journey to their desired destinationâand have a viable afterlife too.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Each consideration has its merits. Each consideration has either an open source solution addressing each problem or a managed solution from a vendor. Evaluate how each best fits and meets all the considerations into your existing machine learning tooling stack.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"But making it part and parcel of your ML model development tooling stack is crucial; it will significantly improve your end-to-end success in putting your models into production.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the next blog in this series, we will examine how you can implement consideration #1 and #2, focusing on some of the tools we suggested.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6pQJ9kqwgmpnC2IaMIDtoj","type":"Asset","createdAt":"2021-11-05T18:35:46.042Z","updatedAt":"2021-11-05T18:35:46.042Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"MLPatternsProduction","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6pQJ9kqwgmpnC2IaMIDtoj/c925f376d94117b1f1dd97b4bf84f72d/MLPatternsProduction.png","details":{"size":203430,"image":{"width":1482,"height":696}},"fileName":"MLPatternsProduction.png","contentType":"image/png"}}},"recommendations":[],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true}},"thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"wSQM6naQrXBOvxNfv5iYe","type":"Asset","createdAt":"2022-03-24T22:19:37.934Z","updatedAt":"2022-03-24T22:19:37.934Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-nodes-dark","file":{"url":"//images.ctfassets.net/xjan103pcp94/wSQM6naQrXBOvxNfv5iYe/ed3965226bd4e0dc9d39da26d95d6e14/blog-recommended-content-nodes-dark.jpg","details":{"size":41506,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-nodes-dark.jpg","contentType":"image/jpeg"}}},"recommendations":null}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BzSmyQ2ojATOq6qOpD4PX","type":"Entry","createdAt":"2021-10-02T19:52:20.790Z","updatedAt":"2021-10-02T19:52:20.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Technology","identifier":"technology"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}}],"hideIntro":true}}],"activeTag":null,"activeType":null,"author":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"kQhxB9LxeWeB3noKuPXiJ","type":"Entry","createdAt":"2020-09-20T16:36:33.304Z","updatedAt":"2020-09-20T16:36:33.304Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Simon Mo","slug":"simon-mo","bio":"Simon Mo is a software engineer at Anyscale. Before Anyscale, he was a student at UC Berkeley participating in research at the RISELab. He focuses on studying and building systems for machine learning, in particular, how to make ML model serving systems more efficient, ergonomic, and scalable. He works on Ray Serve at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"fy7QSQIwnpMZWvM1ndmhx","type":"Asset","createdAt":"2020-09-20T16:36:28.219Z","updatedAt":"2020-09-20T16:36:28.219Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Simon Mo headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/fy7QSQIwnpMZWvM1ndmhx/cdef7ccbc81d0ea4903ed73336e42b48/Simon-Mo-headshot.png","details":{"size":365363,"image":{"width":550,"height":550}},"fileName":"Simon-Mo-headshot.png","contentType":"image/png"}}},"recommendations":[]}},"page":1,"totalPages":1,"allTypes":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KeH0JzrIEpuheazhMAhKf","type":"Entry","createdAt":"2022-06-15T01:31:23.950Z","updatedAt":"2022-06-15T01:31:23.950Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"User Story","slug":"user-story","tintColor":"#6D59C6"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42F9iHs5a05rRrOuqEQwvB","type":"Entry","createdAt":"2022-06-15T01:30:47.322Z","updatedAt":"2022-06-15T01:30:47.322Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Culture","slug":"culture","tintColor":"#39C8E5"}}],"allTags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KwkbI6zRqcE9KD5iKuP8W","type":"Entry","createdAt":"2021-12-05T04:51:33.974Z","updatedAt":"2021-12-05T04:51:33.974Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"RayDP","identifier":"ray_dp"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1OiVmfyPaDqRNavVG5Yp1l","type":"Entry","createdAt":"2021-12-05T04:50:12.541Z","updatedAt":"2021-12-05T04:50:12.541Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Retail","identifier":"retail"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"161LHBXwPkxUmhjt1YbfJH","type":"Entry","createdAt":"2021-12-05T04:49:42.528Z","updatedAt":"2021-12-05T04:49:42.528Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Gaming","identifier":"gaming"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"BDTwwkSW9VtmSkJojGbx8","type":"Entry","createdAt":"2021-12-05T04:49:30.169Z","updatedAt":"2021-12-05T04:49:30.169Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Government","identifier":"government"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4fzKvVOn2R8U6TTi2bcb2R","type":"Entry","createdAt":"2021-12-05T04:49:10.881Z","updatedAt":"2021-12-05T04:49:10.881Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"HLS","identifier":"hls"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"GGg4W2UlqfVP2iPMsc8J1","type":"Entry","createdAt":"2021-12-05T04:44:22.472Z","updatedAt":"2021-12-05T04:44:22.472Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Financial","identifier":"financial"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i6fp6Gfdy5wFeDDOaKcfh","type":"Entry","createdAt":"2021-12-03T22:33:42.248Z","updatedAt":"2021-12-03T22:33:42.248Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Anyscale","identifier":"anyscale"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6chYLcIc6yEB2EtLv2vngw","type":"Entry","createdAt":"2021-11-23T01:06:51.725Z","updatedAt":"2021-11-30T22:20:19.315Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Healthcare","identifier":"healthcare"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BzSmyQ2ojATOq6qOpD4PX","type":"Entry","createdAt":"2021-10-02T19:52:20.790Z","updatedAt":"2021-10-02T19:52:20.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Technology","identifier":"technology"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}}]},"__N_SSP":true},"page":"/blog","query":{"author":"simon-mo"},"buildId":"-oT8_WeW9m_lAxr26cJvY","isFallback":false,"gssp":true,"customServer":true,"scriptLoader":[]}</script><script type="text/javascript" id="hs-script-loader" async="" defer="" src="https://js.hs-scripts.com/20523749.js"></script><script type="text/javascript">
  _linkedin_partner_id = "4210313";
  window._linkedin_data_partner_ids = window._linkedin_data_partner_ids || [];
  window._linkedin_data_partner_ids.push(_linkedin_partner_id);
</script><script type="text/javascript">
  (function(l) { if (!l){window.lintrk = function(a,b){window.lintrk.q.push([a,b])};
  window.lintrk.q=[]} var s = document.getElementsByTagName("script")[0];
  var b = document.createElement("script");
  b.type = "text/javascript";b.async = true;
  b.src = "https://snap.licdn.com/li.lms-analytics/insight.min.js";
  s.parentNode.insertBefore(b, s);})(window.lintrk);
</script><noscript><img height="1" width="1" style="display:none" alt="" src="https://px.ads.linkedin.com/collect/?pid=4210313&amp;fmt=gif"/></noscript></body></html>