<!DOCTYPE html><html lang="en-US"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Blog | Anyscale</title><meta name="description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta property="og:title" content="Blog | Anyscale"/><meta property="og:description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta property="og:site_name" content="Anyscale"/><meta property="og:image" content=""/><meta property="fb:app_id" content=""/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta name="twitter:site" content=""/><meta name="twitter:title" content="Blog | Anyscale"/><meta name="twitter:image" content=""/><meta name="next-head-count" content="14"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><link rel="icon" href="static/favicon.ico"/><script>
(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-P8H6KQG');
</script><script src="https://tag.clearbitscripts.com/v1/pk_7a96be585f731437b35c7bad25c432b7/tags.js"></script><script src="https://www.googleoptimize.com/optimize.js?id=OPT-NPGB9W3"></script><script>
  !function(f,b,e,v,n,t,s)
  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
  n.queue=[];t=b.createElement(e);t.async=!0;
  t.src=v;s=b.getElementsByTagName(e)[0];
  s.parentNode.insertBefore(t,s)}(window,document,'script',
  'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '168099398554987');
  fbq('track', 'PageView');
</script><link rel="stylesheet" href="static/font-awesome-4.7.0/css/font-awesome.min.css"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="true"/><script src="https://fast.wistia.com/assets/external/E-v1.js" async=""></script><meta name="ahrefs-site-verification" content="4be3c5d78cdad8823d038256ac06d4336ae7a84632dac0ed57975703fc14fe6b"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="_next/static/css/dbba207740458c19.css" as="style"/><link rel="stylesheet" href="_next/static/css/dbba207740458c19.css" data-n-g=""/><link rel="preload" href="_next/static/css/13fbfc51931a4b43.css" as="style"/><link rel="stylesheet" href="_next/static/css/13fbfc51931a4b43.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="_next/static/chunks/webpack-94adde92c5e1c75b.js" defer=""></script><script src="_next/static/chunks/framework-95c3ca6de27174dd.js" defer=""></script><script src="_next/static/chunks/main-774d7304f1973b2a.js" defer=""></script><script src="_next/static/chunks/pages/_app-fd8cb5a9775605f5.js" defer=""></script><script src="_next/static/chunks/0f1ac474-1cd8c801d37caef0.js" defer=""></script><script src="_next/static/chunks/1812-a871a62a8e8aa88c.js" defer=""></script><script src="_next/static/chunks/9065-e24d5adee6cbba35.js" defer=""></script><script src="_next/static/chunks/6139-f3c4647afbd26b94.js" defer=""></script><script src="_next/static/chunks/8423-007cfdc371b27898.js" defer=""></script><script src="_next/static/chunks/3167-65b612e959dd1945.js" defer=""></script><script src="_next/static/chunks/9027-e83e1bb65c284840.js" defer=""></script><script src="_next/static/chunks/pages/blog-1eafbb689a124ac5.js" defer=""></script><script src="_next/static/-oT8_WeW9m_lAxr26cJvY/_buildManifest.js" defer=""></script><script src="_next/static/-oT8_WeW9m_lAxr26cJvY/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Rubik&display=swap">@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4i1Uw.woff) format('woff')}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFWkUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFU0UzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFVUUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFWUUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFV0UzdYPFkZVO.woff) format('woff');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href="https://fonts.googleapis.com/css2?family=Outfit:wght@400;500;600;700&display=swap">@font-face{font-family:'Outfit';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4TC1C4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4QK1C4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4e6yC4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4deyC4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,400;0,700;1,400;1,700&display=swap">@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTeEw.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINscg.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKtdU.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7MIU.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGqZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuE6ZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuFKZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGKZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGaZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuF6ZJW9XjDg.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINce_fuJGl18QRY.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINccvfuJGl18QRY.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcdffuJGl18QRY.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINceffuJGl18QRY.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcePfuJGl18QRY.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcdvfuJGl18Q.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdzeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdXeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdLeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhd7eFaxOedfTDw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhd_eFaxOedfTDw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdHeFaxOedc.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIxsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIVsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIJsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AI5sdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AI9sdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIFsdP3pBms.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body class=""><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-P8H6KQG" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next" data-reactroot=""><svg xmlns="http://www.w3.org/2000/svg" class="sr-only"><symbol id="svg-arrow-right-gradient"><path d="M1.5 7.5h18" stroke="url(#paint0_linear_3927_132230)" stroke-width="1.5" stroke-linecap="round"></path><path d="m16.6562 3.65625 3.8442 3.84412-3.8442 3.84413" stroke="url(#paint1_linear_3927_132230)" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><defs><linearGradient id="paint0_linear_3927_132230" x1="1.06972" y1="8.58929" x2="12.2319" y2="-1.53819" gradientUnits="userSpaceOnUse"><stop stop-color="#8B20BE"></stop><stop offset="1" stop-color="#3579FF" stop-opacity=".97"></stop></linearGradient><linearGradient id="paint1_linear_3927_132230" x1="12.377" y1="7.75171" x2="16.9307" y2="11.8684" gradientUnits="userSpaceOnUse"><stop stop-color="#8B20BE"></stop><stop offset="1" stop-color="#3579FF" stop-opacity=".97"></stop></linearGradient></defs></symbol></svg><div class="Banner_container__zEmFV"><a href="https://raysummit.anyscale.com/?utm_source=anyscale&amp;utm_medium=website&amp;utm_campaign=ray_summit_2023&amp;utm_content=homepage_ticker" rel="noreferrer nofollow" target="_blank"><span class="Banner_banner-text__wr6Xi"><span class="Banner_main-text__cpJ6H">Are you passionate about Ray, scalable AI and distributed computing? Join us at Ray Summit!</span><span class="Banner_cta-button__0JCtt"> Find out more! <!-- --> <svg width="23" height="14" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1.5 6.75a.75.75 0 0 0 0 1.5v-1.5Zm18 1.5a.75.75 0 0 0 0-1.5v1.5Zm-18 0h18v-1.5h-18v1.5Z" fill="#fff"></path><path d="M17.187 3.126a.75.75 0 0 0-1.061 1.06l1.06-1.06ZM20.5 7.5l.53.53a.75.75 0 0 0 0-1.06l-.53.53Zm-4.374 3.314a.75.75 0 1 0 1.06 1.06l-1.06-1.06Zm0-6.627L19.97 8.03l1.06-1.061-3.843-3.844-1.061 1.06ZM19.97 6.97l-3.844 3.844 1.06 1.06 3.845-3.843L19.97 6.97Z" fill="#fff"></path></svg></span></span></a><button class="Banner_close-button__9ksfY" type="button" aria-label="Close Banner"></button></div><header class="inner-shadow sticky top-0 z-[1000] h-24 transform py-5 transition-transform duration-500 bg-primary translate-y-0"><div class="container mx-auto flex items-center justify-between"><a href="index.html" class="text-white"><svg width="141" height="32" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><g clip-path="url(#anyscale-logo_svg__a)" fill="currentColor"><path d="M10.653 17.46H3.865C1.697 17.46 0 19.16 0 21.332v6.797c0 2.17 1.697 3.87 3.865 3.87h6.788c2.169 0 3.866-1.7 3.866-3.87V21.33c0-2.171-1.792-3.87-3.866-3.87Zm1.509 10.573c0 .85-.66 1.51-1.509 1.51H3.865a1.49 1.49 0 0 1-1.508-1.51v-6.796c0-.85.66-1.51 1.508-1.51h6.788c.849 0 1.509.66 1.509 1.51v6.796Z"></path><path d="M28.095 11.988h-8.108V3.87C19.892 1.7 18.196 0 16.027 0H3.865C1.697 0 0 1.7 0 3.87v6.608c0 2.171 1.697 3.87 3.865 3.87h13.67v13.687c0 2.171 1.698 3.87 3.866 3.87h6.6c2.168 0 3.865-1.699 3.865-3.87V15.858c0-2.076-1.697-3.775-3.771-3.87Zm-24.23 0a1.49 1.49 0 0 1-1.508-1.51V3.87c0-.85.66-1.51 1.508-1.51h12.162c.849 0 1.509.66 1.509 1.51v8.118H3.866Zm25.738 16.047c0 .85-.66 1.51-1.508 1.51h-6.6a1.49 1.49 0 0 1-1.508-1.51V14.348h8.108c.848 0 1.508.66 1.508 1.51v12.177ZM79.194 17.462l-3.017-8.118h-3.3l4.714 11.327-2.546 5.853h3.112l7.165-17.18H82.21l-3.016 8.118ZM93.335 14.819c-.66-.284-1.414-.472-2.075-.661-.66-.189-1.225-.378-1.79-.66-.378-.19-.566-.473-.566-.85 0-.378.188-.661.472-.85.847-.472 1.79-.378 2.639.094.377.284.566.661.566 1.133h2.828c0-1.133-.566-2.171-1.414-2.832-.943-.755-2.169-1.133-3.3-1.038-.849 0-1.697.094-2.45.472-.66.283-1.227.66-1.604 1.227-.377.472-.565 1.133-.565 1.794 0 .66.188 1.321.565 1.887.377.472.849.85 1.414 1.039.66.283 1.414.472 2.168.66.66.19 1.226.378 1.886.661.377.19.566.472.566.85 0 .377-.189.66-.471.85-.377.283-.85.377-1.32.377-.472 0-1.038-.094-1.415-.472-.377-.283-.565-.66-.66-1.133h-2.922c0 .755.282 1.416.754 1.982.472.567 1.131 1.133 1.791 1.416.849.378 1.698.567 2.546.567.849 0 1.697-.095 2.45-.472.661-.283 1.227-.66 1.604-1.227.377-.567.565-1.133.565-1.794 0-.66-.188-1.321-.66-1.888-.565-.66-1.037-.944-1.602-1.132ZM102.575 11.702c.566 0 1.036.095 1.508.472.377.283.66.755.849 1.227h3.205a5.366 5.366 0 0 0-1.886-3.02 5.348 5.348 0 0 0-3.582-1.039c-1.037 0-2.074.284-3.017.755a5.486 5.486 0 0 0-2.074 2.077 7.427 7.427 0 0 0 0 6.325c.472.85 1.131 1.604 2.074 2.076.943.472 1.98.756 3.017.756 1.32 0 2.546-.378 3.582-1.133a5.366 5.366 0 0 0 1.886-3.02h-3.111c-.283 1.037-1.32 1.698-2.357 1.604-.754 0-1.509-.283-1.98-.944a5.119 5.119 0 0 1 0-5.192 1.914 1.914 0 0 1 1.886-.944ZM118.412 10.1a.405.405 0 0 1-.377.378c-.094 0-.188 0-.188-.095a3.382 3.382 0 0 0-.944-.66c-.659-.378-1.507-.567-2.262-.567-.943 0-1.885.283-2.734.755a4.459 4.459 0 0 0-1.885 2.077c-.472.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.376.85 1.037 1.604 1.885 2.17.849.473 1.697.756 2.64.756.754 0 1.603-.189 2.263-.567.659-.283 1.131-.755 1.602-1.321v1.699h2.923V9.345h-2.923v.755Zm-.377 6.891c-.283.472-.66.944-1.132 1.227-.47.283-1.037.378-1.507.378-.567 0-1.037-.189-1.509-.378-.472-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.094-1.228.472-1.794.848-1.51 2.64-1.982 4.147-1.227h.095c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM55.907 10.1a.406.406 0 0 1-.377.378c-.095 0-.19 0-.19-.095a3.384 3.384 0 0 0-.942-.66c-.66-.378-1.508-.567-2.263-.567-.942 0-1.885.283-2.733.755a4.463 4.463 0 0 0-1.886 2.077c-.471.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.377.85 1.037 1.604 1.886 2.17.848.473 1.697.756 2.64.756.753 0 1.602-.189 2.262-.567.66-.283 1.131-.755 1.603-1.321v1.699h2.922V9.345h-2.922v.755Zm-.472 6.891c-.283.472-.66.944-1.131 1.227-.471.283-1.037.378-1.509.378-.565 0-1.037-.189-1.508-.378-.471-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.095-1.228.472-1.794.848-1.51 2.64-1.982 4.148-1.227h.094c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM126.708 5.477h-2.923v15.575h2.923V5.477ZM140.191 14.914c0-1.038-.189-2.077-.755-3.02-.471-.85-1.131-1.605-2.074-1.983a6.78 6.78 0 0 0-6.033 0 5.485 5.485 0 0 0-2.075 2.077c-.565 1.038-.754 2.077-.754 3.21 0 1.132.189 2.17.754 3.114a5.485 5.485 0 0 0 2.075 2.077c.942.472 1.979.755 3.016.755a6.56 6.56 0 0 0 3.489-1.038c.943-.661 1.602-1.7 1.885-2.738h-3.016c-.377.944-1.321 1.51-2.358 1.416-.659 0-1.414-.283-1.885-.755-.566-.472-.848-1.227-.848-1.982h8.484c.095-.378.095-.755.095-1.133Zm-8.579-.85c.093-.755.377-1.415.942-1.887.566-.472 1.226-.661 1.886-.661.754 0 1.414.189 1.979.66.566.473.849 1.133.849 1.794l-5.656.095ZM67.692 9.159a4.7 4.7 0 0 0-1.98.472c-.377.188-.754.377-1.037.66a.286.286 0 0 1-.377 0c-.094-.094-.094-.094-.094-.188v-.756H61.28v11.705h2.923v-6.418c-.094-.755.188-1.51.754-2.171.471-.472 1.226-.756 1.886-.756.66 0 1.414.284 1.885.755.472.567.754 1.322.66 2.172v6.418h2.923v-6.89c.094-1.322-.283-2.644-1.226-3.682-.848-.85-2.168-1.416-3.394-1.321Z"></path></g><defs><clipPath id="anyscale-logo_svg__a"><path fill="currentColor" d="M0 0h140.19v32H0z"></path></clipPath></defs></svg></a><div class="lg:relative lg:flex lg:flex-row hidden"><div class="max-h-[calc(100vh-192px)] items-center overflow-y-auto py-[1px] shadow-[0_-1px_0_#DAE3F2] lg:flex lg:overflow-visible lg:p-0 lg:shadow-none"><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Products</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="platform.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Anyscale Platform</div></a><a href="ray-open-source.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Open Source</div></a></div></div></div></div></div></div><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Solutions</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="container mx-auto mb-4 lg:mb-0 mt-2 lg:mt-0 lg:mr-12"><div class="header-card h-full rounded-[5px] p-6 lg:w-[296px]"><h5 id="scalable-ai-and-python">Scalable AI and Python</h5>
<p>Leverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more.</p></div></div><div class="grid lg:flex grid-cols-1 md:grid-cols-2 gap-x-10 lg:gap-x-12 container mx-auto"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><p class="text-base leading-normal font-normal text-darkGrey-300 px-6 lg:px-4 mb-4">Scalable AI and Python</p><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2"><a href="data-ingestion.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Data Ingestion</div></a><a href="reinforcement-learning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Reinforcement Learning</div></a><a href="ray-air.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray AIR</div></a><a href="model-serving.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Model Serving</div></a><a href="hyperparameter-tuning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Hyperparameter Tuning</div></a></div></div></div><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><p class="text-base leading-normal font-normal text-darkGrey-300 px-6 lg:px-4 mb-4">Use Cases</p><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2"><a href="large-language-models.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Large Language Models</div></a><a href="demand-forecasting.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Demand Forecasting/Pricing</div></a><a href="industrial-automation.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Industrial Automation</div></a><a href="machine-learning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Scalable ML Platforms</div></a><a href="natural-language-processing.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">NLP</div></a><a href="recommendation-system.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Recommendation System</div></a></div></div></div></div></div></div></div><a class="block font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4 shadow-[0_1px_0_#DAE3F2] lg:shadow-none" href="ray-ecosystem.html"><div class="w-full container mx-auto">Ecosystem</div></a><a class="block font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4 shadow-[0_1px_0_#DAE3F2] lg:shadow-none" href="user-stories.html"><div class="w-full container mx-auto">Success Stories</div></a><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Learn</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="blog/how-ray-solves-common-production-challenges-for-generative-ai.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Blog</div></a><a href="https://raysummit.anyscale.com/?utm_source=anyscale&amp;utm_medium=website&amp;utm_campaign=ray_summit_2023&amp;utm_content=home_nav-pulldown"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Summit 2023</div></a><a href="event-category/rl-summit.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Videos &amp; Webinars</div></a><a href="https://github.com/ray-project/ray-educational-materials"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Training</div></a><a href="https://docs.anyscale.com"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Anyscale Docs</div></a></div></div></div></div></div></div><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Company</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="about.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">About us</div></a><a href="press.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">News</div></a><a href="careers.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Careers</div></a><a href="community.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Community</div></a></div></div></div></div></div></div></div><div class="absolute bottom-0 left-0 right-0 bg-white py-5 shadow-[0_-1px_0_#DAE3F2] lg:relative lg:ml-5 lg:bg-transparent lg:p-0 lg:shadow-none"><div class="container mx-auto flex justify-end"><a class="btn bg-gradient min-w-[320px] justify-center bg-blue-900 text-center lg:min-w-0" href="beta.html">Try It Now</a></div></div></div><div class="cursor-pointer py-4 pl-6 lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M20.05 11H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95ZM20.05 16H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95ZM20.05 6H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95Z" fill="#fff"></path></svg></div></div></header><div class="Layout_page-sections__2__gb overflow-x-hidden"><div class="ArticlesList_container__mBEpW"><div class="ArticlesList_inner__QWc69"><div class="ArticlesList_header__45BKa"><h1>Posts by Eric Liang</h1><div class="ArticlesList_spacer__8l_nL"></div></div><div class="BlogFilters_root__mrUMs"><div class="BlogFilters_inner__87PZK"><div class="BlogFilters_item__1GpID"><div class="wrapper"><select aria-label="Types" class="SelectDropdown_select__hNpf2"><option selected="">All Types</option><option value="news">News</option><option value="culture">Culture</option><option value="engineering">Engineering</option><option value="user-story">User Story</option></select><div class="SelectDropdown_select__hNpf2 SelectDropdown_select-dropdown__Jabke"><button type="button" aria-haspopup="true" aria-expanded="false" style="width:245px">All Types</button><ul role="menu" class="SelectDropdown_menu__3wacL"><li class="SelectDropdown_item__LO5La SelectDropdown_active__YhRi7" tabindex="0" role="menuitem">All Types</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">News</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Culture</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Engineering</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">User Story</li></ul></div></div></div><div class="BlogFilters_item__1GpID"><div class="wrapper"><select aria-label="Tags" class="SelectDropdown_select__hNpf2"><option selected="">All Products / Libraries</option><option value="anyscale">Anyscale</option><option value="ray_core">Ray Core</option><option value="ray-datasets">Ray Datasets</option><option value="ray_train">Ray Train</option><option value="ray-tune">Ray Tune</option><option value="ray_serve">Ray Serve</option><option value="rllib">Ray RLlib</option></select><div class="SelectDropdown_select__hNpf2 SelectDropdown_select-dropdown__Jabke"><button type="button" aria-haspopup="true" aria-expanded="false" style="width:245px">All Products / Libraries</button><ul role="menu" class="SelectDropdown_menu__3wacL"><li class="SelectDropdown_item__LO5La SelectDropdown_active__YhRi7" tabindex="0" role="menuitem">All Products / Libraries</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Anyscale</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Core</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Datasets</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Train</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Tune</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Serve</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray RLlib</li></ul></div></div></div></div></div><div class="empty"><h2>No posts found.</h2><p><a href="blog/how-ray-solves-common-production-challenges-for-generative-ai.html">View all</a></p></div><div class="ArticlesList_list__uP0RC"></div><div class="Pagination_container__FdBHw"></div></div></div></div><footer class="relative bg-primary pt-30 pb-30 md:pt-36 md:pb-[180px] lg:pt-20"><div class="container mx-auto flex flex-col space-y-12 md:space-y-16 lg:flex-row lg:justify-between lg:space-y-0"><div class="flex flex-col gap-16"><div class="flex flex-col gap-6"><div class="h-10 text-white"><svg width="141" height="32" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="origin-top-left scale-125"><g clip-path="url(#anyscale-logo_svg__a)" fill="currentColor"><path d="M10.653 17.46H3.865C1.697 17.46 0 19.16 0 21.332v6.797c0 2.17 1.697 3.87 3.865 3.87h6.788c2.169 0 3.866-1.7 3.866-3.87V21.33c0-2.171-1.792-3.87-3.866-3.87Zm1.509 10.573c0 .85-.66 1.51-1.509 1.51H3.865a1.49 1.49 0 0 1-1.508-1.51v-6.796c0-.85.66-1.51 1.508-1.51h6.788c.849 0 1.509.66 1.509 1.51v6.796Z"></path><path d="M28.095 11.988h-8.108V3.87C19.892 1.7 18.196 0 16.027 0H3.865C1.697 0 0 1.7 0 3.87v6.608c0 2.171 1.697 3.87 3.865 3.87h13.67v13.687c0 2.171 1.698 3.87 3.866 3.87h6.6c2.168 0 3.865-1.699 3.865-3.87V15.858c0-2.076-1.697-3.775-3.771-3.87Zm-24.23 0a1.49 1.49 0 0 1-1.508-1.51V3.87c0-.85.66-1.51 1.508-1.51h12.162c.849 0 1.509.66 1.509 1.51v8.118H3.866Zm25.738 16.047c0 .85-.66 1.51-1.508 1.51h-6.6a1.49 1.49 0 0 1-1.508-1.51V14.348h8.108c.848 0 1.508.66 1.508 1.51v12.177ZM79.194 17.462l-3.017-8.118h-3.3l4.714 11.327-2.546 5.853h3.112l7.165-17.18H82.21l-3.016 8.118ZM93.335 14.819c-.66-.284-1.414-.472-2.075-.661-.66-.189-1.225-.378-1.79-.66-.378-.19-.566-.473-.566-.85 0-.378.188-.661.472-.85.847-.472 1.79-.378 2.639.094.377.284.566.661.566 1.133h2.828c0-1.133-.566-2.171-1.414-2.832-.943-.755-2.169-1.133-3.3-1.038-.849 0-1.697.094-2.45.472-.66.283-1.227.66-1.604 1.227-.377.472-.565 1.133-.565 1.794 0 .66.188 1.321.565 1.887.377.472.849.85 1.414 1.039.66.283 1.414.472 2.168.66.66.19 1.226.378 1.886.661.377.19.566.472.566.85 0 .377-.189.66-.471.85-.377.283-.85.377-1.32.377-.472 0-1.038-.094-1.415-.472-.377-.283-.565-.66-.66-1.133h-2.922c0 .755.282 1.416.754 1.982.472.567 1.131 1.133 1.791 1.416.849.378 1.698.567 2.546.567.849 0 1.697-.095 2.45-.472.661-.283 1.227-.66 1.604-1.227.377-.567.565-1.133.565-1.794 0-.66-.188-1.321-.66-1.888-.565-.66-1.037-.944-1.602-1.132ZM102.575 11.702c.566 0 1.036.095 1.508.472.377.283.66.755.849 1.227h3.205a5.366 5.366 0 0 0-1.886-3.02 5.348 5.348 0 0 0-3.582-1.039c-1.037 0-2.074.284-3.017.755a5.486 5.486 0 0 0-2.074 2.077 7.427 7.427 0 0 0 0 6.325c.472.85 1.131 1.604 2.074 2.076.943.472 1.98.756 3.017.756 1.32 0 2.546-.378 3.582-1.133a5.366 5.366 0 0 0 1.886-3.02h-3.111c-.283 1.037-1.32 1.698-2.357 1.604-.754 0-1.509-.283-1.98-.944a5.119 5.119 0 0 1 0-5.192 1.914 1.914 0 0 1 1.886-.944ZM118.412 10.1a.405.405 0 0 1-.377.378c-.094 0-.188 0-.188-.095a3.382 3.382 0 0 0-.944-.66c-.659-.378-1.507-.567-2.262-.567-.943 0-1.885.283-2.734.755a4.459 4.459 0 0 0-1.885 2.077c-.472.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.376.85 1.037 1.604 1.885 2.17.849.473 1.697.756 2.64.756.754 0 1.603-.189 2.263-.567.659-.283 1.131-.755 1.602-1.321v1.699h2.923V9.345h-2.923v.755Zm-.377 6.891c-.283.472-.66.944-1.132 1.227-.47.283-1.037.378-1.507.378-.567 0-1.037-.189-1.509-.378-.472-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.094-1.228.472-1.794.848-1.51 2.64-1.982 4.147-1.227h.095c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM55.907 10.1a.406.406 0 0 1-.377.378c-.095 0-.19 0-.19-.095a3.384 3.384 0 0 0-.942-.66c-.66-.378-1.508-.567-2.263-.567-.942 0-1.885.283-2.733.755a4.463 4.463 0 0 0-1.886 2.077c-.471.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.377.85 1.037 1.604 1.886 2.17.848.473 1.697.756 2.64.756.753 0 1.602-.189 2.262-.567.66-.283 1.131-.755 1.603-1.321v1.699h2.922V9.345h-2.922v.755Zm-.472 6.891c-.283.472-.66.944-1.131 1.227-.471.283-1.037.378-1.509.378-.565 0-1.037-.189-1.508-.378-.471-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.095-1.228.472-1.794.848-1.51 2.64-1.982 4.148-1.227h.094c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM126.708 5.477h-2.923v15.575h2.923V5.477ZM140.191 14.914c0-1.038-.189-2.077-.755-3.02-.471-.85-1.131-1.605-2.074-1.983a6.78 6.78 0 0 0-6.033 0 5.485 5.485 0 0 0-2.075 2.077c-.565 1.038-.754 2.077-.754 3.21 0 1.132.189 2.17.754 3.114a5.485 5.485 0 0 0 2.075 2.077c.942.472 1.979.755 3.016.755a6.56 6.56 0 0 0 3.489-1.038c.943-.661 1.602-1.7 1.885-2.738h-3.016c-.377.944-1.321 1.51-2.358 1.416-.659 0-1.414-.283-1.885-.755-.566-.472-.848-1.227-.848-1.982h8.484c.095-.378.095-.755.095-1.133Zm-8.579-.85c.093-.755.377-1.415.942-1.887.566-.472 1.226-.661 1.886-.661.754 0 1.414.189 1.979.66.566.473.849 1.133.849 1.794l-5.656.095ZM67.692 9.159a4.7 4.7 0 0 0-1.98.472c-.377.188-.754.377-1.037.66a.286.286 0 0 1-.377 0c-.094-.094-.094-.094-.094-.188v-.756H61.28v11.705h2.923v-6.418c-.094-.755.188-1.51.754-2.171.471-.472 1.226-.756 1.886-.756.66 0 1.414.284 1.885.755.472.567.754 1.322.66 2.172v6.418h2.923v-6.89c.094-1.322-.283-2.644-1.226-3.682-.848-.85-2.168-1.416-3.394-1.321Z"></path></g><defs><clipPath id="anyscale-logo_svg__a"><path fill="currentColor" d="M0 0h140.19v32H0z"></path></clipPath></defs></svg></div><div class="font-light leading-normal text-lightBlue-300 opacity-70 hidden lg:flex"><span>© Anyscale, Inc 2022</span></div></div><div class="z-10 hidden flex-col gap-10 lg:flex"><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Anyscale</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://www.linkedin.com/company/joinanyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M15.784 14.168a1.667 1.667 0 1 1-3.334-.002 1.667 1.667 0 0 1 3.334.002Zm.05 2.9H12.5V27.5h3.334V17.067Zm5.266 0h-3.317V27.5h3.284v-5.475c0-3.05 3.975-3.334 3.975 0V27.5h3.291v-6.609c0-5.141-5.883-4.95-7.266-2.424l.033-1.4Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://www.facebook.com/AnyscaleCompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M21.667 21.247h2.083l.834-3.333h-2.917v-1.667c0-.858 0-1.666 1.666-1.666h1.25v-2.8a23.49 23.49 0 0 0-2.38-.117c-2.263 0-3.87 1.38-3.87 3.917v2.333h-2.5v3.333h2.5v7.084h3.334v-7.084Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://twitter.com/anyscalecompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/anyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Ray</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://twitter.com/raydistributed"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/ray-project/ray"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div></div></div><div class="grid grid-cols-2 gap-x-8 gap-y-12 md:grid-cols-3 md:gap-y-10 md:gap-x-16"><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Products</div><div><ul>
<li><a href="platform.html">Anyscale Compute Platform</a></li>
<li><a href="ray-open-source.html">Ray Open Source</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Events</div><div><ul>
<li><a href="event-category/rl-summit.html">Webinars</a></li>
<li><a href="event-category/rl-summit.html">Meetups</a></li>
<li><a href="event-category/rl-summit.html">Summits</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Company</div><div><ul>
<li><a href="about.html">About Us</a></li>
<li><a href="press.html">News</a></li>
<li><a href="careers.html">Careers</a></li>
<li><a href="community.html">Community</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Learn</div><div><ul>
<li><a href="blog/how-ray-solves-common-production-challenges-for-generative-ai.html">Blog</a></li>
<li><a href="event-category/rl-summit.html">Demos &amp; Webinars</a></li>
<li><a href="event-category/rl-summit.html">Anyscale Academy</a></li>
<li><a href="https://docs.anyscale.com">Anyscale Docs</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Scalable AI and Python</div><div><ul>
<li><a href="data-ingestion.html">Data Ingestion</a></li>
<li><a href="reinforcement-learning.html">Reinforcement Learning</a></li>
<li><a href="ray-air.html">Ray AIR</a></li>
<li><a href="model-serving.html">Model Serving</a></li>
<li><a href="hyperparameter-tuning.html">Hyperparameter Tuning</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Use Cases</div><div><ul>
<li><a href="demand-forecasting.html">Demand Forecasting/Pricing</a></li>
<li><a href="industrial-automation.html">Industrial Automation</a></li>
<li><a href="machine-learning.html">Scalable ML Platforms</a></li>
<li><a href="natural-language-processing.html">NLP</a></li>
<li><a href="recommendation-system.html">Recommendation System</a></li>
</ul>
</div></div></div><div class="grid-col-1 grid gap-12 md:grid-cols-3 md:gap-16 lg:hidden"><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Anyscale</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://www.linkedin.com/company/joinanyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M15.784 14.168a1.667 1.667 0 1 1-3.334-.002 1.667 1.667 0 0 1 3.334.002Zm.05 2.9H12.5V27.5h3.334V17.067Zm5.266 0h-3.317V27.5h3.284v-5.475c0-3.05 3.975-3.334 3.975 0V27.5h3.291v-6.609c0-5.141-5.883-4.95-7.266-2.424l.033-1.4Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://www.facebook.com/AnyscaleCompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M21.667 21.247h2.083l.834-3.333h-2.917v-1.667c0-.858 0-1.666 1.666-1.666h1.25v-2.8a23.49 23.49 0 0 0-2.38-.117c-2.263 0-3.87 1.38-3.87 3.917v2.333h-2.5v3.333h2.5v7.084h3.334v-7.084Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://twitter.com/anyscalecompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/anyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Ray</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://twitter.com/raydistributed"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/ray-project/ray"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="md:self-end md:text-right"><div class="font-light leading-normal text-lightBlue-300 opacity-70"><span>© Anyscale, Inc 2022</span></div></div></div></div><div class="pointer-events-none absolute bottom-0 left-0"><svg width="612" height="297" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="hidden lg:block"><g opacity="0.5" clip-path="url(#dots_svg__a)"><mask id="dots_svg__c" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="0" y="0" width="612" height="297"><path transform="rotate(90 611.5 .5)" fill="url(#dots_svg__b)" stroke="#F2F3F7" d="M611.5.5h296v611h-296z"></path></mask><g mask="url(#dots_svg__c)" stroke="#A2B7D4"><g opacity="0.2" stroke-linecap="round"><path d="M598 350v-688M582 350v-688M566 350v-688M550 350v-688M534 350v-688M518 350v-688M502 350v-688M486 350v-688M470 350v-688M454 350v-688M438 350v-688M422 350v-688M406 350v-688M390 350v-688M374 350v-688M358 350v-688M342 350v-688M326 350v-688M310 350v-688M294 350v-688M278 350v-688M262 350v-688M246 350v-688M230 350v-688M214 350v-688M198 350v-688M182 350v-688M166 350v-688M150 350v-688M134 350v-688M118 350v-688M102 350v-688M86 350v-688M70 350v-688M54 350v-688M38 350v-688M22 350v-688M6 350v-688"></path></g><g opacity="0.2" stroke-linecap="round"><path d="M1014 14H0M1014 30H0M1014 46H0M1014 62H0M1014 78H0M1014 94H0M1014 110H0M1014 126H0M1014 142H0M1014 158H0M1014 174H0M1014 190H4M1014 206H4M1014 222H4M1014 238H4M1014 254H4M1014 270H4M1014 286H4"></path></g><path fill="#4C6996" d="M615.5 12.5v3h-3v-3zM615.5 28.5v3h-3v-3zM615.5 44.5v3h-3v-3zM615.5 60.5v3h-3v-3zM615.5 76.5v3h-3v-3zM615.5 92.5v3h-3v-3zM615.5 108.5v3h-3v-3zM615.5 124.5v3h-3v-3zM615.5 140.5v3h-3v-3zM615.5 156.5v3h-3v-3zM615.5 172.5v3h-3v-3zM615.5 188.5v3h-3v-3zM615.5 204.5v3h-3v-3zM615.5 220.5v3h-3v-3zM615.5 236.5v3h-3v-3zM615.5 252.5v3h-3v-3zM615.5 268.5v3h-3v-3zM615.5 284.5v3h-3v-3zM599.5 12.5v3h-3v-3zM599.5 28.5v3h-3v-3zM599.5 44.5v3h-3v-3zM599.5 60.5v3h-3v-3zM599.5 76.5v3h-3v-3zM599.5 92.5v3h-3v-3zM599.5 108.5v3h-3v-3zM599.5 124.5v3h-3v-3zM599.5 140.5v3h-3v-3zM599.5 156.5v3h-3v-3zM599.5 172.5v3h-3v-3zM599.5 188.5v3h-3v-3zM599.5 204.5v3h-3v-3zM599.5 220.5v3h-3v-3zM599.5 236.5v3h-3v-3zM599.5 252.5v3h-3v-3zM599.5 268.5v3h-3v-3zM599.5 284.5v3h-3v-3zM583.5 12.5v3h-3v-3zM583.5 28.5v3h-3v-3zM583.5 44.5v3h-3v-3zM583.5 60.5v3h-3v-3zM583.5 76.5v3h-3v-3zM583.5 92.5v3h-3v-3zM583.5 108.5v3h-3v-3zM583.5 124.5v3h-3v-3zM583.5 140.5v3h-3v-3zM583.5 156.5v3h-3v-3zM583.5 172.5v3h-3v-3zM583.5 188.5v3h-3v-3zM583.5 204.5v3h-3v-3zM583.5 220.5v3h-3v-3zM583.5 236.5v3h-3v-3zM583.5 252.5v3h-3v-3zM583.5 268.5v3h-3v-3zM583.5 284.5v3h-3v-3zM567.5 12.5v3h-3v-3zM567.5 28.5v3h-3v-3zM567.5 44.5v3h-3v-3zM567.5 60.5v3h-3v-3zM567.5 76.5v3h-3v-3zM567.5 92.5v3h-3v-3zM567.5 108.5v3h-3v-3zM567.5 124.5v3h-3v-3zM567.5 140.5v3h-3v-3zM567.5 156.5v3h-3v-3zM567.5 172.5v3h-3v-3zM567.5 188.5v3h-3v-3zM567.5 204.5v3h-3v-3zM567.5 220.5v3h-3v-3zM567.5 236.5v3h-3v-3zM567.5 252.5v3h-3v-3zM567.5 268.5v3h-3v-3zM567.5 284.5v3h-3v-3zM551.5 12.5v3h-3v-3zM551.5 28.5v3h-3v-3zM551.5 44.5v3h-3v-3zM551.5 60.5v3h-3v-3zM551.5 76.5v3h-3v-3zM551.5 92.5v3h-3v-3zM551.5 108.5v3h-3v-3zM551.5 124.5v3h-3v-3zM551.5 140.5v3h-3v-3zM551.5 156.5v3h-3v-3zM551.5 172.5v3h-3v-3zM551.5 188.5v3h-3v-3zM551.5 204.5v3h-3v-3zM551.5 220.5v3h-3v-3zM551.5 236.5v3h-3v-3zM551.5 252.5v3h-3v-3zM551.5 268.5v3h-3v-3zM551.5 284.5v3h-3v-3zM535.5 12.5v3h-3v-3zM535.5 28.5v3h-3v-3zM535.5 44.5v3h-3v-3zM535.5 60.5v3h-3v-3zM535.5 76.5v3h-3v-3zM535.5 92.5v3h-3v-3zM535.5 108.5v3h-3v-3zM535.5 124.5v3h-3v-3zM535.5 140.5v3h-3v-3zM535.5 156.5v3h-3v-3zM535.5 172.5v3h-3v-3zM535.5 188.5v3h-3v-3zM535.5 204.5v3h-3v-3zM535.5 220.5v3h-3v-3zM535.5 236.5v3h-3v-3zM535.5 252.5v3h-3v-3zM535.5 268.5v3h-3v-3zM535.5 284.5v3h-3v-3zM519.5 12.5v3h-3v-3zM519.5 28.5v3h-3v-3zM519.5 44.5v3h-3v-3zM519.5 60.5v3h-3v-3zM519.5 76.5v3h-3v-3zM519.5 92.5v3h-3v-3zM519.5 108.5v3h-3v-3zM519.5 124.5v3h-3v-3zM519.5 140.5v3h-3v-3zM519.5 156.5v3h-3v-3zM519.5 172.5v3h-3v-3zM519.5 188.5v3h-3v-3zM519.5 204.5v3h-3v-3zM519.5 220.5v3h-3v-3zM519.5 236.5v3h-3v-3zM519.5 252.5v3h-3v-3zM519.5 268.5v3h-3v-3zM519.5 284.5v3h-3v-3zM503.5 12.5v3h-3v-3zM503.5 28.5v3h-3v-3zM503.5 44.5v3h-3v-3zM503.5 60.5v3h-3v-3zM503.5 76.5v3h-3v-3zM503.5 92.5v3h-3v-3zM503.5 108.5v3h-3v-3zM503.5 124.5v3h-3v-3zM503.5 140.5v3h-3v-3zM503.5 156.5v3h-3v-3zM503.5 172.5v3h-3v-3zM503.5 188.5v3h-3v-3zM503.5 204.5v3h-3v-3zM503.5 220.5v3h-3v-3zM503.5 236.5v3h-3v-3zM503.5 252.5v3h-3v-3zM503.5 268.5v3h-3v-3zM503.5 284.5v3h-3v-3zM487.5 12.5v3h-3v-3zM487.5 28.5v3h-3v-3zM487.5 44.5v3h-3v-3zM487.5 60.5v3h-3v-3zM487.5 76.5v3h-3v-3zM487.5 92.5v3h-3v-3zM487.5 108.5v3h-3v-3zM487.5 124.5v3h-3v-3zM487.5 140.5v3h-3v-3zM487.5 156.5v3h-3v-3zM487.5 172.5v3h-3v-3zM487.5 188.5v3h-3v-3zM487.5 204.5v3h-3v-3zM487.5 220.5v3h-3v-3zM487.5 236.5v3h-3v-3zM487.5 252.5v3h-3v-3zM487.5 268.5v3h-3v-3zM487.5 284.5v3h-3v-3zM471.5 12.5v3h-3v-3zM471.5 28.5v3h-3v-3zM471.5 44.5v3h-3v-3zM471.5 60.5v3h-3v-3zM471.5 76.5v3h-3v-3zM471.5 92.5v3h-3v-3zM471.5 108.5v3h-3v-3zM471.5 124.5v3h-3v-3zM471.5 140.5v3h-3v-3zM471.5 156.5v3h-3v-3zM471.5 172.5v3h-3v-3zM471.5 188.5v3h-3v-3zM471.5 204.5v3h-3v-3zM471.5 220.5v3h-3v-3zM471.5 236.5v3h-3v-3zM471.5 252.5v3h-3v-3zM471.5 268.5v3h-3v-3zM471.5 284.5v3h-3v-3zM455.5 12.5v3h-3v-3zM455.5 28.5v3h-3v-3zM455.5 44.5v3h-3v-3zM455.5 60.5v3h-3v-3zM455.5 76.5v3h-3v-3zM455.5 92.5v3h-3v-3zM455.5 108.5v3h-3v-3zM455.5 124.5v3h-3v-3zM455.5 140.5v3h-3v-3zM455.5 156.5v3h-3v-3zM455.5 172.5v3h-3v-3zM455.5 188.5v3h-3v-3zM455.5 204.5v3h-3v-3zM455.5 220.5v3h-3v-3zM455.5 236.5v3h-3v-3zM455.5 252.5v3h-3v-3zM455.5 268.5v3h-3v-3zM455.5 284.5v3h-3v-3zM439.5 12.5v3h-3v-3zM439.5 28.5v3h-3v-3zM439.5 44.5v3h-3v-3zM439.5 60.5v3h-3v-3zM439.5 76.5v3h-3v-3zM439.5 92.5v3h-3v-3zM439.5 108.5v3h-3v-3zM439.5 124.5v3h-3v-3zM439.5 140.5v3h-3v-3zM439.5 156.5v3h-3v-3zM439.5 172.5v3h-3v-3zM439.5 188.5v3h-3v-3zM439.5 204.5v3h-3v-3zM439.5 220.5v3h-3v-3zM439.5 236.5v3h-3v-3zM439.5 252.5v3h-3v-3zM439.5 268.5v3h-3v-3zM439.5 284.5v3h-3v-3zM423.5 12.5v3h-3v-3zM423.5 28.5v3h-3v-3zM423.5 44.5v3h-3v-3zM423.5 60.5v3h-3v-3zM423.5 76.5v3h-3v-3zM423.5 92.5v3h-3v-3zM423.5 108.5v3h-3v-3zM423.5 124.5v3h-3v-3zM423.5 140.5v3h-3v-3zM423.5 156.5v3h-3v-3zM423.5 172.5v3h-3v-3zM423.5 188.5v3h-3v-3zM423.5 204.5v3h-3v-3zM423.5 220.5v3h-3v-3zM423.5 236.5v3h-3v-3zM423.5 252.5v3h-3v-3zM423.5 268.5v3h-3v-3zM423.5 284.5v3h-3v-3zM407.5 12.5v3h-3v-3zM407.5 28.5v3h-3v-3zM407.5 44.5v3h-3v-3zM407.5 60.5v3h-3v-3zM407.5 76.5v3h-3v-3zM407.5 92.5v3h-3v-3zM407.5 108.5v3h-3v-3zM407.5 124.5v3h-3v-3zM407.5 140.5v3h-3v-3zM407.5 156.5v3h-3v-3zM407.5 172.5v3h-3v-3zM407.5 188.5v3h-3v-3zM407.5 204.5v3h-3v-3zM407.5 220.5v3h-3v-3zM407.5 236.5v3h-3v-3zM407.5 252.5v3h-3v-3zM407.5 268.5v3h-3v-3zM407.5 284.5v3h-3v-3zM391.5 12.5v3h-3v-3zM391.5 28.5v3h-3v-3zM391.5 44.5v3h-3v-3zM391.5 60.5v3h-3v-3zM391.5 76.5v3h-3v-3zM391.5 92.5v3h-3v-3zM391.5 108.5v3h-3v-3zM391.5 124.5v3h-3v-3zM391.5 140.5v3h-3v-3zM391.5 156.5v3h-3v-3zM391.5 172.5v3h-3v-3zM391.5 188.5v3h-3v-3zM391.5 204.5v3h-3v-3zM391.5 220.5v3h-3v-3zM391.5 236.5v3h-3v-3zM391.5 252.5v3h-3v-3zM391.5 268.5v3h-3v-3zM391.5 284.5v3h-3v-3zM375.5 12.5v3h-3v-3zM375.5 28.5v3h-3v-3zM375.5 44.5v3h-3v-3zM375.5 60.5v3h-3v-3zM375.5 76.5v3h-3v-3zM375.5 92.5v3h-3v-3zM375.5 108.5v3h-3v-3zM375.5 124.5v3h-3v-3zM375.5 140.5v3h-3v-3zM375.5 156.5v3h-3v-3zM375.5 172.5v3h-3v-3zM375.5 188.5v3h-3v-3zM375.5 204.5v3h-3v-3zM375.5 220.5v3h-3v-3z"></path><rect x="375.5" y="236.5" width="3" height="3" rx="1.5" transform="rotate(90 375.5 236.5)" fill="#4C6996"></rect><path fill="#4C6996" d="M375.5 252.5v3h-3v-3zM375.5 268.5v3h-3v-3zM375.5 284.5v3h-3v-3zM359.5 12.5v3h-3v-3zM359.5 28.5v3h-3v-3zM359.5 44.5v3h-3v-3zM359.5 60.5v3h-3v-3zM359.5 76.5v3h-3v-3zM359.5 92.5v3h-3v-3zM359.5 108.5v3h-3v-3zM359.5 124.5v3h-3v-3zM359.5 140.5v3h-3v-3zM359.5 156.5v3h-3v-3zM359.5 172.5v3h-3v-3zM359.5 188.5v3h-3v-3zM359.5 204.5v3h-3v-3zM359.5 220.5v3h-3v-3zM359.5 236.5v3h-3v-3zM359.5 252.5v3h-3v-3zM359.5 268.5v3h-3v-3zM359.5 284.5v3h-3v-3zM343.5 12.5v3h-3v-3zM343.5 28.5v3h-3v-3zM343.5 44.5v3h-3v-3zM343.5 60.5v3h-3v-3zM343.5 76.5v3h-3v-3zM343.5 92.5v3h-3v-3zM343.5 108.5v3h-3v-3zM343.5 124.5v3h-3v-3zM343.5 140.5v3h-3v-3zM343.5 156.5v3h-3v-3zM343.5 172.5v3h-3v-3zM343.5 188.5v3h-3v-3zM343.5 204.5v3h-3v-3zM343.5 220.5v3h-3v-3zM343.5 236.5v3h-3v-3zM343.5 252.5v3h-3v-3zM343.5 268.5v3h-3v-3zM343.5 284.5v3h-3v-3zM327.5 12.5v3h-3v-3zM327.5 28.5v3h-3v-3zM327.5 44.5v3h-3v-3zM327.5 60.5v3h-3v-3zM327.5 76.5v3h-3v-3zM327.5 92.5v3h-3v-3zM327.5 108.5v3h-3v-3zM327.5 124.5v3h-3v-3zM327.5 140.5v3h-3v-3zM327.5 156.5v3h-3v-3zM327.5 172.5v3h-3v-3zM327.5 188.5v3h-3v-3zM327.5 204.5v3h-3v-3zM327.5 220.5v3h-3v-3zM327.5 236.5v3h-3v-3zM327.5 252.5v3h-3v-3zM327.5 268.5v3h-3v-3zM327.5 284.5v3h-3v-3zM311.5 12.5v3h-3v-3zM311.5 28.5v3h-3v-3zM311.5 44.5v3h-3v-3zM311.5 60.5v3h-3v-3zM311.5 76.5v3h-3v-3zM311.5 92.5v3h-3v-3zM311.5 108.5v3h-3v-3zM311.5 124.5v3h-3v-3zM311.5 140.5v3h-3v-3zM311.5 156.5v3h-3v-3zM311.5 172.5v3h-3v-3zM311.5 188.5v3h-3v-3zM311.5 204.5v3h-3v-3zM311.5 220.5v3h-3v-3zM311.5 236.5v3h-3v-3zM311.5 252.5v3h-3v-3zM311.5 268.5v3h-3v-3zM311.5 284.5v3h-3v-3zM295.5 12.5v3h-3v-3zM295.5 28.5v3h-3v-3zM295.5 44.5v3h-3v-3zM295.5 60.5v3h-3v-3zM295.5 76.5v3h-3v-3zM295.5 92.5v3h-3v-3zM295.5 108.5v3h-3v-3zM295.5 124.5v3h-3v-3zM295.5 140.5v3h-3v-3zM295.5 156.5v3h-3v-3zM295.5 172.5v3h-3v-3zM295.5 188.5v3h-3v-3zM295.5 204.5v3h-3v-3zM295.5 220.5v3h-3v-3zM295.5 236.5v3h-3v-3zM295.5 252.5v3h-3v-3zM295.5 268.5v3h-3v-3zM295.5 284.5v3h-3v-3zM279.5 12.5v3h-3v-3zM279.5 28.5v3h-3v-3zM279.5 44.5v3h-3v-3zM279.5 60.5v3h-3v-3zM279.5 76.5v3h-3v-3zM279.5 92.5v3h-3v-3zM279.5 108.5v3h-3v-3zM279.5 124.5v3h-3v-3zM279.5 140.5v3h-3v-3zM279.5 156.5v3h-3v-3zM279.5 172.5v3h-3v-3zM279.5 188.5v3h-3v-3zM279.5 204.5v3h-3v-3zM279.5 220.5v3h-3v-3zM279.5 236.5v3h-3v-3zM279.5 252.5v3h-3v-3zM279.5 268.5v3h-3v-3zM279.5 284.5v3h-3v-3zM263.5 12.5v3h-3v-3zM263.5 28.5v3h-3v-3zM263.5 44.5v3h-3v-3zM263.5 60.5v3h-3v-3zM263.5 76.5v3h-3v-3zM263.5 92.5v3h-3v-3zM263.5 108.5v3h-3v-3zM263.5 124.5v3h-3v-3zM263.5 140.5v3h-3v-3zM263.5 156.5v3h-3v-3zM263.5 172.5v3h-3v-3zM263.5 188.5v3h-3v-3zM263.5 204.5v3h-3v-3zM263.5 220.5v3h-3v-3zM263.5 236.5v3h-3v-3zM263.5 252.5v3h-3v-3zM263.5 268.5v3h-3v-3zM263.5 284.5v3h-3v-3zM247.5 12.5v3h-3v-3zM247.5 28.5v3h-3v-3zM247.5 44.5v3h-3v-3zM247.5 60.5v3h-3v-3zM247.5 76.5v3h-3v-3zM247.5 92.5v3h-3v-3zM247.5 108.5v3h-3v-3zM247.5 124.5v3h-3v-3zM247.5 140.5v3h-3v-3zM247.5 156.5v3h-3v-3zM247.5 172.5v3h-3v-3zM247.5 188.5v3h-3v-3zM247.5 204.5v3h-3v-3zM247.5 220.5v3h-3v-3zM247.5 236.5v3h-3v-3zM247.5 252.5v3h-3v-3zM247.5 268.5v3h-3v-3zM247.5 284.5v3h-3v-3zM231.5 12.5v3h-3v-3zM231.5 28.5v3h-3v-3zM231.5 44.5v3h-3v-3zM231.5 60.5v3h-3v-3zM231.5 76.5v3h-3v-3zM231.5 92.5v3h-3v-3zM231.5 108.5v3h-3v-3zM231.5 124.5v3h-3v-3zM231.5 140.5v3h-3v-3zM231.5 156.5v3h-3v-3zM231.5 172.5v3h-3v-3zM231.5 188.5v3h-3v-3zM231.5 204.5v3h-3v-3zM231.5 220.5v3h-3v-3zM231.5 236.5v3h-3v-3zM231.5 252.5v3h-3v-3zM231.5 268.5v3h-3v-3zM231.5 284.5v3h-3v-3zM215.5 12.5v3h-3v-3zM215.5 28.5v3h-3v-3zM215.5 44.5v3h-3v-3zM215.5 60.5v3h-3v-3zM215.5 76.5v3h-3v-3zM215.5 92.5v3h-3v-3zM215.5 108.5v3h-3v-3zM215.5 124.5v3h-3v-3zM215.5 140.5v3h-3v-3zM215.5 156.5v3h-3v-3zM215.5 172.5v3h-3v-3zM215.5 188.5v3h-3v-3zM215.5 204.5v3h-3v-3zM215.5 220.5v3h-3v-3zM215.5 236.5v3h-3v-3zM215.5 252.5v3h-3v-3zM215.5 268.5v3h-3v-3zM215.5 284.5v3h-3v-3zM199.5 12.5v3h-3v-3zM199.5 28.5v3h-3v-3zM199.5 44.5v3h-3v-3zM199.5 60.5v3h-3v-3zM199.5 76.5v3h-3v-3zM199.5 92.5v3h-3v-3zM199.5 108.5v3h-3v-3zM199.5 124.5v3h-3v-3zM199.5 140.5v3h-3v-3zM199.5 156.5v3h-3v-3zM199.5 172.5v3h-3v-3zM199.5 188.5v3h-3v-3zM199.5 204.5v3h-3v-3zM199.5 220.5v3h-3v-3zM199.5 236.5v3h-3v-3zM199.5 252.5v3h-3v-3zM199.5 268.5v3h-3v-3zM199.5 284.5v3h-3v-3zM183.5 12.5v3h-3v-3zM183.5 28.5v3h-3v-3zM183.5 44.5v3h-3v-3zM183.5 60.5v3h-3v-3zM183.5 76.5v3h-3v-3zM183.5 92.5v3h-3v-3zM183.5 108.5v3h-3v-3zM183.5 124.5v3h-3v-3zM183.5 140.5v3h-3v-3zM183.5 156.5v3h-3v-3zM183.5 172.5v3h-3v-3zM183.5 188.5v3h-3v-3zM183.5 204.5v3h-3v-3zM183.5 220.5v3h-3v-3zM183.5 236.5v3h-3v-3zM183.5 252.5v3h-3v-3zM183.5 268.5v3h-3v-3zM183.5 284.5v3h-3v-3zM167.5 12.5v3h-3v-3zM167.5 28.5v3h-3v-3zM167.5 44.5v3h-3v-3zM167.5 60.5v3h-3v-3zM167.5 76.5v3h-3v-3zM167.5 92.5v3h-3v-3zM167.5 108.5v3h-3v-3zM167.5 124.5v3h-3v-3zM167.5 140.5v3h-3v-3zM167.5 156.5v3h-3v-3zM167.5 172.5v3h-3v-3zM167.5 188.5v3h-3v-3zM167.5 204.5v3h-3v-3zM167.5 220.5v3h-3v-3zM167.5 236.5v3h-3v-3zM167.5 252.5v3h-3v-3zM167.5 268.5v3h-3v-3zM167.5 284.5v3h-3v-3zM151.5 12.5v3h-3v-3zM151.5 28.5v3h-3v-3zM151.5 44.5v3h-3v-3zM151.5 60.5v3h-3v-3zM151.5 76.5v3h-3v-3zM151.5 92.5v3h-3v-3zM151.5 108.5v3h-3v-3zM151.5 124.5v3h-3v-3zM151.5 140.5v3h-3v-3zM151.5 156.5v3h-3v-3zM151.5 172.5v3h-3v-3zM151.5 188.5v3h-3v-3zM151.5 204.5v3h-3v-3zM151.5 220.5v3h-3v-3zM151.5 236.5v3h-3v-3zM151.5 252.5v3h-3v-3zM151.5 268.5v3h-3v-3zM151.5 284.5v3h-3v-3zM135.5 12.5v3h-3v-3zM135.5 28.5v3h-3v-3zM135.5 44.5v3h-3v-3zM135.5 60.5v3h-3v-3zM135.5 76.5v3h-3v-3zM135.5 92.5v3h-3v-3zM135.5 108.5v3h-3v-3zM135.5 124.5v3h-3v-3zM135.5 140.5v3h-3v-3zM135.5 156.5v3h-3v-3zM135.5 172.5v3h-3v-3zM135.5 188.5v3h-3v-3zM135.5 204.5v3h-3v-3zM135.5 220.5v3h-3v-3zM135.5 236.5v3h-3v-3zM135.5 252.5v3h-3v-3zM135.5 268.5v3h-3v-3zM135.5 284.5v3h-3v-3zM119.5 12.5v3h-3v-3zM119.5 28.5v3h-3v-3zM119.5 44.5v3h-3v-3zM119.5 60.5v3h-3v-3zM119.5 76.5v3h-3v-3zM119.5 92.5v3h-3v-3zM119.5 108.5v3h-3v-3zM119.5 124.5v3h-3v-3zM119.5 140.5v3h-3v-3zM119.5 156.5v3h-3v-3zM119.5 172.5v3h-3v-3zM119.5 188.5v3h-3v-3zM119.5 204.5v3h-3v-3zM119.5 220.5v3h-3v-3zM119.5 236.5v3h-3v-3zM119.5 252.5v3h-3v-3zM119.5 268.5v3h-3v-3zM119.5 284.5v3h-3v-3zM103.5 12.5v3h-3v-3zM103.5 28.5v3h-3v-3zM103.5 44.5v3h-3v-3zM103.5 60.5v3h-3v-3zM103.5 76.5v3h-3v-3zM103.5 92.5v3h-3v-3zM103.5 108.5v3h-3v-3zM103.5 124.5v3h-3v-3zM103.5 140.5v3h-3v-3zM103.5 156.5v3h-3v-3zM103.5 172.5v3h-3v-3zM103.5 188.5v3h-3v-3zM103.5 204.5v3h-3v-3zM103.5 220.5v3h-3v-3zM103.5 236.5v3h-3v-3zM103.5 252.5v3h-3v-3zM103.5 268.5v3h-3v-3zM103.5 284.5v3h-3v-3zM87.5 12.5v3h-3v-3zM87.5 28.5v3h-3v-3zM87.5 44.5v3h-3v-3zM87.5 60.5v3h-3v-3zM87.5 76.5v3h-3v-3zM87.5 92.5v3h-3v-3zM87.5 108.5v3h-3v-3zM87.5 124.5v3h-3v-3zM87.5 140.5v3h-3v-3zM87.5 156.5v3h-3v-3zM87.5 172.5v3h-3v-3zM87.5 188.5v3h-3v-3zM87.5 204.5v3h-3v-3zM87.5 220.5v3h-3v-3zM87.5 236.5v3h-3v-3zM87.5 252.5v3h-3v-3zM87.5 268.5v3h-3v-3zM87.5 284.5v3h-3v-3zM71.5 12.5v3h-3v-3zM71.5 28.5v3h-3v-3zM71.5 44.5v3h-3v-3zM71.5 60.5v3h-3v-3zM71.5 76.5v3h-3v-3zM71.5 92.5v3h-3v-3zM71.5 108.5v3h-3v-3zM71.5 124.5v3h-3v-3zM71.5 140.5v3h-3v-3zM71.5 156.5v3h-3v-3zM71.5 172.5v3h-3v-3zM71.5 188.5v3h-3v-3zM71.5 204.5v3h-3v-3zM71.5 220.5v3h-3v-3zM71.5 236.5v3h-3v-3zM71.5 252.5v3h-3v-3zM71.5 268.5v3h-3v-3zM71.5 284.5v3h-3v-3zM55.5 12.5v3h-3v-3zM55.5 28.5v3h-3v-3zM55.5 44.5v3h-3v-3zM55.5 60.5v3h-3v-3zM55.5 76.5v3h-3v-3zM55.5 92.5v3h-3v-3zM55.5 108.5v3h-3v-3zM55.5 124.5v3h-3v-3zM55.5 140.5v3h-3v-3zM55.5 156.5v3h-3v-3zM55.5 172.5v3h-3v-3zM55.5 188.5v3h-3v-3zM55.5 204.5v3h-3v-3zM55.5 220.5v3h-3v-3zM55.5 236.5v3h-3v-3zM55.5 252.5v3h-3v-3zM55.5 268.5v3h-3v-3zM55.5 284.5v3h-3v-3zM39.5 12.5v3h-3v-3zM39.5 28.5v3h-3v-3zM39.5 44.5v3h-3v-3zM39.5 60.5v3h-3v-3zM39.5 76.5v3h-3v-3zM39.5 92.5v3h-3v-3zM39.5 108.5v3h-3v-3zM39.5 124.5v3h-3v-3zM39.5 140.5v3h-3v-3zM39.5 156.5v3h-3v-3zM39.5 172.5v3h-3v-3zM39.5 188.5v3h-3v-3zM39.5 204.5v3h-3v-3zM39.5 220.5v3h-3v-3zM39.5 236.5v3h-3v-3zM39.5 252.5v3h-3v-3zM39.5 268.5v3h-3v-3zM39.5 284.5v3h-3v-3zM23.5 12.5v3h-3v-3zM23.5 28.5v3h-3v-3zM23.5 44.5v3h-3v-3zM23.5 60.5v3h-3v-3zM23.5 76.5v3h-3v-3zM23.5 92.5v3h-3v-3zM23.5 108.5v3h-3v-3zM23.5 124.5v3h-3v-3zM23.5 140.5v3h-3v-3zM23.5 156.5v3h-3v-3zM23.5 172.5v3h-3v-3zM23.5 188.5v3h-3v-3zM23.5 204.5v3h-3v-3zM23.5 220.5v3h-3v-3zM23.5 236.5v3h-3v-3zM23.5 252.5v3h-3v-3zM23.5 268.5v3h-3v-3zM23.5 284.5v3h-3v-3zM7.5 12.5v3h-3v-3zM7.5 28.5v3h-3v-3zM7.5 44.5v3h-3v-3zM7.5 60.5v3h-3v-3zM7.5 76.5v3h-3v-3zM7.5 92.5v3h-3v-3zM7.5 108.5v3h-3v-3zM7.5 124.5v3h-3v-3zM7.5 140.5v3h-3v-3zM7.5 156.5v3h-3v-3zM7.5 172.5v3h-3v-3zM7.5 188.5v3h-3v-3zM7.5 204.5v3h-3v-3zM7.5 220.5v3h-3v-3zM7.5 236.5v3h-3v-3zM7.5 252.5v3h-3v-3zM7.5 268.5v3h-3v-3zM7.5 284.5v3h-3v-3z"></path></g></g><defs><radialGradient id="dots_svg__b" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="rotate(-136.519 627.256 92.637) scale(929.292 677.674)"><stop stop-color="#D9D9D9"></stop><stop offset="0.574" stop-color="#D9D9D9" stop-opacity="0"></stop></radialGradient><clipPath id="dots_svg__a"><path fill="#fff" transform="rotate(90 306 306)" d="M0 0h297v612H0z"></path></clipPath></defs></svg></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"site":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ke4xTi5OD9gnn2YtRXyCV","type":"Entry","createdAt":"2022-10-14T08:31:23.161Z","updatedAt":"2023-06-06T20:01:24.167Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"site"}},"locale":"en-US"},"fields":{"title":"Anyscale","description":"From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab.","identifier":"site-master-new","footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"16eb9I0yhaIALNOPR8WKTY","type":"Entry","createdAt":"2020-09-27T16:03:59.581Z","updatedAt":"2022-10-27T01:20:31.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer","body":"© Anyscale, Inc 2022","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"C7AXPFHtleWPOZ9BuT73K","type":"Asset","createdAt":"2020-09-28T14:40:50.925Z","updatedAt":"2020-09-28T14:40:50.925Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"anyscale_logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/C7AXPFHtleWPOZ9BuT73K/927f5130df1357db041d7a7908ab572e/Graphic_only__transparent_background_.png","details":{"size":1415,"image":{"width":331,"height":331}},"fileName":"Graphic only (transparent background).png","contentType":"image/png"}}}],"subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1cDdKq2eUfklc6w52dU6dg","type":"Entry","createdAt":"2020-09-27T16:05:42.307Z","updatedAt":"2022-10-27T01:20:26.714Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-1","header":"Products","body":"- [Anyscale Compute Platform](/platform)\n- [Ray Open Source](/ray-open-source)\n"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5tpXhcAJwSdhX8Wqw9HYTt","type":"Entry","createdAt":"2020-09-27T16:06:37.409Z","updatedAt":"2022-10-27T01:20:21.887Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":10,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-2","header":"Events","body":"- [Webinars](/events)\n- [Meetups](/events)\n- [Summits](/events)\n"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3c3LEomScM93hIg3i2zItU","type":"Entry","createdAt":"2020-09-27T16:07:42.856Z","updatedAt":"2022-10-27T01:20:18.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-3","header":"Company","body":"- [About Us](/about)\n- [News](/press)\n- [Careers](/careers)\n- [Community](/community)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6du4SSXPTxDZKoIVNHVVnT","type":"Entry","createdAt":"2022-10-27T01:20:13.320Z","updatedAt":"2022-10-27T01:20:13.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-4","header":"Learn","body":"- [Blog](/blog)\n- [Demos \u0026 Webinars](/events)\n- [Anyscale Academy](/events)\n- [Anyscale Docs](https://docs.anyscale.com)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1hovCXhGcWlgJmrqNzJ79k","type":"Entry","createdAt":"2022-10-27T01:20:09.895Z","updatedAt":"2022-10-27T01:20:09.895Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-5","header":"Scalable AI and Python","body":"- [Data Ingestion](/data-ingestion)\n- [Reinforcement Learning](/reinforcement-learning)\n- [Ray AIR](/ray-air)\n- [Model Serving](/model-serving)\n- [Hyperparameter Tuning](/hyperparameter-tuning)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"YFy5fUqwOD5ShdMaYEvzB","type":"Entry","createdAt":"2022-10-27T01:20:06.631Z","updatedAt":"2022-10-27T01:20:06.631Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-6","header":"Use Cases","body":"- [Demand Forecasting/Pricing](/demand-forecasting)\n- [Industrial Automation](/industrial-automation)\n- [Scalable ML Platforms](/machine-learning)\n- [NLP](/natural-language-processing)\n- [Recommendation System](/recommendation-system)"}}]}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"KBAZ5OavkdZfNTVjdJkp3","type":"Entry","createdAt":"2022-10-14T10:47:29.682Z","updatedAt":"2022-10-14T10:47:29.682Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Products","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KNN1Kmzo7jxSlxX1k74XQ","type":"Entry","createdAt":"2022-10-07T13:14:07.892Z","updatedAt":"2022-10-07T13:14:07.892Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"52Mmru1brS6ZkmItPeIW86","type":"Entry","createdAt":"2022-10-07T13:13:22.198Z","updatedAt":"2022-11-03T01:33:41.860Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Anyscale Platform","link":"/platform"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6cN6ZwdiFNiPcqIhd7EiQY","type":"Entry","createdAt":"2022-10-07T13:13:49.673Z","updatedAt":"2022-10-07T13:13:49.673Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Open Source","link":"/ray-open-source"}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5R1OTMfuLnPqont0Fjn9Xb","type":"Entry","createdAt":"2022-10-07T13:20:19.578Z","updatedAt":"2022-10-14T10:43:21.550Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Solutions","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1mp3IF3ZJE5JAViu3L9E95","type":"Entry","createdAt":"2022-10-07T13:16:00.346Z","updatedAt":"2022-10-07T13:21:01.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Scalable AI and Python","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4VUyKorAkgQP2V295h8eW3","type":"Entry","createdAt":"2022-10-07T13:15:35.949Z","updatedAt":"2022-10-07T13:15:35.949Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Data Ingestion","link":"/data-ingestion"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"Ouf5WgAzYmLovCZFNWqYm","type":"Entry","createdAt":"2022-10-07T13:17:09.728Z","updatedAt":"2022-10-07T13:17:09.728Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Reinforcement Learning","link":"/reinforcement-learning"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5kVPKqUbXBbKmrkvsImcIi","type":"Entry","createdAt":"2022-10-07T13:17:50.307Z","updatedAt":"2022-10-07T13:17:50.307Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray AIR","link":"/ray-air"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5uUidEvWw1oqjWrRcskEW","type":"Entry","createdAt":"2022-10-07T13:18:21.246Z","updatedAt":"2022-10-07T13:18:21.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Model Serving","link":"/model-serving"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Rbw0CrBN1KecPafCPvF4H","type":"Entry","createdAt":"2022-10-07T13:18:45.601Z","updatedAt":"2022-10-07T13:18:45.601Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Hyperparameter Tuning","link":"/hyperparameter-tuning"}}],"card":"##### Scalable AI and Python\n\nLeverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more."}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2RgWCuIiZe2TI6Nx5Ns7sB","type":"Entry","createdAt":"2022-10-07T13:25:03.101Z","updatedAt":"2023-05-31T12:47:25.274Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Use Cases","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3XMbpU1mPdsU7gktltlAzX","type":"Entry","createdAt":"2023-05-31T12:43:54.011Z","updatedAt":"2023-05-31T12:43:54.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Large Language Models","link":"/large-language-models"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6MAYw6HTSmH5JHcOoYjCzC","type":"Entry","createdAt":"2022-10-07T13:22:29.165Z","updatedAt":"2022-10-07T13:22:29.165Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Demand Forecasting/Pricing","link":"/demand-forecasting"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"30BlKhv6h4neaZwjA2jlxO","type":"Entry","createdAt":"2022-10-07T13:23:09.471Z","updatedAt":"2022-10-07T13:23:09.471Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Industrial Automation","link":"/industrial-automation"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2gGwUVxDuBeUso1uQH2gEW","type":"Entry","createdAt":"2022-10-07T13:23:32.145Z","updatedAt":"2022-10-07T13:23:32.145Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Scalable ML Platforms","link":"/machine-learning"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1cbwbmIvP5ZkV9iNEraQGP","type":"Entry","createdAt":"2022-10-07T13:23:59.732Z","updatedAt":"2022-10-07T13:23:59.732Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"NLP","link":"/natural-language-processing"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2A7FMbckpN5bgW1wp5ZuSh","type":"Entry","createdAt":"2022-10-07T13:24:41.861Z","updatedAt":"2022-10-07T13:24:41.861Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Recommendation System","link":"/recommendation-system"}}],"card":"##### Use Cases\n\nLeverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more."}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QNdrsmqz82oiBJvFTQ26n","type":"Entry","createdAt":"2023-04-13T11:37:45.354Z","updatedAt":"2023-04-13T11:37:45.354Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ecosystem","link":"/ray-ecosystem"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i0nHjF3pmjzGAdEE4V5MG","type":"Entry","createdAt":"2021-12-05T02:08:18.719Z","updatedAt":"2022-03-03T02:01:08.894Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Success Stories","link":"/user-stories"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5mFZvzY56aQCY4xvlBDi9W","type":"Entry","createdAt":"2022-10-14T10:46:35.218Z","updatedAt":"2022-10-14T10:46:35.218Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Learn","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"32tQWjMRAI5qci97W3cuie","type":"Entry","createdAt":"2022-10-07T13:28:46.057Z","updatedAt":"2023-03-22T20:31:55.243Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JQbOCOkxHfH2ZPGNNQVbt","type":"Entry","createdAt":"2021-12-05T14:57:28.664Z","updatedAt":"2021-12-05T14:57:28.664Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Blog","link":"/blog"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4qEkJgYkXZmiDq3lsxg0xW","type":"Entry","createdAt":"2023-03-22T20:31:21.666Z","updatedAt":"2023-03-22T20:31:21.666Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Summit 2023","link":"https://raysummit.anyscale.com/?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=home_nav-pulldown"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48sdeWXptwt3UgEZzeTt3k","type":"Entry","createdAt":"2022-10-07T13:26:28.331Z","updatedAt":"2022-10-07T13:26:28.331Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Videos \u0026 Webinars","link":"/events"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6hMfRHFDZUXcJH1tIq0xhY","type":"Entry","createdAt":"2021-10-21T20:59:36.918Z","updatedAt":"2023-03-06T20:24:26.811Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Training","link":"https://github.com/ray-project/ray-educational-materials"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cVWy7hmqbAzJCW6d7cLUx","type":"Entry","createdAt":"2022-10-07T13:28:33.537Z","updatedAt":"2022-10-07T13:28:33.537Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Anyscale Docs","link":"https://docs.anyscale.com"}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Qjs7zNfhrinHTNnzMpVIO","type":"Entry","createdAt":"2022-10-14T10:44:42.403Z","updatedAt":"2022-10-14T10:44:42.403Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Company","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ZFDrG8YGgU0oyNgBOId7X","type":"Entry","createdAt":"2022-10-07T13:32:38.746Z","updatedAt":"2022-10-07T13:32:38.746Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2RD65wmpn4gmL5Bvxu5VOD","type":"Entry","createdAt":"2020-09-08T17:53:32.413Z","updatedAt":"2021-06-20T15:04:11.482Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"About us","link":"/about"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71EHE1t3QrjnQb8hD9O8Ww","type":"Entry","createdAt":"2022-10-07T13:32:07.593Z","updatedAt":"2022-10-07T13:32:07.593Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"News","link":"/press"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3TNUPaz7ViQ6hbiZciBOQV","type":"Entry","createdAt":"2020-09-28T23:19:39.838Z","updatedAt":"2022-05-12T19:18:18.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Careers","link":"/careers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ubc5oYY5moS7HexIp7AjO","type":"Entry","createdAt":"2022-07-22T20:40:53.600Z","updatedAt":"2022-07-22T20:40:53.600Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Community","link":"/community"}}]}}]}}],"bannerText":"Are you passionate about Ray, scalable AI and distributed computing? Join us at Ray Summit!","bannerLink":"https://raysummit.anyscale.com/?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=homepage_ticker","bannerCta":" Find out more! "}},"title":"Blog","slug":"blog","sections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6nWVrKik3UzKQc0m6QjMQ7","type":"Entry","createdAt":"2020-09-01T18:35:40.585Z","updatedAt":"2023-06-20T17:29:52.368Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":59,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"featured-posts","header":"Blog","subheader":"Featured Posts and News","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7xkHJWpQ3mRxrgAQw3oYFk","type":"Entry","createdAt":"2023-06-15T13:47:45.558Z","updatedAt":"2023-06-16T13:24:48.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Introducing the Anyscale Databricks Connector","seoTitle":"Integrating Anyscale Ray with Databricks","slug":"introducing-the-anyscale-databricks-connector","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3e6JquwKuIz6QvCWO0BAz","type":"Entry","createdAt":"2023-06-15T13:16:27.453Z","updatedAt":"2023-06-15T13:16:27.453Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Greene","slug":"eric-greene","link":"https://www.linkedin.com/in/greene/"}}],"publishedDate":"2023-06-15","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Introducing the Anyscale Databricks connector. Simple Data Transfer and Efficient AI Workflows with Anyscale and Databricks.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The Anyscale Databricks Connector is a new Ray Datasets capability that facilitates easy data transfer between Databricks clusters and Anyscale hosted Ray clusters. The connector makes it easy for Data Scientists using Anyscale to leverage their Databricks data lake during machine learning and discovery.  It also enables a simpler way for machine learning engineers to create end-to-end workloads by allowing the entire ML pipeline to be executed within a single Python script. By taking advantage of the highly-scalable nature of Ray and Ray Datasets, machine learning workloads such as training, tuning and batch serving jobs can be executed more quickly and at lower cost, all while taking advantage of the latest advancements in AI through other Ray integrations with Hugging Face, XGBoost, LightGBM and many AI frameworks and libraries.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Key Benefits","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simple access to data","nodeType":"text"},{"data":{},"marks":[],"value":" - Access Databricks data through SQL queries within Anyscale Workspaces Visual Code or JupyterHub environments, empowering data science discovery and development.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Improved data security and governance","nodeType":"text"},{"data":{},"marks":[],"value":" - Ensure data security and governance by directly copying data from Databricks data lakes to Ray clusters, eliminating the need for intermediate steps and maintaining separate controls over sensitive data. All data is encrypted in transit and at rest.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Highly-scalable data exchange","nodeType":"text"},{"data":{},"marks":[],"value":" - Take advantage of the parallel read and write capabilities of Ray Datasets, enabling the exchange of terabytes of data in minutes. This capability significantly speeds up data-intensive operations and enhances scalability while reducing job run times and overall costs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simplified workload development","nodeType":"text"},{"data":{},"marks":[],"value":" - Simplify machine learning workflows by consolidating all the necessary logic into a single script. This script can query features, train and tune models, and score and materialize results back into the data lake, streamlining the entire process.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Unlock the latest AI Innovations","nodeType":"text"},{"data":{},"marks":[],"value":" - Leverage the power of Databricks for querying and joining data, while benefiting from the scalability and simplicity of Ray AIR for machine learning and AI development. Ray AIR provides integration to the latest AI innovations such as pretrained Hugging Face language models.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Simple, Secure and Scalable Data Access","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing the Anyscale Databricks connector, large datasets can be queried from a Databricks SQL warehouse, and quickly transferred into a Ray Dataset distributed across the Ray cluster. The Databricks connector reads query results in chunks of data, in parallel across the Ray cluster. The size of the dataset and the speed at which it can be transferred scales based on the size of your Ray cluster and the number of simultaneous requests the underlying Databricks SQL warehouse supports.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6taTlZK9ffeivO5RCUJiQ4","type":"Asset","createdAt":"2023-06-15T13:27:43.116Z","updatedAt":"2023-06-15T13:27:43.116Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"1 - Anyscale Databricks Architecture","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6taTlZK9ffeivO5RCUJiQ4/c5fc0bfef768f0fb6e8785bf71ded1e2/1_-_Anyscale_Databricks_Arch.jpg","details":{"size":95989,"image":{"width":761,"height":478}},"fileName":"1 - Anyscale Databricks Arch.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Anyscale Databricks connectors, parallel reads and writes and overall speed will scale with the size of the Ray cluster. The benchmarks for reading and writing demonstrate how the cluster scales out to support larger data sets.  ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2KimYEe845rk6fb2oYaCmz","type":"Asset","createdAt":"2023-06-15T13:35:36.597Z","updatedAt":"2023-06-15T13:35:36.597Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2 - Anyscale Databricks Table Example","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2KimYEe845rk6fb2oYaCmz/8302017d6535b5d676d3dfb50dc12cbd/2_-_Anyscale_Databricks_Table_Example.jpg","details":{"size":218942,"image":{"width":2178,"height":1002}},"fileName":"2 - Anyscale Databricks Table Example.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Simplified ML Development","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing the Databricks connector from within an Anyscale Workspaces, Data Scientists and Machine Learning Engineers have a unified experience while developing workloads. The data query, feature engineering, training, tuning and inference can all be executed within a single Python script that scales across a Ray cluster. Ray integrates with the latest AI and Machine learning libraries, enabling the most advanced ML workloads that work with 3rd party libs  like Hugging Face, XGBoost, LightGBM, and most PyTorch and TensorFlow model architectures.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7c7IX8TpJKbaXdnLIwoD88","type":"Asset","createdAt":"2023-06-15T13:39:22.298Z","updatedAt":"2023-06-15T13:39:22.298Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"3 - Anyscale Databricks UI","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7c7IX8TpJKbaXdnLIwoD88/f17fb8c8884ce197c9ad35a8ce3f6b4e/3_-_Anyscale_Databricks_Product_UI.jpg","details":{"size":242920,"image":{"width":2226,"height":1230}},"fileName":"3 - Anyscale Databricks Product UI.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2muX02EO70X2MXN8wBRyUJ","type":"Asset","createdAt":"2023-06-15T13:41:08.580Z","updatedAt":"2023-06-15T13:41:08.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"4 - Anyscale Databricks Product UI 2","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2muX02EO70X2MXN8wBRyUJ/cc5301c192c66e632cb3141a9e5bde55/4_-_Anyscale_Databricks_Product_UI_2.jpg","details":{"size":383181,"image":{"width":1860,"height":1282}},"fileName":"4 - Anyscale Databricks Product UI 2.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlock new use cases with AI and ML ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nAnyscale and Ray integrate with most open source AI and ML libraries, enabling the latest innovation in AI to be applied to Databricks data. Ray makes working with Hugging face, XGBoost, LightGBM, TensorFlow and PyTorch and SciKit Learn a unified experience, with the added benefits of scaling with distributed training, tuning and serving.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical ML workload to train a LightGBM can be implemented in 20 lines of code.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6rUeSQ88fvbTd68jkhrii0","type":"Asset","createdAt":"2023-06-15T13:42:58.984Z","updatedAt":"2023-06-15T13:42:58.984Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"5 - Anyscale Databricks deployment","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6rUeSQ88fvbTd68jkhrii0/43de8dc64eaf8792ad73ac6faf8b4551/5_-_Anyscale_Databricks_deployment.jpg","details":{"size":272363,"image":{"width":1470,"height":1402}},"fileName":"5 - Anyscale Databricks deployment.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Want to see for yourself? ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/signup"},"content":[{"data":{},"marks":[],"value":"Request a Trial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" today!\n","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71IpYQvsxplh7okIL7RJVx","type":"Asset","createdAt":"2023-06-15T13:50:54.646Z","updatedAt":"2023-06-16T13:19:09.178Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"locale":"en-US"},"fields":{"title":"0 -Anyscale Databricks headline image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/71IpYQvsxplh7okIL7RJVx/b193e65fbc2d6500e3ccb2162d323080/0_-_Updated_Databricks_Anyscale_Image.png","details":{"size":90723,"image":{"width":1500,"height":1000}},"fileName":"0 - Updated Databricks Anyscale Image.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2GdxAQ2xXKoNlzKc7DXvLS","type":"Entry","createdAt":"2023-05-31T19:04:09.560Z","updatedAt":"2023-06-01T03:29:01.792Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Announcing Aviary: Open Source Multi-LLM Serving","slug":"announcing-aviary-open-source-multi-llm-serving-solution","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-05-31","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Exec Summary","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": Open source LLMs are getting better every day, so today we’re open sourcing Aviary to make testing, evaluating and deploying Open Source LLMs easier – we found it harder than we thought it should be so we used Ray Serve to fix it. ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Try it out with Aviary Explorer ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://aviary.anyscale.com/"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Github Repo ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/aviary"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Video demo and explanation ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.youtube.com/watch?v=WmqPfQOXJ-4"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Slack support ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Forum support ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/c/llms-generative-ai/27"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Want a managed version of Aviary? Sign up ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://bit.ly/run-aviary"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". \n\nWe’re excited to announce the release of Aviary: a new open source project that simplifies and enables easy self-hosted serving of multiple LLM models efficiently. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We’re big fans of open source LLMs here at Anyscale. The rate of improvement of open source LLMs has been ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.semianalysis.com/p/google-we-have-no-moat-and-neither"},"content":[{"nodeType":"text","value":"nothing short of phenomenal","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". This has given the AI community many options beyond the big “closed” players like OpenAI, Anthropic, Cohere and more. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Why are companies exploring self hosted open source LLMs? There are a few reasons: ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Cost: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"LLMs are very expensive to operate. A single query can cost tens of cents. Using open source can be considerably cheaper. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Latency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"By colocating LLMs and business logic, sometimes even on the same machine, latency – one of the biggest issues with deploying LLMs – can be kept low.  ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Transparency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Organizations want to understand exactly what is happening in their models. Open source allows them to understand what’s happening inside them.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Deployment flexibility: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Open Source models can be deployed on premise, in the cloud using the user’s cloud resources, or as part of a SaaS offering.  ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Data Control: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Data governance issues are much easier to guarantee if the data does not leave your control. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Customization:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" Customizing/fine-tuning on proprietary data to enable domain specific answers at high quality.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We were writing some tools for ourselves to really understand the answers to the questions above, as well as the advantages of closed models, mainly focused around quality. A funny thing happened on the way, however: we discovered that while the open source models have improved, the infrastructure for serving LLMs has not kept up. We started building our own library for loading the models, autoscaling them efficiently, etc on top of Ray Serve, dealing with variations between the models on things like stop tokens etc. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It was then that we realized, actually the rest of the LLM community might benefit from this library as well. So, today we are releasing Aviary as open source. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary helps leverage the advantages of open source models by building on the solid foundation of ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io"},"content":[{"nodeType":"text","value":"Ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", the popular framework for scalable AI. In particular, it takes advantage of ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"nodeType":"text","value":"Ray Serve","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", the highly flexible serving framework that is part of Ray. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary does this by: ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Providing an extensive suite of pre-configured open source LLMs, with reasonable defaults that work out of the box. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Integrating acceleration approaches like DeepSpeed with the packaged LLMs. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Simplifying the deployment of multiple LLMs within a single unified framework. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Simplifying the addition of new LLMs to within minutes in most cases","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Offering unique autoscaling support, including scale-to-zero (a first in open source). ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We’ve also included a demo Gradio frontend that shows off what’s possible with these capabilities, as well as some command line tools to address the evaluation questions that originally motivated this project. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"You can see these features in the video below: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jx26kCBQB1GRKEulIDxoi","type":"Entry","createdAt":"2023-05-31T02:10:20.974Z","updatedAt":"2023-05-31T02:10:20.974Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Aviary Announcement Video","videoUrl":"https://www.youtube.com/watch?v=WmqPfQOXJ-4"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"While existing solutions have some of these features individually and we are grateful to build on their foundations (such as Hugging Face’s text-generation-inference), none of the existing solutions brings these capabilities together in a way that is convenient for users. We are also planning to continue to expand the feature set of Aviary, adding support for features like streaming, continuous batching and others.  ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary is also open to community contributions – especially for adding new LLMs. We’ll then redeploy these in production so the rest of the world can use those LLMs too. We will also be supporting Aviary through Slack, Discourse, GitHub issues and an LLM-based RayBot. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"At the same time we understand that not everyone wants to take on the additional challenges of maintaining their own aviary of models. For that reason, we will also be offering a hosted version of Aviary that builds on top of our Anyscale managed Ray platform and offers additional features around deployment including:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Using spot instances with on-demand fallback for large potential savings","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Zero downtime upgrades with no dropped requests","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Faster deployment and autoscaling","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Improved observability tools. ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We are also announcing that Aviary will be available at no additional charge for existing Anyscale customers via our workspaces solution, and we’re actively onboarding new Aviary customers now. If you’d like to deploy Aviary, please reach out to us ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://bit.ly/run-aviary"},"content":[{"nodeType":"text","value":"here","marks":[],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1c2aG5pfLWU6YenPBZIHvI","type":"Asset","createdAt":"2023-05-31T19:00:13.845Z","updatedAt":"2023-05-31T19:00:13.845Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Aviary Light Mode","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1c2aG5pfLWU6YenPBZIHvI/742f647b979b4eb4b682e37fc35c71a7/aviary-light-mode.png","details":{"size":712180,"image":{"width":3030,"height":1750}},"fileName":"aviary-light-mode.png","contentType":"image/png"}}},"mainImageFit":"contain","hideIntro":true,"showMainImage":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UsGSYssf1ebf8N5mRbNxT","type":"Entry","createdAt":"2023-05-17T17:26:50.771Z","updatedAt":"2023-05-24T20:17:43.464Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Numbers every LLM Developer should know","slug":"num-every-llm-developer-should-know","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-05-17","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"When I was at Google, there was a document put together by Jeff Dean, the legendary engineer, called ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://brenocon.com/dean_perf.html"},"content":[{"nodeType":"text","value":"Numbers every Engineer should know","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Prompts","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"40-90%: Amount saved by appending “Be Concise” to your prompt","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money [1]. This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money. ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"1.3: Average tokens per word","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document will be about 1000 tokens.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Prices","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Prices [2] are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark. ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~50: Cost Ratio of GPT-4 to GPT-3.5 Turbo","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"What this means is that for many practical applications, it’s much better to use GPT-4 for things like generation and then use that data to fine tune a smaller model. It is roughly 50 times cheaper to use GPT-3.5-Turbo [3] than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output)  – so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example. ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"5: Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x less [4] than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x! ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"10: Cost Ratio of OpenAI embedding to Self-Hosted embedding ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using HuggingFace’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees). ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"6: Cost Ratio of OpenAI base vs fine tuned model queries","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model. ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"1: Cost Ratio of Self-Hosted base vs fine-tuned model queries ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Training and Fine Tuning\n","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://arxiv.org/abs/2302.13971"},"content":[{"nodeType":"text","value":"LLaMa paper","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray"},"content":[{"nodeType":"text","value":"bread and butter","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"! Contact us if you’d like to learn more). The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model. ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"\u003c 0.001: Cost ratio of fine tuning vs training from scratch","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using"},"content":[{"nodeType":"text","value":"6B parameter model for about $7","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $40. However, fine tuning is one thing and training from scratch is another [5].\n","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"GPU Memory","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning. ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices. ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"2x number of parameters: Typical GPU memory requirements of an LLM for serving","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy there is too much loss of resolution. Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical. ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~1GB: Typical GPU memory requirements of an embedding model","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/"},"content":[{"nodeType":"text","value":"sentence transformers","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". OpenAI also has its own embeddings that they provide commercially. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU. ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"\u003e10x: Throughput improvement from batching LLM requests ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second.  The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point. ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~1 MB: GPU Memory required for 1 token of output with a 13B parameter model","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"embedded-entry-inline","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1ACoDuIrrm71E0BWViBO4Y","type":"Entry","createdAt":"2023-05-17T17:26:34.593Z","updatedAt":"2023-05-17T17:26:34.593Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"inlineImage"}},"locale":"en-US"},"fields":{"name":"numbers-cheatsheet","image":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1AkaPcJlWoSpqixtqeKcD1","type":"Asset","createdAt":"2023-05-17T16:48:22.989Z","updatedAt":"2023-05-17T20:48:45.978Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"numbers-cheatsheet","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1AkaPcJlWoSpqixtqeKcD1/9505980d855c36120b4818980745fd00/Screenshot_2023-05-17_at_1.46.09_PM.png","details":{"size":550573,"image":{"width":2194,"height":1734}},"fileName":"Screenshot 2023-05-17 at 1.46.09 PM.png","contentType":"image/png"}}},"isRetina2x":false}}},"content":[]},{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Next Steps","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Review the up-to-date metrics referenced in this blog in the following ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/llm-numbers"},"content":[{"nodeType":"text","value":"Github repo","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". \n\nSee our earlier ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"nodeType":"text","value":"blog series on solving Generative AI infrastructure","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" ","marks":[{"type":"underline"}],"data":{}},{"nodeType":"text","value":"and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"nodeType":"text","value":"using LangChain with Ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in learning more about Ray, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io/"},"content":[{"nodeType":"text","value":"Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://docs.ray.io/"},"content":[{"nodeType":"text","value":"Docs.Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nTo connect with the Ray community join #LLM on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"Ray Slack","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" or our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"Discuss forum","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://www.anyscale.com/platform"},"content":[{"nodeType":"text","value":"Anyscale.com/Platform ","marks":[],"data":{}}]},{"nodeType":"text","value":"and click the 'Try it now' button","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Summit 2023:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"nodeType":"text","value":"Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"nodeType":"text","value":"practical training focused on LLMs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"hr","data":{},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[1] Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2022-05-14. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[2] Retrieved from ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://openai.com/pricing"},"content":[{"nodeType":"text","value":"http://openai.com/pricing","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on 2022-05-14.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[3] ","marks":[],"data":{}},{"nodeType":"text","value":"GPT-4","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). ","marks":[],"data":{}},{"nodeType":"text","value":"GPT-3.5 Turbo","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": 0.2c/1k tokens.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[4] This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[5] 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3ZnD84ZZAfH60U5ncBebaO","type":"Asset","createdAt":"2023-05-17T15:56:36.431Z","updatedAt":"2023-05-17T18:04:30.622Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"numbers-cover-image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3ZnD84ZZAfH60U5ncBebaO/227f68207ec7731e789f342e7ec320e8/Screenshot_2023-05-17_at_10.12.01_AM.png","details":{"size":445922,"image":{"width":2348,"height":1616}},"fileName":"Screenshot 2023-05-17 at 10.12.01 AM.png","contentType":"image/png"}}},"mainImageFit":"contain","hideIntro":true,"showMainImage":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3vUo7K4aHmhARGamZX7Ujj","type":"Entry","createdAt":"2023-05-11T15:17:24.618Z","updatedAt":"2023-06-16T21:13:49.520Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Streaming distributed execution across CPUs and GPUs","seoTitle":"Streaming distributed execution across CPUs and GPUs","slug":"streaming-distributed-execution-across-cpus-and-gpus","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2aJRcdEj9iAktAnqn0wucE","type":"Entry","createdAt":"2021-02-16T07:30:38.421Z","updatedAt":"2021-02-16T07:30:38.421Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Stephanie Wang","slug":"stephanie-wang","link":"https://github.com/stephanie-wang"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2edothjNBpyQEu2aX55KRl","type":"Entry","createdAt":"2023-05-10T15:30:16.699Z","updatedAt":"2023-05-10T15:30:16.699Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Cheng Su","slug":"cheng-su"}}],"publishedDate":"2023-05-11","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"In a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/offline-batch-inference-comparing-ray-apache-spark-and-sagemaker"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"previous blog,","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" we showed how Ray Data can provide speedups over SageMaker and frameworks like Apache Spark for large-scale batch inference workloads. This blog post delves further into how Ray Data streaming works and how to use it for your own ML pipelines.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Some of the most demanding machine learning (ML) use cases we have encountered involve pipelines that span both CPU and GPU devices in distributed environments. These situations arise in various workloads, including:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Batch inference, which involves a CPU-intensive preprocessing stage (e.g., video decoding or image resizing) before utilizing a GPU-intensive model to make predictions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed training, where similar CPU-heavy transformations are required to prepare or augment the dataset prior to GPU training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Interestingly, in many of these workloads, the preprocessing steps often prove to be the bottleneck. This can happen when preprocessing requires ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"parallelism across multiple nodes","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"significant memory to buffer results","nodeType":"text"},{"data":{},"marks":[],"value":" between the CPU and GPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For instance, consider the decoding of compressed images in memory. A typical JPEG decompression ratio is 10:1, which implies that the output memory size may be ten times that of the input, resulting in substantial memory pressure in addition to CPU load. The challenges become more complex with other data modalities, such as video; for example, H264 can decompress at a 2000:1 ratio, producing 200GB of frame outputs for a 100MB file input. This means that practitioners trying to offload CPU-heavy preprocessing onto multiple CPUs or machines have to deal with intermediate results that are far larger than their already sizable source datasets:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7ntuAY4zsUXtrtIxSofnyg","type":"Asset","createdAt":"2023-05-10T15:33:44.406Z","updatedAt":"2023-05-10T18:55:03.562Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"streaming_figure_1","description":"Figure 1. Decoding of compressed images with large images spilled to disk","file":{"url":"//images.ctfassets.net/xjan103pcp94/7ntuAY4zsUXtrtIxSofnyg/1fdb9631162d6b66777d205d1d3a7e1c/Screenshot_from_2023-05-10_11-00-43.png","details":{"size":64895,"image":{"width":1493,"height":436}},"fileName":"Screenshot from 2023-05-10 11-00-43.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Executed naively, these compute and memory-intensive workloads fail to fully utilize expensive GPU hardware. For example, we could buffer 200GB of frame outputs on disk, but that could add minutes of overheads. Even if this is overlapped with GPU computation, we can end up with long pauses on the GPU. This motivates fully streamed execution in the cluster setting, which avoids such delays by streaming intermediate data through cluster memory:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"40VpN8OKxEM7wHohV802gE","type":"Asset","createdAt":"2023-05-10T15:36:35.326Z","updatedAt":"2023-05-10T18:56:06.767Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_2","description":"Figure 2. Decoding intermediate data streamed through memory","file":{"url":"//images.ctfassets.net/xjan103pcp94/40VpN8OKxEM7wHohV802gE/c07086bfe677dc5e3109496f5a4ec083/Screenshot_from_2023-05-10_11-01-00.png","details":{"size":35974,"image":{"width":1199,"height":268}},"fileName":"Screenshot from 2023-05-10 11-01-00.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we guide you through our development of a versatile streaming backend for Ray Data, and show examples of how to implement the demanding use cases mentioned earlier. We discuss the performance benefits of implementing pipelined execution across CPUs and GPUs compared to bulk processing, and discuss its broader applicability to various workloads, including distributed training.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pipelined execution in a cluster","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To understand the necessity and advantages of pipelined (or streaming) execution across CPU and GPU devices, let's first examine how a ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Bulk_synchronous_parallel"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"bulk synchronous parallel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (BSP) framework might handle batch inference. Because of its simplicity and generality, BSP is a common way frameworks (e.g., MapReduce, Apache Spark) parallelize distributed computations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Note that pipelining within a single machine is already commonly used in data loading libraries such as ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Torch DataLoader","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://petastorm.readthedocs.io/en/latest/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Petastorm.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" Here, we discuss pipelining computations across an entire cluster of machines.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical batch inference job consists of the following operations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6mkS4HlXNovPW6Dl387jYe","type":"Asset","createdAt":"2023-05-10T15:40:10.724Z","updatedAt":"2023-05-10T19:28:57.091Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"locale":"en-US"},"fields":{"title":"streaming_figure_3","description":"Figure 3. A typical batch inference job sequential operations","file":{"url":"//images.ctfassets.net/xjan103pcp94/6mkS4HlXNovPW6Dl387jYe/1a707558e51ac00b776ae6f5d808b74e/Screenshot_from_2023-05-10_12-23-58.png","details":{"size":33844,"image":{"width":1526,"height":244}},"fileName":"Screenshot from 2023-05-10 12-23-58.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To implement such an inference job, you'd write the code using Ray Data (or another similar framework):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1kYVOacE6epR4czS5YoOHX","type":"Entry","createdAt":"2023-05-10T15:41:24.239Z","updatedAt":"2023-05-10T15:41:24.239Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_1","body":"import ray\n\n# Define model and preprocessor.\nmodel = ...\npreprocess_fn = ...\n\n# Define the inference function.\ndef model_fn(batch: Dict[str, np.ndarray]):\n    return {\"results\": model(batch)}\n\n# The following snippet implements the pipeline.\nray.data.read_parquet(...) \\           # 1. Load\n    .map_batches(preprocess_fn) \\      # 2. Preprocess\n    .map_batches(model_fn) \\           # 3. Inference\n    .write_parquet(...)                # 4. Save\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a BSP framework, each of these operations can be modeled as a 1:1 transformation, which can be fused together into a single stage. Each task within the stage will execute the mentioned operations locally in sequence:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4oQ40RlSK1AF7Y4ZnVGV8J","type":"Asset","createdAt":"2023-05-10T15:44:10.631Z","updatedAt":"2023-05-10T18:59:16.654Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_4","description":"Figure 4. BSP framework operations modeled as a 1:1 transformation","file":{"url":"//images.ctfassets.net/xjan103pcp94/4oQ40RlSK1AF7Y4ZnVGV8J/a0413e90a55ec69d7d0ba49554a7f2a0/Screenshot_from_2023-05-10_11-07-39.png","details":{"size":103326,"image":{"width":1511,"height":583}},"fileName":"Screenshot from 2023-05-10 11-07-39.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This execution strategy is ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"memory optimal","nodeType":"text"},{"data":{},"marks":[],"value":", as there is no intermediate data and execution is inherently pipelined (since there is only a single stage). However, challenges emerge in a heterogeneous setting, where certain pipeline steps may prefer to run on CPUs or GPUs independently. In Ray Data, such inference pipelines are expressed with transformations that launch actors scheduled onto GPUs ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"(num_gpus=1 and compute=ActorPoolStrategy)","nodeType":"text"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3nLf8c4wMq9KInsI4ca7HB","type":"Entry","createdAt":"2023-05-10T15:46:05.506Z","updatedAt":"2023-05-10T16:00:09.475Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_2","body":"import ray\nfrom ray.data import ActorPoolStrategy\n\nModel = ...\npreprocess_fn = ...\n\n# Define the model as a stateful class with cached setup.\nclass MyModelCallableCls:\n    def __init__(self):\n        self.model = Model(...)\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e\n        return {\"results\": self.model(batch)}\n\n# Modify the pipeline to use GPU actors.\nray.data.read_parquet(...) \\\n    .map_batches(preprocess_fn) \\\n    .map_batches(\n          MyModelCallableCls,\n          num_gpus=1,\n          compute=ActorPoolStrategy(size=N)) \\\n    .write_parquet(...)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the above code snippet, the load and preprocessing steps run on CPU (Stage 1), inference runs on GPUs (Stage 2), and then the result saving runs on CPU again (Stage 3). This configuration leads to spilling of data to remote storage when intermediate results (e.g., decoded video frames) exceed cluster memory sizes. Hence, we see that ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"BSP is not memory optimal for heterogeneous workloads","nodeType":"text"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"66FIu8nzl2kZRB1Ht5xZFL","type":"Asset","createdAt":"2023-05-10T15:49:26.155Z","updatedAt":"2023-05-10T19:00:24.815Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_5","description":"Figure 5. BSP is not memory optimal for heterogeneous workloads","file":{"url":"//images.ctfassets.net/xjan103pcp94/66FIu8nzl2kZRB1Ht5xZFL/295ab9176fab5d04ee47ac53c5e1b418/Screenshot_from_2023-05-10_11-02-18.png","details":{"size":162611,"image":{"width":1750,"height":730}},"fileName":"Screenshot from 2023-05-10 11-02-18.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"These unnecessary overheads can be avoided with end-to-end pipelining (i.e., streaming) across the cluster, which we present in the next section.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data streaming","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In Ray 2.4, we've changed the default execution strategy of Ray Data to streaming from bulk synchronous. These streaming Datasets are fully backwards compatible with the existing API, i.e., they can be transformed lazily with map operations, support shuffle operations, and also caching / materialization in memory:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5CSJWRWgN6pQwxcFMBhMSr","type":"Entry","createdAt":"2023-05-10T15:50:53.654Z","updatedAt":"2023-05-10T15:59:44.870Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_3","body":"# Create a dataset over parquet files\nds: ray.data.Dataset = ray.data.read_parquet(...)\n\n# Transform the dataset\nds = ds.map_batches(my_preprocess_fn)\nds = ds.map_batches(my_model_fn)\n\n# Iterate over dataset batches in streaming fashion\nfor batch in ds.iter_batches():\n   print(batch)\n\n# Materialize all contents of the dataset in memory\nds = ds.materialize()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In other words, the Ray Data API now leverages streaming execution for improved performance on large datasets, with the same simple transformation API as in previous Ray versions.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Video processing example","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To more fully understand the capabilities of this API, let's walk through what this looks like in an example video processing pipeline. In the pipeline, we'll first decode video frames, annotate each frame with an ML model, apply a classification model to the annotated frames, and then save the results to JSON:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"s1kmF3kQraHomGuDsvOQt","type":"Asset","createdAt":"2023-05-10T15:55:25.682Z","updatedAt":"2023-05-10T19:30:26.377Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"streaming_figure_6","description":"Figure 6. A pipeline decoding video frames, annotating frames, applying classification, and saving results to JSON.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/s1kmF3kQraHomGuDsvOQt/9385b71522afe76eebc6342f54387ffa/Screenshot_from_2023-05-10_12-24-36.png","details":{"size":38452,"image":{"width":1478,"height":230}},"fileName":"Screenshot from 2023-05-10 12-24-36.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's start by declaring the components of our pipeline. For brevity, we'll just provide stub classes here, but you can find the ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/ericl/ac086f0a9bdf41b6101dca665045dfec"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"full skeleton here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (and the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference docs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") that can be adapted to your own workloads. Data between these steps is kept in dicts of numpy arrays:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3DoR7DQ01JPe4k0MjPdubR","type":"Entry","createdAt":"2023-05-10T15:57:40.210Z","updatedAt":"2023-05-10T15:59:24.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_5","body":"# Declare a function that decodes frames.\ndef decode_frames(batch: Dict[str, np.ndarray]) -\u003e Dict:\n    video_data = batch[\"bytes\"]\n    ...\n    return {\"frames\": decoded_frames}\n\n# Declare a model that annotates decoded frame data.\nclass FrameAnnotator:\n    def __init__(self):\n        self.model = ...\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e Dict:\n        frames = batch[\"frames\"]\n        return {\"annotated_frames\": self.model(frames)}\n\n# Declare a model that classifies annotated frame data.\nclass FrameClassifier:\n    def __init__(self):\n        self.model = ...\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e Dict:\n        frames = batch[\"annotated_frames\"]\n        return {\"results\": self.model(frames)}","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next, we'll use Ray Data to connect these steps together. This takes just a few lines of code, and we can customize the resource requirements per step to configure CPUs and GPUs:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1U2wSU4PEADDsQpVQqJJAa","type":"Entry","createdAt":"2023-05-10T15:59:06.340Z","updatedAt":"2023-05-10T15:59:06.340Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_6","body":"# Create a dataset from video binary data.\nds = ray.data.read_binary(...)\n\n# Apply the decode step. We can customize the resources per\n# task. Here each decode task requests 4 CPUs from Ray.\nds = ds.map_batches(decode_frames, num_cpus=4)\n\n# Apply the annotation step, using an actor pool of size 5.\n# Each actor runs on a CPU.\nds = ds.map_batches(\n    FrameAnnotator,\n    compute=ActorPoolStrategy(size=5))\n\n# Apply the classification step, using a pool of 2 GPU actors,\n# and a fixed data batch size of 64 for the actor calls.\nds = ds.map_batches(\n    FrameClassifier,\n    num_gpus=1,\n    batch_size=64,\n    compute=ActorPoolStrategy(size=2))\n\n# Trigger execution and write outputs to json.\nds.write_json(\"/tmp/output\")\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The configured streaming topology from above looks like this logically. Since each step has a unique resource requirement / task vs actor strategy, here every step becomes its own stage (Ray Data fuses steps with equivalent resource requirements):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2XaeSiV04QKLILBLAlD0t5","type":"Asset","createdAt":"2023-05-11T15:44:03.608Z","updatedAt":"2023-05-11T15:44:03.608Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"streaming_figure_7","description":"Figure 7: Streaming execution across CPUs and GPUs","file":{"url":"//images.ctfassets.net/xjan103pcp94/2XaeSiV04QKLILBLAlD0t5/d75fceb54323c7587312028b40a4b37c/Screenshot_from_2023-05-10_11-02-51.png","details":{"size":68124,"image":{"width":1445,"height":299}},"fileName":"Screenshot from 2023-05-10 11-02-51.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We run the above script (","nodeType":"text"},{"data":{"uri":"https://gist.github.com/ericl/ac086f0a9bdf41b6101dca665045dfec"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GitHub gist","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") in an Anyscale workspace with 2 GPU nodes and 5 CPU nodes (you can use the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/getting-started.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray cluster launcher","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to also launch a cluster for this):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3hsRFIiW2RGkFM5Igr2jVT","type":"Entry","createdAt":"2023-05-10T16:04:25.585Z","updatedAt":"2023-05-10T16:06:19.918Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_7","body":"$ python workload.py\n\n2023-05-02 15:10:01,105 INFO streaming_executor.py:91 -- Executing DAG\nInputDataBuffer[Input] -\u003e TaskPoolMapOperator[MapBatches(decode_frames)] -\u003e \nActorPoolMapOperator[MapBatches(FrameAnnotator)] -\u003e\nActorPoolMapOperator[MapBatches(FrameClassifier)] -\u003e \nTaskPoolMapOperator[Write]\n2023-05-02 15:10:01,128 INFO actor_pool_map_operator.py:114 -- MapBatches(FrameAnnotator): Waiting for 5 pool actors to start...\n2023-05-02 15:10:02,691 INFO actor_pool_map_operator.py:114 -- MapBatches(FrameClassifier): Waiting for 2 pool actors to start...\n\nRunning: 25.0/112.0 CPU, 2.0/2.0 GPU, 33.19 GiB/32.27 GiB object_store_memory:  28%|███▍        | 285/1000 [01:40\u003c03:12,  3.71it/s]","language":"Shell"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"While the workload is running, we can view the observability graphs in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-core/ray-dashboard.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Dashboard","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". We can see from the actor metrics that our 7 worker actors were active for the entire run (RUNNING_TASK), and that Ray Data kept them busy with MapBatches(FrameClassifier) as well as MapBatches(FrameAnnotator) tasks. Because of the streaming execution strategy, tasks for all stages run concurrently:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5MDgPW8470uVoy6MFBqowq","type":"Asset","createdAt":"2023-05-10T16:09:18.740Z","updatedAt":"2023-05-10T16:09:27.512Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_8","description":"Figure 8. View active tasks and actors for this job in the Ray Dashboard","file":{"url":"//images.ctfassets.net/xjan103pcp94/5MDgPW8470uVoy6MFBqowq/39f6300387d895145313c0caee8149cf/image3.png","details":{"size":77415,"image":{"width":1216,"height":516}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can also inspect hardware utilization in the Ray dashboard. We'll focus on the network activity. We can see two nodes receiving ~500MiB/s (these are the two GPU nodes hosting the FrameClassifier actors), and a number of other nodes driving significant traffic from 100-200MiB/s, likely sending data to the GPU nodes or between the decode and FrameAnnotator steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6QEOgHncDDrSl3SKl9YzWF","type":"Asset","createdAt":"2023-05-10T16:12:30.030Z","updatedAt":"2023-05-10T19:54:24.559Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"streaming_figure_9","description":"Figure 9. Ray Dashboard showing network activity","file":{"url":"//images.ctfassets.net/xjan103pcp94/6QEOgHncDDrSl3SKl9YzWF/bae6e33fce2749f2ade8afd73681e16b/Screenshot_from_2023-05-10_12-42-27.png","details":{"size":107840,"image":{"width":1635,"height":707}},"fileName":"Screenshot from 2023-05-10 12-42-27.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The overall pipeline finishes in ~5 minutes, successfully processing ~350GB of video frame data total on a heterogeneous cluster of CPUs and GPUs. We see that Ray Data was able to pipeline the entire execution across the cluster to make the best use of memory.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Discussion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Optimizations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"In order to seamlessly execute such streaming topologies, Ray Data provides a number of optimizations under the hood. This includes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Memory stability","nodeType":"text"},{"data":{},"marks":[],"value":": Ray Data relies on the underlying Ray scheduler to schedule tasks and actors, but still needs to manage back-pressure across the streaming topology to bound memory usage and avoid object store spilling. It does this by only scheduling new tasks if it would keep the streaming execution under ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/data-internals.html#streaming-execution"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"configured resources limits","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Intuitively, enforcing a cap on intermediate result memory usage is needed to avoid degrading to bulk execution.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data locality","nodeType":"text"},{"data":{},"marks":[],"value":": While Ray will already place tasks on nodes where their input arguments are local, Ray Data's streaming backend extends this to optimize the scheduling of actor tasks. For example, in the above example, a lot of network traffic is avoided between the `decode_frames` and `FrameAnnotator` steps by routing decoded frames to actors that are on the same node.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Fault tolerance","nodeType":"text"},{"data":{},"marks":[],"value":": Ray Data leverages Ray's built-in ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-core/fault-tolerance.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"fault tolerance","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in order to handle object loss in large jobs. When objects are lost, they are recomputed based on their task lineage tracked by Ray. If actors were needed to produce these objects, Ray restarts these actors prior to re-submitting the task.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scalability","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"To give an idea of the upper scalability envelope of Ray Data streaming, we ran a number of synthetic benchmarks that stressed Ray's object manager and Ray Data's ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/execution/streaming_executor.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"streaming executor","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" processing a 20TiB array dataset. Under the hood, Ray Data is using ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.remote() / ray.wait() ","nodeType":"text"},{"data":{},"marks":[],"value":"to orchestrate the streaming topology, which means it is similarly scalable as Ray itself:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2vKUP0VZYl0yPNspQmfU05","type":"Asset","createdAt":"2023-05-10T16:17:37.701Z","updatedAt":"2023-05-10T16:17:37.701Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"streaming_figure_10","description":"Figure 10. Synthetic benchmarks stressing Ray's object manager and Ray Data's streaming executor processing a 20TiB array dataset","file":{"url":"//images.ctfassets.net/xjan103pcp94/2vKUP0VZYl0yPNspQmfU05/0566c92beb5ea743d9ecc3404a551705/image9.png","details":{"size":44857,"image":{"width":1636,"height":640}},"fileName":"image9.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"These benchmarks were run on a cluster of 500 machines, and test pipelines with between one to four stages similar to the video streaming example. We see that for the simplest pipeline on this cluster, Ray Data can process input data at a rate exceeding 1TiB/s. For more complex multi-stage pipelines, Ray Data can still sustain about 100-200GiB/s of end-to-end throughput on a large cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed Training","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"A significant portion of ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" users today are also using Ray Data as a distributed data backend for ML training. Ray Data streaming allows these users to efficiently work with datasets that are larger than cluster memory for training, leveraging both CPU and GPU nodes to speed up their jobs. We are working on enhancing Ray Train's API to natively work with Ray Data streaming.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What's next?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Today, early users are taking advantage of the Ray Data streaming backend to create efficient large-scale inference pipelines over unstructured data, including video and audio data. To learn more about how to build your own pipeline, check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference tutorial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in our docs. We'll also be following up with a blog on more unstructured application examples. To learn more generally about streaming in Ray Data, check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/data.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"library docs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and let us know what ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/issues"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"improvements","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" you'd like to see to Ray Data.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Also, we will be presenting at the ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Register now","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" — early bird registration is open until","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" June 30, 2023.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5qTVayH4YnKM7pGL8CiSwc","type":"Asset","createdAt":"2023-05-11T15:46:46.691Z","updatedAt":"2023-05-11T15:46:46.691Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_streaming_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5qTVayH4YnKM7pGL8CiSwc/99806986301b595308c6c89a8bad6249/images_and_diagrams__13_.png","details":{"size":83586,"image":{"width":960,"height":540}},"fileName":"images and diagrams (13).png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1n9cmtmuJ3wQPGW1TtXZ4t","type":"Entry","createdAt":"2023-05-08T15:57:32.674Z","updatedAt":"2023-05-09T00:38:09.625Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Building a Self Hosted Question Answering Service using LangChain + Ray in 20 minutes","slug":"building-a-self-hosted-question-answering-service-using-langchain-ray","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-05-08","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 3 of a blog series. In this blog, we’ll show you how to build an LLM question and answering service. In future parts, we will optimize the code and measure performance: cost, latency and throughput.","body":{"nodeType":"document","data":{},"content":[{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4laoMTyctlD4QnuM9KzTI6","type":"Entry","createdAt":"2023-05-08T20:47:41.452Z","updatedAt":"2023-05-08T20:47:41.452Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Building a Question Answering Chatbot","videoUrl":"https://youtu.be/Sy-Xp-sdlh0"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\nThis blog post builds on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"nodeType":"text","value":"Part 1","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" of our LangChain series, where we built a semantic search service in about 100 lines. Still, search is so … 2022. What if we wanted to build a question answering service? ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"One option is we could just ask the LLM directly without any background at all. Unfortunately one of the biggest problems with LLMs is not just ignorance (“I don’t know”) but hallucination (“I think I know but I actually don’t ","marks":[],"data":{}},{"nodeType":"text","value":"at all","marks":[{"type":"italic"}],"data":{}},{"nodeType":"text","value":".”) This is perhaps the biggest issue facing LLMs at the current time. The way we overcome that is by combining factual information from our search engine and the capabilities of an LLM together. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Again, as we demonstrated before, there is a powerful combination in Ray + LangChain. LangChain provides a chain that is well suited to this (Retrieval QA). To give a fuller picture of how the pieces come together, we’re going to implement some parts that could usually just as easily be wrapped. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The Question Answering Service we will build will query the results from our Search Engine, and then use an LLM to summarize the results of the search. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Previously we had shown this diagram for how to serve semantic search results: ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1yspRkXhEt1xIRdiuztbyh","type":"Asset","createdAt":"2023-05-08T15:43:10.951Z","updatedAt":"2023-05-08T15:43:10.951Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"qna-save-search-queries","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1yspRkXhEt1xIRdiuztbyh/f5a50d1046b085b95cd18742e51d5393/qna-save-search-queries.png","details":{"size":243832,"image":{"width":1600,"height":794}},"fileName":"qna-save-search-queries.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We are going to augment that now by creating a chain that consists of the above stage, then generating a prompt, and feeding that to an LLM to generate the answer. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Hence, the resulting system we are trying to build looks like this: ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mD1pRX9Do1SDRCUor1Yex","type":"Asset","createdAt":"2023-05-08T15:44:48.379Z","updatedAt":"2023-05-08T15:44:48.379Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"qna-answering-questions-with","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2mD1pRX9Do1SDRCUor1Yex/d4041f6d0d296c73edd77de26e076a17/qna-answering-questions-with.png","details":{"size":170666,"image":{"width":1556,"height":1024}},"fileName":"qna-answering-questions-with.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In other words, we take the vector search results, we take a prompt template, generate the prompt and then pass that to the LLM. Today we will use ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/Stability-AI/StableLM"},"content":[{"nodeType":"text","value":"StableLM","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" but you can easily swap in whatever model you want to. \n\nBefore we get started, it’s worth noting that you can find the source code to this project in our LangChain Ray examples repo at: ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/langchain-ray"},"content":[{"nodeType":"text","value":"https://github.com/ray-project/langchain-ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 1: The Prompt Template","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The prompt template is derived from the suggested one from StableLM, but modified for our use case. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In serve.py, we declare the following template:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2MvraLhg4SD47Hq41r5Jd7","type":"Entry","createdAt":"2023-05-08T15:46:38.818Z","updatedAt":"2023-05-08T15:46:38.818Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-1","body":"TEMPLATE = \"\"\"\n\u003c|SYSTEM|\u003e# StableLM Tuned (Alpha version)\n- You are a helpful, polite, fact-based agent for answering questions about Ray. \n- Your answers include enough detail for someone to follow through on your suggestions. \n\u003c|USER|\u003e\n\nPlease answer the following question using the context provided. If you don't know the answer, just say that you don't know. Base your answer on the context below. Say \"I don't know\" if the answer does not appear to be in the context below. \n\nQUESTION: {question} \nCONTEXT: \n{context}\n\nANSWER: \u003c|ASSISTANT|\u003e\n\"\"\"","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Let’s go through the template. The first part is the “\u003c|SYSTEM|\u003e” tag. You can think of this as setting the “personality” of the LLM. LLMs are trained to treat the system tag differently. Not all LLMs support this, but OpenAI and StableLM do.  The second part is the “\u003c|USER|\u003e” tag which is the question we want to ask. Note that the question and context are “templatized” – we will provide them from another source. Finally, since LLMs generate outputs by continuing on from the input, we say to the LLM “OK, here’s where you take over.” ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The question will come from the user’s query. The context will use what we built last time: the search results from our semantic search. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 2: Setting up the embeddings and the LLM","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now let’s have a look at the __init__ method for our deployment. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3A4WRFfgvE3dNjwHACGYRB","type":"Entry","createdAt":"2023-05-08T15:47:56.437Z","updatedAt":"2023-05-08T15:47:56.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-2","body":"def __init__(self):\n       #... the same code from Part 1 .. \n       self.llm = StableLMPipeline.from_model_id(model_id=\"stabilityai/stablelm-tuned-alpha-7b\",\n                                                    task=\"text-generation\", model_kwargs=\n                                                    {\"device_map\":\"auto\", \"torch_dtype\": torch.float16})\n       WandbTracer.init({\"project\": \"wandb_prompts_2\"})\n       self.chain = load_qa_chain(\n           llm=self.llm,\n           chain_type=\"stuff\",\n           prompt=PROMPT)","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"As you can see, we’ve added just 3 lines. \n\nThe first line creates a new StableLM LLM. In this case we had to write a little bit of glue code because we wanted to specify using ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://en.wikipedia.org/wiki/Half-precision_floating-point_format"},"content":[{"nodeType":"text","value":"float16","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" (halving the memory consumption of the model). We are working with the authors of Langchain to make this unnecessary. The key line from that file is this one: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"lthNpTGxWTEV31PX65mYb","type":"Entry","createdAt":"2023-05-08T15:50:17.006Z","updatedAt":"2023-05-08T15:50:17.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-3","body":" response = self.pipeline(prompt, temperature=0.1, max_new_tokens=256, do_sample=True)\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Here we specify the maximum number of tokens, and that we want it to pretty much answer the question the same way every time, and that we want to do one word at a time. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The second line sets up our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.wandb.ai/ref/python/integrations/wandbtracer"},"content":[{"nodeType":"text","value":"tracing with Weights and Biases","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". This is completely optional, but will allow us to visualize the input. You can find out more about Weights and Biases ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://wandb.ai"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The third thing we do is create a new chain that is specifically designed for answering questions. We specify the LLM to use, the prompt to use and finally the “chain type” – for now we set this to “stuff” but there are other options like “map_reduce”, and also pass in the prompt. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 3: Respond to questions","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We’ve now got our Langchain ready, now all we have to do is write the code that uses the chain to answer questions! ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4Z72OU7xq4tPFzV8zLks6","type":"Entry","createdAt":"2023-05-08T15:51:41.475Z","updatedAt":"2023-05-08T15:51:41.475Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-4","body":"   def answer_question(self, query):\n       search_results = self.db.similarity_search(query)\n       print(f'Results from db are: {search_results}')\n       result = self.chain({\"input_documents\": search_results, \"question\":query})\n\n       print(f'Result is: {result}')\n       return result[\"output_text\"]","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"You’ll notice the first line is identical to our previous version. Now we execute the chain with both our search results and the question being fed into the template. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 4: Go!","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now let’s get this started. If you’re using Weights and Biases, don’t forget to log in using wandb login. To start, let’s do serve run serve:deployment. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now it’s started, let’s use a simple query script to test it.","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4XXeXIabNrMpB1kEND5KBK","type":"Entry","createdAt":"2023-05-08T15:53:45.904Z","updatedAt":"2023-05-08T15:53:45.904Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-5","body":"$ python query.py 'What are placement groups?'\nPlacement groups are a way for users to group resources together and schedule tasks or actors on those resources. They allow users to reserve resources across multiple nodes and can be used for gang-scheduling actors or for spreading resources apart. Placement groups are represented by a list of bundles, which are used to group resources together and schedule tasks or actors. After the placement group is created, tasks or actors can be scheduled according to the placement group and even on individual bundles.\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Yay! It works (well, mostly, that part at the end is a bit weird)! Let’s also check that it doesn’t make stuff up: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12iqa2l4xNWuK18GLTu9Ly","type":"Entry","createdAt":"2023-05-08T15:54:22.876Z","updatedAt":"2023-05-08T15:54:22.876Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-6","body":"$ python query.py 'How do I make fried rice?'\nI don't know.\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Let’s just check the prompt that was sent to StableLM. This is where Weights and Biases comes in. Pulling up our interface we can find the prompt that was sent: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"64oHWc7Rnz0fgCIBOR8gEF","type":"Entry","createdAt":"2023-05-08T15:55:06.135Z","updatedAt":"2023-05-08T15:55:06.135Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-7","body":"\u003c|SYSTEM|\u003e# StableLM Tuned (Alpha version)\n- You are a helpful, polite, fact-based agent for answering questions about Ray. \n- Your answers include enough detail for someone to follow through on your suggestions. \n\u003c|USER|\u003e\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\n\nPlease answer the following question using the context provided. \nQUESTION: What are placement groups? \nCONTEXT: \nPlacement Groups#\nPlacement groups allow users to atomically reserve groups of resources across multiple nodes (i.e., gang scheduling).\nThey can be then used to schedule Ray tasks and actors packed as close as possible for locality (PACK), or spread apart\n(SPREAD). Placement groups are generally used for gang-scheduling actors, but also support tasks.\nHere are some real-world use cases:\n\nray list placement-groups provides the metadata and the scheduling state of the placement group.\nray list placement-groups --detail provides statistics and scheduling state in a greater detail.\nNote\nState API is only available when you install Ray is with pip install \"ray[default]\"\nInspect Placement Group Scheduling State#\nWith the above tools, you can see the state of the placement group. The definition of states are specified in the following files:\nHigh level state\nDetails\n\nPlacement groups are represented by a list of bundles. For example, {\"CPU\": 1} * 4 means you’d like to reserve 4 bundles of 1 CPU (i.e., it reserves 4 CPUs).\nBundles are then placed according to the placement strategies across nodes on the cluster.\nAfter the placement group is created, tasks or actors can be then scheduled according to the placement group and even on individual bundles.\nCreate a Placement Group (Reserve Resources)#\n\nSee the User Guide for Objects.\nPlacement Groups#\nPlacement groups allow users to atomically reserve groups of resources across multiple nodes (i.e., gang scheduling). They can be then used to schedule Ray tasks and actors packed as close as possible for locality (PACK), or spread apart (SPREAD). Placement groups are generally used for gang-scheduling actors, but also support tasks.\nSee the User Guide for Placement Groups.\nEnvironment Dependencies#\n\nANSWER: \u003c|ASSISTANT|\u003e\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We can see the search results that we made in are being included. StableLM is then using this to synthesize its answer. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Conclusion","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this blog post we showed how we could build on the simple search engine we built in the previous blog in this series and make a retrieval-based question answering service. It didn’t need us to do much: we needed to bring up a new LLM (StableLM),  we needed to generate a prompt with the search results in it, and then feed that result to the LLM asking it to derive an answer from it. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Next Steps","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Review the code and data used in this blog in the following ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/langchain-ray/tree/main/open_source_LLM_retrieval_qa"},"content":[{"nodeType":"text","value":"Github repo","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". \n\nIf you are interested in learning more about Ray, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io/"},"content":[{"nodeType":"text","value":"Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://docs.ray.io/"},"content":[{"nodeType":"text","value":"Docs.Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\n","marks":[],"data":{}},{"nodeType":"text","value":"See our earlier ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"nodeType":"text","value":"blog series on solving Generative AI infrastructure","marks":[],"data":{}}]},{"nodeType":"text","value":" with Ray.","marks":[],"data":{}},{"nodeType":"text","value":"\n\nTo connect with the Ray community join #LLM on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"Ray Slack","marks":[],"data":{}}]},{"nodeType":"text","value":" or our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"Discuss forum","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://www.anyscale.com/platform"},"content":[{"nodeType":"text","value":"Anyscale.com/Platform ","marks":[],"data":{}}]},{"nodeType":"text","value":"and click the 'Try it now' button","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Summit 2023:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"nodeType":"text","value":"Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"nodeType":"text","value":"practical training focused on LLMs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mD1pRX9Do1SDRCUor1Yex","type":"Asset","createdAt":"2023-05-08T15:44:48.379Z","updatedAt":"2023-05-08T15:44:48.379Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"qna-answering-questions-with","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2mD1pRX9Do1SDRCUor1Yex/d4041f6d0d296c73edd77de26e076a17/qna-answering-questions-with.png","details":{"size":170666,"image":{"width":1556,"height":1024}},"fileName":"qna-answering-questions-with.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2C7QtMnASHfVdag9xYamXh","type":"Entry","createdAt":"2023-05-04T15:03:36.052Z","updatedAt":"2023-06-06T14:20:09.992Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker","seoTitle":"Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker","slug":"offline-batch-inference-comparing-ray-apache-spark-and-sagemaker","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DwU0wh6AmJoGv8JOy6n0Q","type":"Entry","createdAt":"2020-11-11T23:34:15.165Z","updatedAt":"2021-02-05T22:22:08.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","slug":"amog-kamsetty","link":"https://www.linkedin.com/in/amogkamsetty/","bio":"Amog is a software engineer at Anyscale. He helps maintain and develop two machine learning libraries: Ray Tune for distributed hyperparameter tuning and RaySGD for distributed deep learning training.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J134jX2ybZRLPfcAIGLTI","type":"Asset","createdAt":"2020-11-13T00:05:05.533Z","updatedAt":"2020-11-13T00:05:05.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J134jX2ybZRLPfcAIGLTI/12589bb0c6792051050ebc6e1e83af25/Amog_Kamsetty_.jpg","details":{"size":57660,"image":{"width":480,"height":640}},"fileName":"Amog Kamsetty .jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}}],"publishedDate":"2023-05-04","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Update 5/10: ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"Thanks to the Databricks Apache Spark developers for pointing out the prefetching flag ","nodeType":"text"},{"data":{},"marks":[{"type":"code"},{"type":"italic"}],"value":"spark.databricks.execution.pandasUDF.prefetch.maxBatches ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"available in Databricks Runtime that can be used with the Iterator API. It is also possible to implement this manually in Spark open source with background threads. With prefetching set to 4 batches, Spark reaches ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"},{"type":"italic"}],"value":"159.86 images/sec","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":". The code can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/blob/main/spark/code/torch-batch-inference-s3-10G-standard-iterator-databricks-prefetch.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"italic"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"TL;DR","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As more companies use large scale machine learning (ML) models for training and evaluation, offline batch inference becomes an essential workload. A number of challenges come with it: managing compute infrastructure; optimizing use of all heterogeneous resources; and transferring data from storage to hardware accelerators. Addressing these challenges, Ray performs significantly better as it can coordinate clusters of diverse resources, allowing for better utilization of the specific resource requirements of the workload.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we conduct a comparison of three different solutions for offline batch inference: AWS ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker Batch Transform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Our experiments demonstrate that Ray Data achieves speeds up to ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"17x ","nodeType":"text"},{"data":{},"marks":[],"value":"faster than SageMaker Batch Transform and","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" 2x","nodeType":"text"},{"data":{},"marks":[],"value":" faster than Spark for offline image classification. Ray Data also scales effectively to terabyte-sized datasets. The code for these benchmarks is publicly available and can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introduction","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46bNG3QTa1atN02rsemfe8","type":"Asset","createdAt":"2023-05-03T22:20:44.214Z","updatedAt":"2023-05-03T22:20:44.214Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_1_batch_offline_inference","description":"Figure 1. General parallel batch processing architecture\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/46bNG3QTa1atN02rsemfe8/120bf3aac26ae94257b41ac674f42e94/image3.png","details":{"size":48540,"image":{"width":960,"height":540}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Offline batch inference is a critical workload for many AI products, especially with the growing usage of pre-trained foundation models. At its core, it seems like a simple problem: given a trained model and a dataset, get model predictions for each data point. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, there are many challenges to doing this at scale:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 1: ","nodeType":"text"},{"data":{},"marks":[],"value":"Managing compute infrastructure and cloud clusters, especially when needing to use ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"heterogeneous clusters","nodeType":"text"},{"data":{},"marks":[],"value":", consisting of different instance types to maximize throughput.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 2: ","nodeType":"text"},{"data":{},"marks":[],"value":"Parallelizing data processing and utilizing all resources (CPUs \u0026 GPUs) in the cluster at any given point in time.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 3: ","nodeType":"text"},{"data":{},"marks":[],"value":"Efficiently transferring data between cloud storage, CPU RAM for preprocessing, and GPU RAM for model inference, especially for unstructured data like images and text. This has ramifications for both performance and developer velocity.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 4:","nodeType":"text"},{"data":{},"marks":[],"value":" A user experience that makes it easy to iterate and develop while working at scale.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What does the industry recommend for offline batch inference?","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The industry hasn’t converged on a standard solution. ","nodeType":"text"},{"data":{"uri":"https://twitter.com/benhamner/status/1605730034450169857"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"It seems like every ML practitioner has their own favorite approach.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5hRMwuEGUS4nJ7eV5riIK9","type":"Asset","createdAt":"2023-05-03T22:23:14.590Z","updatedAt":"2023-05-03T22:23:14.590Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"tweet_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5hRMwuEGUS4nJ7eV5riIK9/05b76abcbe4ea58ae3839fd319858594/image4.png","details":{"size":177304,"image":{"width":1798,"height":588}},"fileName":"image4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We see that there are three categories of solutions that attempt to address the above challenges. However, only category three addresses all the challenges.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Batch Services: ","nodeType":"text"},{"data":{},"marks":[],"value":"Cloud providers such as AWS, GCP, and Azure provide batch services to manage compute infrastructure for you. Some newer products like ","nodeType":"text"},{"data":{"uri":"https://modal.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Modal Labs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provide a user experience layer around cloud providers to abstract away even more complexity. Regardless of which service you choose, the process is the same: you provide your code, and the service runs your code on each node in a cluster. However, while infrastructure management is necessary (Challenge #1), it is not enough. These services have limitations, such as a lack of software libraries to address Challenges #2, #3, and #4 which makes them suitable only for experienced users who can write their own optimized batch inference code.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Online Inference solutions: ","nodeType":"text"},{"data":{},"marks":[],"value":" Solutions like ","nodeType":"text"},{"data":{"uri":"https://www.bentoml.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Bento ML","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provide APIs to make it easy to write performant inference code and can abstract away infrastructure complexities. But they are designed for online inference rather than offline batch inference—two different problems with different sets of requirements. These solutions often don’t perform well in the offline case, leading inference service providers like ","nodeType":"text"},{"data":{"uri":"https://modelserving.com/blog/unifying-real-time-and-batch-inference-with-bentoml-and-spark"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Bento ML to integrating with Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for offline inference.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed data systems:","nodeType":"text"},{"data":{},"marks":[],"value":" These solutions are designed to handle large amounts of data and are better suited to handle all three challenges listed above. They break large datasets into reasonable batches for processing and can map a given function (i.e., the model) over the dataset efficiently, effectively handling the challenges of offline batch inference at scale. Examples include ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ultimately, we have observed that Ray Data is the best practical solution for offline batch inference for modern deep learning applications, both in terms of performance and user experience.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We ran the following comparisons and experiments to corroborate the above observations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing AWS SageMaker Batch Transform, Apache Spark, and Ray Data performance and UX on an image classification use case","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling up Ray Data batch inference to 10 TB, showing 90%+ GPU utilization","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Image Classification: SageMaker Batch Transform vs. Apache Spark vs. Ray Data","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We ran an image classification task from the ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1911.02549"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLPerf Inference Benchmark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" suite in the offline setting. This benchmark uses images from the ","nodeType":"text"},{"data":{"uri":"https://image-net.org/challenges/LSVRC/2012/2012-downloads.php#Images"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"ImageNet 2012 Dataset","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and a pre-trained ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch ResNet50 model.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We timed a full end-to-end batch inference workload. With images stored in the parquet format in a public S3 bucket, the workload involved the following three steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Reading images from S3","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Simple CPU preprocessing (resizing, cropping, normalization)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Model inference on GPU","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"While all Ray experiments were run on Anyscale, Spark experiments were run on Databricks runtime v12.0, with Apache Spark 3.3.1. More details on the full configurations can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"in the repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In particular for Spark, we used two different setups:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Single-cluster","nodeType":"text"},{"data":{},"marks":[],"value":": Both the CPU preprocessing and GPU inference were done on the same cluster. We tried both versions of the PySpark Pandas UDF API: ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.pandas_udf.html#pyspark.sql.functions.pandas_udf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Series-based and Iterator-based","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Multi-cluster","nodeType":"text"},{"data":{},"marks":[],"value":": CPU preprocessing is done on one cluster, the preprocessed data is saved to a shared file system, and GPU inference is done on a different cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"All the source code for the experiments conducted above is available ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", including additional details on various other configurations that we attempted.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":" Results of throughput from experiments","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"When running with a 10 GB dataset on a single g4dn.12xlarge instance (48 CPUs, 4 GPUs), we observe the following throughput (images/sec) results for ResNet-50 batch inference:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Urc9cqOTvtI8d4w9DjiEm","type":"Asset","createdAt":"2023-05-04T00:09:19.405Z","updatedAt":"2023-05-04T00:09:19.405Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_5_table","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Urc9cqOTvtI8d4w9DjiEm/42718e584dcc4d0d16592749b0782b4f/Screen_Shot_2023-05-03_at_5.01.12_PM.png","details":{"size":91393,"image":{"width":1211,"height":541}},"fileName":"Screen Shot 2023-05-03 at 5.01.12 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The above throughput differences translate directly into cost savings as the batch inference job with Ray can run much faster—so the faster it finishes the fewer resources are consumed and the cheaper it gets. Additionally, Ray is fully open source.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2dNPqjBTFiDZn2gcXkdir9","type":"Asset","createdAt":"2023-05-03T22:50:03.518Z","updatedAt":"2023-05-03T22:50:03.518Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 1","description":"Figure 1. Throughput difference among three different solutions for 10GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/2dNPqjBTFiDZn2gcXkdir9/f07a675dcb4f124e29aeb2f6618dbf15/image1.png","details":{"size":163062,"image":{"width":1334,"height":948}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"SageMaker Batch Transform for batch inference","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As we can see, SageMaker Batch Transform is not well suited for batch inference, both for throughput and cost.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While SageMaker handles","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" Challenge #1 ","nodeType":"text"},{"data":{},"marks":[],"value":"to some degree and abstracts away compute infrastructure management, it doesn’t fully address the remaining challenges","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The low-throughput results are primarily because while SageMaker Batch Transform claims to be a ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"solution for batch inference on large datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", under the hood, it uses the same architecture as for online serving systems. It starts an HTTP server, and deploys your model as an endpoint. And then for each image, a separate request is sent to the server.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"SageMaker also does not provide support for batching across ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"multiple files","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and even if you batched your data beforehand, the max payload per request is 100 MB, nowhere near enough to fully saturate your GPUs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In terms of developer velocity, when running these benchmarks, we ran into a few additional challenges with SageMaker. In particular, the need to spin up a new cluster every time we ran a script and the difficult to parse stack traces slowed down developer iterations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While online inference solutions have their place, the additional overhead they have, as well as the inability to fully utilize cluster resources and saturate GPUs by maximizing batch sizes and effectively pipelining data make them highly inefficient for offline batch inference.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing Spark and Ray for batch inference","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Clearly, as we observed above, shoehorning online inference solutions for offline use cases is not the right approach. Let’s compare the two distributed data system approaches: Spark and Ray Data. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We see that Ray Data has","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" 2x ","nodeType":"text"},{"data":{},"marks":[],"value":"faster throughput than Spark. Let’s break down how Ray Data and Spark fare for the challenges mentioned earlier:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #2: Hybrid CPU and GPU workload","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deep learning workloads involve ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"both CPU and GPU computation","nodeType":"text"},{"data":{},"marks":[],"value":". For optimal parallelization, all the CPUs and GPUs in the cluster should be fully utilized at any given time. However, this is a limitation of Spark scheduling as all the stages are fused together, regardless of their resource requirements. This means that in the single cluster setup, the total parallelism is limited by the number of GPUs, leading to underutilization of CPUs. This also means that GPUs are completely idle during the reading and preprocessing steps. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"When using a separate CPU and GPU cluster, data needs to be persisted and orchestrated between stages. Overall, for these CPU+GPU workloads, Spark does not fully address ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"challenge #2","nodeType":"text"},{"data":{},"marks":[],"value":" and cannot utilize all resources in the cluster concurrently. We attempted to use Spark’s stage level scheduling feature to address this issue, but were ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/tree/main/spark#stage-level-scheduling"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"unsuccessful","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6m5c6zQiMVCithTLEIazb8","type":"Asset","createdAt":"2023-05-03T23:02:01.263Z","updatedAt":"2023-05-03T23:02:01.263Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 2","description":"Figure 2. How Ray data efficiently pipelines and optimizes hardware: CPUs/GPUs","file":{"url":"//images.ctfassets.net/xjan103pcp94/6m5c6zQiMVCithTLEIazb8/1be9788a7aa54707a42f38023597ae2f/5mb.gif","details":{"size":4991804,"image":{"width":3260,"height":1182}},"fileName":"5mb.gif","contentType":"image/gif"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data, on the other hand, can independently scale the CPU preprocessing and GPU inference steps. It also streams data ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"through these stages","nodeType":"text"},{"data":{},"marks":[],"value":", increasing CPUs and GPUs utilization and reducing out-of-memory errors (OOMs), and fine-grained resource allocation for ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html#scaling-and-resource-allocation"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"autoscaling and management","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #3: Large, multi-dimensional tensors","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deep learning workloads often involve dealing with complex data types like images, audio, or video, which are represented by multi-dimensional ","nodeType":"text"},{"data":{"uri":"https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"arrays","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Having first-class support for these data types is necessary for better performance and developer experience. Ray Data has Numpy batch format as a first-class citizen, and does not require Pandas, which is suited for deep learning and leads to higher performance and more memory efficiency, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-2-2-improved-developer-experience-performance-and-stability"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"as shown in previous benchmarks","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #4: Ray is Python native","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another fundamental difference is that Ray is Python native whereas Spark is not, which also impacts performance. We ran a microbenchmark that applies a ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"time.sleep(1)","nodeType":"text"},{"data":{},"marks":[],"value":" UDF. Surprisingly, running the UDF takes ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/tree/main/spark#microbenchmark"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"80 seconds","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"; we believe due to JVM\u003c\u003ePyarrow overhead! On Ray Data, the same microbenchmark takes a little ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/blob/main/ray/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"under 5 seconds","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Not to mention, this makes Spark development more difficult. In particular when debugging, the recommended way is to include debug information in the ","nodeType":"text"},{"data":{"uri":"https://stackoverflow.com/a/57175768"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"output pandas dataframe rather than using print statements","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Even as we scale to a larger 300 GB dataset using 4 nodes and 16 GPUs, we see that these performance differences still hold:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5wDKGXZNHB2Hlt6JEMMKOt","type":"Asset","createdAt":"2023-05-03T23:04:40.956Z","updatedAt":"2023-05-03T23:04:40.956Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_3_batch_inference","description":"Figure 3. Throughput difference between Spark and Ray solution for 300 GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/5wDKGXZNHB2Hlt6JEMMKOt/da27ee6ea7e5773abef42eb291dd1a97/image5.png","details":{"size":92846,"image":{"width":1102,"height":830}},"fileName":"image5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"While big data processing systems are the best solutions for offline inference, for deep learning workloads, Ray Data outperforms Spark.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling up Ray Data and maximizing performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we test the scalability of Ray Data and attempt to maximize performance by using a ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"heterogeneous cluster","nodeType":"text"},{"data":{},"marks":[],"value":" consisting of a mix of GPU nodes and CPU-only nodes. Deep learning workloads involving complex data types are often memory-bound, so by using additional CPU-only nodes, we add more total memory to our cluster to better maximize GPU utilization. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlike SageMaker or Spark, Ray works natively with heterogeneous clusters consisting of different instance types, fully addressing ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"challenge #1.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We run the same benchmark, except with 10 TB worth of images, using a single cluster of 10 g4dn.12xlarge instances and 10 m5.16xlarge instances.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Throughput: 11,580.958 img/sec","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"As we can see below, Ray Data achieves over 90%+ GPU utilization throughout the workload run:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"16XJHcynegHxIdae1hupAF","type":"Asset","createdAt":"2023-05-03T23:07:22.875Z","updatedAt":"2023-05-03T23:07:22.875Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"batch_inference_figure_4","description":"Figure 4. GPU utilization of all 40 GPUs in the cluster. We are roughly at 90%+ GPU utilization during the entire duration of the workload.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/16XJHcynegHxIdae1hupAF/8eab9567ac78a8d558bcd04218f7e1ca/image2.png","details":{"size":466601,"image":{"width":1999,"height":621}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nThe ability to scale to many GPUs and large data sizes without needing to rewrite code and not sacrificing on throughput is what sets Ray Data apart compared to other solutions for deep learning workloads.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog, we identified four technical challenges for doing batch inference at scale and investigated three commonly cited industry solutions for offline batch inferences. Using the cited approaches, we compared Ray, Spark, and Amazon SageMaker Batch Transform.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We concluded that Ray Data best meets all four challenges:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data abstracts away compute infrastructure management, and can maximize performance for memory-bound workloads as it natively supports heterogeneous clusters.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"streams","nodeType":"text"},{"data":{},"marks":[],"value":" data from cloud storage -\u003e CPU -\u003e GPU, ensuring that all resources in the cluster are utilized at any given point in time. This improves throughput and reduces overall cost.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data has native support for multi-dimensional tensors, which is vital for deep learning workloads.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data is Python native and programmatic, making it easy to develop and debug.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Overall, Ray Data is the best option for deep learning offline batch inference. In our image classification benchmarks, as shown in the figures above, Ray Data significantly outperforms SageMaker Batch Transform (by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"17x)","nodeType":"text"},{"data":{},"marks":[],"value":" and Spark (by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"2x ","nodeType":"text"},{"data":{},"marks":[],"value":"and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"3x","nodeType":"text"},{"data":{},"marks":[],"value":") while linearly scaling to TB level data sizes. All code for these benchmarks can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Lastly, as a result of higher throughput and Ray as an open source solution, Ray lowers the overall costs on top of the base EC2 cost.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What’s Next","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stay tuned for a follow up post where we dive into Ray Data streaming's design and how to use it for your own ML pipelines. Meanwhile, you can get started with Ray Data for batch inference ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Alternatively, ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfC6rmBeb9pLPSKRf1e7VCy7BtkjSGu20gh0xjQhdo6rcUflQ/viewform"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"fill out this form","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to connect with us if you are interested in working on a proof of concept (POC) for your offline batch inference workload with Ray.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have our ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit 2023 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"early-bird registration open. Secure your spot, and save some money.\n\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n\n\n\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2dNPqjBTFiDZn2gcXkdir9","type":"Asset","createdAt":"2023-05-03T22:50:03.518Z","updatedAt":"2023-05-03T22:50:03.518Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 1","description":"Figure 1. Throughput difference among three different solutions for 10GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/2dNPqjBTFiDZn2gcXkdir9/f07a675dcb4f124e29aeb2f6618dbf15/image1.png","details":{"size":163062,"image":{"width":1334,"height":948}},"fileName":"image1.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"51oIQOHymWExFWIRYQRXge","type":"Entry","createdAt":"2023-05-02T17:47:40.421Z","updatedAt":"2023-05-12T07:23:26.789Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Turbocharge LangChain: guide to 20x faster embedding","slug":"turbocharge-langchain-now-guide-to-20x-faster-embedding","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DwU0wh6AmJoGv8JOy6n0Q","type":"Entry","createdAt":"2020-11-11T23:34:15.165Z","updatedAt":"2021-02-05T22:22:08.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","slug":"amog-kamsetty","link":"https://www.linkedin.com/in/amogkamsetty/","bio":"Amog is a software engineer at Anyscale. He helps maintain and develop two machine learning libraries: Ray Tune for distributed hyperparameter tuning and RaySGD for distributed deep learning training.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J134jX2ybZRLPfcAIGLTI","type":"Asset","createdAt":"2020-11-13T00:05:05.533Z","updatedAt":"2020-11-13T00:05:05.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J134jX2ybZRLPfcAIGLTI/12589bb0c6792051050ebc6e1e83af25/Amog_Kamsetty_.jpg","details":{"size":57660,"image":{"width":480,"height":640}},"fileName":"Amog Kamsetty .jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6G9xTnF76ZUKKvgcVOqhtm","type":"Entry","createdAt":"2022-08-23T02:58:28.957Z","updatedAt":"2023-05-02T04:18:43.884Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Philipp Moritz","slug":"philipp-moritz","link":"https://www.linkedin.com/in/philipp-moritz-61419682/","bio":"Co-founder and CTO at Anyscale"}}],"publishedDate":"2023-05-03","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 2([part 1 here](https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray)) of a blog series. In this blog, we’ll show you how to turbocharge embeddings. In future parts, we will show you how to combine a vector database and an LLM to create a fact-based question answering service. Additionally, we will optimize the code and measure performance: cost, latency and throughput.","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7nxbgQNlTcYZECfJ0vAabS","type":"Entry","createdAt":"2023-05-02T17:16:29.953Z","updatedAt":"2023-05-02T17:16:29.953Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"LangChain + Ray Tutorial: How to Generate Embeddings For 33,000 Pages for $1","videoUrl":"https://youtu.be/hGnZajytlac"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Generating embeddings from documents is a critical step for LLM workflows. Many LLM apps are being built today through retrieval based similarity search:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Documents are embedded and stored in a vector database.  ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Incoming queries are used to pull semantically relevant passages from the vector database, and these passages are used as context for LLMs to answer the query.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"previous blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we showed how to use ","nodeType":"text"},{"data":{"uri":"https://github.com/hwchase17/langchain"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to do step 1, and we also showed how to parallelize this step by using Ray tasks for faster embedding creation.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we take it one step further, scaling out to many more documents. Continue reading to see how to use ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a distributed data processing system that’s a part of the Ray framework, to generate and store embeddings for 2,000 PDF documents from cloud storage, parallelizing across 20 GPUs, all in under 4 minutes and in less than 100 lines of code.\n\nWhile in this walkthrough we use ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to read PDF files from S3 cloud storage, it also supports a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/api/input_output.html#input-output"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"wide number of other data formats","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" like text data, parquet, images, and can read data from a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/api/input_output.html#input-output"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"variety of sources","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" like MongoDB and SQL Databases.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Why do I need to parallelize this?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2CHfkngLbr34XsDZ7IKBAW","type":"Asset","createdAt":"2023-05-02T02:38:14.040Z","updatedAt":"2023-05-02T02:38:14.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Why do I need to parallelize this?","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2CHfkngLbr34XsDZ7IKBAW/af3491b43c8afcdb307890881b2adf5c/embedding-why-do-I-need-to-parallelize-this.png","details":{"size":153788,"image":{"width":1600,"height":1004}},"fileName":"embedding-why-do-I-need-to-parallelize-this.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"LangChain provides all the tools and the integrations for building LLM applications, including loading, embedding, and storing documents. While LangChain works great for quickly getting started with a handful of documents, when you want to scale your corpus up to thousands or more documents, this can quickly become unwieldy.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Naively using a for loop to do this for each document within a corpus of a 2,000 documents takes 75 minutes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4eHSySPomCQud1Y6m1yiyB","type":"Entry","createdAt":"2023-05-02T02:45:13.859Z","updatedAt":"2023-05-02T04:15:57.187Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Embedded-Code-Snippet-1","body":"import os\nfrom tqdm import tqdm\n\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=100,\n    length_function=len,\n)\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nhf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n\n# Put your directory containing PDFs here\ndirectory = '/tmp/data/'\npdf_documents = [os.path.join(directory, filename) for filename in os.listdir(directory)]\n\nlangchain_documents = []\nfor document in tqdm(pdf_documents):\n    try:\n        loader = PyPDFLoader(document)\n        data = loader.load()\n        langchain_documents.extend(data)\n    except Exception:\n        continue\n\nprint(\"Num pages: \", len(langchain_documents))\nprint(\"Splitting all documents\")\nsplit_docs = text_splitter.split_documents(langchain_documents)\n\nprint(\"Embed and create vector index\")\ndb = FAISS.from_documents(split_docs, embedding=hf)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Clearly, if you want to iterate quickly and try out different multiple document corpuses, splitting techniques, chunk sizes, or embedding models, just doing this in a for loop won’t cut it.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Instead, for faster development, you need to horizontally scale, and for this you need a framework to make this parallelization very easy. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"By using Ray Data, we can define our embedding generation pipeline and execute it in a few lines of code, and it will automatically scale out, leveraging the compute capabilities of all the CPUs and GPUs in our cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stages of our Data Pipeline","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this example, we want to generate embeddings for our document corpus consisting of the top 2,000 arxiv papers on “large language models”. There are over 30,000 pages in all these documents. The code for generating this dataset can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/blob/main/embedding_pdf_documents/arxiv_dataset_generation.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J2BxxlXHAsmjVdrjCqLdn","type":"Asset","createdAt":"2023-05-02T02:52:04.314Z","updatedAt":"2023-05-02T02:52:04.314Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Stages of our Data Pipeline","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J2BxxlXHAsmjVdrjCqLdn/2ee02bfb6d2d96c7be6ab848c757d45e/embedded-stages-of-our-pipeline.png","details":{"size":127047,"image":{"width":1512,"height":556}},"fileName":"embedded-stages-of-our-pipeline.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let’s take a look at the stages of our data pipeline:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Load the PDF documents from our S3 bucket as raw bytes","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use PyPDF to convert those bytes into string text","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use LangChain’s text splitter to split the text into chunks","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use a pre-trained sentence-transformers model to embed each chunk","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Store the embeddings and the original text into a FAISS vector store","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The full data pipeline was run on 5 g4dn.12xlarge instances on AWS EC2, consisting of 20 GPUs in total. The code for the full data pipeline can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/blob/main/embedding_pdf_documents/embedding_ray.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Starting the Ray Cluster","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Follow the steps ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/blob/main/embedding_pdf_documents/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to set up a multi-node Ray cluster on AWS.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray clusters can also be started on GCP, Azure, or other cloud providers. See the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/getting-started.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Cluster documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for full info. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Alternatively, you can use ","nodeType":"text"},{"data":{"uri":"https://docs.anyscale.com/develop/workspaces/get-started"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Workspaces on Anyscale","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to manage your Ray clusters.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now that we have the cluster setup, let’s go through the steps in our script.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Installing Dependencies","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"First, we need to install the necessary dependencies on all the nodes in our Ray cluster. We can do this via Ray’s ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#id1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"runtime environment","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" feature.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4EbdJbi2f8hfsYWczaTcqZ","type":"Entry","createdAt":"2023-05-02T03:46:04.461Z","updatedAt":"2023-05-02T03:46:04.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Embedding-code-snippet-2","body":"import ray\n\nray.init(runtime_env={\"pip\": [\"langchain\", \"pypdf\", \"sentence_transformers\", \"transformers\"]})","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 1","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Load the 2,143 documents from our S3 bucket as raw bytes.The S3 bucket contains unmodified PDF files that have been downloaded from arxiv.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can easily do this via Ray Data’s read APIs, which creates a Ray Dataset object. Ray Datasets are lazy. Further operations can be chained and the stages are run only when execution is triggered.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Cuzf28zxzpUhECAe0PxyR","type":"Entry","createdAt":"2023-05-02T03:47:03.011Z","updatedAt":"2023-05-02T17:11:29.904Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-3","body":"from ray.data.datasource import FileExtensionFilter\n\n# Filter out non-PDF files.\nds = ray.data.read_binary_files(\"s3://ray-llm-batch-inference/\", partition_filter=FileExtensionFilter(\"pdf\"))","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 2","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Use PyPDF to load in the raw bytes and parse them as string text. We also skip over any documents or pages that are unparseable. Even after skipping these, we still have over 33,642 pages in our dataset.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We use the pypdf library directly to read PDFs directly from bytes rather than file paths. Once ","nodeType":"text"},{"data":{"uri":"https://github.com/hwchase17/langchain/pull/3915"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://github.com/hwchase17/langchain/pull/3915","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is merged, LangChain’s PyPdfLoader can be used directly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"60msecWkdwI269c9fYmRih","type":"Entry","createdAt":"2023-05-02T03:49:06.119Z","updatedAt":"2023-05-02T04:15:18.317Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-4","body":"def convert_to_text(pdf_bytes: bytes):\n    pdf_bytes_io = io.BytesIO(pdf_bytes)\n\n    try:\n        pdf_doc = PdfReader(pdf_bytes_io)\n    except pypdf.errors.PdfStreamError:\n        # Skip pdfs that are not readable.\n        # We still have over 30,000 pages after skipping these.\n        return []\n\n    text = []\n    for page in pdf_doc.pages:\n        try:\n            text.append(page.extract_text())\n        except binascii.Error:\n            # Skip all pages that are not parseable due to malformed characters.\n            print(\"parsing failed\")\n    return text\n\n# We use `flat_map` as `convert_to_text` has a 1-\u003eN relationship.\n# It produces N strings for each PDF (one string per page).\n# Use `map` for 1-\u003e1 relationship.\nds = ds.flat_map(convert_to_text)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 3","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Split the text into chunks using LangChain’s TextSplitter abstraction. After applying this transformation, the 33,642 pages are split into 144,411 chunks. Each chunk will then be encoded into an embedding in Step 4.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3S1Tcod9rihoVKoyg9RMMz","type":"Entry","createdAt":"2023-05-02T03:50:32.629Z","updatedAt":"2023-05-02T04:14:39.402Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-5","body":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ndef split_text(page_text: str):\n    # Use chunk_size of 1000.\n    # We felt that the answer we would be looking for would be \n    # around 200 words, or around 1000 characters.\n    # This parameter can be modified based on your documents and use case.\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000, chunk_overlap=100, length_function=len\n    )\n    split_text: List[str] = text_splitter.split_text(page_text)\n\n    split_text = [text.replace(\"\\n\", \" \") for text in split_text]\n    return split_text\n\n# We use `flat_map` as `split_text` has a 1-\u003eN relationship.\n# It produces N output chunks for each input string.\n# Use `map` for 1-\u003e1 relationship.\nds = ds.flat_map(split_text)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 4","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Then, we can embed each of our chunks using a pre-trained sentence transformer model on GPUs. Here, we leverage Ray Actors for stateful computation, allowing us to initialize a model only once per GPU, rather than for every single batch.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"At the end of this stage, we have 144,411 encodings by running 20 model replicas across 20 GPUs, each processing a batch of 100 chunks at a time to maximize GPU utilization.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Dp9uXgJznvx0VgoNwZt0e","type":"Entry","createdAt":"2023-05-02T03:53:27.990Z","updatedAt":"2023-05-02T04:13:54.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-6","body":"class Embed:\n    def __init__(self):\n        # Specify \"cuda\" to move the model to GPU.\n        self.transformer = SentenceTransformer(model_name, device=\"cuda\")\n\n    def __call__(self, text_batch: List[str]):\n        # We manually encode using sentence_transformer since LangChain\n        # HuggingfaceEmbeddings does not support specifying a batch size yet.\n        embeddings = self.transformer.encode(\n            text_batch,\n            batch_size=100,  # Large batch size to maximize GPU utilization.\n            device=\"cuda\",\n        ).tolist()\n\n        return list(zip(text_batch, embeddings))\n\n# Use `map_batches` since we want to specify a batch size to maximize GPU utilization.\nds = ds.map_batches(\n    Embed,\n    # Large batch size to maximize GPU utilization.\n    # Too large a batch size may result in GPU running out of memory.\n    # If the chunk size is increased, then decrease batch size.\n    # If the chunk size is decreased, then increase batch size.\n    batch_size=100,  # Large batch size to maximize GPU utilization.\n    compute=ray.data.ActorPoolStrategy(min_size=20, max_size=20),  # I have 20 GPUs in my cluster\n    num_gpus=1,  # 1 GPU for each actor.\n)\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We use the `sentence_transformers` library directly so that we can provide a specific batch size. Once ","nodeType":"text"},{"data":{"uri":"https://github.com/hwchase17/langchain/pull/3914"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://github.com/hwchase17/langchain/pull/3914","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is merged, LangChain’s `HuggingfaceEmbeddings` can be used instead.\n\n","nodeType":"text"},{"data":{},"marks":[{"type":"underline"}],"value":"Stage 5","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we can execute this Data Pipeline by iterating through it, and we store the results in a persisted FAISS vector database for future querying.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3WstnFWdDedO9wxSKYDGR8","type":"Entry","createdAt":"2023-05-02T03:54:47.846Z","updatedAt":"2023-05-02T04:17:51.737Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-7","body":"from langchain import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\ntext_and_embeddings = []\nfor output in ds.iter_rows():\n    text_and_embeddings.append(output)\n\nvectore_store = FAISS.from_embeddings(\n    text_and_embeddings,\n    # Provide the embedding model to embed the query.\n    # The documents are already embedded.\n    embedding=HuggingFaceEmbeddings(model_name=model_name)\n)\n\n# Persist the vector store.\nvectore_store.save_local(\"faiss_index\")","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Execution","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Executing this code, we see that all 20 GPUs are utilized at near 100% utilization. And what would normally take over an hour to run, can now be done in under 4 minutes! If you use AWS spot instances, this would only cost $0.95 total.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5rAh6B2lrImLTTXvpBMeHG","type":"Asset","createdAt":"2023-05-02T04:00:42.271Z","updatedAt":"2023-05-02T04:00:42.271Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Execution-1","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5rAh6B2lrImLTTXvpBMeHG/70baf2935bfad28c70d5fb310fe15ef1/embedding-execution-1.png","details":{"size":9146,"image":{"width":352,"height":130}},"fileName":"embedding-execution-1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1yk9oF8K2IIuK88xWBTfGr","type":"Asset","createdAt":"2023-05-02T04:01:19.132Z","updatedAt":"2023-05-02T04:43:16.984Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"locale":"en-US"},"fields":{"title":"Embedding - Execution 2","file":{"url":"//images.ctfassets.net/xjan103pcp94/1yk9oF8K2IIuK88xWBTfGr/ee185a15ff08445d14f3e47ed7f0f2b9/embedding-execution-2.jpg","details":{"size":127225,"image":{"width":1097,"height":780}},"fileName":"embedding-execution-2.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Querying the Vector Database","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can now load in our persisted FAISS database, and query it for similarity search. Let’s see the top document that’s most relevant to the “prompt engineering” query:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3DulKBBh4pLynITCBlacTf","type":"Entry","createdAt":"2023-05-02T04:04:27.586Z","updatedAt":"2023-05-02T04:04:27.586Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-8","body":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain import FAISS\n\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nquery_embedding = HuggingFaceEmbeddings(model_name=model_name)\ndb = FAISS.load_local(\"faiss_index\", query_embedding)\ndocuments = db.similarity_search(query=\"prompt engineering\", k=1)\n[doc.page_content for doc in documents]","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"SMSzMi5hzq9x7y2pUs2Ja","type":"Entry","createdAt":"2023-05-02T04:05:20.246Z","updatedAt":"2023-05-02T04:05:35.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-9","body":"['Prompt Engineering for Job Classiﬁcation 7 5 LLMs \u0026 Prompt Engineering Table '\n '3. Overview of the various prompt modiﬁcations explored in thi s study. '\n 'Short name Description Baseline Provide a a job posting and asking if it is '\n 'ﬁt for a graduate. CoT Give a few examples of accurate classiﬁcation before '\n 'queryi ng. Zero-CoT Ask the model to reason step-by-step before providing '\n 'its an swer. rawinst Give instructions about its role and the task by adding '\n 'to the user msg. sysinst Give instructions about its role and the task as a '\n 'system msg. bothinst Split instructions with role as a system msg and task '\n 'as a user msg. mock Give task instructions by mocking a discussion where it '\n 'ackn owledges them. reit Reinforce key elements in the instructions by '\n 'repeating the m. strict Ask the model to answer by strictly following a '\n 'given templat e. loose Ask for just the ﬁnal answer to be given following a '\n 'given temp late. right Asking the model to reach the right conclusion.']\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next Steps","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"See part 3 ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/building-a-self-hosted-question-answering-service-using-langchain-ray"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Review the code and data used in this blog in the following ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/tree/main/embedding_pdf_documents"},"content":[{"data":{},"marks":[],"value":"Github repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\n","nodeType":"text"},{"data":{},"marks":[],"value":"See our earlier ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[],"value":"blog series on solving Generative AI infrastructure","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with Ray.","nodeType":"text"},{"data":{},"marks":[],"value":"\n\nIf you are interested in learning more about Ray, see ","nodeType":"text"},{"data":{"uri":"http://ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"http://docs.ray.io/"},"content":[{"data":{},"marks":[],"value":"Docs.Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nTo connect with the Ray community join #LLM on the ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"data":{},"marks":[],"value":"Ray Slack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or our ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[],"value":"Discuss forum","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","nodeType":"text"},{"data":{"uri":"http://www.anyscale.com/platform"},"content":[{"data":{},"marks":[],"value":"Anyscale.com/Platform ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"and click the 'Try it now' button","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray Summit 2023:","nodeType":"text"},{"data":{},"marks":[],"value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"practical training focused on LLMs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J2BxxlXHAsmjVdrjCqLdn","type":"Asset","createdAt":"2023-05-02T02:52:04.314Z","updatedAt":"2023-05-02T02:52:04.314Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Stages of our Data Pipeline","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J2BxxlXHAsmjVdrjCqLdn/2ee02bfb6d2d96c7be6ab848c757d45e/embedded-stages-of-our-pipeline.png","details":{"size":127047,"image":{"width":1512,"height":556}},"fileName":"embedded-stages-of-our-pipeline.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2rfQivy2LoE7OTc9bGVGvm","type":"Entry","createdAt":"2023-04-27T13:59:53.933Z","updatedAt":"2023-06-06T14:28:19.779Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Announcing Ray 2.4.0: Infrastructure for LLM training, tuning, inference, and serving","seoTitle":"Announcing Ray 2.4.0: Infrastructure for LLM training, tuning, inference, and serving","slug":"announcing-ray-2-4-0-infrastructure-for-llm-training-tuning-inference-and","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6TqWiyUibvC9ETfyYljP8b","type":"Entry","createdAt":"2022-02-11T15:42:52.165Z","updatedAt":"2022-02-11T15:42:52.165Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jiajun Yao","slug":"jiajun-yao"}}],"publishedDate":"2023-04-27","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.4.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2.4 release ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"features exciting improvements across the Ray ecosystem. In this blog post, we highlight new features and examples for Generative AI workloads and share four overall additions:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements to Ray data for ease of use, stability, and observability","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Improved Serve observability","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Introduce RLlib’s module for custom reinforcement learning","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Improved Ray scalability for large clusters","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Generative AI model examples and new features using Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Over the past few months, we have seen a flurry of innovative activities around ","nodeType":"text"},{"data":{"uri":"https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-generative-ai"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"generative AI models","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Large_language_model"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"large language models (LLMs)","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". To continue our steadfast commitment to make Ray a pivotal compute substrate for ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"generative AI workloads","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and address the challenges (as explained in our ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"blog series","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"), we have added significant enhancements in this release to ensure that these open source LLM models and workloads are accessible to the open source community and performant with Ray. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QDe7dun9hKhLCoGFSXKh9","type":"Asset","createdAt":"2023-04-26T19:23:25.596Z","updatedAt":"2023-04-26T19:23:25.596Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_image","file":{"url":"//images.ctfassets.net/xjan103pcp94/1QDe7dun9hKhLCoGFSXKh9/d9f93233f6ee72608e34735eac545a74/image1.png","details":{"size":154908,"image":{"width":960,"height":540}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Generative AI Workloads:","nodeType":"text"},{"data":{},"marks":[],"value":" First, with 2.4 we are releasing a set of new working examples showing how to use Ray with ","nodeType":"text"},{"data":{"uri":"https://stablediffusionweb.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stable Diffusion","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and LLMs like ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/transformers/model_doc/gptj"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GPT-J","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Below we tabulate the workload of each of these models and their respective links. More are underway with respect to Ray’s integration with ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"49AVYyPR2gbRD9P3NOH7My","type":"Entry","createdAt":"2023-04-26T19:44:22.407Z","updatedAt":"2023-04-26T22:30:40.568Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"generative_ai_features_and_examples","header":"Table 1. New LLM features, examples, and tutorials","body":"| Generative AI Workload| Description| Links|\n| ---------- | ---------- | ---------- |\n|Fine-tuning GPT-J|In this example, we showcase how to use [Ray Train](https://docs.ray.io/en/releases-2.4.0/train/train.html) for fine-tuning [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) using [DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed) and Hugging Face. GPT-J is a GPT-2-like causal language mode, with 6B parameters, trained on the PILE dataset.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html)|\n|Fine-tuning Dreambooth|This example shows how to fine-tune a [DreamBooth](https://dreambooth.github.io/) model using [Ray Train](https://docs.ray.io/en/releases-2.4.0/train/train.html). Dreambooth is a technique to fine-tune diffusion models (like Stable Diffusion) by injecting a custom subject to the model.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/dreambooth_finetuning.html)|\n|Scalable Batch Inference  with GPT-J|This example showcases how to use[ Ray Data](https://docs.ray.io/en/releases-2.4.0/data/dataset.html) for scalable batch inference with  GPT-J.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_batch_prediction.html)| Scalable Batch Inference  with Stable Diffusion|In this example, showcase how to use [Ray Data](https://docs.ray.io/en/releases-2.4.0/data/dataset.html) for scalable batch inference with [Stable Diffusion](https://stablediffusionweb.com/). Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from CompVis, Stability AI and LAION.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/stablediffusion_batch_prediction.html)|\n|Serving GPT-J|In this example, we showcase how to use [Ray Serve](https://docs.ray.io/en/releases-2.4.0/serve/index.html) for serving GPT-J.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_serving.html)|\n|Serving Stable Diffusion|In this example, we showcase how to use [Ray Serve](https://docs.ray.io/en/releases-2.4.0/serve/index.html) for serving [Stable Diffusion.](https://stablediffusionweb.com/)|[Source and documentation](https://docs.ray.io/en/master/serve/tutorials/stable-diffusion.html)|\n|Ray integration with LangChain|In this tutorial, we showcase how to build an open source search engine with Ray and [LangChain](https://python.langchain.com/en/latest/index.html)|[Source and blog](https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray)|"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray AIR new trainers:","nodeType":"text"},{"data":{},"marks":[],"value":" Second, to further enable additional large model workloads on Ray, we’re also releasing an ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"AccelerateTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", allowing you to run ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/accelerate"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"HuggingFace Accelerate","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/accelerate/usage_guides/deepspeed"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"DeepSpeed","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on Ray with minimal code changes. This Trainer integrates with the rest of the Ray ecosystem—including the ability to run distributed ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"hyperparameter tuning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with each trial dispatched as a distributed training job.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"See how ","nodeType":"text"},{"data":{"uri":"https://github.com/CarperAI/trlx/blob/9bc08369ca9ec83342c4d7755205dab1a7723006/trlx/sweep.py#L325-L340"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"CarperAI/trlx","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" uses this for RLHF hyperparameter sweeps, and take a peek at the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html#ray-train-huggingface-accelerate-acceleratetrainer"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" too.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4CHwg0j8NDYgH399cu2hGC","type":"Entry","createdAt":"2023-04-26T21:23:30.012Z","updatedAt":"2023-04-26T21:23:30.012Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"accelerate_trainer_code_snippet","body":"from ray.train.huggingface.accelerate import AccelerateTrainer\n\ndef train_func():\n    ...\n\ntrainer = AccelerateTrainer(\n    train_func,\n    accelerate_config={}  # you can pass in a config filepath here as well\n    scaling_config=ScalingConfig(\n        trainer_resources={\"CPU\": 0},\n        num_workers=4,\n        use_gpu=True,\n        resources_per_worker={\"CPU\": 2},\n    ),\n)\n\nresult = trainer.fit()\n\n# Integration with Ray Tune\ntuner = tune.Tuner(\n    trainer,\n    param_space={\"lr\": tune.grid_search([0.1, 0.2, 0.3]),\n)\n\nresults = tuner.fit()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can see the full example for AccelerateTrainer ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html#ray-train-huggingface-accelerate-acceleratetrainer"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Updated LightningTrainer:","nodeType":"text"},{"data":{},"marks":[],"value":" Third, in the broader deep learning space, we’re introducing the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.lightning.LightningTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LightningTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", allowing you to scale your ","nodeType":"text"},{"data":{"uri":"https://lightning.ai/docs/pytorch/stable//index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch Lightning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on Ray. As part of our continued effort for seamless integration and ease of use, we have enhanced and replaced our existing ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray_lightning","nodeType":"text"},{"data":{},"marks":[],"value":" integration, which was widely adopted, with the latest changes to PyTorch Lighting. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The new ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"LightningTrainer","nodeType":"text"},{"data":{},"marks":[],"value":" provides better compatibility with the other Ray libraries: Ray Tune or Ray Data or Ray Serve directly with Ray connecting APIs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Check out ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.lightning.LightningTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"the documentation for this component","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or take a look ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/examples/lightning/lightning_mnist_example.html#lightning-mnist-example"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"at an example","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1p8EMLSFTFDI0RuBceMrPe","type":"Entry","createdAt":"2023-04-26T21:27:50.764Z","updatedAt":"2023-04-26T21:27:50.764Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"updated_lightning_trainder_code_snippet","body":"from pytorch_lightning.loggers.csv_logs import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\nfrom ray.train.lightning import (\n    LightningTrainer,\n    LightningConfigBuilder,\n    LightningCheckpoint,\n)\n\nclass MNISTClassifier:\n\n    …\n\nclass MNISTDataModule:\n    …\n\nlightning_config = (\n    LightningConfigBuilder()\n    .module(MNISTClassifier, lr=1e-3, feature_dim=128)\n    .trainer(\n        max_epochs=10,\n        accelerator=\"cpu\",\n        log_every_n_steps=100,\n        logger=CSVLogger(\"logs\"),\n    )\n    .fit_params(datamodule=datamodule)\n    .checkpointing(monitor=\"val_accuracy\", mode=\"max\", save_top_k=3)\n    .build()\n)\nscaling_config = ScalingConfig(\n    num_workers=4, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n)\n\nrun_config = RunConfig(\n    name=\"ptl-mnist-example\",\n    local_dir=\"/tmp/ray_results\",\n    checkpoint_config=CheckpointConfig(\n        num_to_keep=3,\n        checkpoint_score_attribute=\"val_accuracy\",\n        checkpoint_score_order=\"max\",\n    ),\n)\ntrainer = LightningTrainer(\n    lightning_config=lightning_config,\n    scaling_config=scaling_config,\n    run_config=run_config,\n)\n\nresult = trainer.fit()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"See the full source code ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/examples/lightning/lightning_mnist_example.html#lightning-mnist-example"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements in Ray Data ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The enhancements and features in the release cut across a few dimensions: stability, ease of use, and observability.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data and ease of use: ","nodeType":"text"},{"data":{},"marks":[],"value":"Data ingestion and preprocessing for machine learning (ML) at scale are central to any end-to-end ML. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To enable ease of use, the default backend execution model for Ray Data is streaming-based under the hood. As such developers need not worry about instructing Ray Data to use the streaming execution backend; it’s the default execution mode. Second, the default streaming backend lends itself well to large scale workloads such as setting up distributed training data streaming and batch inference or preprocessing. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4fRyCVJVYH8NAjkcFhTgDX","type":"Entry","createdAt":"2023-04-26T21:30:40.030Z","updatedAt":"2023-04-26T21:30:40.030Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_data_code_snippet","body":"import ray\nfrom ray.data import ActorPoolStrategy\n\nimport time\n\ndef preprocess_func(x):\n   # Do some work with the data x\n   x = ...\n   time.sleep(0.1)\n   return x\n\nfor _ in (\n   ray.data.range_tensor(5000, shape=(80, 80, 3), parallelism=200)\n   .map_batches(preprocess_func, compute=ActorPoolStrategy(size=4))\n   .map_batches(preprocess_func, compute=ActorPoolStrategy(size=3))\n   .map_batches(preprocess_func, num_cpus=1)\n   .iter_batches()\n\n):\n\n   pass","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This launches a simple 4-stage pipeline. We use different compute args for each stage, which forces them to be run as separate operators instead of getting fused together. You should see a log message indicating streaming execution is being used.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While the above example is using dummy tasks, these sort of operator pipelines are a common pattern Ray users desire to scale:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6T6VTlZroKCZ6eyyII0HC1","type":"Asset","createdAt":"2023-04-26T21:34:22.865Z","updatedAt":"2023-04-26T21:34:22.865Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_data_distributed_image","description":"Figure 1. Ray data streaming data with ActorPool ","file":{"url":"//images.ctfassets.net/xjan103pcp94/6T6VTlZroKCZ6eyyII0HC1/d7202c977c1a0c25eeb4563ab5c50319/image3.png","details":{"size":106561,"image":{"width":1249,"height":515}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray data and SQL","nodeType":"text"},{"data":{},"marks":[],"value":": Additionally, we have extended Ray Data’s ability to fetch data from a common ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/creating-datastreams.html#reading-from-sql-databases"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SQL data source","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", with ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"read_sql(...)","nodeType":"text"},{"data":{},"marks":[],"value":", enabling you to connect to a common SQL data store.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3imkshawnZw7DglAUBNTN3","type":"Entry","createdAt":"2023-04-26T21:36:16.889Z","updatedAt":"2023-04-26T21:36:16.889Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_data_sql_connector_code_snippet","body":"import mysql.connector\n\nimport ray\n\ndef create_connection():\n    return mysql.connector.connect(\n        user=\"admin\",\n        password=...,\nhost=\"example-mysql-database.c2c2k1yfll7o.us-west-2.rds.amazonaws.com\",\n      connection_timeout=30,\n      database=\"example\",\n    )\n\n# Get all movies\ndatastream = ray.data.read_sql(\"SELECT * FROM movie\", create_connection)\n# Get movies after the year 1980\ndatastream = ray.data.read_sql(\n    \"SELECT title, score FROM movie WHERE year \u003e= 1980\", create_connection\n)\n# Get the number of movies per year\ndatastream = ray.data.read_sql(\n    \"SELECT year, COUNT(*) FROM movie GROUP BY year\", create_connection\n)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements to Serve observability","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In 2.4, we are adding new functionality for observability and monitoring for Ray Serve applications. You can use the Ray dashboard to get a high-level overview of your Ray cluster and Ray Serve application’s states. This includes details such as:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"the number of deployment replicas currently running","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"logs for your Serve controller, deployment replicas, and HTTP proxies","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"the Ray nodes (i.e., machines) running in your Ray cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can access the Ray dashboard at port 8265 at your cluster’s URI. For example, if you’re running Ray Serve locally, use ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"http://localhost:8265","nodeType":"text"},{"data":{},"marks":[],"value":" in your browser.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3CtBAZRebvQVRJsbp63rD2","type":"Asset","createdAt":"2023-04-26T21:39:28.459Z","updatedAt":"2023-04-26T21:39:28.459Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_serve_observebility","description":"Figure 1. Serve observesability metrics in the Ray Dashboard.","file":{"url":"//images.ctfassets.net/xjan103pcp94/3CtBAZRebvQVRJsbp63rD2/0951a7ad1f4d4ec1ae2a3ba60a3af218/image2.png","details":{"size":146211,"image":{"width":1999,"height":1040}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"For more information on the dashboard, read the documentation for the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/ray-core/ray-dashboard.html#dash-serve-view"},"content":[{"data":{},"marks":[],"value":"Serve page","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"RLlib’s new module for custom reinforcement","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In RLlib, we are introducing a new ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/rllib-rlmodule.html#rl-modules-alpha"},"content":[{"data":{},"marks":[],"value":"RLModule","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" abstraction (alpha). RLModule API provides a unified way to define custom reinforcement learning models in RLlib. This API enables you to design and implement your own models to suit specific needs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To maintain consistency and usability, RLlib offers a standardized approach for defining module objects for both single-agent and multi-agent reinforcement learning environments through the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/package_ref/rl_modules.html"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"SingleAgentRLModuleSpec","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/package_ref/doc/ray.rllib.core.rl_module.marl_module.MultiAgentRLModuleSpec.html#ray.rllib.core.rl_module.marl_module.MultiAgentRLModuleSpec"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"MultiAgentRLModuleSpec","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" classes. The built-in RLModules in RLlib follow this consistent design pattern, making it easier for you to understand and utilize these modules.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1w6lK0sBUAVl4BmOGBQcQ9","type":"Entry","createdAt":"2023-04-26T21:48:25.122Z","updatedAt":"2023-04-26T21:48:25.122Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"rllib_multiagent_code_snippet","body":"import gymnasium as gym\nfrom ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec\nfrom ray.rllib.core.testing.torch.bc_module import DiscreteBCTorchModule\n\nenv = gym.make(\"CartPole-v1\")\n\nspec = SingleAgentRLModuleSpec(\n    module_class=DiscreteBCTorchModule,\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    model_config_dict={\"fcnet_hiddens\": [64]},\n)\n\nmodule = spec.build()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is an experimental module that serves as a general replacement for ModelV2, and is subject to change. It will eventually match the functionality of the previous stack. If you only use high-level RLlib APIs such as ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html#ray.rllib.algorithms.algorithm.Algorithm"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Algorithm","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", you should not experience significant changes, except for a few new parameters to the configuration object. Read more about it in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/rllib/rllib-rlmodule.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Core scaling to 2000 nodes","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We’ve also invested time in making sure Ray can support larger scale workloads. With this release, we are announcing official support for Ray clusters to up to 2000 nodes. See","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/release/benchmarks/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":" scalability envelope","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for more details.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"First and foremost, we want to thank all contributors for their valuable contributions to this new release of ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.4.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2.4","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Your enduring support continues to foster the wider use of Ray adoption.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This release is a milestone in our efforts to offer Ray as a performant compute infrastructure for LLM training, inference, and serving. We shared a number of examples that speak to these efforts.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Additionally, we enhanced Ray Data, improved Serve observability, introduced a new RLlib customization module, and scaled Ray core to support large workloads, up to 2000 nodes. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Have a go at the latest release with pip install “ray[default]” and let us know of your feedback. We’re always interested to hear from you – feel free to reach out to us on ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Github","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discuss","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What’s Next?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stay tuned for more Generative AI how-to blog series as we continue our efforts to make Ray the platform choice and compute substrate for all your Generative AI workloads. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you missed our four-part blog series on Generative AI, in which lay out our vision, position, and solution on how Ray addresses the challenges associated with OSS Generative AI workloads, peruse the links below. Much of the above Generative AI examples and features stem from this series.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"🔗 ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-ray-solves-common-production-challenges-for-generative-ai"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"How Ray solves common production challenges for generative AI infrastructure","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"🔗 ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Training 175B Parameter Language Models at 1000 GPU scale with Alpa and Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"🔗 ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/faster-stable-diffusion-fine-tuning-with-ray-air"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Faster stable diffusion fine-tuning with Ray AIR","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"🔗 ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"How to fine tune and serve LLMs simply, quickly and cost effectively using Ray + DeepSpeed + HuggingFace","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"🔗 ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Building an LLM open source search engine in 100 lines using LangChain and Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Join our ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/community"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Community","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the ","nodeType":"text"},{"data":{"uri":"https://forms.gle/9TSdDYUgxYs8SA9e8"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray #LLM slack channel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have our ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit 2023 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"early-bird registration open. Secure your spot, and save some money.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QDe7dun9hKhLCoGFSXKh9","type":"Asset","createdAt":"2023-04-26T19:23:25.596Z","updatedAt":"2023-04-26T19:23:25.596Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_image","file":{"url":"//images.ctfassets.net/xjan103pcp94/1QDe7dun9hKhLCoGFSXKh9/d9f93233f6ee72608e34735eac545a74/image1.png","details":{"size":154908,"image":{"width":960,"height":540}},"fileName":"image1.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"574o0Bh7HzHlCEc6AXphfF","type":"Entry","createdAt":"2023-04-19T16:00:08.661Z","updatedAt":"2023-05-31T18:34:15.880Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":16,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Building an LLM open source search engine in 100 lines using LangChain and Ray","seoTitle":"Building an LLM Open-Source Search Engine in 100 Lines","slug":"llm-open-source-search-engine-langchain-ray","description":"In part 1 of a new blog series, we show how to build a search engine in 100 lines using LLM embeddings and a vector database.","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-04-18","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 1 of a blog series. In this blog, we’ll introduce you to LangChain and Ray Serve and how to use them to build a search engine using LLM embeddings and a vector database. In future parts, we will show you how to turbocharge embeddings and how to combine a vector database and an LLM to create a fact-based question answering service. Additionally, we will optimize the code and measure performance: cost, latency and throughput.\n\n\u003cscript type=\"application/ld+json\"\u003e{\n\"@context\": \"http://schema.org\",\n\"@type\": \"VideoObject\",\n\"name\": \"Open Source LLM Search Engine with LangChain on Ray\",\n\"description\": \"Waleed, Head of Engineering at Anyscale, explains how to use LangChain and Ray Serve to build a search engine using LLM embeddings and a vector database. Blog: https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray\",\n\"thumbnailUrl\": \"https://i.ytimg.com/vi/v7a8SR-sZpI/default.jpg\",\n\"uploadDate\": \"2023-04-19T16:00:41Z\",\n\"duration\": \"PT7M36S\",\n\"embedUrl\": \"https://www.youtube.com/embed/v7a8SR-sZpI\",\n\"interactionCount\": \"4540\"\n}\u003c/script\u003e","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"b9V0HkkRoYEqCnIrMO9kC","type":"Entry","createdAt":"2023-04-19T17:16:15.326Z","updatedAt":"2023-04-19T17:16:15.326Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"LLM Search Engine with Langchain","videoUrl":"https://www.youtube.com/watch?v=v7a8SR-sZpI"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog, we'll cover:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"An introduction to ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and show why it’s awesome.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"An explanation of how Ray complements ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" by: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Showing how with a few minor changes, we can speed parts of the process up by a factor of 4x or more","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Making ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"’s capabilities available in the cloud using Ray Serve","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Using self-hosted models by running ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the model all in the same Ray cluster without having to worry about maintaining individual machines.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introduction","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a very powerful framework for ML orchestration, but with great power comes voluminous documentation. 120 megabytes in fact. How can we make that documentation more accessible? ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Answer: make it searchable! It used to be that creating your own high quality search results was hard. But by using ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we can build it in about 100 lines of code. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is where ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" comes in. ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provides an amazing suite of tools for everything around LLMs. It’s kind of like ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/"},"content":[{"data":{},"marks":[],"value":"HuggingFace","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" but specialized for LLMs. There are tools (chains) for prompting, indexing, generating and summarizing text. While an amazing tool, using Ray with it can make ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" even more powerful. In particular, it can: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Simply and quickly help you deploy a ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" service. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Rather than relying on remote API calls, allow Chains to run co-located and auto-scalable with the LLMs itself. This brings all the advantages we ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"discussed in a previous blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": lower cost, lower latency, and control over your data.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Building the index","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4OzISThpksdKgjZ0gVJUiB","type":"Asset","createdAt":"2023-04-19T16:59:49.253Z","updatedAt":"2023-05-01T09:40:14.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"doc-index-starter-langchain","description":"Build a document index easily with Ray and Langchain","file":{"url":"//images.ctfassets.net/xjan103pcp94/4OzISThpksdKgjZ0gVJUiB/85bb7fccdfef1df3d061c57e9af1062a/index-langchain.jpg","details":{"size":28918,"image":{"width":900,"height":381}},"fileName":"index-langchain.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"First we will build the index via the following steps. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Download the content we want to index locally. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Read the content and cut it into tiny little pieces (about a sentence each). This is because it is easier to match queries against pieces of a page rather than the whole page.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use the ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/sentence-transformers"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Sentence Transformers","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" library from HuggingFace to generate a vector representation of each sentence. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Embed those vectors in a Vector database (we use ","nodeType":"text"},{"data":{"uri":"https://github.com/facebookresearch/faiss"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FAISS","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", but you could use whatever you like). ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The amazing thing about this code is how simple it is - ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/d06097768abbea54d59e5d3ed4f045f3"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"See Here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". As you will see, thanks to LangChain, all the heavy lifting is done for us. Let’s pick a few excerpts. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Assuming we’ve downloaded the Ray docs, this is all we have to do to read all the docs in: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5XcvTPG6LASLRu3arklXJ0","type":"Entry","createdAt":"2023-04-17T08:22:33.719Z","updatedAt":"2023-04-17T08:22:33.719Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-1","body":"loader = ReadTheDocsLoader(\"docs.ray.io/en/master/\")\ndocs = loader.load() ","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The next step is to break each document into little chunks. LangChain uses splitters to do this. So all we have to do is this: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"72vZr3kpoioEza3WTS1vfT","type":"Entry","createdAt":"2023-04-17T08:23:19.379Z","updatedAt":"2023-04-17T08:23:19.379Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-2","body":"chunks = text_splitter.create_documents(\n    [doc.page_content for doc in docs], \n    metadatas=[doc.metadata for doc in docs])","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We want to preserve the metadata of what the original URL was, so we make sure to retain the metadata along with these documents. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now we have the chunks we can embed them as vectors. LLM providers do offer APIs for doing this remotely (and this is how most people use LangChain). But, with just a little bit of glue we can download Sentence Transformers from HuggingFace and run them locally (inspired by LangChain’s support for llama.cpp). ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/aea1d312d68c9431949442cc562d5f2c"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Here’s the glue code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"By doing so, we reduce latency, stay on open source technologies, and don’t need a HuggingFace key or to pay for API usage. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have the embeddings, now we can use a vector database – in this case FAISS – to store the embeddings. Vector databases are optimized for doing quick searches in high dimensional spaces. Again, LangChain makes this effortless. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4YljOvEjiKrfhFRmpA1PDG","type":"Entry","createdAt":"2023-04-17T08:23:35.223Z","updatedAt":"2023-04-17T08:23:35.223Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-3","body":"from langchain.vectorstores import FAISS\n\ndb = FAISS.from_documents(chunks, embeddings)\ndb.save_local(FAISS_INDEX_PATH)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"And that’s it. The code for this is ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/d0915e52cbe56dff328f5c00ded21107"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Now we can build the store.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"39FzcaaXnaSmciROgXSD0x","type":"Entry","createdAt":"2023-04-17T08:29:45.440Z","updatedAt":"2023-04-17T08:29:45.440Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-python-build-snippet","body":"% python build_vector_store.py"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This takes about 8 minutes to execute. Most of that time is spent doing the embeddings. Of course, it’s not a big deal in this case, but imagine if you were indexing hundreds of gigabytes instead of hundreds of megabytes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Accelerating indexing using Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"[Note: This is a slightly more advanced topic and can be skipped on first reading. It just shows how we can do it more quickly – 4x to 8x times more quickly]","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How can we speed up indexing? The great thing is that embedding is easy to parallelize. What if we: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Sliced the list of chunks into 8 shards. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Embedded each of the 8 shards separately.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Merge the shards. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7tDpD5Q7nxtRyX9lgDvbkI","type":"Asset","createdAt":"2023-04-17T08:04:57.120Z","updatedAt":"2023-04-17T08:04:57.120Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"langchain-ray-accelerated-indexing","description":"Build a document index 4-8x faster with Ray","file":{"url":"//images.ctfassets.net/xjan103pcp94/7tDpD5Q7nxtRyX9lgDvbkI/6209fbd875c5cd379c2289ef6f6554f0/Screen_Shot_2023-04-16_at_6.20.10_PM.png","details":{"size":277144,"image":{"width":2284,"height":936}},"fileName":"Screen Shot 2023-04-16 at 6.20.10 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"One key thing to realize is that embedding is GPU accelerated, so if we want to do this, we need 8 GPUs. Thanks to Ray, those 8 GPUS don’t have to be on the same machine. But even on a single machine, there are significant advantages to using Ray. And one does not have to go to the complexity of setting up a Ray cluster, all you need to do is ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"pip install ray[default]","nodeType":"text"},{"data":{},"marks":[],"value":" and then ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"import ray","nodeType":"text"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This requires some minor surgery to the code. Here’s what we have to do.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"First, create a task that creates the embedding and then uses it to index a shard. Note the Ray annotation and us telling us each task will need a whole GPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3NmJuC8SstZJpoHLrzrLgg","type":"Entry","createdAt":"2023-04-17T08:23:48.410Z","updatedAt":"2023-04-17T08:23:48.410Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-4","body":"@ray.remote(num_gpus=1)\ndef process_shard(shard): \n    embeddings = LocalHuggingFaceEmbeddings('multi-qa-mpnet-base-dot-v1')\n    result = FAISS.from_documents(shard, embeddings)\n    return result\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next, split the workload in the shards. NumPy to the rescue! This is a single line: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7lotIjqpm0Sm03BPHE4t2n","type":"Entry","createdAt":"2023-04-17T08:24:01.572Z","updatedAt":"2023-04-17T08:24:01.572Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-5","body":"shards = np.array_split(chunks, db_shards)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Then, create one task for each shard and wait for the results.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7m8iFk9DvJOBZc3r4maPTY","type":"Entry","createdAt":"2023-04-17T08:24:13.248Z","updatedAt":"2023-04-17T08:24:13.248Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-6","body":"futures = [process_shard.remote(shards[i]) for i in range(db_shards)]\nresults = ray.get(futures)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, let’s merge the shards together. We do this using simple linear merging. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"21KuagZ35WVzVVJ7Xq4qHy","type":"Entry","createdAt":"2023-04-17T08:24:27.839Z","updatedAt":"2023-04-17T08:29:23.908Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-7","body":"db = results[0]\nfor i in range(1,db_shards):\n    db.merge_from(results[i])","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/4c41f3ee66040f57d34c6a40e42b5969"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Here’s what the sped up code looks like.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"You might be wondering, does this actually work? We ran some tests on a g4dn.metal instance with 8 GPUs. The original code took 313 seconds to create the embeddings, the new code took 70 seconds, that’s about a 4.5x improvement. There’s still some one-time overheads to creating tasks, setting up the GPUs etc. This reduces as the data increases. For example, we did a simple test with 4 times the data, and it was around 80% of the theoretical maximum performance (ie. 6.5x faster vs theoretical maximum of 8x faster from the 8 GPUs). ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can use the Ray Dashboard to see how hard those GPUs are working. Sure enough they’re all close to 100% running the process_shard method we just wrote. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"14IoasmHxwEeQdAEGJqlyO","type":"Asset","createdAt":"2023-04-17T08:10:19.509Z","updatedAt":"2023-04-17T21:38:58.348Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"accelerated-index-langchain-dashboard","description":"Dashboard shows that GPU utilization is maxed out across all instances","file":{"url":"//images.ctfassets.net/xjan103pcp94/14IoasmHxwEeQdAEGJqlyO/5ae6e6739e258252a78c889ca7959683/raydash.png","details":{"size":278628,"image":{"width":2442,"height":842}},"fileName":"raydash.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"It turns out merging vector databases  is pretty fast, taking only 0.3 seconds for all 8 shards to be merged. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Serving","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1g6zBePU72Rmz5MBH2reaB","type":"Asset","createdAt":"2023-04-17T08:08:50.696Z","updatedAt":"2023-04-17T08:08:50.696Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Serving-Queries-Ray-Langchain","description":"Serve search queries with Ray and Langchain","file":{"url":"//images.ctfassets.net/xjan103pcp94/1g6zBePU72Rmz5MBH2reaB/db400e9bbbc445d7214d45658f81992f/Screen_Shot_2023-04-16_at_9.42.46_PM.png","details":{"size":380753,"image":{"width":2718,"height":1348}},"fileName":"Screen Shot 2023-04-16 at 9.42.46 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Serving is another area where the combination of LangChain and Ray Serve shows its power. This is just scratching the surface: we’ll explore amazing capabilities like independent auto scaling and request batching in our next blog post in the series. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The steps required to do this are: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Load the FAISS database we created and the instantiate the embedding","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Start using FAISS to do similarity searches. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve makes this magically easy. Ray uses a “deployment” to wrap a simple python class. The __init__ method does the loading and then __call__ is what actually does the work. Ray takes care of connecting it to the internet, bringing up a service, http and so on. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Here’s a simplified version of the code: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7GwywaSkEi9NnwUhk3wMMD","type":"Entry","createdAt":"2023-04-17T08:24:42.996Z","updatedAt":"2023-04-17T08:24:42.996Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-8","body":"@serve.deployment\nclass VectorSearchDeployment:\n    def __init__(self):\n        self.embeddings = … \n        self.db = FAISS.load_local(FAISS_INDEX_PATH, self.embeddings)\n\n    def search(self,query): \n        results = self.db.max_marginal_relevance_search(query)\n        retval = \u003csome string processing of the results\u003e\n        return retval\n\n    async def __call__(self, request: Request) -\u003e List[str]:\n        return self.search(request.query_params[\"query\"])\n\ndeployment = VectorSearchDeployment.bind()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"That’s it! ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let’s now start this service with the command line (of course Serve has more deployment options than this, but this is an easy way):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"o0cCaJMv4yeC2ikcQuJhf","type":"Entry","createdAt":"2023-04-17T08:29:00.307Z","updatedAt":"2023-04-17T08:29:00.307Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-serve-snippet","body":"% serve run serve_vector_store:deployment"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now we can write a simple python script to query the service to get relevant vectors(it’s just a web server running on port 8000). ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3E0la6cbL8n7UPpvve36nJ","type":"Entry","createdAt":"2023-04-17T08:24:56.873Z","updatedAt":"2023-04-17T08:24:56.873Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-9","body":"import requests\nimport sys\nquery = sys.argv[1]\nresponse = requests.post(f'http://localhost:8000/?query={query}')\nprint(response.content.decode())","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"And now let’s try it out: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SL7auZuJzHEyaq8fVIifv","type":"Entry","createdAt":"2023-04-17T08:25:16.327Z","updatedAt":"2023-04-17T21:14:41.421Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-10","body":"$ python query.py 'Does Ray Serve support batching?'\nFrom http://docs.ray.io/en/master/serve/performance.html\n\nYou can check out our microbenchmark instructions\nto benchmark Ray Serve on your hardware.\nRequest Batching#\n====\n\nFrom http://docs.ray.io/en/master/serve/performance.html\n\nYou can enable batching by using the ray.serve.batch decorator. Let’s take a look at a simple example by modifying the MyModel class to accept a batch.\nfrom ray import serve\nimport ray\n@serve.deployment\nclass Model:\n    def __call__(self, single_sample: int) -\u003e int:\n        return single_sample * 2\n====\n\nFrom http://docs.ray.io/en/master/ray-air/api/doc/ray.train.lightgbm.LightGBMPredictor.preferred_batch_format.html\n\nnative batch format.\nDeveloperAPI: This API may change across minor Ray releases.\n====\n\nFrom http://docs.ray.io/en/master/serve/performance.html\n\nMachine Learning (ML) frameworks such as Tensorflow, PyTorch, and Scikit-Learn support evaluating multiple samples at the same time.\nRay Serve allows you to take advantage of this feature via dynamic request batching.\n===="}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We showed in the above code just how easy it is to build key components of an LLM-based search engine and serve its responses to the entire world by combining the power of LangChain and Ray Serve. And we didn’t have to deal with a single pesky API key! ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Tune in for Part 2, where we will show how to turn this into a chatgpt-like answering system. We’ll use open source LLMs like Dolly 2.0 to do that. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"And finally we’ll share Part 3 where we’ll talk about scalability and cost. The above is fine for a few hundred queries per second, but what if you need to scale to a lot more? And are the claims about latency correct? ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next Steps","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"See part 2 ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/turbocharge-langchain-now-guide-to-20x-faster-embedding"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nReview the code and data used in this blog in the following ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/tree/main/open_source_LLM_search_engine"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Github repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". \n\nSee our earlier ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[],"value":"blog series on solving Generative AI infrastructure","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with Ray.\n\nIf you are interested in learning more about Ray, see ","nodeType":"text"},{"data":{"uri":"http://ray.io"},"content":[{"data":{},"marks":[],"value":"Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"http://docs.ray.io"},"content":[{"data":{},"marks":[],"value":"Docs.Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To connect with the Ray community join #LLM on the ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"data":{},"marks":[],"value":"Ray Slack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or our ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[],"value":"Discuss forum","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","nodeType":"text"},{"data":{"uri":"http://www.anyscale.com/platform"},"content":[{"data":{},"marks":[],"value":"Anyscale.com/Platform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and click the 'Try it now' button","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray Summit 2023:","nodeType":"text"},{"data":{},"marks":[],"value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"practical training focused on LLMs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4OzISThpksdKgjZ0gVJUiB","type":"Asset","createdAt":"2023-04-19T16:59:49.253Z","updatedAt":"2023-05-01T09:40:14.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"doc-index-starter-langchain","description":"Build a document index easily with Ray and Langchain","file":{"url":"//images.ctfassets.net/xjan103pcp94/4OzISThpksdKgjZ0gVJUiB/85bb7fccdfef1df3d061c57e9af1062a/index-langchain.jpg","details":{"size":28918,"image":{"width":900,"height":381}},"fileName":"index-langchain.jpg","contentType":"image/jpeg"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7bptc6mEv7bhFZvq5AOXqc","type":"Entry","createdAt":"2023-04-18T20:36:43.249Z","updatedAt":"2023-04-19T16:00:08.646Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Why I Joined Anyscale: Solving Cutting-Edge Problems in a Time of Enormous Change","seoTitle":"why i joined anyscale by sidney rabsatt","slug":"why-i-joined-anyscale-solving-cutting-edge-problems-in-a-time-of-enormous","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"461P1q4TML68SzYxOG9sxm","type":"Entry","createdAt":"2023-04-18T20:05:20.525Z","updatedAt":"2023-04-18T20:05:20.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Sidney Rabsatt","slug":"sidney-rabsatt","link":"https://www.linkedin.com/in/sidney-rabsatt/"}}],"publishedDate":"2023-04-19","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42F9iHs5a05rRrOuqEQwvB","type":"Entry","createdAt":"2022-06-15T01:30:47.322Z","updatedAt":"2022-06-15T01:30:47.322Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Culture","slug":"culture","tintColor":"#39C8E5"}},"intro":"Why Sidney Rabsatt joined Anyscale as Head of Product.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Throughout my career, I've had the opportunity to work on cutting-edge problems across some of the most impactful technological transitions of our time. From the early days of the World Wide Web to the rise of Mobile and Cloud computing, I’ve worked on numerous commercial products across Networking, Observability, and App/Cloud Infrastructure and on some of the most widely-adopted open-source projects including Wireshark, Nginx, and now Ray. I've been deeply involved in resolving the complexities and, at times, unexpected infrastructure obstacles that technologists encounter, while empowering companies to fully benefit from these advancements.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7uPFFDyMQg5GZ5FU5kM8rw","type":"Asset","createdAt":"2023-04-18T20:34:23.318Z","updatedAt":"2023-04-18T20:34:23.318Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Sidney Rabsatt Headshot","description":"Picture of Sidney Rabsatt, Anyscale Head of Product","file":{"url":"//images.ctfassets.net/xjan103pcp94/7uPFFDyMQg5GZ5FU5kM8rw/225783b86665705f9c198768754f8722/Sidney_pic.jpeg","details":{"size":501555,"image":{"width":3540,"height":2832}},"fileName":"Sidney pic.jpeg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"AI","nodeType":"text"},{"data":{},"marks":[],"value":" promises to be the most complex transition across the most dimensions that affect our lives. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"I joined Anyscale to embrace and be part of solving those challenges. I joined to make AI available to all organizations, to give people better tools, and do it responsibly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray and Anyscale work at the core of the best known and most advanced AI applications such as Generative AI with OpenAI’s ChatGPT and Prediction for Uber rides / ETAs, not to mention what Spotify, Instacart and others are doing. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The team is one of the strongest in the industry, comprised of AI/ML PhD’s and experienced AI experts from top schools like UC Berkeley and companies like Uber, Google, Amazon, Meta, and more!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Our technology is proven and continues to evolve rapidly with strong community involvement. And the opportunity is vast to define how best to develop and run AI. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"AI will touch and shape our lives in many ways and joining Anyscale to lead Product is how I’m getting involved.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7uPFFDyMQg5GZ5FU5kM8rw","type":"Asset","createdAt":"2023-04-18T20:34:23.318Z","updatedAt":"2023-04-18T20:34:23.318Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Sidney Rabsatt Headshot","description":"Picture of Sidney Rabsatt, Anyscale Head of Product","file":{"url":"//images.ctfassets.net/xjan103pcp94/7uPFFDyMQg5GZ5FU5kM8rw/225783b86665705f9c198768754f8722/Sidney_pic.jpeg","details":{"size":501555,"image":{"width":3540,"height":2832}},"fileName":"Sidney pic.jpeg","contentType":"image/jpeg"}}},"mainImageFit":"contain","hideIntro":true,"showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7e5A9XUzg0NfneWobPOKxf","type":"Entry","createdAt":"2023-04-10T23:01:28.472Z","updatedAt":"2023-05-31T18:31:32.938Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":12,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"How to fine tune and serve LLMs simply, quickly and cost effectively using Ray + DeepSpeed + HuggingFace","seoTitle":"How to Fine-Tune a 6 Billion Parameter LLM for Less Than $7","slug":"how-to-fine-tune-and-serve-llms","description":"In part 4 of our Generative AI series, we share how to build a system for fine-tuning \u0026 serving LLMs in 40 minutes or less.","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2yKvtARlwNkkIYOYLFgkhB","type":"Entry","createdAt":"2022-06-22T19:54:47.912Z","updatedAt":"2022-06-22T19:54:47.912Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jun Gong","slug":"jun-gong"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BGRfsYFKUgIpGjaXTUYml","type":"Entry","createdAt":"2021-08-09T15:38:43.390Z","updatedAt":"2021-08-09T15:38:43.390Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Antoni Baum","slug":"antoni-baum","link":"https://www.linkedin.com/in/yard1/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}}],"publishedDate":"2023-04-10","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 4 of our blog series on Generative AI. In the previous blog posts we explained:\n1.[Why Ray is a sound platform for Generative AI](https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure)\n2.[we showed how it can push the performance limits](https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray)\n3.[how you can use Ray for stable diffusion](https://www.anyscale.com/blog/faster-stable-diffusion-fine-tuning-with-ray-air). \n","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this blog, we share a  practical approach on how you can use the combination of HuggingFace, DeepSpeed, and Ray to build a system for fine-tuning and serving LLMs, in 40 minutes for less than $7 for a 6 billion parameter model. In particular, we illustrate the following:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Using these three components, you can simply and quickly put together an open-source LLM fine-tuning and serving system. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"By taking advantage of Ray’s distributed capabilities, we show how this can be both more cost-effective and faster than using a single large (and often ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://aiascendant.substack.com/p/taiwan-is-pandora-gpus-are-unobtainium"},"content":[{"nodeType":"text","value":"unobtainable","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":") machine.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Here’s what we’ll be doing: ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Discussing why you might want to run your own LLM instead of using one of the new API providers. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing you the evolving tech stack we are seeing for cost-effective LLM fine-tuning and serving, combining HuggingFace, DeepSpeed, Pytorch, and Ray. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing you 40 lines of Python code that can enable you to serve a 6 billion parameter GPT-J model. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing you, for less than $7, how you can fine-tune the model to sound more medieval using the works of Shakespeare by doing it in a distributed fashion on low-cost machines, which is considerably more cost-effective than using a single large powerful machine. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing how you can serve the fine-tuned 6B LLM compiled model binary. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing how the fine-tuned model compares to a prompt engineering approach with large systems. ","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Why would I want to run my own LLM? ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"There are ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://openai.com"},"content":[{"nodeType":"text","value":"many","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://anthropic.com/"},"content":[{"nodeType":"text","value":"many","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://cohere.ai/"},"content":[{"nodeType":"text","value":"providers","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" of LLM APIs online. Why would you want to run your own? There are a few reasons: ","marks":[],"data":{}}]},{"nodeType":"ordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Cost, especially for fine-tuned inference","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": For example, OpenAI charges 12c per 1000 tokens (about 700 words) for a fine-tuned model on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://openai.com/pricing"},"content":[{"nodeType":"text","value":"Davinci","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". It’s important to remember that many user interactions require multiple backend calls (e.g. one to help with the prompt generation, post-generation moderation, etc), so it’s very possible that a single interaction with an end user could cost a few dollars. For many applications, this is cost prohibitive. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Latency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"using these LLMs is especially slow. A GPT-3.5 query for example can take up to 30 seconds. Combine a few round trips from your data center to theirs and it is possible for a query to take minutes. Again, this makes many applications impossible. Bringing the processing in-house allows you to optimize the stack for your application, e.g. by using low-resolution models, tightly packing queries to GPUs, and so on. We have heard from users that optimizing their workflow has often resulted in a 5x or more latency improvement. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Data Security \u0026 Privacy: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"In order to get the response from these APIs, you have to send them a lot of data for many applications (e.g. send a few snippets of internal documents and ask the system to summarize them). Many of the API providers reserve the right to use those instances for retraining. Given the sensitivity of organizational data and also frequent legal constraints like data residency, this is especially limiting. One, particularly concerning recent development, is the ability to ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://arxiv.org/pdf/2301.13188.pdf"},"content":[{"nodeType":"text","value":"regenerate training data from learned models","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", and people ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.techradar.com/news/samsung-workers-leaked-company-secrets-by-using-chatgpt"},"content":[{"nodeType":"text","value":"unintentionally disclosing secret information","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"How to create and run your own LLM ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The LLM space is an incredibly fast-moving space, and it is currently evolving very rapidly. What we are seeing is a particular technology stack that combines multiple technologies: ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70GhsrIVrh0Qz0fNDUp4A8","type":"Asset","createdAt":"2023-04-07T14:17:45.223Z","updatedAt":"2023-04-07T14:17:45.223Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"llm-stack","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/70GhsrIVrh0Qz0fNDUp4A8/458e076a02c4745369c683851c378536/Screenshot_2023-04-07_at_10.17.23_AM.png","details":{"size":144021,"image":{"width":1073,"height":663}},"fileName":"Screenshot 2023-04-07 at 10.17.23 AM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"What we’ve also seen is a reluctance to go beyond a single machine for training. In part, because there is a perception that moving to multiple machines is seen as complicated. The good news is this is where ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io"},"content":[{"nodeType":"text","value":"Ray.io","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" shines (ba-dum-tish). It simplifies cross-machine coordination and orchestration aspects using not much more than Python and Ray decorators, but also is a great framework for composing this entire stack together. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\nRecent results on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html"},"content":[{"nodeType":"text","value":"Dolly","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/lm-sys/FastChat"},"content":[{"nodeType":"text","value":"Vicuna","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" (both trained on Ray or trained on models built with Ray like ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://huggingface.co/EleutherAI/gpt-j-6b"},"content":[{"nodeType":"text","value":"GPT-J","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":") are small LLMs (relatively speaking – say the open source model GPT-J-6B with 6 billion parameters) that can be incredibly powerful when fine-tuned on the right data. The key is fine-tuning and the right data parts. So you do not always need to use the latest and greatest model with 150 billion-plus parameters to get useful results. Let’s get started! \n","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Serving a pre-existing model with Ray for text generation","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The detailed steps on how to serve the GPT-J model with Ray can be found ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/gptj_serving.html"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", so let’s highlight some of the aspects of how we do that. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"56imF50uP87DrWXWfjxNqV","type":"Entry","createdAt":"2023-04-07T18:37:36.999Z","updatedAt":"2023-04-07T18:47:21.234Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"serve-llm","body":"   @serve.deployment(ray_actor_options={\"num_gpus\":1})\n   classPredictDeployment:\n     def__init__(self, model_id:str, revision:str=None):\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n        import torch\n        self.model = AutoModelForCausalLM.from_pretrained(\n            \"EleutherAI/gpt-j-6B\",\n            revision=revision,\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True,\n            device_map=\"auto\",  # automatically makes use of all GPUs available to the Actor\n        )\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Serving in Ray happens in actors, in this case, one called PredictDeployment. This code shows the __init__ method of the action that downloads the model from Hugging Face. To launch the model on the current node, we simply do: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7jOn5zhnrYvHQZg3fqmwjE","type":"Entry","createdAt":"2023-04-07T18:39:26.501Z","updatedAt":"2023-04-07T18:47:32.494Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"serve-cmd","body":"deployment = PredictDeployment.bind(model_id=model_id, revision=revision)\nserve.run(deployment)\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"That starts a service on port 8000 of the local machine. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We can now query that service using a few lines of Python","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6HQg2dtvaXrNsKuUvSIX3S","type":"Entry","createdAt":"2023-04-07T18:41:24.773Z","updatedAt":"2023-04-07T18:47:42.594Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"serve-query","body":"import requests\nprompt = (\n    “Once upon a time, there was a horse. “\n)\nsample_input = {\"text\": prompt}\noutput = requests.post(\"http://localhost:8000/\", json=[sample_input]).json()\nprint(output)","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"And sure enough, this prints out a continuation of the above opening. Each time it runs, there is something slightly different. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\"Once upon a time, there was a horse.","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"But this particular horse was too big to be put into a normal stall. Instead, the animal was moved into an indoor pasture, where it could take a few hours at a time out of the stall. The problem was that this pasture was so roomy that the horse would often get a little bored being stuck inside. The pasture also didn’t have a roof, and so it was a good place for snow to accumulate.\"","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is certainly an interesting direction and story … but now we want to set it in the medieval era. What can we do? ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Fine Tuning Your LLM","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now that we’ve shown how to serve a model, how do we fine-tune it to be more medieval? What about if we train it on 2500 lines from Shakespeare? ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is where ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.deepspeed.ai/"},"content":[{"nodeType":"text","value":"DeepSpeed","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" comes in. DeepSpeed is a set of optimized algorithms for training and fine-tuning networks. The problem is that DeepSpeed doesn’t have an orchestration layer. This is not so much of a problem on a single machine, but if you want to use multiple machines, this typically involves a bunch of bespoke ssh commands, complex managed keys, and so on. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is where Ray can help. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html"},"content":[{"nodeType":"text","value":"page","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" in the Ray documentation discusses how to fine-tune it to sound more like something from the 15th century with a bit of flair. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Let’s go through the key parts. First, we load the data from hugging face","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12gknw4CuwTUwzyVwR5MEH","type":"Entry","createdAt":"2023-04-07T18:42:15.242Z","updatedAt":"2023-04-07T18:47:51.606Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hf-load-data","body":"from datasets import load_dataset\nprint(\"Loading tiny_shakespeare dataset\")\ncurrent_dataset = load_dataset(\"tiny_shakespeare\")\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Skipping the tokenization code, here’s the heart of the code that we will run for each worker. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5633wVi5PVmd43ciiJRLrv","type":"Entry","createdAt":"2023-04-07T18:43:13.431Z","updatedAt":"2023-04-07T18:48:00.199Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hf-trainer","body":"def trainer_init_per_worker(train_dataset, eval_dataset=None,**config):\n    # Use the actual number of CPUs assigned by Ray\n    model = GPTJForCausalLM.from_pretrained(model_name, use_cache=False)\n    model.resize_token_embeddings(len(tokenizer))\n    enable_progress_bar()\n    metric = evaluate.load(\"accuracy\")\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=compute_metrics,\n        tokenizer=tokenizer,\n        data_collator=default_data_collator,\n    )\n    return trainer\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\nAnd now we create a Ray AIR HuggingFaceTrainer that orchestrates the distributed run and wraps around multiple copies of the training loop above: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"Qo7c20UkHRTIAgy2zvQ4G","type":"Entry","createdAt":"2023-04-07T18:43:58.073Z","updatedAt":"2023-04-07T18:48:09.302Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hf-trainer2","body":"trainer = HuggingFaceTrainer(\n    trainer_init_per_worker=trainer_init_per_worker,\n    trainer_init_config={\n        \"batch_size\":16,  # per device\n        \"epochs\":1,\n    },\n    scaling_config=ScalingConfig(\n        num_workers=num_workers,\n        use_gpu=use_gpu,\n        resources_per_worker={\"GPU\":1,\"CPU\": cpus_per_worker},\n    ),\n    datasets={\"train\": ray_datasets[\"train\"],\"evaluation\": ray_datasets[\"validation\"]},\n    preprocessor=Chain(splitter, tokenizer),\n)\nresults = trainer.fit()\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"While there is some complexity here, it is not much more complex than the code to get it to run on a simple machine. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Fine-tuning and Performance","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"One of the most important topics related to LLMs is the question of cost. In this particular case, the costs are small (in part because we ran only one epoch of fine-tuning, depending on the problem 1-10 epochs of fine-tuning are used, and also in part because this dataset is not so large). But running the tests on different configurations shows us that understanding performance is not always easy. The below shows some benchmarking results with different configurations of machines on AWS. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3k8l58AKMayxIzIhrO7Btr","type":"Entry","createdAt":"2023-04-07T14:39:44.461Z","updatedAt":"2023-04-07T14:39:44.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"fine-tune-performance","body":"\n| Configuration| #instances| Time (mins)| Total Cost (on-demand)|Total Cost (spot)| Cost Ratio|\n| ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n| 16 x g4dn.4xlarge (1 x T4 16GB GPU)|16|48|$15.41|__$6.17__|100%|\n| 32 x g4dn.4xlarge (1 x T4 16GB GPU)|32|__30__|$19.26|$7.71|125%|\n| 1 x p3.16xlarge (8 x V100 16GB GPU)|1|44|$17.95|$9.27|150%|\n| 1 x g5.48xlarge (8 x A10G 24GB GPU)|1|84|$22.81|$10.98|178%|\n"}}},"content":[]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3C7MbWRZ8oYJoE00IW4mIi","type":"Asset","createdAt":"2023-04-11T16:45:53.578Z","updatedAt":"2023-04-11T16:45:53.578Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"llm-graph","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3C7MbWRZ8oYJoE00IW4mIi/93f686b238a84d2ef64abe3aa7670791/Screenshot_2023-04-11_at_12.44.46_PM.png","details":{"size":76511,"image":{"width":1047,"height":644}},"fileName":"Screenshot 2023-04-11 at 12.44.46 PM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Note:","marks":[{"type":"underline"}],"data":{}},{"nodeType":"text","value":" we tried to run the same test with A100s, but we were unable to obtain the p4d machines to do so. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Looking at these numbers, we see some surprises: ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Perhaps the most obvious machine to use – the g5.48xlarge – the machine with the highest on-paper performance – is both the most expensive and the slowest at almost twice the price when using spot instances. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The p3.16xlarge with its use of NVLink between the GPUs is a considerably better option. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Most surprising of all, using multiple machines is both the ","marks":[],"data":{}},{"nodeType":"text","value":"cheapest ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"and the ","marks":[],"data":{}},{"nodeType":"text","value":"fastest ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"option. ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The exact same code is running on all the machines, and aside from tweaking the number of GPU workers, nothing else was changed. Using multiple machines gave us the cheapest (16 machines) and the fastest (32 machines) option of the ones we benchmarked. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is the beauty and power of Ray. The code itself was simple enough, and in fact, was able to use a standard library –  DeepSpeed – with no modifications. So it was no more complex in this case than a single machine. Simultaneously, it gave more options and flexibility to optimize to be both cheaper and faster than a single machine.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Closing the loop: Serving the fine-tuned model","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now that we have a fine-tuned model, let’s try to serve it. The only change we need to make is to (a) copy the model to s3 from the fine-tuning process and (b) load it from there. In other words, the only change from the previous code we started with originally is: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"202WfToEkr4LafSdvnynbx","type":"Entry","createdAt":"2023-04-07T18:45:09.318Z","updatedAt":"2023-04-07T18:48:20.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"load-model","body":"        checkpoint = Checkpoint.from_uri(\n             \"s3://demo-pretrained-model/gpt-j-6b-shakespeare\"\n        )\n        with checkpoint.as_directory() as dir:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                dir,\n                torch_dtype=torch.float16,\n                low_cpu_mem_usage=True,\n                device_map=\"auto\")\n            self.tokenizer = AutoTokenizer.from_pretrained(dir)\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"And now let’s try querying it again: ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Once upon a time there was a horse. This horse was in my youth, a little unruly, but yet the best of all. I have, sir; I know every horse in the field, and the best that I have known is the dead. And now I thank the gods, and take my leave.","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"As you can see, it definitely has more of a Shakespearean flavor. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Conclusion","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We have shown a new tech stack that combines Ray, HuggingFace, DeepSpeed, and PyTorch to make a system that: ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Makes it simple and quick to deploy as a service. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Can be used to cost-effectively fine-tune and is actually most cost-effective when using multiple machines without the complexity. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"How fine-tuning – even a single epoch – can change the output of a trained model. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Deploying a fine-tuned model is only marginally harder than deploying a standard one. ","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Next Steps","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If you want to use LangChain + Ray to serve LLM's, see our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"nodeType":"text","value":"LangChain blog series","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in learning more about Ray, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io"},"content":[{"nodeType":"text","value":"Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://docs.ray.io"},"content":[{"nodeType":"text","value":"Docs.Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"To connect with the Ray community join #LLM on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"Ray Slack","marks":[],"data":{}}]},{"nodeType":"text","value":" or our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"Discuss forum","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in our Ray hosted service for ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/model-serving"},"content":[{"nodeType":"text","value":"ML Training and Serving","marks":[],"data":{}}]},{"nodeType":"text","value":", see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://www.anyscale.com/platform"},"content":[{"nodeType":"text","value":"Anyscale.com/Platform","marks":[],"data":{}}]},{"nodeType":"text","value":" and click the 'Try it now' button","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Summit 2023:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"nodeType":"text","value":"Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"nodeType":"text","value":"practical training focused on LLMs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"19zr72hDLSKFt8vQMz3hb6","type":"Asset","createdAt":"2023-04-11T00:38:37.097Z","updatedAt":"2023-04-11T00:38:37.097Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"fine-tune-stack","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/19zr72hDLSKFt8vQMz3hb6/fd9f6b83a9fe5b66456ae54ecf9bb04d/fine-tune-stack.png","details":{"size":344489,"image":{"width":1716,"height":1180}},"fileName":"fine-tune-stack.png","contentType":"image/png"}}}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70ZthWUkgA42DqmZ1GVmuM","type":"Entry","createdAt":"2022-06-15T16:41:59.066Z","updatedAt":"2022-06-15T16:43:47.038Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"blog-types-tags","body":"This section is used to order the \"Types\" and \"Tags\" that show up for filters on the Blog Index","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"56lORqEsxxZXgpuGzAhJBC","type":"Entry","createdAt":"2022-06-15T16:42:23.797Z","updatedAt":"2022-06-15T16:44:24.730Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"list"}},"locale":"en-US"},"fields":{"title":"Types","identifier":"blog-type-filters","items":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42F9iHs5a05rRrOuqEQwvB","type":"Entry","createdAt":"2022-06-15T01:30:47.322Z","updatedAt":"2022-06-15T01:30:47.322Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Culture","slug":"culture","tintColor":"#39C8E5"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KeH0JzrIEpuheazhMAhKf","type":"Entry","createdAt":"2022-06-15T01:31:23.950Z","updatedAt":"2022-06-15T01:31:23.950Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"User Story","slug":"user-story","tintColor":"#6D59C6"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1fDHWgr5HgjPURy6aaDlnB","type":"Entry","createdAt":"2022-06-15T16:42:41.243Z","updatedAt":"2022-06-22T15:37:31.744Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"list"}},"locale":"en-US"},"fields":{"title":"Products / Libraries","identifier":"blog-tag-filters","items":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i6fp6Gfdy5wFeDDOaKcfh","type":"Entry","createdAt":"2021-12-03T22:33:42.248Z","updatedAt":"2021-12-03T22:33:42.248Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Anyscale","identifier":"anyscale"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}}]}}]}}],"recommendations":[],"articles":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3vUo7K4aHmhARGamZX7Ujj","type":"Entry","createdAt":"2023-05-11T15:17:24.618Z","updatedAt":"2023-06-16T21:13:49.520Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Streaming distributed execution across CPUs and GPUs","seoTitle":"Streaming distributed execution across CPUs and GPUs","slug":"streaming-distributed-execution-across-cpus-and-gpus","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2aJRcdEj9iAktAnqn0wucE","type":"Entry","createdAt":"2021-02-16T07:30:38.421Z","updatedAt":"2021-02-16T07:30:38.421Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Stephanie Wang","slug":"stephanie-wang","link":"https://github.com/stephanie-wang"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2edothjNBpyQEu2aX55KRl","type":"Entry","createdAt":"2023-05-10T15:30:16.699Z","updatedAt":"2023-05-10T15:30:16.699Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Cheng Su","slug":"cheng-su"}}],"publishedDate":"2023-05-11","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"In a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/offline-batch-inference-comparing-ray-apache-spark-and-sagemaker"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"previous blog,","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" we showed how Ray Data can provide speedups over SageMaker and frameworks like Apache Spark for large-scale batch inference workloads. This blog post delves further into how Ray Data streaming works and how to use it for your own ML pipelines.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Some of the most demanding machine learning (ML) use cases we have encountered involve pipelines that span both CPU and GPU devices in distributed environments. These situations arise in various workloads, including:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Batch inference, which involves a CPU-intensive preprocessing stage (e.g., video decoding or image resizing) before utilizing a GPU-intensive model to make predictions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed training, where similar CPU-heavy transformations are required to prepare or augment the dataset prior to GPU training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Interestingly, in many of these workloads, the preprocessing steps often prove to be the bottleneck. This can happen when preprocessing requires ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"parallelism across multiple nodes","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"significant memory to buffer results","nodeType":"text"},{"data":{},"marks":[],"value":" between the CPU and GPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For instance, consider the decoding of compressed images in memory. A typical JPEG decompression ratio is 10:1, which implies that the output memory size may be ten times that of the input, resulting in substantial memory pressure in addition to CPU load. The challenges become more complex with other data modalities, such as video; for example, H264 can decompress at a 2000:1 ratio, producing 200GB of frame outputs for a 100MB file input. This means that practitioners trying to offload CPU-heavy preprocessing onto multiple CPUs or machines have to deal with intermediate results that are far larger than their already sizable source datasets:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7ntuAY4zsUXtrtIxSofnyg","type":"Asset","createdAt":"2023-05-10T15:33:44.406Z","updatedAt":"2023-05-10T18:55:03.562Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"streaming_figure_1","description":"Figure 1. Decoding of compressed images with large images spilled to disk","file":{"url":"//images.ctfassets.net/xjan103pcp94/7ntuAY4zsUXtrtIxSofnyg/1fdb9631162d6b66777d205d1d3a7e1c/Screenshot_from_2023-05-10_11-00-43.png","details":{"size":64895,"image":{"width":1493,"height":436}},"fileName":"Screenshot from 2023-05-10 11-00-43.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Executed naively, these compute and memory-intensive workloads fail to fully utilize expensive GPU hardware. For example, we could buffer 200GB of frame outputs on disk, but that could add minutes of overheads. Even if this is overlapped with GPU computation, we can end up with long pauses on the GPU. This motivates fully streamed execution in the cluster setting, which avoids such delays by streaming intermediate data through cluster memory:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"40VpN8OKxEM7wHohV802gE","type":"Asset","createdAt":"2023-05-10T15:36:35.326Z","updatedAt":"2023-05-10T18:56:06.767Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_2","description":"Figure 2. Decoding intermediate data streamed through memory","file":{"url":"//images.ctfassets.net/xjan103pcp94/40VpN8OKxEM7wHohV802gE/c07086bfe677dc5e3109496f5a4ec083/Screenshot_from_2023-05-10_11-01-00.png","details":{"size":35974,"image":{"width":1199,"height":268}},"fileName":"Screenshot from 2023-05-10 11-01-00.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we guide you through our development of a versatile streaming backend for Ray Data, and show examples of how to implement the demanding use cases mentioned earlier. We discuss the performance benefits of implementing pipelined execution across CPUs and GPUs compared to bulk processing, and discuss its broader applicability to various workloads, including distributed training.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pipelined execution in a cluster","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To understand the necessity and advantages of pipelined (or streaming) execution across CPU and GPU devices, let's first examine how a ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Bulk_synchronous_parallel"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"bulk synchronous parallel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (BSP) framework might handle batch inference. Because of its simplicity and generality, BSP is a common way frameworks (e.g., MapReduce, Apache Spark) parallelize distributed computations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Note that pipelining within a single machine is already commonly used in data loading libraries such as ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Torch DataLoader","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://petastorm.readthedocs.io/en/latest/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Petastorm.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" Here, we discuss pipelining computations across an entire cluster of machines.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical batch inference job consists of the following operations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6mkS4HlXNovPW6Dl387jYe","type":"Asset","createdAt":"2023-05-10T15:40:10.724Z","updatedAt":"2023-05-10T19:28:57.091Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"locale":"en-US"},"fields":{"title":"streaming_figure_3","description":"Figure 3. A typical batch inference job sequential operations","file":{"url":"//images.ctfassets.net/xjan103pcp94/6mkS4HlXNovPW6Dl387jYe/1a707558e51ac00b776ae6f5d808b74e/Screenshot_from_2023-05-10_12-23-58.png","details":{"size":33844,"image":{"width":1526,"height":244}},"fileName":"Screenshot from 2023-05-10 12-23-58.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To implement such an inference job, you'd write the code using Ray Data (or another similar framework):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1kYVOacE6epR4czS5YoOHX","type":"Entry","createdAt":"2023-05-10T15:41:24.239Z","updatedAt":"2023-05-10T15:41:24.239Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_1","body":"import ray\n\n# Define model and preprocessor.\nmodel = ...\npreprocess_fn = ...\n\n# Define the inference function.\ndef model_fn(batch: Dict[str, np.ndarray]):\n    return {\"results\": model(batch)}\n\n# The following snippet implements the pipeline.\nray.data.read_parquet(...) \\           # 1. Load\n    .map_batches(preprocess_fn) \\      # 2. Preprocess\n    .map_batches(model_fn) \\           # 3. Inference\n    .write_parquet(...)                # 4. Save\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a BSP framework, each of these operations can be modeled as a 1:1 transformation, which can be fused together into a single stage. Each task within the stage will execute the mentioned operations locally in sequence:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4oQ40RlSK1AF7Y4ZnVGV8J","type":"Asset","createdAt":"2023-05-10T15:44:10.631Z","updatedAt":"2023-05-10T18:59:16.654Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_4","description":"Figure 4. BSP framework operations modeled as a 1:1 transformation","file":{"url":"//images.ctfassets.net/xjan103pcp94/4oQ40RlSK1AF7Y4ZnVGV8J/a0413e90a55ec69d7d0ba49554a7f2a0/Screenshot_from_2023-05-10_11-07-39.png","details":{"size":103326,"image":{"width":1511,"height":583}},"fileName":"Screenshot from 2023-05-10 11-07-39.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This execution strategy is ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"memory optimal","nodeType":"text"},{"data":{},"marks":[],"value":", as there is no intermediate data and execution is inherently pipelined (since there is only a single stage). However, challenges emerge in a heterogeneous setting, where certain pipeline steps may prefer to run on CPUs or GPUs independently. In Ray Data, such inference pipelines are expressed with transformations that launch actors scheduled onto GPUs ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"(num_gpus=1 and compute=ActorPoolStrategy)","nodeType":"text"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3nLf8c4wMq9KInsI4ca7HB","type":"Entry","createdAt":"2023-05-10T15:46:05.506Z","updatedAt":"2023-05-10T16:00:09.475Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_2","body":"import ray\nfrom ray.data import ActorPoolStrategy\n\nModel = ...\npreprocess_fn = ...\n\n# Define the model as a stateful class with cached setup.\nclass MyModelCallableCls:\n    def __init__(self):\n        self.model = Model(...)\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e\n        return {\"results\": self.model(batch)}\n\n# Modify the pipeline to use GPU actors.\nray.data.read_parquet(...) \\\n    .map_batches(preprocess_fn) \\\n    .map_batches(\n          MyModelCallableCls,\n          num_gpus=1,\n          compute=ActorPoolStrategy(size=N)) \\\n    .write_parquet(...)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the above code snippet, the load and preprocessing steps run on CPU (Stage 1), inference runs on GPUs (Stage 2), and then the result saving runs on CPU again (Stage 3). This configuration leads to spilling of data to remote storage when intermediate results (e.g., decoded video frames) exceed cluster memory sizes. Hence, we see that ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"BSP is not memory optimal for heterogeneous workloads","nodeType":"text"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"66FIu8nzl2kZRB1Ht5xZFL","type":"Asset","createdAt":"2023-05-10T15:49:26.155Z","updatedAt":"2023-05-10T19:00:24.815Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_5","description":"Figure 5. BSP is not memory optimal for heterogeneous workloads","file":{"url":"//images.ctfassets.net/xjan103pcp94/66FIu8nzl2kZRB1Ht5xZFL/295ab9176fab5d04ee47ac53c5e1b418/Screenshot_from_2023-05-10_11-02-18.png","details":{"size":162611,"image":{"width":1750,"height":730}},"fileName":"Screenshot from 2023-05-10 11-02-18.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"These unnecessary overheads can be avoided with end-to-end pipelining (i.e., streaming) across the cluster, which we present in the next section.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data streaming","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In Ray 2.4, we've changed the default execution strategy of Ray Data to streaming from bulk synchronous. These streaming Datasets are fully backwards compatible with the existing API, i.e., they can be transformed lazily with map operations, support shuffle operations, and also caching / materialization in memory:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5CSJWRWgN6pQwxcFMBhMSr","type":"Entry","createdAt":"2023-05-10T15:50:53.654Z","updatedAt":"2023-05-10T15:59:44.870Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_3","body":"# Create a dataset over parquet files\nds: ray.data.Dataset = ray.data.read_parquet(...)\n\n# Transform the dataset\nds = ds.map_batches(my_preprocess_fn)\nds = ds.map_batches(my_model_fn)\n\n# Iterate over dataset batches in streaming fashion\nfor batch in ds.iter_batches():\n   print(batch)\n\n# Materialize all contents of the dataset in memory\nds = ds.materialize()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In other words, the Ray Data API now leverages streaming execution for improved performance on large datasets, with the same simple transformation API as in previous Ray versions.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Video processing example","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To more fully understand the capabilities of this API, let's walk through what this looks like in an example video processing pipeline. In the pipeline, we'll first decode video frames, annotate each frame with an ML model, apply a classification model to the annotated frames, and then save the results to JSON:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"s1kmF3kQraHomGuDsvOQt","type":"Asset","createdAt":"2023-05-10T15:55:25.682Z","updatedAt":"2023-05-10T19:30:26.377Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"streaming_figure_6","description":"Figure 6. A pipeline decoding video frames, annotating frames, applying classification, and saving results to JSON.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/s1kmF3kQraHomGuDsvOQt/9385b71522afe76eebc6342f54387ffa/Screenshot_from_2023-05-10_12-24-36.png","details":{"size":38452,"image":{"width":1478,"height":230}},"fileName":"Screenshot from 2023-05-10 12-24-36.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's start by declaring the components of our pipeline. For brevity, we'll just provide stub classes here, but you can find the ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/ericl/ac086f0a9bdf41b6101dca665045dfec"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"full skeleton here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (and the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference docs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") that can be adapted to your own workloads. Data between these steps is kept in dicts of numpy arrays:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3DoR7DQ01JPe4k0MjPdubR","type":"Entry","createdAt":"2023-05-10T15:57:40.210Z","updatedAt":"2023-05-10T15:59:24.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_5","body":"# Declare a function that decodes frames.\ndef decode_frames(batch: Dict[str, np.ndarray]) -\u003e Dict:\n    video_data = batch[\"bytes\"]\n    ...\n    return {\"frames\": decoded_frames}\n\n# Declare a model that annotates decoded frame data.\nclass FrameAnnotator:\n    def __init__(self):\n        self.model = ...\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e Dict:\n        frames = batch[\"frames\"]\n        return {\"annotated_frames\": self.model(frames)}\n\n# Declare a model that classifies annotated frame data.\nclass FrameClassifier:\n    def __init__(self):\n        self.model = ...\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e Dict:\n        frames = batch[\"annotated_frames\"]\n        return {\"results\": self.model(frames)}","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next, we'll use Ray Data to connect these steps together. This takes just a few lines of code, and we can customize the resource requirements per step to configure CPUs and GPUs:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1U2wSU4PEADDsQpVQqJJAa","type":"Entry","createdAt":"2023-05-10T15:59:06.340Z","updatedAt":"2023-05-10T15:59:06.340Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_6","body":"# Create a dataset from video binary data.\nds = ray.data.read_binary(...)\n\n# Apply the decode step. We can customize the resources per\n# task. Here each decode task requests 4 CPUs from Ray.\nds = ds.map_batches(decode_frames, num_cpus=4)\n\n# Apply the annotation step, using an actor pool of size 5.\n# Each actor runs on a CPU.\nds = ds.map_batches(\n    FrameAnnotator,\n    compute=ActorPoolStrategy(size=5))\n\n# Apply the classification step, using a pool of 2 GPU actors,\n# and a fixed data batch size of 64 for the actor calls.\nds = ds.map_batches(\n    FrameClassifier,\n    num_gpus=1,\n    batch_size=64,\n    compute=ActorPoolStrategy(size=2))\n\n# Trigger execution and write outputs to json.\nds.write_json(\"/tmp/output\")\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The configured streaming topology from above looks like this logically. Since each step has a unique resource requirement / task vs actor strategy, here every step becomes its own stage (Ray Data fuses steps with equivalent resource requirements):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2XaeSiV04QKLILBLAlD0t5","type":"Asset","createdAt":"2023-05-11T15:44:03.608Z","updatedAt":"2023-05-11T15:44:03.608Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"streaming_figure_7","description":"Figure 7: Streaming execution across CPUs and GPUs","file":{"url":"//images.ctfassets.net/xjan103pcp94/2XaeSiV04QKLILBLAlD0t5/d75fceb54323c7587312028b40a4b37c/Screenshot_from_2023-05-10_11-02-51.png","details":{"size":68124,"image":{"width":1445,"height":299}},"fileName":"Screenshot from 2023-05-10 11-02-51.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We run the above script (","nodeType":"text"},{"data":{"uri":"https://gist.github.com/ericl/ac086f0a9bdf41b6101dca665045dfec"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GitHub gist","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") in an Anyscale workspace with 2 GPU nodes and 5 CPU nodes (you can use the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/getting-started.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray cluster launcher","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to also launch a cluster for this):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3hsRFIiW2RGkFM5Igr2jVT","type":"Entry","createdAt":"2023-05-10T16:04:25.585Z","updatedAt":"2023-05-10T16:06:19.918Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_7","body":"$ python workload.py\n\n2023-05-02 15:10:01,105 INFO streaming_executor.py:91 -- Executing DAG\nInputDataBuffer[Input] -\u003e TaskPoolMapOperator[MapBatches(decode_frames)] -\u003e \nActorPoolMapOperator[MapBatches(FrameAnnotator)] -\u003e\nActorPoolMapOperator[MapBatches(FrameClassifier)] -\u003e \nTaskPoolMapOperator[Write]\n2023-05-02 15:10:01,128 INFO actor_pool_map_operator.py:114 -- MapBatches(FrameAnnotator): Waiting for 5 pool actors to start...\n2023-05-02 15:10:02,691 INFO actor_pool_map_operator.py:114 -- MapBatches(FrameClassifier): Waiting for 2 pool actors to start...\n\nRunning: 25.0/112.0 CPU, 2.0/2.0 GPU, 33.19 GiB/32.27 GiB object_store_memory:  28%|███▍        | 285/1000 [01:40\u003c03:12,  3.71it/s]","language":"Shell"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"While the workload is running, we can view the observability graphs in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-core/ray-dashboard.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Dashboard","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". We can see from the actor metrics that our 7 worker actors were active for the entire run (RUNNING_TASK), and that Ray Data kept them busy with MapBatches(FrameClassifier) as well as MapBatches(FrameAnnotator) tasks. Because of the streaming execution strategy, tasks for all stages run concurrently:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5MDgPW8470uVoy6MFBqowq","type":"Asset","createdAt":"2023-05-10T16:09:18.740Z","updatedAt":"2023-05-10T16:09:27.512Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_8","description":"Figure 8. View active tasks and actors for this job in the Ray Dashboard","file":{"url":"//images.ctfassets.net/xjan103pcp94/5MDgPW8470uVoy6MFBqowq/39f6300387d895145313c0caee8149cf/image3.png","details":{"size":77415,"image":{"width":1216,"height":516}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can also inspect hardware utilization in the Ray dashboard. We'll focus on the network activity. We can see two nodes receiving ~500MiB/s (these are the two GPU nodes hosting the FrameClassifier actors), and a number of other nodes driving significant traffic from 100-200MiB/s, likely sending data to the GPU nodes or between the decode and FrameAnnotator steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6QEOgHncDDrSl3SKl9YzWF","type":"Asset","createdAt":"2023-05-10T16:12:30.030Z","updatedAt":"2023-05-10T19:54:24.559Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"streaming_figure_9","description":"Figure 9. Ray Dashboard showing network activity","file":{"url":"//images.ctfassets.net/xjan103pcp94/6QEOgHncDDrSl3SKl9YzWF/bae6e33fce2749f2ade8afd73681e16b/Screenshot_from_2023-05-10_12-42-27.png","details":{"size":107840,"image":{"width":1635,"height":707}},"fileName":"Screenshot from 2023-05-10 12-42-27.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The overall pipeline finishes in ~5 minutes, successfully processing ~350GB of video frame data total on a heterogeneous cluster of CPUs and GPUs. We see that Ray Data was able to pipeline the entire execution across the cluster to make the best use of memory.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Discussion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Optimizations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"In order to seamlessly execute such streaming topologies, Ray Data provides a number of optimizations under the hood. This includes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Memory stability","nodeType":"text"},{"data":{},"marks":[],"value":": Ray Data relies on the underlying Ray scheduler to schedule tasks and actors, but still needs to manage back-pressure across the streaming topology to bound memory usage and avoid object store spilling. It does this by only scheduling new tasks if it would keep the streaming execution under ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/data-internals.html#streaming-execution"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"configured resources limits","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Intuitively, enforcing a cap on intermediate result memory usage is needed to avoid degrading to bulk execution.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data locality","nodeType":"text"},{"data":{},"marks":[],"value":": While Ray will already place tasks on nodes where their input arguments are local, Ray Data's streaming backend extends this to optimize the scheduling of actor tasks. For example, in the above example, a lot of network traffic is avoided between the `decode_frames` and `FrameAnnotator` steps by routing decoded frames to actors that are on the same node.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Fault tolerance","nodeType":"text"},{"data":{},"marks":[],"value":": Ray Data leverages Ray's built-in ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-core/fault-tolerance.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"fault tolerance","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in order to handle object loss in large jobs. When objects are lost, they are recomputed based on their task lineage tracked by Ray. If actors were needed to produce these objects, Ray restarts these actors prior to re-submitting the task.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scalability","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"To give an idea of the upper scalability envelope of Ray Data streaming, we ran a number of synthetic benchmarks that stressed Ray's object manager and Ray Data's ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/execution/streaming_executor.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"streaming executor","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" processing a 20TiB array dataset. Under the hood, Ray Data is using ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.remote() / ray.wait() ","nodeType":"text"},{"data":{},"marks":[],"value":"to orchestrate the streaming topology, which means it is similarly scalable as Ray itself:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2vKUP0VZYl0yPNspQmfU05","type":"Asset","createdAt":"2023-05-10T16:17:37.701Z","updatedAt":"2023-05-10T16:17:37.701Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"streaming_figure_10","description":"Figure 10. Synthetic benchmarks stressing Ray's object manager and Ray Data's streaming executor processing a 20TiB array dataset","file":{"url":"//images.ctfassets.net/xjan103pcp94/2vKUP0VZYl0yPNspQmfU05/0566c92beb5ea743d9ecc3404a551705/image9.png","details":{"size":44857,"image":{"width":1636,"height":640}},"fileName":"image9.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"These benchmarks were run on a cluster of 500 machines, and test pipelines with between one to four stages similar to the video streaming example. We see that for the simplest pipeline on this cluster, Ray Data can process input data at a rate exceeding 1TiB/s. For more complex multi-stage pipelines, Ray Data can still sustain about 100-200GiB/s of end-to-end throughput on a large cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed Training","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"A significant portion of ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" users today are also using Ray Data as a distributed data backend for ML training. Ray Data streaming allows these users to efficiently work with datasets that are larger than cluster memory for training, leveraging both CPU and GPU nodes to speed up their jobs. We are working on enhancing Ray Train's API to natively work with Ray Data streaming.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What's next?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Today, early users are taking advantage of the Ray Data streaming backend to create efficient large-scale inference pipelines over unstructured data, including video and audio data. To learn more about how to build your own pipeline, check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference tutorial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in our docs. We'll also be following up with a blog on more unstructured application examples. To learn more generally about streaming in Ray Data, check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/data.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"library docs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and let us know what ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/issues"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"improvements","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" you'd like to see to Ray Data.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Also, we will be presenting at the ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Register now","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" — early bird registration is open until","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" June 30, 2023.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5qTVayH4YnKM7pGL8CiSwc","type":"Asset","createdAt":"2023-05-11T15:46:46.691Z","updatedAt":"2023-05-11T15:46:46.691Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_streaming_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5qTVayH4YnKM7pGL8CiSwc/99806986301b595308c6c89a8bad6249/images_and_diagrams__13_.png","details":{"size":83586,"image":{"width":960,"height":540}},"fileName":"images and diagrams (13).png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2C7QtMnASHfVdag9xYamXh","type":"Entry","createdAt":"2023-05-04T15:03:36.052Z","updatedAt":"2023-06-06T14:20:09.992Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker","seoTitle":"Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker","slug":"offline-batch-inference-comparing-ray-apache-spark-and-sagemaker","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DwU0wh6AmJoGv8JOy6n0Q","type":"Entry","createdAt":"2020-11-11T23:34:15.165Z","updatedAt":"2021-02-05T22:22:08.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","slug":"amog-kamsetty","link":"https://www.linkedin.com/in/amogkamsetty/","bio":"Amog is a software engineer at Anyscale. He helps maintain and develop two machine learning libraries: Ray Tune for distributed hyperparameter tuning and RaySGD for distributed deep learning training.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J134jX2ybZRLPfcAIGLTI","type":"Asset","createdAt":"2020-11-13T00:05:05.533Z","updatedAt":"2020-11-13T00:05:05.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J134jX2ybZRLPfcAIGLTI/12589bb0c6792051050ebc6e1e83af25/Amog_Kamsetty_.jpg","details":{"size":57660,"image":{"width":480,"height":640}},"fileName":"Amog Kamsetty .jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}}],"publishedDate":"2023-05-04","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Update 5/10: ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"Thanks to the Databricks Apache Spark developers for pointing out the prefetching flag ","nodeType":"text"},{"data":{},"marks":[{"type":"code"},{"type":"italic"}],"value":"spark.databricks.execution.pandasUDF.prefetch.maxBatches ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"available in Databricks Runtime that can be used with the Iterator API. It is also possible to implement this manually in Spark open source with background threads. With prefetching set to 4 batches, Spark reaches ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"},{"type":"italic"}],"value":"159.86 images/sec","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":". The code can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/blob/main/spark/code/torch-batch-inference-s3-10G-standard-iterator-databricks-prefetch.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"italic"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"TL;DR","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As more companies use large scale machine learning (ML) models for training and evaluation, offline batch inference becomes an essential workload. A number of challenges come with it: managing compute infrastructure; optimizing use of all heterogeneous resources; and transferring data from storage to hardware accelerators. Addressing these challenges, Ray performs significantly better as it can coordinate clusters of diverse resources, allowing for better utilization of the specific resource requirements of the workload.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we conduct a comparison of three different solutions for offline batch inference: AWS ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker Batch Transform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Our experiments demonstrate that Ray Data achieves speeds up to ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"17x ","nodeType":"text"},{"data":{},"marks":[],"value":"faster than SageMaker Batch Transform and","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" 2x","nodeType":"text"},{"data":{},"marks":[],"value":" faster than Spark for offline image classification. Ray Data also scales effectively to terabyte-sized datasets. The code for these benchmarks is publicly available and can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introduction","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46bNG3QTa1atN02rsemfe8","type":"Asset","createdAt":"2023-05-03T22:20:44.214Z","updatedAt":"2023-05-03T22:20:44.214Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_1_batch_offline_inference","description":"Figure 1. General parallel batch processing architecture\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/46bNG3QTa1atN02rsemfe8/120bf3aac26ae94257b41ac674f42e94/image3.png","details":{"size":48540,"image":{"width":960,"height":540}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Offline batch inference is a critical workload for many AI products, especially with the growing usage of pre-trained foundation models. At its core, it seems like a simple problem: given a trained model and a dataset, get model predictions for each data point. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, there are many challenges to doing this at scale:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 1: ","nodeType":"text"},{"data":{},"marks":[],"value":"Managing compute infrastructure and cloud clusters, especially when needing to use ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"heterogeneous clusters","nodeType":"text"},{"data":{},"marks":[],"value":", consisting of different instance types to maximize throughput.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 2: ","nodeType":"text"},{"data":{},"marks":[],"value":"Parallelizing data processing and utilizing all resources (CPUs \u0026 GPUs) in the cluster at any given point in time.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 3: ","nodeType":"text"},{"data":{},"marks":[],"value":"Efficiently transferring data between cloud storage, CPU RAM for preprocessing, and GPU RAM for model inference, especially for unstructured data like images and text. This has ramifications for both performance and developer velocity.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 4:","nodeType":"text"},{"data":{},"marks":[],"value":" A user experience that makes it easy to iterate and develop while working at scale.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What does the industry recommend for offline batch inference?","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The industry hasn’t converged on a standard solution. ","nodeType":"text"},{"data":{"uri":"https://twitter.com/benhamner/status/1605730034450169857"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"It seems like every ML practitioner has their own favorite approach.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5hRMwuEGUS4nJ7eV5riIK9","type":"Asset","createdAt":"2023-05-03T22:23:14.590Z","updatedAt":"2023-05-03T22:23:14.590Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"tweet_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5hRMwuEGUS4nJ7eV5riIK9/05b76abcbe4ea58ae3839fd319858594/image4.png","details":{"size":177304,"image":{"width":1798,"height":588}},"fileName":"image4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We see that there are three categories of solutions that attempt to address the above challenges. However, only category three addresses all the challenges.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Batch Services: ","nodeType":"text"},{"data":{},"marks":[],"value":"Cloud providers such as AWS, GCP, and Azure provide batch services to manage compute infrastructure for you. Some newer products like ","nodeType":"text"},{"data":{"uri":"https://modal.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Modal Labs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provide a user experience layer around cloud providers to abstract away even more complexity. Regardless of which service you choose, the process is the same: you provide your code, and the service runs your code on each node in a cluster. However, while infrastructure management is necessary (Challenge #1), it is not enough. These services have limitations, such as a lack of software libraries to address Challenges #2, #3, and #4 which makes them suitable only for experienced users who can write their own optimized batch inference code.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Online Inference solutions: ","nodeType":"text"},{"data":{},"marks":[],"value":" Solutions like ","nodeType":"text"},{"data":{"uri":"https://www.bentoml.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Bento ML","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provide APIs to make it easy to write performant inference code and can abstract away infrastructure complexities. But they are designed for online inference rather than offline batch inference—two different problems with different sets of requirements. These solutions often don’t perform well in the offline case, leading inference service providers like ","nodeType":"text"},{"data":{"uri":"https://modelserving.com/blog/unifying-real-time-and-batch-inference-with-bentoml-and-spark"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Bento ML to integrating with Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for offline inference.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed data systems:","nodeType":"text"},{"data":{},"marks":[],"value":" These solutions are designed to handle large amounts of data and are better suited to handle all three challenges listed above. They break large datasets into reasonable batches for processing and can map a given function (i.e., the model) over the dataset efficiently, effectively handling the challenges of offline batch inference at scale. Examples include ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ultimately, we have observed that Ray Data is the best practical solution for offline batch inference for modern deep learning applications, both in terms of performance and user experience.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We ran the following comparisons and experiments to corroborate the above observations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing AWS SageMaker Batch Transform, Apache Spark, and Ray Data performance and UX on an image classification use case","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling up Ray Data batch inference to 10 TB, showing 90%+ GPU utilization","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Image Classification: SageMaker Batch Transform vs. Apache Spark vs. Ray Data","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We ran an image classification task from the ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1911.02549"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLPerf Inference Benchmark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" suite in the offline setting. This benchmark uses images from the ","nodeType":"text"},{"data":{"uri":"https://image-net.org/challenges/LSVRC/2012/2012-downloads.php#Images"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"ImageNet 2012 Dataset","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and a pre-trained ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch ResNet50 model.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We timed a full end-to-end batch inference workload. With images stored in the parquet format in a public S3 bucket, the workload involved the following three steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Reading images from S3","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Simple CPU preprocessing (resizing, cropping, normalization)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Model inference on GPU","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"While all Ray experiments were run on Anyscale, Spark experiments were run on Databricks runtime v12.0, with Apache Spark 3.3.1. More details on the full configurations can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"in the repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In particular for Spark, we used two different setups:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Single-cluster","nodeType":"text"},{"data":{},"marks":[],"value":": Both the CPU preprocessing and GPU inference were done on the same cluster. We tried both versions of the PySpark Pandas UDF API: ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.pandas_udf.html#pyspark.sql.functions.pandas_udf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Series-based and Iterator-based","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Multi-cluster","nodeType":"text"},{"data":{},"marks":[],"value":": CPU preprocessing is done on one cluster, the preprocessed data is saved to a shared file system, and GPU inference is done on a different cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"All the source code for the experiments conducted above is available ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", including additional details on various other configurations that we attempted.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":" Results of throughput from experiments","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"When running with a 10 GB dataset on a single g4dn.12xlarge instance (48 CPUs, 4 GPUs), we observe the following throughput (images/sec) results for ResNet-50 batch inference:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Urc9cqOTvtI8d4w9DjiEm","type":"Asset","createdAt":"2023-05-04T00:09:19.405Z","updatedAt":"2023-05-04T00:09:19.405Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_5_table","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Urc9cqOTvtI8d4w9DjiEm/42718e584dcc4d0d16592749b0782b4f/Screen_Shot_2023-05-03_at_5.01.12_PM.png","details":{"size":91393,"image":{"width":1211,"height":541}},"fileName":"Screen Shot 2023-05-03 at 5.01.12 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The above throughput differences translate directly into cost savings as the batch inference job with Ray can run much faster—so the faster it finishes the fewer resources are consumed and the cheaper it gets. Additionally, Ray is fully open source.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2dNPqjBTFiDZn2gcXkdir9","type":"Asset","createdAt":"2023-05-03T22:50:03.518Z","updatedAt":"2023-05-03T22:50:03.518Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 1","description":"Figure 1. Throughput difference among three different solutions for 10GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/2dNPqjBTFiDZn2gcXkdir9/f07a675dcb4f124e29aeb2f6618dbf15/image1.png","details":{"size":163062,"image":{"width":1334,"height":948}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"SageMaker Batch Transform for batch inference","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As we can see, SageMaker Batch Transform is not well suited for batch inference, both for throughput and cost.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While SageMaker handles","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" Challenge #1 ","nodeType":"text"},{"data":{},"marks":[],"value":"to some degree and abstracts away compute infrastructure management, it doesn’t fully address the remaining challenges","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The low-throughput results are primarily because while SageMaker Batch Transform claims to be a ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"solution for batch inference on large datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", under the hood, it uses the same architecture as for online serving systems. It starts an HTTP server, and deploys your model as an endpoint. And then for each image, a separate request is sent to the server.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"SageMaker also does not provide support for batching across ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"multiple files","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and even if you batched your data beforehand, the max payload per request is 100 MB, nowhere near enough to fully saturate your GPUs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In terms of developer velocity, when running these benchmarks, we ran into a few additional challenges with SageMaker. In particular, the need to spin up a new cluster every time we ran a script and the difficult to parse stack traces slowed down developer iterations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While online inference solutions have their place, the additional overhead they have, as well as the inability to fully utilize cluster resources and saturate GPUs by maximizing batch sizes and effectively pipelining data make them highly inefficient for offline batch inference.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing Spark and Ray for batch inference","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Clearly, as we observed above, shoehorning online inference solutions for offline use cases is not the right approach. Let’s compare the two distributed data system approaches: Spark and Ray Data. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We see that Ray Data has","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" 2x ","nodeType":"text"},{"data":{},"marks":[],"value":"faster throughput than Spark. Let’s break down how Ray Data and Spark fare for the challenges mentioned earlier:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #2: Hybrid CPU and GPU workload","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deep learning workloads involve ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"both CPU and GPU computation","nodeType":"text"},{"data":{},"marks":[],"value":". For optimal parallelization, all the CPUs and GPUs in the cluster should be fully utilized at any given time. However, this is a limitation of Spark scheduling as all the stages are fused together, regardless of their resource requirements. This means that in the single cluster setup, the total parallelism is limited by the number of GPUs, leading to underutilization of CPUs. This also means that GPUs are completely idle during the reading and preprocessing steps. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"When using a separate CPU and GPU cluster, data needs to be persisted and orchestrated between stages. Overall, for these CPU+GPU workloads, Spark does not fully address ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"challenge #2","nodeType":"text"},{"data":{},"marks":[],"value":" and cannot utilize all resources in the cluster concurrently. We attempted to use Spark’s stage level scheduling feature to address this issue, but were ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/tree/main/spark#stage-level-scheduling"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"unsuccessful","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6m5c6zQiMVCithTLEIazb8","type":"Asset","createdAt":"2023-05-03T23:02:01.263Z","updatedAt":"2023-05-03T23:02:01.263Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 2","description":"Figure 2. How Ray data efficiently pipelines and optimizes hardware: CPUs/GPUs","file":{"url":"//images.ctfassets.net/xjan103pcp94/6m5c6zQiMVCithTLEIazb8/1be9788a7aa54707a42f38023597ae2f/5mb.gif","details":{"size":4991804,"image":{"width":3260,"height":1182}},"fileName":"5mb.gif","contentType":"image/gif"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data, on the other hand, can independently scale the CPU preprocessing and GPU inference steps. It also streams data ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"through these stages","nodeType":"text"},{"data":{},"marks":[],"value":", increasing CPUs and GPUs utilization and reducing out-of-memory errors (OOMs), and fine-grained resource allocation for ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html#scaling-and-resource-allocation"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"autoscaling and management","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #3: Large, multi-dimensional tensors","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deep learning workloads often involve dealing with complex data types like images, audio, or video, which are represented by multi-dimensional ","nodeType":"text"},{"data":{"uri":"https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"arrays","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Having first-class support for these data types is necessary for better performance and developer experience. Ray Data has Numpy batch format as a first-class citizen, and does not require Pandas, which is suited for deep learning and leads to higher performance and more memory efficiency, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-2-2-improved-developer-experience-performance-and-stability"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"as shown in previous benchmarks","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #4: Ray is Python native","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another fundamental difference is that Ray is Python native whereas Spark is not, which also impacts performance. We ran a microbenchmark that applies a ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"time.sleep(1)","nodeType":"text"},{"data":{},"marks":[],"value":" UDF. Surprisingly, running the UDF takes ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/tree/main/spark#microbenchmark"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"80 seconds","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"; we believe due to JVM\u003c\u003ePyarrow overhead! On Ray Data, the same microbenchmark takes a little ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/blob/main/ray/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"under 5 seconds","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Not to mention, this makes Spark development more difficult. In particular when debugging, the recommended way is to include debug information in the ","nodeType":"text"},{"data":{"uri":"https://stackoverflow.com/a/57175768"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"output pandas dataframe rather than using print statements","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Even as we scale to a larger 300 GB dataset using 4 nodes and 16 GPUs, we see that these performance differences still hold:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5wDKGXZNHB2Hlt6JEMMKOt","type":"Asset","createdAt":"2023-05-03T23:04:40.956Z","updatedAt":"2023-05-03T23:04:40.956Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_3_batch_inference","description":"Figure 3. Throughput difference between Spark and Ray solution for 300 GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/5wDKGXZNHB2Hlt6JEMMKOt/da27ee6ea7e5773abef42eb291dd1a97/image5.png","details":{"size":92846,"image":{"width":1102,"height":830}},"fileName":"image5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"While big data processing systems are the best solutions for offline inference, for deep learning workloads, Ray Data outperforms Spark.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling up Ray Data and maximizing performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we test the scalability of Ray Data and attempt to maximize performance by using a ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"heterogeneous cluster","nodeType":"text"},{"data":{},"marks":[],"value":" consisting of a mix of GPU nodes and CPU-only nodes. Deep learning workloads involving complex data types are often memory-bound, so by using additional CPU-only nodes, we add more total memory to our cluster to better maximize GPU utilization. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlike SageMaker or Spark, Ray works natively with heterogeneous clusters consisting of different instance types, fully addressing ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"challenge #1.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We run the same benchmark, except with 10 TB worth of images, using a single cluster of 10 g4dn.12xlarge instances and 10 m5.16xlarge instances.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Throughput: 11,580.958 img/sec","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"As we can see below, Ray Data achieves over 90%+ GPU utilization throughout the workload run:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"16XJHcynegHxIdae1hupAF","type":"Asset","createdAt":"2023-05-03T23:07:22.875Z","updatedAt":"2023-05-03T23:07:22.875Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"batch_inference_figure_4","description":"Figure 4. GPU utilization of all 40 GPUs in the cluster. We are roughly at 90%+ GPU utilization during the entire duration of the workload.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/16XJHcynegHxIdae1hupAF/8eab9567ac78a8d558bcd04218f7e1ca/image2.png","details":{"size":466601,"image":{"width":1999,"height":621}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nThe ability to scale to many GPUs and large data sizes without needing to rewrite code and not sacrificing on throughput is what sets Ray Data apart compared to other solutions for deep learning workloads.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog, we identified four technical challenges for doing batch inference at scale and investigated three commonly cited industry solutions for offline batch inferences. Using the cited approaches, we compared Ray, Spark, and Amazon SageMaker Batch Transform.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We concluded that Ray Data best meets all four challenges:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data abstracts away compute infrastructure management, and can maximize performance for memory-bound workloads as it natively supports heterogeneous clusters.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"streams","nodeType":"text"},{"data":{},"marks":[],"value":" data from cloud storage -\u003e CPU -\u003e GPU, ensuring that all resources in the cluster are utilized at any given point in time. This improves throughput and reduces overall cost.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data has native support for multi-dimensional tensors, which is vital for deep learning workloads.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data is Python native and programmatic, making it easy to develop and debug.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Overall, Ray Data is the best option for deep learning offline batch inference. In our image classification benchmarks, as shown in the figures above, Ray Data significantly outperforms SageMaker Batch Transform (by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"17x)","nodeType":"text"},{"data":{},"marks":[],"value":" and Spark (by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"2x ","nodeType":"text"},{"data":{},"marks":[],"value":"and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"3x","nodeType":"text"},{"data":{},"marks":[],"value":") while linearly scaling to TB level data sizes. All code for these benchmarks can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Lastly, as a result of higher throughput and Ray as an open source solution, Ray lowers the overall costs on top of the base EC2 cost.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What’s Next","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stay tuned for a follow up post where we dive into Ray Data streaming's design and how to use it for your own ML pipelines. Meanwhile, you can get started with Ray Data for batch inference ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Alternatively, ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfC6rmBeb9pLPSKRf1e7VCy7BtkjSGu20gh0xjQhdo6rcUflQ/viewform"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"fill out this form","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to connect with us if you are interested in working on a proof of concept (POC) for your offline batch inference workload with Ray.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have our ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit 2023 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"early-bird registration open. Secure your spot, and save some money.\n\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n\n\n\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2dNPqjBTFiDZn2gcXkdir9","type":"Asset","createdAt":"2023-05-03T22:50:03.518Z","updatedAt":"2023-05-03T22:50:03.518Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 1","description":"Figure 1. Throughput difference among three different solutions for 10GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/2dNPqjBTFiDZn2gcXkdir9/f07a675dcb4f124e29aeb2f6618dbf15/image1.png","details":{"size":163062,"image":{"width":1334,"height":948}},"fileName":"image1.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}}],"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Tvd0c4cpDmpGTCTHTyU4e","type":"Entry","createdAt":"2023-03-22T01:13:56.900Z","updatedAt":"2023-04-24T18:00:47.830Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"How Ray solves common production challenges for generative AI infrastructure","seoTitle":"How Ray Solves Generative AI and LLM Infrastructure Challenges","slug":"ray-common-production-challenges-for-generative-ai-infrastructure","description":"Learn the infrastructure challenges for supporting workloads production deployments around foundation models, and how Ray tackles these challenges.","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BGRfsYFKUgIpGjaXTUYml","type":"Entry","createdAt":"2021-08-09T15:38:43.390Z","updatedAt":"2021-08-09T15:38:43.390Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Antoni Baum","slug":"antoni-baum","link":"https://www.linkedin.com/in/yard1/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2yKvtARlwNkkIYOYLFgkhB","type":"Entry","createdAt":"2022-06-22T19:54:47.912Z","updatedAt":"2022-06-22T19:54:47.912Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jun Gong","slug":"jun-gong"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7LfbvZJGJ9Hpn6IYnNfxR2","type":"Entry","createdAt":"2020-09-15T02:35:09.220Z","updatedAt":"2021-06-16T18:45:23.355Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Kai Fricke","slug":"kai-fricke","link":"https://www.linkedin.com/in/kaifricke/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}}],"publishedDate":"2023-03-20","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 1 of our generative AI blog series. In this post, we talk about how to use Ray to productionize common generative model workloads. An upcoming blog will deep dive into why projects like [Alpa](https://github.com/alpa-projects/alpa) are using Ray to scale large models.","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Generative image and language models promise to change how businesses approach design, support, development, and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.gartner.com/en/articles/beyond-chatgpt-the-future-of-generative-ai-for-enterprises"},"content":[{"nodeType":"text","value":"more","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". This blog focuses on the infrastructure challenges for supporting workloads production deployments around foundation models, and how ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.ray.io/"},"content":[{"nodeType":"text","value":"Ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", a leading solution for ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-overview/use-cases.html"},"content":[{"nodeType":"text","value":"scaling ML workloads","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", tackles these challenges. We finish with a roadmap for improvements we're undertaking to make things even easier.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Today, Ray is used by leading AI organizations to ","marks":[],"data":{}},{"nodeType":"text","value":"train","marks":[{"type":"italic"}],"data":{}},{"nodeType":"text","value":" large language models (LLM) at scale (e.g., by ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://thenewstack.io/how-ray-a-distributed-ai-framework-helps-power-chatgpt/"},"content":[{"nodeType":"text","value":"OpenAI to train ChatGPT ","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/ray-summit-2022-stories-large-language-models"},"content":[{"nodeType":"text","value":"Cohere to train their models","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.eleuther.ai/"},"content":[{"nodeType":"text","value":"EleutherAI","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" to train ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://huggingface.co/EleutherAI/gpt-j-6b"},"content":[{"nodeType":"text","value":"GPT-J","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/alpa-projects/alpa"},"content":[{"nodeType":"text","value":"Alpa","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" for multi-node training and serving). However, one of the reasons why these models are so exciting is that open-source versions can be fine-tuned and deployed to address particular problems without needing to be trained from scratch. Indeed, users in the community are increasingly asking how to use Ray for the orchestration of their own generative AI workloads, building off foundation models trained by larger players.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In the table below, we highlight common \"production-scale\" needs (which typically span from 1-100 nodes) in green. This includes questions such as: ","marks":[],"data":{}}]},{"nodeType":"ordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"How do I scale batch inference to terabytes of unstructured data?","marks":[{"type":"bold"}],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"How do I create a fine-tuning service that can spin up new jobs on demand?","marks":[{"type":"bold"}],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"How do I deploy a model that spans multiple GPUs on multiple nodes?","marks":[{"type":"bold"}],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this blog post, we will focus primarily on these green-highlighted workloads.","marks":[{"type":"bold"}],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4J7gYiqhavWhjroHvBAzEf","type":"Asset","createdAt":"2023-03-22T01:07:54.116Z","updatedAt":"2023-03-22T01:07:54.116Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"1-100-llm-nodes","description":"Different scale workloads for generative AI.","file":{"url":"//images.ctfassets.net/xjan103pcp94/4J7gYiqhavWhjroHvBAzEf/6da675628226e059a37604b28e77e059/Pasted_Graphic.png","details":{"size":496992,"image":{"width":2168,"height":934}},"fileName":"Pasted Graphic.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If you are interested in taking a look at the code examples of using Ray for generative AI workloads, check out ","marks":[{"type":"italic"}],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/index.html#text-nlp"},"content":[{"nodeType":"text","value":"our examples page","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"text","value":". ","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Generative models: the latest evolution of ML workloads\n","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6AYwkNlrt4ZrjzMovQS1q","type":"Asset","createdAt":"2023-03-20T18:53:11.300Z","updatedAt":"2023-03-20T18:53:11.300Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray-cluster","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6AYwkNlrt4ZrjzMovQS1q/58f7fe77a61f1550a68c397ba0edf50e/Screenshot_2023-03-20_at_2.52.32_PM.png","details":{"size":128408,"image":{"width":1045,"height":545}},"fileName":"Screenshot 2023-03-20 at 2.52.32 PM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Fig. Based on the latest deep learning models, the compute requirements of generative AI mirror those of traditional ML platforms, only greater.","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We think of generative model workloads as magnifying the inherent computational challenges behind ML workloads. This comes along two axes:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Cost","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": Generative models are larger and more computationally expensive to train and serve. For example, an XGBoost / scikit-learn model may take 1-10ms per record, while the most basic inference on these generative models will take 1000ms or more--- a factor of \u003e100x. This change in cost makes going distributed a default requirement for companies that want to use generative models in production.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Complexity","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": In part because of cost, new computational requirements such as model parallel serving or incremental status updates (i.e., reporting partial results) emerge from use cases such as text generation and image diffusion.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Let's also break down this trend more specifically by workload. The following table summarizes common AI workloads, and how infrastructure requirements have evolved over time for these different use cases:","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2ogzkzEaUy3LApS1ShYVKI","type":"Asset","createdAt":"2023-03-20T19:48:23.806Z","updatedAt":"2023-03-20T19:48:23.806Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"workload table","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2ogzkzEaUy3LApS1ShYVKI/6c606dfc16c9fc7e5cc0ffffe035b7bd/Screenshot_2023-03-20_at_3.48.12_PM.png","details":{"size":120978,"image":{"width":1120,"height":751}},"fileName":"Screenshot 2023-03-20 at 3.48.12 PM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"As the table shows, the cost and complexity of these workloads has grown over time--- and this is in spite of the rapid improvements in GPU hardware in the industry. Let's dive next into the challenges practitioners may face with using generative AI for these workloads.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Challenges in generative AI infrastructure","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Generative AI infrastructure presents new challenges for distributed training, online serving, and offline inference workloads.","marks":[],"data":{}}]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"1. Distributed training","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Some of the largest scale generative model training is being done on Ray today:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://thenewstack.io/how-ray-a-distributed-ai-framework-helps-power-chatgpt/"},"content":[{"nodeType":"text","value":"OpenAI","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" uses Ray to coordinate the training of ChatGPT and other models.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/alpa-projects/alpa"},"content":[{"nodeType":"text","value":"Alpa project","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" uses Ray to coordinate training and serving of data, model, and pipeline-parallel computations with JAX as the underlying framework.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/ray-summit-2022-stories-large-language-models"},"content":[{"nodeType":"text","value":"Cohere","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.eleuther.ai/"},"content":[{"nodeType":"text","value":"EleutherAI","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" use Ray to train their large language models at scale along with PyTorch and JAX.","marks":[],"data":{}}]}]}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3qoGKzophjj3OHLzVUe0wh","type":"Asset","createdAt":"2023-03-20T19:03:23.573Z","updatedAt":"2023-03-20T19:03:23.573Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"alpa","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3qoGKzophjj3OHLzVUe0wh/216510e4110506bcb4d508d690c0266c/Screenshot_2023-03-20_at_3.03.02_PM.png","details":{"size":109490,"image":{"width":1125,"height":266}},"fileName":"Screenshot 2023-03-20 at 3.03.02 PM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Fig. ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://ai.googleblog.com/2022/05/alpa-automated-model-parallel-deep.html"},"content":[{"nodeType":"text","value":"Alpa","marks":[{"type":"underline"},{"type":"italic"}],"data":{}}]},{"nodeType":"text","value":" uses Ray as the underlying substrate to schedule GPUs for distributed training of large models, including generative AI models.","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Common challenges for distributed training for generative models include:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"How to effectively partition the model across multiple accelerators?","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"How to setup your training to be tolerant of failures on preemptible instances? ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In an upcoming blog, we will cover how projects are leveraging Ray to solve challenges for distributed training.","marks":[],"data":{}}]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"2. Online serving and fine-tuning","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We are also seeing a growing interest in using Ray for generative AI workloads at medium scales (e.g., 1-100 nodes). Typically, users at this scale are interested in using Ray to scale out existing training or inference workloads they can already run on one node (e.g., using  ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.deepspeed.ai/"},"content":[{"nodeType":"text","value":"DeepSpeed","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://huggingface.co/docs/accelerate/index"},"content":[{"nodeType":"text","value":"Accelerate","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", or a variety of other common single-node frameworks). In other words, they want to run many copies of a workload for purposes of deploying an online inference, fine-tuning, or training service.","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5z8My3V5rCzk4jj34clDh4","type":"Asset","createdAt":"2023-03-20T19:04:55.663Z","updatedAt":"2023-03-20T19:04:55.663Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"gpu-cost","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5z8My3V5rCzk4jj34clDh4/9b4febe3c451314b51b4e97d9f06f5e1/Screenshot_2023-03-20_at_3.04.28_PM.png","details":{"size":32550,"image":{"width":917,"height":548}},"fileName":"Screenshot 2023-03-20 at 3.04.28 PM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Fig. A100 GPUs, while providing much more GRAM per GPU, cost much more per gigabyte of GPU memory than A10 or T4 GPUs. Multi-node Ray clusters can hence serve generative workloads at a significantly lower cost when GRAM is the bottleneck.","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Doing this form of scale-out itself can be incredibly tricky to get right and costly to implement. For example, consider the task of scaling a fine-tuning or online inference service for multi-node language models. There are many details to get right, such as optimizing data movement, fault tolerance, and autoscaling of model replicas. Frameworks such as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.deepspeed.ai/"},"content":[{"nodeType":"text","value":"DeepSpeed","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://huggingface.co/docs/accelerate/index"},"content":[{"nodeType":"text","value":"Accelerate","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" handle the sharding of model operators, but not the execution of higher-level applications invoking these models.","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"64PKDu2I6TwPO7SupD2GDD","type":"Asset","createdAt":"2023-03-20T19:06:20.509Z","updatedAt":"2023-03-20T20:09:27.252Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"layered-stack","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/64PKDu2I6TwPO7SupD2GDD/808f2cbe5998498d16b2b7037693a095/Screenshot_2023-03-20_at_4.09.08_PM.png","details":{"size":49005,"image":{"width":824,"height":363}},"fileName":"Screenshot 2023-03-20 at 4.09.08 PM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Fig. Layered stack diagram for deployments of generative model workloads.","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Even for single-node workloads, users can often benefit from distribution as it is much cheaper to deploy a small cluster of GPUs than a single high-end device (e.g., A100 GPU) for hosting models. This is because lower-end GPUs typically cost less per gigabyte of memory. However, it is challenging to scale deployments involving many machines. It is also difficult to drive high utilization out of the box without libraries such as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/serve/index.html"},"content":[{"nodeType":"text","value":"Ray Serve","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/data/dataset.html"},"content":[{"nodeType":"text","value":"Ray Data","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"3. Offline batch inference","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"On the offline side, batch inference for these models also has challenges in requiring data-intensive preprocessing followed by GPU-intensive model evaluation. Companies like ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://engineering.fb.com/2022/09/19/ml-applications/data-ingestion-machine-learning-training-meta/"},"content":[{"nodeType":"text","value":"Meta","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.tensorflow.org/api_docs/python/tf/data/experimental/service"},"content":[{"nodeType":"text","value":"Google","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" build custom services (","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://engineering.fb.com/2022/09/19/ml-applications/data-ingestion-machine-learning-training-meta/"},"content":[{"nodeType":"text","value":"DPP","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.tensorflow.org/api_docs/python/tf/data/experimental/service"},"content":[{"nodeType":"text","value":"tf.data service","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":")","marks":[],"data":{}},{"nodeType":"text","value":" ","marks":[{"type":"italic"}],"data":{}},{"nodeType":"text","value":"to perform this at scale in heterogeneous CPU/GPU clusters. While in the past such services were the rarity, we are more and more often seeing users ask how to do this in the context of generative AI inference. These users now also need to tackle the distributed systems challenges of scheduling, observability, and fault tolerance.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"How Ray addresses these challenges","marks":[],"data":{}}]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"Existing Ray Features","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Core scheduling: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"To orchestrate the large-scale distributed computations required for training generative models from scratch, Ray Core has flexible support for ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-core/scheduling/index.html"},"content":[{"nodeType":"text","value":"scheduling","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" tasks and actors on CPUs and GPUs. This capability is especially useful by those using Ray for large-scale model training. ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-core/scheduling/placement-group.html"},"content":[{"nodeType":"text","value":"Placement groups","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" enable users to reserve groups of GPU or CPU resources to place replicas of large models for multi-node fine-tuning, inference, or training.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Train","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": Ray's ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/train/train.html"},"content":[{"nodeType":"text","value":"Train","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" library provides out of the box Trainer classes that can run popular frameworks such as distributed TensorFlow, Torch, XGBoost, Horovod, and more. To make it easier to get started with generative models, we are adding integrations with popular frameworks such as HuggingFace Accelerate, DeepSpeed, and Alpa to Train, as well as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray/issues/33021"},"content":[{"nodeType":"text","value":"RLHF support","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\n","marks":[],"data":{}},{"nodeType":"text","value":"Ray Serve: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Ray ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/serve/index.html"},"content":[{"nodeType":"text","value":"Serve","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" provides a first-class API for scaling model deployment graphs. Because Serve runs within Ray, it also has a great degree of flexibility in running ad-hoc or auxiliary computations. For example, users have launched Ray sub-tasks from Serve to fine-tune models, and coordinated the return of incremental results through named actors also running in Ray.","marks":[],"data":{}}]},{"nodeType":"heading-3","data":{},"content":[{"nodeType":"text","value":"Upcoming Features","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Data streaming backend","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": To make large-scale batch inference on mixed CPU and GPU node clusters easier to run at scale, we're working on adding first-class streaming inference support to Ray Data: ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/enhancements/pull/18"},"content":[{"nodeType":"text","value":"https://github.com/ray-project/enhancements/pull/18","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Async requests in Ray Serve","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": We are looking to enhance Ray Serve to natively handle long-running async jobs such as fine-tuning requests. These requests typically take minutes to run, much longer than normal ML inference jobs of \u003c10s, which often means users have to turn to using an external job queue: ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray/issues/32292"},"content":[{"nodeType":"text","value":"https://github.com/ray-project/ray/issues/32292","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"New examples we're releasing","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Of course, a big difference exists between workable in theory and working in practice. To bridge this gap, we're also announcing the initial release of several generative AI examples, which we will expand over time as new models and frameworks emerge.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The linked examples may require you to ","marks":[{"type":"italic"}],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-overview/installation.html#daily-releases-nightlies"},"content":[{"nodeType":"text","value":"use a nightly version of Ray","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"text","value":" -- links and examples will be available on a stable version of Ray with Ray 2.4.","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Stable Diffusion","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/dreambooth_finetuning.html"},"content":[{"nodeType":"text","value":"Fine-Tuning","marks":[],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/serve/tutorials/stable-diffusion.html"},"content":[{"nodeType":"text","value":"Serving","marks":[],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/stablediffusion_batch_prediction.html"},"content":[{"nodeType":"text","value":"Batch Prediction","marks":[],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Streaming Batch Prediction: ","marks":[],"data":{}},{"nodeType":"text","value":"coming in Ray 2.5","marks":[{"type":"italic"}],"data":{}}]}]}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"GPT-J","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html"},"content":[{"nodeType":"text","value":"Fine-Tuning","marks":[],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/gptj_serving.html"},"content":[{"nodeType":"text","value":"Serving","marks":[],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/gptj_batch_prediction.html"},"content":[{"nodeType":"text","value":"Batch Prediction","marks":[],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Streaming Batch Prediction: ","marks":[],"data":{}},{"nodeType":"text","value":"coming in Ray 2.5","marks":[{"type":"italic"}],"data":{}}]}]}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"OPT-66B","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Fine-Tuning: ","marks":[],"data":{}},{"nodeType":"text","value":"coming in Ray 2.5","marks":[{"type":"italic"}],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Serving: ","marks":[],"data":{}},{"nodeType":"text","value":"coming in Ray 2.5","marks":[{"type":"italic"}],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Batch Prediction: ","marks":[],"data":{}},{"nodeType":"text","value":"coming in Ray 2.5","marks":[{"type":"italic"}],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Streaming Batch Prediction: ","marks":[],"data":{}},{"nodeType":"text","value":"coming in Ray 2.5","marks":[{"type":"italic"}],"data":{}}]}]}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Conclusion","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In summary, we believe generative models are accelerating the need for a flexible, unified framework such as Ray for running ML computations. The Ray team plans to add enhanced APIs for working with these types of expensive models at scale and examples and integrations with popular models and community frameworks to make it easy to get started. Get started with Ray ML use cases ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-overview/use-cases.html"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", and take a look at our","marks":[],"data":{}},{"nodeType":"text","value":" ","marks":[{"type":"italic"}],"data":{}},{"nodeType":"text","value":"code examples of using Ray for generative AI workloads on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/index.html#text-nlp"},"content":[{"nodeType":"text","value":"our examples page","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"64PKDu2I6TwPO7SupD2GDD","type":"Asset","createdAt":"2023-03-20T19:06:20.509Z","updatedAt":"2023-03-20T20:09:27.252Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"layered-stack","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/64PKDu2I6TwPO7SupD2GDD/808f2cbe5998498d16b2b7037693a095/Screenshot_2023-03-20_at_4.09.08_PM.png","details":{"size":49005,"image":{"width":824,"height":363}},"fileName":"Screenshot 2023-03-20 at 4.09.08 PM.png","contentType":"image/png"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"SFpqDfxhd1Fkl9YvB2WUM","type":"Entry","createdAt":"2022-03-29T00:09:03.580Z","updatedAt":"2022-03-29T00:09:03.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Deep Dive: Data ingest in a third-generation ML architecture","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5945GdUwJCPlj6wHOPlw7Y","type":"Entry","createdAt":"2021-11-30T16:50:41.093Z","updatedAt":"2022-06-22T16:09:22.594Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Deep Dive: Data Ingest in a Third Generation ML Architecture","slug":"deep-dive-data-ingest-in-a-third-generation-ml-architecture","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12vKMQ3j3q45idk83ZoPYA","type":"Entry","createdAt":"2021-10-01T20:23:37.378Z","updatedAt":"2021-10-01T20:23:37.378Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Chen Shen","slug":"chen-shen"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48lK0gPUqCqrmxWiQi8ve8","type":"Entry","createdAt":"2021-02-16T07:25:36.331Z","updatedAt":"2021-02-16T07:25:36.331Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Clark Zinzow","slug":"clark-zinzow","link":"https://www.linkedin.com/in/clarkzinzow/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2021-11-30","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Distributed libraries allow improved performance by exploiting the full bandwidth of distributed memory, and giving greater programmability. But how does that actually work? What does the code look like?\n\nIn this post, we’ll be looking at a concrete example with code samples: ML ingest with Ray Datasets and Ray Train.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"This is part 3 of our series on ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"third generation ML architectures","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". In the previous post, we talked about how distributed libraries allow improved ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"performance","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" by exploiting the full bandwidth of distributed memory, and giving greater programmability. But how does that ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"actually work","nodeType":"text"},{"data":{},"marks":[],"value":"? What does the code look like?","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this post, we’ll be looking at a concrete example with code samples: ML ingest with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/user_guide.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"We show how these distributed libraries can be woven together with just a few lines of Python--- a key capability not possible in 2nd gen architectures.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"We examine how Datasets and Train use the interoperable primitives of Ray tasks, actors, and objects to enable this composable architecture.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Runnable scripts are available that can be adapted for use on your own Ray cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Small Data Training","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To set the stage, let's consider ML training in the small-data setting. These kinds of pipelines are quite simple since all data fits in memory, and the overhead of shuffling is minimal. You can express it as just a few lines of pseudocode:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3ZZZM2MDEx3oRdMKvIblVb","type":"Entry","createdAt":"2021-11-29T23:28:30.694Z","updatedAt":"2021-11-29T23:28:50.762Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"small data training","body":"data = load_data()\npreprocess(data)\nfor each epoch:\n    random_shuffle(data)\n    train_one_epoch(data)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's review the steps above:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Loading","nodeType":"text"},{"data":{},"marks":[],"value":": Small data is typically read from files on local disk into memory. It may be streamed from files in some cases.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Preprocessing","nodeType":"text"},{"data":{},"marks":[],"value":": Apply simple transformations (i.e., feature engineering).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Shuffling","nodeType":"text"},{"data":{},"marks":[],"value":": Randomly permute the order of items in the dataset. Shuffling randomly for each epoch is ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1709.10432"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"important","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for stochastic gradient descent.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Training","nodeType":"text"},{"data":{},"marks":[],"value":": Fit the model over the data (e.g., using a framework like ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://horovod.readthedocs.io/en/stable/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Horovod","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":").","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Big Data Training Challenges","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Training models over big data adds additional needs around (1) distributed preprocessing, (2) distributed shuffling to improve convergence rates, and (3) pipelined execution with ML training:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed preprocessing","nodeType":"text"},{"data":{},"marks":[],"value":": The data ingestion requirements of large-scale training can be substantial, motivating specialized systems such as ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/2108.09373"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"DPP","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" from Facebook and ","nodeType":"text"},{"data":{"uri":"https://eng.uber.com/petastorm/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Petastorm","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" from Uber. The systems allow for preprocessing to be off-loaded to separate nodes in the cluster distinct from the GPU machines. Some portion of preprocessing can be done offline, but it is desirable for data to be \"minimally preprocessed\" for flexibility.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed shuffling","nodeType":"text"},{"data":{},"marks":[],"value":": It is important for the dataset to be shuffled (randomly re-ordered) for each epoch of training. This can significantly ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1709.10432"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"improve the convergence of SGD","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", but is challenging in the ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/MapReduce"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"distributed setting","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". While ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"global shuffling","nodeType":"text"},{"data":{},"marks":[],"value":" is optimal, typically solutions like TensorFlow/Pytorch data loaders and Petastorm only perform local shuffling due to the engineering complexity of stitching together large-scale data shuffles with ML training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Pipelining data processing and training","nodeType":"text"},{"data":{},"marks":[],"value":": Due to limited cluster memory sizes and the need for random per-epoch shuffles, we see that preprocessing and shuffle computations may need to be interleaved with training. This is only possible today in specialized systems like DPP (e.g., you cannot trivially connect Spark's distributed shuffle with Horovod, since they are separate distributed systems).\nIn other words, our simple pipeline became hard since components necessarily become distributed and pipelined for performance:","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In other words, our simple pipeline became hard since components necessarily become distributed and pipelined for performance:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"10WGpzv003QrG6Mz73dzUo","type":"Entry","createdAt":"2021-11-29T23:32:46.395Z","updatedAt":"2021-11-29T23:32:46.395Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"data = load_data()         # larger than cluster memory :(","body":"data = load_data()         # larger than cluster memory :(\npreprocess(data)           # distributed transforms :(\nfor each epoch:\n    random_shuffle(data)   # distributed shuffle :(\n    train_one_epoch(data)  # pipelined with above distributed steps :(","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Second Generation Approach","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's briefly consider how we could compose existing distributed systems to solve this distributed ingest problem. We need to set up a Spark cluster for data processing, a Horovod cluster for training, a coordinator service for control plane operations, and external storage for data plane communication.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6jWNBb88EkWu12CCvGYDst","type":"Asset","createdAt":"2021-11-30T00:22:56.327Z","updatedAt":"2021-11-30T21:06:31.778Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"2nd Generation - Data Ingest Problem","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6jWNBb88EkWu12CCvGYDst/88488a650b7eb5eb6d88bda7e37ca7c1/dataIngestProblem.png","details":{"size":90348,"image":{"width":1406,"height":666}},"fileName":"dataIngestProblem.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The training pipeline would work in the following steps. First, the coordinator service would (1) submit a shuffle job to the Spark cluster, which (2) reads and writes data out to external storage. Next, the Horovod data reader (e.g., Petastorm) would (3) fetch the written dataset location from the coordinator and (4) read the shuffle data for training. These steps would repeat for each epoch of training, and can run concurrently to optimize execution latencies.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The disadvantages of the 2nd generation approach are:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Lack of programmability","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": need to setup and manage 3+ separate distributed systems. It's also hard to orchestrate with workflow systems due to the interleaving of shuffle with training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Performance overhead","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": intermediate data written to external storage since it needs to cross between distributed systems.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Third Generation Approach","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In contrast, with a 3rd gen architecture we can compose the entire data ingest pipeline using distributed ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"libraries","nodeType":"text"},{"data":{},"marks":[],"value":". In the snippet below ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/examples/train_linear_dataset_example.html#train-linear-dataset-example"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"(see here for a full runnable example)","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we compose a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset-pipeline.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Dataset Pipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with a distributed ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/user_guide.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train Job","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"78pALMO2tFH7mvFI7WL9wK","type":"Entry","createdAt":"2021-11-30T00:24:17.532Z","updatedAt":"2021-11-30T00:24:17.532Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Third Generation Approach","body":"from ray.train import Trainer, get_dataset_shard\n\n# Distributed Preprocessing and Shuffle\npipe = ray.data.read_parquet(path).window(size).repeat()\npipe = pipe.map_batches(preprocess)\npipe = pipe.random_shuffle_each_window()\n\n# Ray Train Function\ndef train_func():\n    model = NeuralNetworkModel(...)\n    model = train.torch.prepare_model(model)\n    for epoch_data in get_dataset_shard().iter_epochs():\n        model.fit(epoch_data.to_torch(...))\n\n# Compose and Run\ntrainer = Trainer(num_workers=3, backend=\"torch\", use_gpu=True)\nresult = trainer.run(train_func, dataset=dataset_pipeline)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The above snippet, while simplified, is able to express the aforementioned ML ingest and training pipeline with just a few lines of Python code--- without any need to wrangle distributed systems. Under the hood, the Dataset and Train libraries leverage Ray Tasks and Actors respectively to execute distributed data preprocessing and ML training. We are able to compose them by just passing a reference to the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"dataset_pipeline","nodeType":"text"},{"data":{},"marks":[],"value":" object to Train:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"n0VLbvBnx8WyFKvQJDDyC","type":"Asset","createdAt":"2021-11-30T00:27:03.418Z","updatedAt":"2021-11-30T21:09:59.864Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"3rdGenTasks andActors","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/n0VLbvBnx8WyFKvQJDDyC/b40de05028d9740c1b3f51af4e20eaeb/3rdGenTasks_andActors.png","details":{"size":148336,"image":{"width":1366,"height":922}},"fileName":"3rdGenTasks andActors.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The above figure illustrates the tasks and actors created by the above code snippet.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Compared to the 2nd gen approach, the 3rd gen approach achieves:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Lower operational and development overheads","nodeType":"text"},{"data":{},"marks":[],"value":": developers can compose and ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"customize","nodeType":"text"},{"data":{},"marks":[],"value":" the entire distributed training system in a single script thanks to the programmability of a 3rd gen architecture.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Better performance","nodeType":"text"},{"data":{},"marks":[],"value":": as we'll see in the case studies, this approach reduces overheads by allowing data to be passed in-memory.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Code Walkthrough","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"So how does it work? Let's walk through the above example starting with the system requirements.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Requirements for the Example","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"For performance, we want data to be passed in-memory between preprocessing, shuffle, and training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"We should support ingestion of a dataset that is larger than memory. In the example below, we'll assume a 2TB dataset, and a cluster with 1TB of memory.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Support for heterogeneous clusters (e.g., a cluster with GPU training nodes and CPU preprocessing nodes).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Part 1: Windowed data loading","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's look at the first part of the code above, which creates a data loading pipeline.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xvSklEPrmcWNId9RfBs5j","type":"Entry","createdAt":"2021-11-30T00:28:20.053Z","updatedAt":"2021-11-30T00:28:20.053Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"pipe = ray.data.read_parquet(path).window(size).repeat()","body":"pipe = ray.data.read_parquet(path).window(size).repeat()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This uses the Ray Dataset library to create a DatasetPipeline reading our parquet data from disk. Since the dataset (2TB) is larger than our cluster memory (1TB), we use the .window() function to process windows of size=200GB at a time, leaving extra memory headroom for execution. Since we want to loop over the dataset indefinitely, we use the .repeat() operator after that.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Part 2: Preprocessing and shuffle pipeline","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The second part of the pipeline is applying the distributed transform and shuffling operations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"K22lS3qMNQAVVakEyi5mO","type":"Entry","createdAt":"2021-11-30T00:29:09.699Z","updatedAt":"2021-11-30T00:29:09.699Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Preprocessing and shuffle pipeline","body":"pipe = pipe.map_batches(preprocess)\npipe = pipe.random_shuffle()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is telling Ray to transform records in the pipeline with a given ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"preprocess","nodeType":"text"},{"data":{},"marks":[],"value":" function, and then shuffling the entire window randomly (e.g., 200GB at a time), to avoid going out of core. So far, nothing has been executed beyond reading the file metadata--- we're building up a logical pipeline.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Part 3: Ray Train Setup","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next we define the code that is run on each GPU worker and implements distributed training. Each worker can read a particular split of the pipeline we defined by calling ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"get_dataset_shard","nodeType":"text"},{"data":{},"marks":[],"value":". It sets up a model using ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"train.torch.prepare_model","nodeType":"text"},{"data":{},"marks":[],"value":" to participate in distributed training. Then, it trains over the data in each epoch (repeat) of the dataset.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4OVxuAmQZAAr2zL5vrYVnO","type":"Entry","createdAt":"2021-11-30T00:29:55.709Z","updatedAt":"2021-11-30T00:29:55.709Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"train_func","body":"def train_func():\n    model = NeuralNetworkModel(...)\n    model = train.torch.prepare_model(model)\n    for epoch_data in get_dataset_shard().iter_epochs():\n        model.fit(epoch_data.to_torch(...))","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To create the training actors, we create a ray.train.Trainer that requires 3 GPU workers:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"BIl5itQv7ZPDqNfgrNZSP","type":"Entry","createdAt":"2021-11-30T00:30:22.961Z","updatedAt":"2021-11-30T00:30:22.961Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"trainer = Trainer(num_workers=3, backend=\"torch\", use_gpu=True)","body":"trainer = Trainer(num_workers=3, backend=\"torch\", use_gpu=True)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"At this point, our pipeline is fully defined, and our training actors have been created and assigned GPUs in the cluster, we just need to run it.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Part 3: Running everything","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"This line of code triggers the execution of the entire pipeline:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"j221k8qfddcLQDzWQVpcA","type":"Entry","createdAt":"2021-11-30T00:31:25.195Z","updatedAt":"2021-11-30T00:31:25.195Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Part 3: Running everything","body":"result = trainer.run(train_func, dataset=dataset_pipeline)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"So what's happening in the cluster?","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train sends actor method calls to each actor to run its given training function.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Each actor pulls data from the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"DatasetPipeline","nodeType":"text"},{"data":{},"marks":[],"value":" shard given to it (each pipeline shard contains a handle to a coordinator actor created by Datasets for this DatasetPipeline instance).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"This triggers actor calls to the coordinator actor.\na. The coordinator schedules execution of the next window of the pipeline, e.g., Ray tasks that use CPU nodes in the cluster to:\n      i. load data for the window (200GB)\n      ii. preprocess the data\n      iii. randomly shuffle the data\n      iv. split up the data and assign splits to trainer actors\nb. The coordinator returns to the trainer actors object references to their assigned Dataset split.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The trainer actors ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.get()","nodeType":"text"},{"data":{},"marks":[],"value":" data blocks from their Dataset split and generate mini-batches to pass to the underlying learning library (i.e., PyTorch).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can visualize the overall dataflow in the following timeline diagram. Once data is loaded, shuffle and execution proceed in a fully pipelined way, leveraging tasks running on CPU nodes to implement shuffling, and actors running on GPUs for training:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"51nJaf8KaCjBBQOzS4W6mU","type":"Asset","createdAt":"2021-11-30T05:53:49.763Z","updatedAt":"2021-11-30T05:53:49.763Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"dataflow","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/51nJaf8KaCjBBQOzS4W6mU/d490293c792df1d7803eb30f13682b1d/dataflow.png","details":{"size":90897,"image":{"width":1210,"height":432}},"fileName":"dataflow.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Try it out yourself with these examples in Ray 1.8:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/examples/train_linear_dataset_example.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://docs.ray.io/en/master/train/examples/train_linear_dataset_example.html","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/examples/big_data_ingestion.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://docs.ray.io/en/master/data/examples/big_data_ingestion.html","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Benchmarks","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a previous blog post, we discussed the ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"performance advantages","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" of passing data in-memory and with pipelining and showed improvements in an ablation study. Since then, Datasets has been used by several open source users to implement large-scale shuffled ML ingest. We present two case studies from our ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/presentation/d/1zANPlmrxQkjPU62I-p92oFO3rJrmjVhs73hL4YbM4C4"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyData Dataset talk","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" showing significant performance improvements:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Case Study 1: high-tech ML platform startup","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Dask-on-Ray → Datasets → Horovod","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Dask-on-Ray and Datasets was ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"8x faster","nodeType":"text"},{"data":{},"marks":[],"value":" than Pandas + S3+ Petastorm, even on a single machine.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Benchmark:","nodeType":"text"},{"data":{},"marks":[],"value":" ","nodeType":"text"},{"data":{"uri":"http://ludwig.ai/"},"content":[{"data":{},"marks":[],"value":"Ludwig AI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" model, NYC Taxi dataset (5 GB subset), single g4dn.4xlarge instance","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UdwUzwL7HNFGOSUgHydvq","type":"Asset","createdAt":"2021-11-30T07:55:45.520Z","updatedAt":"2021-11-30T21:14:24.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Shuffled Data Benchmark","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2UdwUzwL7HNFGOSUgHydvq/3bdea36e01ae221aa6d802001c57a528/shuffledDataBenchmark.png","details":{"size":79641,"image":{"width":1108,"height":656}},"fileName":"shuffledDataBenchmark.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Case Study 2: large transport tech company","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"S3 → Datasets → Horovod","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Datasets from S3 was ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"4x faster","nodeType":"text"},{"data":{},"marks":[],"value":" than Petastorm from S3","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Benchmark:","nodeType":"text"},{"data":{},"marks":[],"value":" 1.5 TB synthetic tabular dataset, 16 nodes (40 vCPUs, 180 GB RAM), 2 shuffle windows ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Gl6shcoyCtt7P1xHuYWO0","type":"Asset","createdAt":"2021-11-30T07:57:18.136Z","updatedAt":"2021-11-30T07:57:18.136Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Petastorm Datasets","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6Gl6shcoyCtt7P1xHuYWO0/63479b61e30fb33db13df73f72e42da4/petastormDatasets.png","details":{"size":28808,"image":{"width":626,"height":290}},"fileName":"petastormDatasets.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We believe 3rd gen ML architectures will help engineers develop and standardize infrastructure for large-scale ML apps. This blog demonstrated that with just a single Python script, we can connect distributed data preprocessing with training in a highly performant way.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Moreover, in true 3rd-gen fashion, we were able to do the above without building a specialized system. We used Ray to interleave execution of two independent distributed libraries--- a key capability not possible in 2nd gen architectures. This composability is possible since both libraries are built on the common and interoperable primitives of Ray tasks, actors, and objects.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While we're just getting started with ML ingest-- look for new examples and performance enhancements as Ray Datasets graduates from beta in the next few months--- this is just one aspect of programmable distributed compute with Ray. Check out other use cases in Tuning, Training, Serving, and more here: ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://www.ray.io/","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7hIMsn7WDpX9zGqRwfZnvu","type":"Asset","createdAt":"2021-11-30T21:13:58.335Z","updatedAt":"2021-11-30T21:13:58.335Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"3rdGenTasks andActors","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7hIMsn7WDpX9zGqRwfZnvu/0f04538b77d8cc926e7021a3be138720/3rdGenTasks_andActors.png","details":{"size":148336,"image":{"width":1366,"height":922}},"fileName":"3rdGenTasks andActors.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}}],"hideIntro":true,"recommendations":[]}},"url":"https://www.anyscale.com/blog/deep-dive-data-ingest-in-a-third-generation-ml-architecture","ctaText":"Read more","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"qyo3m0dc8CvMNCfhxDy5e","type":"Asset","createdAt":"2022-03-24T21:30:36.648Z","updatedAt":"2022-03-24T21:30:36.648Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-code-dark","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/qyo3m0dc8CvMNCfhxDy5e/409ed6f79aea5822b10aaa365318a005/blog-recommended-content-code-dark.jpg","details":{"size":39745,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-code-dark.jpg","contentType":"image/jpeg"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7mGbmNOjhusYL5rRQl4O3T","type":"Entry","createdAt":"2022-03-25T22:15:11.684Z","updatedAt":"2022-03-29T00:10:02.325Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Why third-generation ML platforms are more performant","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3rZB4NJNYoF4vsTsX2K2Jm","type":"Entry","createdAt":"2021-10-06T16:00:05.641Z","updatedAt":"2022-06-22T16:20:14.305Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Why Third Generation ML Platforms are More Performant","slug":"why-third-generation-ml-platforms-are-more-performant","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12vKMQ3j3q45idk83ZoPYA","type":"Entry","createdAt":"2021-10-01T20:23:37.378Z","updatedAt":"2021-10-01T20:23:37.378Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Chen Shen","slug":"chen-shen"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2021-10-06","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"In a previous blog post, we defined a \"3rd generation ML platform\" as one that offered full programmability for ML workflows. Key to a 3rd generation platform is the concept of a programmable compute layer. In this blog, we report on emerging patterns of distributed compute we see in advanced ML platform workloads. We show how Ray, a leading programmable compute layer, improves performance by 3-9x in relevant production workloads.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"In a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"previous blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we defined a \"3rd generation ML platform\" as one that offered full programmability for ML workflows. Key to a 3rd generation platform is the","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" ","nodeType":"text"},{"data":{},"marks":[],"value":"concept of a ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"programmable compute layer","nodeType":"text"},{"data":{},"marks":[],"value":". In this blog, we report on emerging patterns of distributed compute we see in advanced ML platform workloads. We show how ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a leading programmable compute layer, improves performance by 3-9x in relevant production workloads.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ecYQnlH7n2Tcv8Ovf1IEZ","type":"Asset","createdAt":"2021-10-01T20:28:28.089Z","updatedAt":"2021-10-05T04:17:24.678Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"2ndVS3rdGeneration","description":"A \"2nd generation\" (left) vs \"3rd generation\" (right) ML platform. Light-blue boxes represent clusters, and light-purple represents libraries. The 3rd generation platform eliminates cluster compute silos and improves performance and programmability.","file":{"url":"//images.ctfassets.net/xjan103pcp94/6ecYQnlH7n2Tcv8Ovf1IEZ/79da29e55ec74c834913c2072c6ebf8d/Fig1.png","details":{"size":76050,"image":{"width":2156,"height":420}},"fileName":"Fig1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance Overheads in Second Generation Platforms","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed ML workflows are typically composed from a few types of compute patterns: collective (i.e., a set of processes communicating with each other like in ","nodeType":"text"},{"data":{"uri":"https://github.com/horovod/horovod"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"distributed SGD","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"), chaining (i.e., a ","nodeType":"text"},{"data":{"uri":"https://docs.metaflow.org/metaflow/basics#linear"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"sequential workflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" of tasks run one after the other), and nesting (i.e., tasks that kick off other tasks, commonly seen in ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"hyperparameter tuning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7KYcuMZ98l5DdtDi10vHxp","type":"Asset","createdAt":"2021-10-01T20:43:35.650Z","updatedAt":"2021-10-05T04:21:32.297Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"collectingChainingNesting","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7KYcuMZ98l5DdtDi10vHxp/594784d15b520df3924340af95bc0b86/CollectiveChainingNesting.png","details":{"size":64947,"image":{"width":2180,"height":570}},"fileName":"CollectiveChainingNesting.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"first generation platforms","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ML workflows were implemented as custom-built systems optimized for a specific workflow. In second generation platforms, flexibility was achieved by relying on ","nodeType":"text"},{"data":{"uri":"https://neptune.ai/blog/best-workflow-and-pipeline-orchestration-tools"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"workflow orchestrators","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to provide chaining and nesting, gluing together separate systems that internally implement high-optimized collective operations.","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":" ","nodeType":"text"},{"data":{},"marks":[],"value":"The following figure illustrates a 2nd generation distributed ML platform. Each step is run as a separate cluster by a workflow orchestrator (e.g., ","nodeType":"text"},{"data":{"uri":"https://engineering.fb.com/2016/05/09/core-data/introducing-fblearner-flow-facebook-s-ai-backbone/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FBLearner Flow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker Steps","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://metaflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Metaflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.kubeflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"KubeFlow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"60TSWgITq3zG5xHqP8z3Xi","type":"Asset","createdAt":"2021-10-01T20:44:24.648Z","updatedAt":"2021-10-05T04:24:47.644Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"WorkflowOrchestrator","description":"A \"2nd generation\" distributed ML platform. A workflow orchestrator is needed to connect components implemented as separate distributed systems, limiting the programmability--- and performance--- of the platform.","file":{"url":"//images.ctfassets.net/xjan103pcp94/60TSWgITq3zG5xHqP8z3Xi/badf3cbb8c64666b880e7b28d407e6bf/WorkflowOrchestrator.png","details":{"size":72420,"image":{"width":1676,"height":420}},"fileName":"WorkflowOrchestrator.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, this 2nd generation architecture imposes performance overheads and limits programmability. This is due to:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Scheduling overheads. ","nodeType":"text"},{"data":{},"marks":[],"value":"These platforms rely on separately scheduled VMs or containers per step to distribute the workload. Each step may launch its own distributed framework (e.g., Spark or Distributed PyTorch). This leads to several seconds to minutes of scheduling overhead per step, and prevents optimizations like pipelining.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data movement overheads","nodeType":"text"},{"data":{},"marks":[],"value":". The overhead between steps can be reduced substantially if data is kept in memory between steps when possible and not materialized to storage.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Programmability overheads","nodeType":"text"},{"data":{},"marks":[],"value":". Expressing fine-grained nesting or pipelining can require substantial changes to distributed systems code that are rarely accessible to end-users of existing ML platforms.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Chaining and Nesting can be Performance Bottlenecks","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In many cases, chaining and nesting are ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"not","nodeType":"text"},{"data":{},"marks":[],"value":" performance bottlenecks. This is because the data transferred is small, or scheduling overhead is small compared to execution time. However, there are a growing number of scenarios where bottlenecks do arise. Here we overview several use cases that benefit from optimized chaining and nesting.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BYEdrkn0NDWNOEzPmtSIm","type":"Entry","createdAt":"2021-10-01T21:05:45.584Z","updatedAt":"2021-10-06T20:32:36.631Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":30,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"Compare 2nd vs 3rd Generation","body":"\n|  Use case   | 2nd generation platforms  | 3rd generation platforms | \n| ---------- | ---------- | ---------- |\n| __Data Ingest for Model Training (Chaining):__ Chaining data processing and ML training is simple when the data can fit onto a single machine. It becomes challenging at [large scale](https://arxiv.org/abs/2108.09373), when you want to [shuffle data globally](https://speakerdeck.com/anyscale/per-epoch-shuffling-data-loader-mix-it-up-as-you-train-clark-zinzow-anyscale) during training, or pipeline data preparation with training to [reduce latency](https://docs.ray.io/en/latest/data/dataset-pipeline.html). Although materializing data to cluster storage provides certain benefits, frequent I/O to cluster storage introduces a lot of overhead. | ![01_twineditSimpleChain](//images.contentful.com/xjan103pcp94/45y4oFh98EwSKMRaxzHcJ/93fee7c4c8c217df3a41173f52c6a733/01_twineditSimpleChain.png) | ![01_twineditPipelined](//images.contentful.com/xjan103pcp94/whcbju48HM91FOIIyMHrC/2e2b019b20786f16f84b6479dbeed31b/01_twineditPipelined.png)\n|  __Serving Pipelines (Chaining):__ Similarly, passing data efficiently is critical to the performance of disaggregated [model serving pipelines](https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns). In a disaggregated pipeline, large models are split into multiple parts that can be deployed and scaled separately (e.g., fully-connected networks vs memory-intensive embedding lookups). Predictions can be composed of multiple models in a DAG structure (e.g., [RoboVision](https://robovision.ai/) uses a 5-model stack for vehicle detection pipeline).   | ![02_2nd_4x2_22font_SimpleServing](//images.contentful.com/xjan103pcp94/4UyLFxAEkYrZUIP0AWDXbO/cf95c047d9d4df80066ff766b12ee006/02_2nd_4x2_22font_SimpleServing.png) | ![02_3rd_4x2_22font_DisaggregatedModelServingPipeline](//images.contentful.com/xjan103pcp94/30rncKJXFtnXBK12vcHKyD/7915253df3ff9c68c2faea56e4a45342/02_3rd_4x2_22font_DisaggregatedModelServingPipeline.png)\n|  __Batch Scoring (Nesting):__ Scoring a model on a large dataset can be quite slow without [distributed computation](https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/ai/batch-scoring-python), which naturally generates a nested workload during the course of training. Nesting can be difficult to express, let alone efficiently, in traditional workflow orchestrators.  | ![03_2nd_4x2_22font_SequentuilScoring](//images.contentful.com/xjan103pcp94/5uWbLSWU26EZp5Q9kkEMMh/41c443ee64006e8ee8d88eb20cd65f70/03_2nd_4x2_22font_SequentuilScoring.png)  | ![03_3rd_4x2_22font_DistributedScoring](//images.contentful.com/xjan103pcp94/1IxBk5YiSbdFPRUYneqBKy/8c39b2d0586bfa4af2ab84063a191433/Screen_Shot_2021-10-05_at_6.55.50_PM.png)\n| __Hyperparameter Tuning (Nesting):__ Nesting also occurs naturally during hyperparameter tuning, which seeks to explore many variants of existing workloads. Nesting is needed for [distributed trials](https://docs.ray.io/en/latest/tune/index.html), and efficiency is important for supporting lightweight trials and [population based approaches](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-scheduler-pbt).  | ![04_2nd_4x2_22font_SimpleNesting](//images.contentful.com/xjan103pcp94/6BHEgjIjRWRIymCYfQ1xKc/234d498153325597f75936d9e95cbb96/04_2nd_4x2_22font_SimpleNesting.png)  | ![04_3rd_4x2_22font_DistributedtrialsPBP](//images.contentful.com/xjan103pcp94/2oh4v2FUq0C5RS2FlwBSah/5708b8ccf119b334af0b4584921633a3/04_3rd_4x2_22font_DistributedtrialsPBP.png)\n| __Workflow DAGs (Both Chaining and Nesting):__ Finally, both chaining and nesting occur naturally in higher level workflow DAGs implemented by workflow orchestrators. Here two bottlenecks can occur: (1) if large amounts of data is passed or shared between workflow steps, it is desirable to pass data at memory-speed rather than reading and writing to cluster storage, and (2) if steps are small, scheduling overhead can dominate workflow run time. | ![05_2nd_4x2_22font_DAG](//images.contentful.com/xjan103pcp94/5pHdKYmzsuBM6WAAa2WAur/b45d46823841e0d089d2d80628bb7265/05_2nd_4x2_22font_DAG.png)  | ![05_3rd_4x2_22font_inMemoryLow-overhead](//images.contentful.com/xjan103pcp94/DZmefXHDWCnvBWm0m05wG/aa46b9911cf20c6f4281a8e3763592a9/05_3rd_4x2_22font_inMemoryLow-overhead.png)"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Third Generation Platforms Accelerate Chaining and Nesting","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"A ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"third generation ML platform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" eliminates the above performance bottlenecks. Users and builders of third generation platforms are able to:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Implement chaining and nesting of distributed steps with minimal scheduling and data movement overheads.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Programmatically author ML workflows, weaving together steps like ingest, transform, and training without needing to wrangle separate distributed systems.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is possible with the use of a ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"programmable compute layer ","nodeType":"text"},{"data":{},"marks":[],"value":"such as ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which can serve as a replacement to (or ","nodeType":"text"},{"data":{"uri":"https://www.astronomer.io/blog/airflow-ray-data-science-story"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"accelerator for","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") workflow orchestrators. In a 3rd gen platform, distributed logic such as ","nodeType":"text"},{"data":{"uri":"https://medium.com/distributed-computing-with-ray/data-processing-support-in-ray-ae8da34dce7e"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"data processing","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is implemented as ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/ray-libraries.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"libraries","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" within the compute layer:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5b9o8llgwycHgDtV9c1r8m","type":"Asset","createdAt":"2021-10-04T20:57:30.411Z","updatedAt":"2021-10-04T20:57:30.411Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"3rdGenDistributed","description":"A \"3rd generation\" distributed ML platform. The programmable compute layer allows distributed steps to be tightly woven together in code, eliminating the overheads of separate clusters.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5b9o8llgwycHgDtV9c1r8m/ab3f90039ba3ac7cf2577c1658aad85c/3rdGenDistributed.png","details":{"size":33191,"image":{"width":720,"height":216}},"fileName":"3rdGenDistributed.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Benchmarks","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this section we highlight some of the performance gains users have seen by leveraging Ray's support for efficient chaining and nesting of computations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Uber: Shuffled ML Ingest Pipeline","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a previous blog post, Uber introduced how they were leveraging Ray for ","nodeType":"text"},{"data":{"uri":"https://eng.uber.com/horovod-ray/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"elastic scheduling and training with Horovod","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". In a follow-up project, another team is using Ray's ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/dataset-pipeline.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Dataset Pipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to implement shuffled ML ingest, where data is globally shuffled across cluster CPU workers per iteration of GPU training. Here we analyze the importance of pipelining and in-memory data exchange. We ran a training workload reading 500GB of data on a cluster of 70 CPU nodes and 16 GPU nodes, and find ingest throughput is 3x higher with the pipelining and in-memory data exchange enabled by Ray:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"soBLd3ZStUVqfRH8BCxtR","type":"Asset","createdAt":"2021-10-04T20:59:23.745Z","updatedAt":"2021-10-05T03:52:09.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Shuffled Ingest","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/soBLd3ZStUVqfRH8BCxtR/bafe8e1c993e605a88ae521cdfc99584/Shuffled_Ingest_Throughput.png","details":{"size":88039,"image":{"width":1938,"height":810}},"fileName":"Shuffled Ingest Throughput.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Wildlife Studios: Chained Model Pipelines","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Mobile gaming giant ","nodeType":"text"},{"data":{"uri":"https://wildlifestudios.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Wildlife Studios’","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" legacy system for serving revenue-generating in-game offers was not scaling to meet their latency and cost requirements. After switching to Ray Serve, their Dynamic Offers team was able to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/wildlife-studios-serves-in-game-offers-3x-faster-at-1-10th-the-cost-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"serve offers three times faster","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". The Ray Serve architecture provided support for parallel inference on multiple models in a pipeline, decreasing latency and minimizing idle machines:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1qt32KDRT1K8MzgWLfQ2iq","type":"Asset","createdAt":"2021-10-04T21:00:42.387Z","updatedAt":"2021-10-05T03:52:32.315Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"p95","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1qt32KDRT1K8MzgWLfQ2iq/eaae25d1ae4d466294c568fbe380a0e6/P95_Latency_in_Production_Cluster.png","details":{"size":82299,"image":{"width":1804,"height":784}},"fileName":"P95 Latency in Production Cluster.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Anastasia.ai: Nested Model Evaluation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://anastasia.ai/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Anastasia","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provides a powerful platform that enables organizations to operate AI capacities at scale with a fraction of the resources and effort traditionally required. They were able to accelerate a demand prediction problem using ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and got up to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-anastasia-implements-ray-and-anyscale-to-speed-up-ml-processes-9x"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"9x performance gains","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" due to fine-grained re-use of resources compared to a coarse grained orchestrator:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4ASH8c3Ia98IGL4o1u1FSS","type":"Asset","createdAt":"2021-10-04T21:02:21.018Z","updatedAt":"2021-10-05T03:58:00.087Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"RelativeTimeEvaluate384CoreCluster","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4ASH8c3Ia98IGL4o1u1FSS/5f9db5896146bc065699d6caacb5739e/Relative_Time_to_Evaluate_on_384-core_Cluster.png","details":{"size":75697,"image":{"width":2020,"height":720}},"fileName":"Relative Time to Evaluate on 384-core Cluster.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog, we showed how a programmable compute layer such as Ray can provide 3-9x performance improvements for production ML workloads, eliminating bottlenecks found in 2nd generation production architectures. This is in addition to the productivity and operational benefits of having a programmable architecture.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"As a final note, the ideas presented here apply generally to distributed programming as well as to ML workloads. If you're interested in the performance and programmability of ML applications and distributed computing, check out the ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray project","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and consider ","nodeType":"text"},{"data":{"uri":"https://jobs.lever.co/anyscale"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"joining us","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fTEska4fx1SFm7wlBB7JF","type":"Asset","createdAt":"2021-10-06T16:49:59.640Z","updatedAt":"2021-10-06T16:49:59.640Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"mlplatformCropped","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5fTEska4fx1SFm7wlBB7JF/cb6fc1348b8a28bc0b91ef9dfb080adb/mlplatformCropped.png","details":{"size":84401,"image":{"width":926,"height":500}},"fileName":"mlplatformCropped.png","contentType":"image/png"}}},"recommendations":[],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}}],"hideIntro":true}},"url":"https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant","ctaText":"Read more","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mI1ydQdbxC8VN9jq5SE9O","type":"Asset","createdAt":"2022-03-24T22:19:37.854Z","updatedAt":"2022-03-24T22:46:25.932Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-recommended-content-rl-robot-bubble-dark","file":{"url":"//images.ctfassets.net/xjan103pcp94/2mI1ydQdbxC8VN9jq5SE9O/b7509b9f4f23ddf47620aa465a919914/blog-recommended-content-rl-robot-bubble-dark.jpg","details":{"size":40730,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-rl-robot-bubble-dark.jpg","contentType":"image/jpeg"}}},"recommendations":null}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"59TsRspGZTiG6lcUWkrke1","type":"Entry","createdAt":"2022-12-18T00:47:12.553Z","updatedAt":"2023-02-23T15:52:47.946Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Training One Million Machine Learning Models in Record Time with Ray","seoTitle":"Training 1 Million ML Models in Record Time | Anyscale","slug":"training-one-million-machine-learning-models-in-record-time-with-ray","description":"More companies are needing to train \u0026 deploy many small ML models, often hundreds or thousands. Learn how Ray and Anyscale make that possible.","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4QtW45NTCt8MQYWOaE1TX2","type":"Entry","createdAt":"2020-09-14T18:41:27.491Z","updatedAt":"2022-08-23T02:59:22.436Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Robert Nishihara","slug":"robert-nishihara","link":"https://www.linkedin.com/in/robert-nishihara-b6465444/","bio":"Co-founder and CEO at Anyscale"}}],"publishedDate":"2022-12-17","intro":"Ray and Anyscale are used by companies like Instacart to speed up machine learning training workloads (often demand forecasting) by 10x compared with tools like Celery, AWS Batch, SageMaker, Vertex AI, Dask, and more.","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This blog focuses on scaling ","marks":[],"data":{}},{"nodeType":"text","value":"many model training","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":". While much of the buzz is around large model training, in recent years, more and more companies have found themselves needing to train and deploy many smaller machine learning models, often hundreds or thousands.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Our team has worked with hundreds of companies looking to scale machine learning in production, and this blog post aims to cover the motivation and some best practices for training many models. Using the approaches described here, companies have seen order-of-magnitude performance and scalability wins (e.g., ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.youtube.com/watch?v=3t26ucTy0Rs"},"content":[{"nodeType":"text","value":"12x for Instacart","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/how-anastasia-implements-ray-and-anyscale-to-speed-up-ml-processes-9x"},"content":[{"nodeType":"text","value":"9x for Anastasia","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":") relative to frameworks like Celery, AWS Batch, AWS SageMaker, Vertex AI, Dask, and more.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this blog, we’ll cover:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Why companies are doing many model training","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"How to use Ray to train multiple models","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The properties of Ray that enable efficient many model training","marks":[],"data":{}}]}]}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5LJcaD764pXNWoid6S0fiJ","type":"Asset","createdAt":"2022-12-15T05:36:31.358Z","updatedAt":"2022-12-15T05:36:31.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Many_model_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5LJcaD764pXNWoid6S0fiJ/7a9d050768f73b04603a440b43c9027d/image.png","details":{"size":222897,"image":{"width":612,"height":459}},"fileName":"image.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Why train many ML models?","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"While cutting edge applications of machine learning are leading to an explosion in model size, the need for many models cuts across industries.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"As a few examples:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Models per geographical zone at Instacart: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Instacart uses machine learning for a huge variety of tasks including delivery ETA prediction, planning the supply of couriers, and optimizing routing models for shoppers. For each use case, one model is trained for each zone, often a geographical location like a zip code. Separate models are also trained for different attributes like the time of the day. This results in tens of thousands of models. Using the approach in this blog post, ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.youtube.com/watch?v=3t26ucTy0Rs"},"content":[{"nodeType":"text","value":"Instacart reduced training times for these models by 12x compared to AWS batch and Celery","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", from days down to hours.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Search and ranking for ecommerce:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" Ecommerce companies we work with use machine learning for applications ranging from query suggestion to ranking to personalization. One of the most important use cases is product classification, in which every new product sold on the site is tagged with hundreds or thousands of attributes (e.g., soft, green, kitchen, holiday, …) often based on pictures of the product. These attributes are used to improve product search. It’s very common to use one model per attribute, leading to an explosion of models being trained and deployed.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Per customer and per product forecasting and modeling:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" One B2B analytics company we work with trains per-customer models to forecast growth as well as churn. As a result, they need to frequently retrain and redeploy thousands of models. Another multinational manufacturing company trains predictive models for each of tens of thousands of products. Using the approach in this blog post, both companies were able to achieve significant scalability and performance wins relative to their previous solutions, often by an order of magnitude. As another forecasting example, ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://nixtla.github.io/statsforecast/examples/ets_ray_m5.html"},"content":[{"nodeType":"text","value":"Nixtla ran a benchmark training 30,000 models","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". In a separate experiment, they trained ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/how-nixtla-uses-ray-to-accurately-predict-more-than-a-million-time-series"},"content":[{"nodeType":"text","value":"one million models in 30 minutes","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Finding the best model among many (hyperparameter tuning): ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Another B2B analytics company we work with experiments with thousands of types of models (both different types of models as well as different model hyperparameters) so that they can choose the best one, and they do this for every single one of their customers. This is essentially a massive hyperparameter tuning job.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Let’s go over the techniques these companies use to achieve these results. We’ll describe a couple of the most popular methods.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"These approaches use Ray to scale many model training. For an overview of Ray, check out the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/"},"content":[{"nodeType":"text","value":"Ray documentation","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" or ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://towardsdatascience.com/modern-parallel-and-distributed-python-a-quick-tutorial-on-ray-99f8d70369b8?gi=55808edf2eda"},"content":[{"nodeType":"text","value":"this introductory blog post","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Later in this blog post, we include links showing how companies like OpenAI, Uber, Shopify, Instacart, Netflix, Lyft, Cruise, and Bytedance are scaling their machine learning workloads with Ray.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"How to train many models at scale","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"While there are a growing number of ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.uber.com/blog/horovod-ray/"},"content":[{"nodeType":"text","value":"blog posts","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/train/examples.html"},"content":[{"nodeType":"text","value":"tutorials","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on the challenges of training ","marks":[],"data":{}},{"nodeType":"text","value":"large","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" ML models, there are considerably fewer covering the details and approaches for training ","marks":[],"data":{}},{"nodeType":"text","value":"many","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" ML models. We’ve seen a huge variety of approaches ranging from services like AWS Batch, SageMaker, and Vertex AI to homegrown solutions built around open source tools like Celery or Redis.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray removes a lot of the performance overhead of handling these challenging use cases, and as a result users often report significant performance gains when switching to Ray. Here we’ll go into the next level of detail about how that works.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Approach 1: Using Ray Core (1 task per file)","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Suppose you already have a big list of model training configurations (e.g., paths to individual model data files for each zip code or SKU) and just need to parallelize execution of a model training function. Let's run this with Ray. In the following example, we'll train 1 million scikit-learn models on different input files.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"First, the serial implementation:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5YVIp0eVXQrWKXW95vFsD","type":"Entry","createdAt":"2022-12-15T05:39:43.488Z","updatedAt":"2022-12-15T05:39:43.488Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Many_model_blog-serial_implementation","body":"# For reading from cloud storage paths.\nfrom smart_open import smart_open\nimport pandas as pd\n\ndef train_model(file_path: str):\n    data = pd.read_csv(smart_open(file_path, \"r\"))\n\n    ## Train your model here.\n    from sklearn.linear_model import LinearRegression\n    lr = LinearRegression()\n    # (Column names are anonymized)\n    lr.fit(data[[\"id4\", \"id5\"]], data[\"v3\"])\n\n    ## Write outputs.\n    # smart_open(output, \"w\").write(pickle.dumps(lr))\n\nmodels_to_train = [\n\tf\"s3://air-example-data/h2oai_1m_files/file_{i:07}.csv\"\n\tfor i in range(1000000)\n]\n\n# This will take much too long serially.\nfor file in models_to_train:\n    print(\"Training model serially\", file)\n    train_model(file)","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We could parallelize this directly using ","marks":[],"data":{}},{"nodeType":"text","value":"ray.remote","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" functions, but to make things easier we'll use Ray's multiprocessing library--- one of Ray's distributed libraries:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1aBo2oVxAqhuAjpePckL0a","type":"Entry","createdAt":"2022-12-15T05:41:00.410Z","updatedAt":"2022-12-15T05:41:00.410Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Many_model_blog-ray_multiprocessing_implementation","body":"from ray.util.multiprocessing import Pool\nimport tqdm\n\n# Create a pool, where each worker is assigned 1 CPU by Ray.\npool = Pool(ray_remote_args={\"num_cpus\": 1})\n\n# Use the pool to run `train_model` on the data, in batches of 10.\niterator = pool.imap_unordered(train_model, models_to_train, chunksize=10)\n\n# Track the progress using tqdm and retrieve the results into a list.\nlist(tqdm.tqdm(iterator, total=1000000))","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Wrapping the call to the pool will give us a nice progress bar to monitor progress. Internally, Ray dispatches tasks to workers in the cluster, automatically handling issues such as fault tolerance and batching optimizations:","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We can see in the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-core/ray-dashboard.html"},"content":[{"nodeType":"text","value":"Ray dashboard","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" the above job takes just a few minutes to run on a 10-node cluster. Each task reads the input file from S3 before training the model (a no-op in the example code above):","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3W5qPqoYD5VEaVUXgPRrg7","type":"Asset","createdAt":"2022-12-15T05:54:21.146Z","updatedAt":"2022-12-15T05:54:21.146Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Many_model_blog-combined_plot","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3W5qPqoYD5VEaVUXgPRrg7/6ee656d0f757dcda6d87dfabdcd4860f/image.png","details":{"size":388081,"image":{"width":1828,"height":824}},"fileName":"image.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It takes about 10 minutes to train 1 million models from individual data files using Ray's multiprocessing library. That was pretty simple. But what if your data isn't already organized by model?","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Approach 2: Using Ray Data (grouping data by key)","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Often your data will be in some other format, such as parquet files (e.g., the output of a separate ETL job), and will not be already partitioned by model. You could turn to a separate Spark cluster to generate those nice single-model files we had in Approach 1.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"To keep things in the same script, here we'll show how to use ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"nodeType":"text","value":"Ray's Dataset library","marks":[],"data":{}}]},{"nodeType":"text","value":" to group a dataset by customer ID, prior to training a model on each customer. First, we load the data using the ","marks":[],"data":{}},{"nodeType":"text","value":"read_csv","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" method of Ray Datasets, and group it by our desired grouping key:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"VDuXzXGJL2033Kazd3IZl","type":"Entry","createdAt":"2022-12-15T05:47:11.728Z","updatedAt":"2022-12-15T05:47:11.728Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Many_model_blog-read_data","body":"import ray\n\nds = ray.data.read_csv(\"s3://air-example-data/h2oai_benchmark\")\n# Repartition the single CSV into 500 blocks to increase the parallelism.\nds = ds.repartition(500)\n\n# Compute our (dummy) `customer_id` key as the concatenation of the\n# `id3` and `v1` columns and then group the dataset by it.\nds = ds.add_column(\"customer_id\", lambda r: r[\"id3\"] + r[\"v1\"].astype(str))\nds = ds.groupby(\"customer_id\")","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We'll use the special ","marks":[],"data":{}},{"nodeType":"text","value":"map_groups()","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" function on GroupedDataset, which takes in a batch of data and returns one or more results. We'll return a single trained model for each batch of data:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3mW0tWsrzmwCKfTOlnTCOO","type":"Entry","createdAt":"2022-12-15T05:47:56.587Z","updatedAt":"2022-12-18T00:39:18.926Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Many_model_blog-data_train","body":"def train_model(data):\n    ## Train your model here.\n    from sklearn.linear_model import LinearRegression\n    lr = LinearRegression()\n    lr.fit(data[[\"id4\", \"id5\"]], data[\"v3\"])\n\n    # Return a single dict of the model and stats, etc.\n    return [{\n        \"coef\": lr.coef_,\n        \"intercept\": lr.intercept_,\n        \"customer_id\": data[\"customer_id\"][0],\n    }]\n\n# Execute the model training.\nds = ds.map_groups(train_model)\n\n# Write the results to destination files.\n# ds.write_parquet(\"s3://path/to/output\")","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"When this executes, Ray's Dataset library will execute the distributed shuffle operations needed to group the data by customer ID, and then can parallel apply the given model training function to generate the model.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The takeaway here is that Ray's swiss army knife of libraries allows us to scale this task to a cluster in just Python, without having to wrangle multiple distributed systems or services.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"How to Train Multiple Larger Models","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Suppose we wanted to train multiple larger models, each requiring a GPU, or perhaps multiple processes to train in parallel. Ray's resource-based scheduling lets us handle that seamlessly.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"For models requiring GPU resources, we could tell Ray to schedule tasks onto GPUs by specifying ","marks":[],"data":{}},{"nodeType":"text","value":"num_gpus=1","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" as a remote arg. For example,","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4drU9N8D3iCpKufSB05Y8z","type":"Entry","createdAt":"2022-12-15T05:49:45.510Z","updatedAt":"2022-12-15T05:49:45.510Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Many_model_training-resource_example","body":"# Create a pool that assigns each worker a GPU.\nPool(ray_remote_args={\"num_gpus\": 1})\n\n# Execute the model training, telling Ray to assign 1 GPU per task.\nds = ds.map_groups(train_model, num_gpus=1)","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Similarly, if we wanted to use multiple CPUs per model (e.g., configuring sklearn to use 8 CPUs per model), we could set ","marks":[],"data":{}},{"nodeType":"text","value":"num_cpus=8","marks":[{"type":"code"}],"data":{}},{"nodeType":"text","value":" as a remote arg to tell Ray to reserve 8 CPU slots per training task:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6HIeKhdALvBcDZpZW5SCVW","type":"Entry","createdAt":"2022-12-15T05:50:30.357Z","updatedAt":"2022-12-15T05:50:30.357Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Many_model_blog-cpu_resource_example","body":"# Create a pool that assigns each worker 8 CPUs.\nPool(ray_remote_args={\"num_cpus\": 8})\n\n# Execute the model training, telling Ray to reserve 8 CPUs per task.\nds = ds.map_groups(train_model, num_cpus=8)","language":"python"}}},"content":[]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Using Ray AIR for distributed training","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"For higher-level distributed training APIs, you may want to pull from ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-air/getting-started.html"},"content":[{"nodeType":"text","value":"Ray AIR","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" libraries such as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/train/train.html"},"content":[{"nodeType":"text","value":"Ray Train","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"nodeType":"text","value":"Ray Tune","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", which are designed to manage the execution of multiple computationally intensive model training jobs. Train runs ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/train/train.html"},"content":[{"nodeType":"text","value":"single distributed training jobs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" and can work with Tune to run multiple of these jobs at once.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Tune can also run many simple model training jobs, but is a bit less scalable in this dimension than the other approaches (think hundreds of models instead of millions) . Here's an example of using Tune to execute the Approach 1 example from above (see also the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/tune/tutorials/tune-run.html"},"content":[{"nodeType":"text","value":"Tune Experiments User Guide","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"):","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7048vvDoug0Vr7heoLlRz5","type":"Entry","createdAt":"2022-12-15T05:51:09.061Z","updatedAt":"2022-12-15T05:51:09.061Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Many_model_blog-tune_example","body":"from ray import tune\n\n# For reading from cloud storage paths.\nfrom smart_open import smart_open\nimport pandas as pd\n\ndef trainable_func(config: dict):\n    data = pd.read_csv(smart_open(config[\"file_path\"], \"r\"))\n\n    ## Train your model here.\n    from sklearn.linear_model import LinearRegression\n    lr = LinearRegression()\n    lr.fit(data[[\"id4\", \"id5\"]], data[\"v3\"])\n\n    # Return a single dict of the model and stats, etc.\n    return {\n        \"coef\": lr.coef_,\n        \"intercept\": lr.intercept_,\n        \"customer_id\": data[\"customer_id\"][0],\n    }\n\n# Tune is designed for up to thousands of trials.\nparam_space = {\n    \"file_path\": tune.grid_search([\n\tf\"s3://air-example-data/h2oai_1m_files/file_{i:07}.csv\"\n\tfor i in range(1000)\n    ])\n}\n\ntuner = tune.Tuner(trainable_func, param_space=param_space)\nprint(tuner.fit())","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Compared to directly using Ray tasks, Tune ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/tune/tutorials/tune-run.html#comparison-to-ray-remote"},"content":[{"nodeType":"text","value":"offers","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":":","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Status reporting and tracking, including integrations and callbacks to common monitoring tools.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Checkpointing of trials for fine-grained fault-tolerance.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Gang scheduling of multi-worker trials.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"To get started training many models using Ray, follow the ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-overview/use-cases.html"},"content":[{"nodeType":"text","value":"end-to-end tutorials","marks":[{"type":"bold"}],"data":{}}]},{"nodeType":"text","value":" to build your project.","marks":[{"type":"bold"}],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Why Ray is great for training many models","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"To summarize, Ray is great for many-model training for a few reasons.","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Flexible scheduling","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": Ray provides lower overhead for per-model training task (milliseconds, instead of seconds to minutes). This is in contrast to training frameworks like SageMaker that launch docker containers or nodes per task. In the examples above, Ray is able to multiplex tasks onto individual processes, for lower per-task overhead.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Unification:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" Ray's collection of built-in libraries means you can performantly handle both data preprocessing and training with just a few lines of Python, instead of spinning up new distributed systems. This is what enables the example in Approach 2 to run as a single Ray script.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray also excels at other use cases that stretch the boundaries of existing frameworks. For example, distributed training of large single models, or distributed serving of many different models. More on these topics in future blog posts.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"More about Ray","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"More relevant material for learning about Ray","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"To get started with Ray, check out our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray"},"content":[{"nodeType":"text","value":"GitHub","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/"},"content":[{"nodeType":"text","value":"documentation","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" or ask questions on our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"forums","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In addition to the use cases described in this blog, Ray includes libraries for scaling ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/train/train.html"},"content":[{"nodeType":"text","value":"training","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/rllib/index.html"},"content":[{"nodeType":"text","value":"reinforcement learning","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"nodeType":"text","value":"hyperparameter tuning","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"nodeType":"text","value":"model serving","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"nodeType":"text","value":"data ingest \u0026 processing","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Read the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/many-models-batch-training-at-scale-with-ray-core"},"content":[{"nodeType":"text","value":"sequel blog ","marks":[],"data":{}}]},{"nodeType":"text","value":"that demonstrates how to train thousands of models per feature such as geographical location in record time. ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Learn about how ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/ray-overview/ray-libraries.html"},"content":[{"nodeType":"text","value":"Ray integrates with the rest of the machine learning ecosystem","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", including training frameworks like PyTorch, TensorFlow, Horovod, XGBoost, Scikit-learn, Hugging Face, and LightGBM. Ray also integrates with MLOps frameworks like Weights \u0026 Biases, MLFlow, Arize and data platforms like Snowflake and Databricks.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Learn how ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.youtube.com/watch?v=CqiL5QQnN64\u0026list=PLzTswPQNepXmLUiL4F_1VHrPcCz1OeILw\u0026index=3"},"content":[{"nodeType":"text","value":"OpenAI","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" trains their largest models including ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://openai.com/blog/chatgpt/"},"content":[{"nodeType":"text","value":"ChatGPT","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" using Ray.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"To learn more about how companies like ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.youtube.com/watch?v=GUxIzOpHfuk\u0026list=PLzTswPQNepXmLUiL4F_1VHrPcCz1OeILw\u0026index=5"},"content":[{"nodeType":"text","value":"Uber","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.youtube.com/watch?v=kbvzvdKH7bc\u0026list=PLzTswPQNepXmLUiL4F_1VHrPcCz1OeILw\u0026index=6"},"content":[{"nodeType":"text","value":"Shopify","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.youtube.com/watch?v=For8yLkZP5w\u0026list=PLzTswPQNepXmLUiL4F_1VHrPcCz1OeILw\u0026index=7"},"content":[{"nodeType":"text","value":"Cohere","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/ray-summit-2022/agenda/sessions/111"},"content":[{"nodeType":"text","value":"Netflix","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/ray-summit-2022/agenda/sessions/187"},"content":[{"nodeType":"text","value":"Lyft","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/ray-summit-2022/agenda/sessions/194"},"content":[{"nodeType":"text","value":"Cruise","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/ray-summit-2022/agenda/sessions/166"},"content":[{"nodeType":"text","value":"Bytedance","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", and others are building on Ray, check out the talks from the recent ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/ray-summit-2022/agenda"},"content":[{"nodeType":"text","value":"2022 Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5LJcaD764pXNWoid6S0fiJ","type":"Asset","createdAt":"2022-12-15T05:36:31.358Z","updatedAt":"2022-12-15T05:36:31.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Many_model_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5LJcaD764pXNWoid6S0fiJ/7a9d050768f73b04603a440b43c9027d/image.png","details":{"size":222897,"image":{"width":612,"height":459}},"fileName":"image.png","contentType":"image/png"}}},"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4vczdRU8OXqZutjMAsuIUV","type":"Entry","createdAt":"2022-02-14T17:00:58.083Z","updatedAt":"2022-06-22T15:58:47.605Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Ray Datasets for large-scale machine learning ingest and scoring","slug":"ray-datasets-for-machine-learning-training-and-scoring","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48lK0gPUqCqrmxWiQi8ve8","type":"Entry","createdAt":"2021-02-16T07:25:36.331Z","updatedAt":"2021-02-16T07:25:36.331Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Clark Zinzow","slug":"clark-zinzow","link":"https://www.linkedin.com/in/clarkzinzow/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3MwXLiQy2kRWe7mVi2RjRw","type":"Entry","createdAt":"2021-02-16T07:28:19.034Z","updatedAt":"2021-02-16T07:28:19.034Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Alex Wu","slug":"alex-wu","link":"https://github.com/wuisawesome"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6TqWiyUibvC9ETfyYljP8b","type":"Entry","createdAt":"2022-02-11T15:42:52.165Z","updatedAt":"2022-02-11T15:42:52.165Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jiajun Yao","slug":"jiajun-yao"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12vKMQ3j3q45idk83ZoPYA","type":"Entry","createdAt":"2021-10-01T20:23:37.378Z","updatedAt":"2021-10-01T20:23:37.378Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Chen Shen","slug":"chen-shen"}}],"publishedDate":"2022-02-14","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},"intro":"We're happy to introduce Ray Datasets: A data loading and preprocessing library built on Ray that leverages Ray’s task, actor, and object APIs to enable large-scale machine learning ingest, training, and inference within a single Python application.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"We're happy to introduce Ray Datasets, a data loading and preprocessing library built on Ray. Datasets leverages Ray’s task, actor, and object APIs to enable large-scale machine learning (ML) ingest, training, and inference, all within a single Python application.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Dus0MGXn2d7LxspFtlL2Y","type":"Asset","createdAt":"2022-02-11T15:48:59.662Z","updatedAt":"2022-02-11T15:48:59.662Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-ray-datasets-1","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6Dus0MGXn2d7LxspFtlL2Y/a0adc3d04cd940c35518d2da8a6dd1bc/blog-ray-datasets-1.png","details":{"size":54803,"image":{"width":863,"height":227}},"fileName":"blog-ray-datasets-1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a nutshell, Datasets:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Is the standard way to ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"load distributed data into Ray","nodeType":"text"},{"data":{},"marks":[],"value":", supporting popular storage backends and file formats.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Supports common ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"ML preprocessing operations ","nodeType":"text"},{"data":{},"marks":[],"value":"including basic parallel data transformations such as map, batched map, and filter, and global operations such as sort, shuffle, groupby, and stats aggregations.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Supports operations requiring stateful setup and GPU acceleration.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Works seamlessly with Ray-integrated ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"data processing libraries","nodeType":"text"},{"data":{},"marks":[],"value":" (Spark, Pandas, NumPy, Dask, Mars) and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"ML frameworks","nodeType":"text"},{"data":{},"marks":[],"value":" (TensorFlow, Torch, Horovod).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we will survey the current state of distributed training and model scoring pipelines and give an overview of Ray Datasets and how it solves problems present in the status quo. If that leaves you wanting more, be sure to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/events/2022/02/16/ray-datasets-scalable-data-preprocessing-for-distributed-ml"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"register for our upcoming webinar","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", where you can get a first-hand look at Datasets in action. With that, let’s dive in!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Current ML training and inference pipelines","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The status quo pipeline","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Today, users often stitch together their training pipeline using various distributed compute frameworks. While this approach has its advantages in re-using existing systems, there are several drawbacks, which we covered in our blog post on ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/deep-dive-data-ingest-in-a-third-generation-ml-architecture"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"data ingest in third-gen ML architectures","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5x3G3Dh3D8MYyPDW8nPYYd","type":"Asset","createdAt":"2022-02-11T15:50:18.588Z","updatedAt":"2022-02-11T15:50:18.588Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-ray-datasets-2","description":"Training pipeline status quo: workflow orchestration frameworks are required to orchestrate this multi-language, multi-job pipeline with intermediate data persistence.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5x3G3Dh3D8MYyPDW8nPYYd/28600a4cc8f3e9957189157502f8ce95/blog-ray-datasets-2.png","details":{"size":28054,"image":{"width":481,"height":229}},"fileName":"blog-ray-datasets-2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The vision","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray is enabling ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"simple Python scripts","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to replace these pipelines, avoiding their tradeoffs and also ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"improving performance","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Ray Datasets are a key part of this vision, acting as the \"distributed Arrow\" format for exchanging data between distributed steps in Ray.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introduction to Ray Datasets","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Datasets in a nutshell","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Datasets is fundamentally a distributed dataset abstraction, where the underlying data blocks (partitions) are distributed across a Ray cluster, sitting in distributed memory.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4VXwe4LNX21gLTE3kyHHi7","type":"Asset","createdAt":"2022-02-11T15:50:55.408Z","updatedAt":"2022-02-11T15:50:55.408Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-ray-datasets-3","description":"A Dataset holds references to one or more in-memory data blocks, distributed across a Ray cluster.","file":{"url":"//images.ctfassets.net/xjan103pcp94/4VXwe4LNX21gLTE3kyHHi7/caa38501011c089e58e79c081e7c646d/blog-ray-datasets-3.png","details":{"size":13891,"image":{"width":342,"height":257}},"fileName":"blog-ray-datasets-3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This distributed representation allows for the Dataset to be built by distributed parallel tasks, each pulling a block’s worth of data from a source (e.g., S3) and putting the block into the node’s local object store, with the client-side Dataset object holding references to the distributed blocks. Operations on the client-side Dataset object then result in parallel operations on those blocks.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A Dataset’s blocks can contain data of any modality, including text, arbitrary binary bytes (e.g., images), and numerical data; however, the full power of Datasets is unlocked when used with tabular data. In this case, each block consists of a partition of a distributed table, and these row-based partitions are represented as Arrow Tables under the hood, yielding a distributed Arrow dataset.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3gyPc0fA7WxX87umcI9XcJ","type":"Asset","createdAt":"2022-02-11T15:51:31.856Z","updatedAt":"2022-02-11T15:51:31.856Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-ray-datasets-4","description":"Visualization of a Dataset that has three Arrow table blocks, with each block holding 1000 rows.","file":{"url":"//images.ctfassets.net/xjan103pcp94/3gyPc0fA7WxX87umcI9XcJ/eb227f90cb147ca2d2036f1cb1dc9f71/blog-ray-datasets-4.png","details":{"size":83115,"image":{"width":1467,"height":566}},"fileName":"blog-ray-datasets-4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"How does Datasets fit into my training pipeline?","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Datasets is ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"not","nodeType":"text"},{"data":{},"marks":[],"value":" intended as a replacement for generic data processing systems like Spark. Datasets is meant to be the last-mile bridge between ETL pipelines and distributed applications running on Ray.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Ev5f6vUnfU2puvaXpTGqK","type":"Asset","createdAt":"2022-02-11T15:52:09.183Z","updatedAt":"2022-02-11T15:52:09.183Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-ray-datasets-5","description":"Ray Datasets is the last-mile data bridge to a Ray cluster.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Ev5f6vUnfU2puvaXpTGqK/fedc6e6bd286b2e9a99ac6fe189a3b72/blog-ray-datasets-5.png","details":{"size":73764,"image":{"width":1469,"height":459}},"fileName":"blog-ray-datasets-5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This bridge becomes extra powerful when using Ray-integrated DataFrame libraries for your data processing stage, as this allows you to run a full data-to-ML pipeline on top of Ray, eliminating the need to materialize data to external storage as an intermediate step. Ray serves as the universal compute substrate for your ML pipeline, with Datasets forming the distributed data bridge between pipeline stages.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2phJrxN7J0EZTgyNHp8Dl3","type":"Asset","createdAt":"2022-02-11T15:52:42.026Z","updatedAt":"2022-02-11T15:52:42.026Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-ray-datasets-6","description":"When the entire pipeline runs on Ray, distributed data can seamlessly pass from relational data processing to model training without touching a disk or centralized data broker.","file":{"url":"//images.ctfassets.net/xjan103pcp94/2phJrxN7J0EZTgyNHp8Dl3/78cf77af5c38e7ad2d2b6ebe5d8f4b98/blog-ray-datasets-6.png","details":{"size":85323,"image":{"width":1455,"height":428}},"fileName":"blog-ray-datasets-6.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Check out our ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/deep-dive-data-ingest-in-a-third-generation-ml-architecture"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"blog post on ingest in third-generation ML architectures","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for more on how this works under the hood.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Basic features","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scalable parallel I/O","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"Datasets aims to be a universal parallel data loader, data writer, and exchange format, providing a narrow data waist for Ray applications and libraries to interface with.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3lgwQGVC9HiTL7jDsHiM9W","type":"Asset","createdAt":"2022-02-11T15:53:13.835Z","updatedAt":"2022-02-11T15:53:13.835Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-ray-datasets-7","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3lgwQGVC9HiTL7jDsHiM9W/c06e7f31b2a371372d1d2088e988490c/blog-ray-datasets-7.png","details":{"size":49443,"image":{"width":595,"height":366}},"fileName":"blog-ray-datasets-7.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is accomplished by heavily leveraging Arrow’s I/O layer, using Ray’s high-throughput task execution to parallelize Arrow’s high-performance single-threaded I/O. The Datasets I/O layer has scaled to ","nodeType":"text"},{"data":{"uri":"https://www.youtube.com/watch?v=h7svj_oAY14"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"multi-petabyte data ingest jobs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in production at Amazon.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Datasets’ scalable I/O is all available behind a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"dead-simple API","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", expressible via a single call: ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.data.read_\u003cformat\u003e()","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Data format compatibility","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Arrow’s I/O layer comes support for many of your favorite tabular file formats (JSON, CSV, Parquet) and storage backends (local disk, S3, GCS, Azure Blog Storage, HDFS). Beyond tabular data, we’ve added support for parallel reads and writes of NumPy, text, and binary files. This comprehensive support for reading many formats from ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html#datasource-compatibility-matrices"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"many external sources","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", coupled with the extremely scalable parallelization scheme, makes Datasets the preferred way to ingest large amounts of data into a Ray cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2NWFJyy9Ei50jXhIjIi8KL","type":"Entry","createdAt":"2022-02-17T15:11:04.059Z","updatedAt":"2022-02-17T15:15:52.128Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"Ray Datasets blog code example TEST","body":"\u003cscript src=\"https://gist.github.com/chandler-anyscale/7e363fe70d6fb7b3a2d53adb8d2b80de.js\"\u003e\u003c/script\u003e","classNames":"gray_background","styling":["condensed"]}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1PThAXsbQlcnwFMIMTDZ2Y","type":"Entry","createdAt":"2022-02-11T15:54:13.271Z","updatedAt":"2022-02-11T15:54:13.271Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Ray datasets code example 1","body":"# Read structured data from disk, cloud storage, etc.\nray.data.read_parquet(\"s3://path/to/parquet\")\nray.data.read_json(\"...\")\nray.data.read_csv(\"...\")\nray.data.read_text(\"...\")\n\n# Read tensor / image / file data.\nray.data.read_numpy(\"...\")\nray.data.read_binary_files(\"...\")\n\n# Create from in-memory objects.\nray.data.from_objects([list, of, python, objects])\nray.data.from_pandas([list, of, pandas, dfs])\nray.data.from_numpy([list, of, numpy, arrays])\nray.data.from_arrow([list, of, arrow, tables])"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Data framework compatibility","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"In addition to storage I/O, Datasets also allows for bidirectional in-memory data exchange with many popular distributed frameworks when they are run on Ray, such as Spark, Dask, Modin, and Mars, as well as Pandas and NumPy for small local in-memory data. For convenient ingestion of data into model trainers, Datasets provides an exchange API for both PyTorch and TensorFlow, yielding the familiar framework-specific datasets, ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"torch.util.data.IterableDataset","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://www.tensorflow.org/api_docs/python/tf/data/Dataset"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"tf.data.Dataset","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71B56Ib91KsKA3JQc88C61","type":"Entry","createdAt":"2022-02-11T15:54:42.654Z","updatedAt":"2022-02-11T15:54:42.654Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Ray datasets, code example 2","body":"# Convert from existing DataFrames.\nray.data.from_spark(spark_df)\nray.data.from_dask(dask_df)\nray.data.from_modin(modin_df)\n\n# Convert to DataFrames and ML datasets.\ndataset.to_spark()\ndataset.to_dask()\ndataset.to_modin()\ndataset.to_torch()\ndataset.to_tf()\n\n# Convert to objects in the shared memory object store.\ndataset.to_numpy_refs()\ndataset.to_arrow_refs()\ndataset.to_pandas_refs()"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Last-mile preprocessing","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"Datasets offers convenient ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/dataset-ml-preprocessing.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"data preprocessing functionality","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for common last-mile transformations that you wish to perform right before training your model or doing batch inference. “Last-mile preprocessing” covers transformations that differ across models or that involve per-run or per-epoch randomness. Datasets allow you to do these operations in parallel while keeping everything in (distributed) memory with ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":".map_batches(fn)","nodeType":"text"},{"data":{},"marks":[],"value":", with no need to persist the results back to storage before starting to train your model or do batch inference.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7EdvgcT1AxIARgiuBAVucL","type":"Entry","createdAt":"2022-02-11T15:55:10.675Z","updatedAt":"2022-02-11T15:55:10.675Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Ray datasets, code example 3","body":"# Simple transforms.\ndataset.map(fn)\ndataset.flat_map(fn)\ndataset.map_batches(fn)\ndataset.filter(fn)\n\n# Aggregate operations.\ndataset.repartition()\ndataset.groupby()\ndataset.aggregate()\ndataset.sort()\n\n# ML Training utilities.\ndataset.random_shuffle()\ndataset.split()\ndataset.iter_batches()"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stateful GPU tasks","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"To enable ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/advanced-pipelines.html#example-pipelined-batch-inference"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference on large datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", Datasets supports running stateful computations on GPUs. This is quite simple: instead of calling ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":".map_batches(fn)","nodeType":"text"},{"data":{},"marks":[],"value":" with a stateless function, call ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":".map_batches(callable_cls, compute=\"actors\")","nodeType":"text"},{"data":{},"marks":[],"value":". The callable class will be instantiated on a Ray actor, and re-used multiple times to transform input batches for inference:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6kxuQLugIWMJt1UWG1Kfx7","type":"Entry","createdAt":"2022-02-11T15:56:02.840Z","updatedAt":"2022-02-16T22:35:37.789Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Ray datasets, code example 4","body":"# Example of GPU batch inference on an ImageNet model.\ndef preprocess(image: bytes) -\u003e bytes:\n    return image\n\nclass BatchInferModel:\n    def __init__(self):\n        self.model = ImageNetModel()\n    def __call__(self, batch: pd.DataFrame) -\u003e pd.DataFrame:\n        return self.model(batch)\n\nds = ray.data.read_binary_files(\"s3://bucket/image-dir\")\n\n# Preprocess the data.\nds = ds.map(preprocess)\n# -\u003e Map Progress: 100%|████████████████████| 200/200 [00:00\u003c00:00, 1123.54it/s]\n\n# Apply GPU batch inference with actors, and assign each actor a GPU using\n# ``num_gpus=1`` (any Ray remote decorator argument can be used here).\nds = ds.map_batches(BatchInferModel, compute=\"actors\", batch_size=256, num_gpus=1)\n# -\u003e Map Progress (16 actors 4 pending): 100%|██████| 200/200 [00:07, 27.60it/s]\n\n# Save the results.\nds.repartition(1).write_json(\"s3://bucket/inference-results\")","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pipelined compute with Datasets","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Reading into, transforming, and consuming/writing out of your Dataset creates a series of execution stages. By default, these stages are eagerly executed via blocking calls, which provides an easy-to-understand bulk synchronous parallel execution model and maximal parallelism for each stage:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2prFH2YsPnw9N42t92NEVf","type":"Asset","createdAt":"2022-02-11T15:56:29.468Z","updatedAt":"2022-02-11T15:56:29.468Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-ray-datasets-8","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2prFH2YsPnw9N42t92NEVf/205d40321d6cc1e3f0bd206883e854a8/blog-ray-datasets-8.png","details":{"size":11960,"image":{"width":726,"height":162}},"fileName":"blog-ray-datasets-8.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, this doesn’t allow you to overlap computation across stages: when the first data block is done loading, we can’t start transforming it until all other data blocks are done being loaded as well. If different stages require different resources, this lock-step execution may over-saturate the current stage’s resources while leaving all other stage’s resources idle. Pipelining solves this problem:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7BOR16tisROxG7FAEunQm2","type":"Asset","createdAt":"2022-02-11T15:56:57.945Z","updatedAt":"2022-02-11T15:56:57.945Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-ray-datasets-9","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7BOR16tisROxG7FAEunQm2/b8eac507b5ba1e318cc4edc66b1906a0/blog-ray-datasets-9.png","details":{"size":9896,"image":{"width":727,"height":121}},"fileName":"blog-ray-datasets-9.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pipelining is natively supported in the Datasets API: simply call ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":".window()","nodeType":"text"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":".repeat()","nodeType":"text"},{"data":{},"marks":[],"value":" to generate a DatasetPipeline that can be read, transformed, and written just like a normal Dataset. This means you can easily incrementally process or stream data for ML training and inference. Read more about it in our ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/key-concepts.html#dataset-pipelines"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"pipelining docs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Why build Datasets on top of Ray, anyway?","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray has a robust ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"distributed dataplane","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", combining decentralized scheduling with a best-in-class distributed object layer, featuring:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Efficient zero-copy reads","nodeType":"text"},{"data":{},"marks":[],"value":" via shared memory for workers on the same node, obviating the need for serialization when sharing data across worker processes.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Locality-aware scheduling","nodeType":"text"},{"data":{},"marks":[],"value":", where data-intensive tasks are scheduled onto the node that has the most of the task’s required data already local.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Resilient object transfer protocols","nodeType":"text"},{"data":{},"marks":[],"value":", with a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/analyzing-memory-management-and-performance-in-dask-on-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"memory manager","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" that ensures prefetching and forward progress while bounding the amount of memory usage.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"A fast data transport implementation","nodeType":"text"},{"data":{},"marks":[],"value":", transferring data chunks in parallel in order to maximize transfer throughput.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Given Ray's distributed dataplane, building a library like Datasets becomes comparatively simple. Datasets delegates most of the heavy lifting to the Ray dataplane, focusing on providing higher-level features such as convenient APIs, data format support, and stage pipelining.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How is the community using Datasets?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The Ray Datasets project is still in its early stages. Post-beta, we plan to add a number of additional features, including:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Support for more data formats and integrations","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Reducing object store memory overhead in large-scale pipelines","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Improved performance and scalability of shuffle (scaling to 100+TB)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"That said, users are already finding Datasets to be providing distinct advantages today.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Case study 1: ML training and inference vs Petastorm/Pandas","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"One organization utilizing Ray for their ML infra has found Datasets to be effective at speeding up their training and inference workloads at small scales:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"At training","nodeType":"text"},{"data":{},"marks":[],"value":", Ray Datasets was 8x faster than Pandas + S3 + Petastorm when used from a single GPU instance, showing reduction in serialization overheads:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6fMjPc2UFwEjfplk211ODU","type":"Asset","createdAt":"2022-02-11T15:57:42.065Z","updatedAt":"2022-02-11T15:57:42.065Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-ray-datasets-10","description":"Benchmark: NYC Taxi dataset (5 GB subset), single g4dn.4xlarge instance.","file":{"url":"//images.ctfassets.net/xjan103pcp94/6fMjPc2UFwEjfplk211ODU/f0a87904f0728348a7c1929668b38a11/blog-ray-datasets-10.png","details":{"size":13417,"image":{"width":600,"height":371}},"fileName":"blog-ray-datasets-10.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"At inference","nodeType":"text"},{"data":{},"marks":[],"value":", Dask-on-Ray + Datasets + Torch was 5x faster than Pandas + Torch again even when evaluating on a single machine:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ToadmAmqvyfvXj7i7dr82","type":"Asset","createdAt":"2022-02-11T15:58:12.695Z","updatedAt":"2022-02-11T20:35:57.001Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-ray-datasets-11","description":"Benchmark: NYC Taxi dataset (5 GB subset), single r5d.4xlarge instance","file":{"url":"//images.ctfassets.net/xjan103pcp94/6ToadmAmqvyfvXj7i7dr82/d1cf0a834341d4140c7b9358a4ce3d84/blog-ray-datasets-11.png","details":{"size":35679,"image":{"width":600,"height":371}},"fileName":"blog-ray-datasets-11.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Case study 2: ML ingest at large scale","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another ML platform group evaluating a larger scale S3 → Datasets → Horovod data pipeline found significant gains when scaling their ingest pipeline to a cluster of machines. In this use case, not only did Datasets provide higher throughput, but better shuffle quality since it supported a true distributed shuffle.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UySspyDDjP9XhS2oS1Z93","type":"Entry","createdAt":"2022-02-16T17:12:45.324Z","updatedAt":"2022-02-16T17:12:45.324Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"Ray Datasets blog table","body":"|           | Aggregate throughput |\n|-----------|----------------------|\n| Petastorm | 2.16 GB/s            |\n| Datasets  | 8.18 GB/s            |\n\nBenchmark: 1.5 TB synthetic tabular dataset, 16 nodes (40 vCPUs, 180 GB RAM)."}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To summarize, Datasets simplifies ML pipelines by providing a flexible and scalable API for working with data within Ray. We're just getting started with Datasets, but users are already finding it effective for training and inference at a variety of scales. ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Check out the documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/events/2022/02/16/ray-datasets-scalable-data-preprocessing-for-distributed-ml"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"register for our upcoming webinar","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to get a first-hand look at Datasets in action.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"This post is based on Alex Wu and Clark Zinzow's talk, “","nodeType":"text"},{"data":{"uri":"https://www.youtube.com/watch?v=wl4tvru9_Cg"},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Unifying Data preprocessing and training with Ray Datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":",” from PyData Global 2021.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JAMW625TMPlNr83HdK3Oc","type":"Asset","createdAt":"2022-02-11T16:06:40.807Z","updatedAt":"2022-06-22T19:14:12.523Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"blog-ray-datasets-thumb","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5JAMW625TMPlNr83HdK3Oc/261027acff0343875679da74359a2f7f/1372461_Blog_image_or_illustration_-7_062222.jpg","details":{"size":734176,"image":{"width":1500,"height":1000}},"fileName":"1372461_Blog image or illustration -7_062222.jpg","contentType":"image/jpeg"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}}],"hideIntro":true,"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5945GdUwJCPlj6wHOPlw7Y","type":"Entry","createdAt":"2021-11-30T16:50:41.093Z","updatedAt":"2022-06-22T16:09:22.594Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Deep Dive: Data Ingest in a Third Generation ML Architecture","slug":"deep-dive-data-ingest-in-a-third-generation-ml-architecture","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12vKMQ3j3q45idk83ZoPYA","type":"Entry","createdAt":"2021-10-01T20:23:37.378Z","updatedAt":"2021-10-01T20:23:37.378Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Chen Shen","slug":"chen-shen"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48lK0gPUqCqrmxWiQi8ve8","type":"Entry","createdAt":"2021-02-16T07:25:36.331Z","updatedAt":"2021-02-16T07:25:36.331Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Clark Zinzow","slug":"clark-zinzow","link":"https://www.linkedin.com/in/clarkzinzow/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2021-11-30","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Distributed libraries allow improved performance by exploiting the full bandwidth of distributed memory, and giving greater programmability. But how does that actually work? What does the code look like?\n\nIn this post, we’ll be looking at a concrete example with code samples: ML ingest with Ray Datasets and Ray Train.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"This is part 3 of our series on ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"third generation ML architectures","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". In the previous post, we talked about how distributed libraries allow improved ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"performance","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" by exploiting the full bandwidth of distributed memory, and giving greater programmability. But how does that ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"actually work","nodeType":"text"},{"data":{},"marks":[],"value":"? What does the code look like?","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this post, we’ll be looking at a concrete example with code samples: ML ingest with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/user_guide.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"We show how these distributed libraries can be woven together with just a few lines of Python--- a key capability not possible in 2nd gen architectures.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"We examine how Datasets and Train use the interoperable primitives of Ray tasks, actors, and objects to enable this composable architecture.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Runnable scripts are available that can be adapted for use on your own Ray cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Small Data Training","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To set the stage, let's consider ML training in the small-data setting. These kinds of pipelines are quite simple since all data fits in memory, and the overhead of shuffling is minimal. You can express it as just a few lines of pseudocode:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3ZZZM2MDEx3oRdMKvIblVb","type":"Entry","createdAt":"2021-11-29T23:28:30.694Z","updatedAt":"2021-11-29T23:28:50.762Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"small data training","body":"data = load_data()\npreprocess(data)\nfor each epoch:\n    random_shuffle(data)\n    train_one_epoch(data)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's review the steps above:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Loading","nodeType":"text"},{"data":{},"marks":[],"value":": Small data is typically read from files on local disk into memory. It may be streamed from files in some cases.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Preprocessing","nodeType":"text"},{"data":{},"marks":[],"value":": Apply simple transformations (i.e., feature engineering).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Shuffling","nodeType":"text"},{"data":{},"marks":[],"value":": Randomly permute the order of items in the dataset. Shuffling randomly for each epoch is ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1709.10432"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"important","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for stochastic gradient descent.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Training","nodeType":"text"},{"data":{},"marks":[],"value":": Fit the model over the data (e.g., using a framework like ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://horovod.readthedocs.io/en/stable/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Horovod","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":").","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Big Data Training Challenges","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Training models over big data adds additional needs around (1) distributed preprocessing, (2) distributed shuffling to improve convergence rates, and (3) pipelined execution with ML training:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed preprocessing","nodeType":"text"},{"data":{},"marks":[],"value":": The data ingestion requirements of large-scale training can be substantial, motivating specialized systems such as ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/2108.09373"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"DPP","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" from Facebook and ","nodeType":"text"},{"data":{"uri":"https://eng.uber.com/petastorm/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Petastorm","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" from Uber. The systems allow for preprocessing to be off-loaded to separate nodes in the cluster distinct from the GPU machines. Some portion of preprocessing can be done offline, but it is desirable for data to be \"minimally preprocessed\" for flexibility.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed shuffling","nodeType":"text"},{"data":{},"marks":[],"value":": It is important for the dataset to be shuffled (randomly re-ordered) for each epoch of training. This can significantly ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1709.10432"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"improve the convergence of SGD","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", but is challenging in the ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/MapReduce"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"distributed setting","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". While ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"global shuffling","nodeType":"text"},{"data":{},"marks":[],"value":" is optimal, typically solutions like TensorFlow/Pytorch data loaders and Petastorm only perform local shuffling due to the engineering complexity of stitching together large-scale data shuffles with ML training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Pipelining data processing and training","nodeType":"text"},{"data":{},"marks":[],"value":": Due to limited cluster memory sizes and the need for random per-epoch shuffles, we see that preprocessing and shuffle computations may need to be interleaved with training. This is only possible today in specialized systems like DPP (e.g., you cannot trivially connect Spark's distributed shuffle with Horovod, since they are separate distributed systems).\nIn other words, our simple pipeline became hard since components necessarily become distributed and pipelined for performance:","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In other words, our simple pipeline became hard since components necessarily become distributed and pipelined for performance:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"10WGpzv003QrG6Mz73dzUo","type":"Entry","createdAt":"2021-11-29T23:32:46.395Z","updatedAt":"2021-11-29T23:32:46.395Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"data = load_data()         # larger than cluster memory :(","body":"data = load_data()         # larger than cluster memory :(\npreprocess(data)           # distributed transforms :(\nfor each epoch:\n    random_shuffle(data)   # distributed shuffle :(\n    train_one_epoch(data)  # pipelined with above distributed steps :(","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Second Generation Approach","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's briefly consider how we could compose existing distributed systems to solve this distributed ingest problem. We need to set up a Spark cluster for data processing, a Horovod cluster for training, a coordinator service for control plane operations, and external storage for data plane communication.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6jWNBb88EkWu12CCvGYDst","type":"Asset","createdAt":"2021-11-30T00:22:56.327Z","updatedAt":"2021-11-30T21:06:31.778Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"2nd Generation - Data Ingest Problem","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6jWNBb88EkWu12CCvGYDst/88488a650b7eb5eb6d88bda7e37ca7c1/dataIngestProblem.png","details":{"size":90348,"image":{"width":1406,"height":666}},"fileName":"dataIngestProblem.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The training pipeline would work in the following steps. First, the coordinator service would (1) submit a shuffle job to the Spark cluster, which (2) reads and writes data out to external storage. Next, the Horovod data reader (e.g., Petastorm) would (3) fetch the written dataset location from the coordinator and (4) read the shuffle data for training. These steps would repeat for each epoch of training, and can run concurrently to optimize execution latencies.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The disadvantages of the 2nd generation approach are:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Lack of programmability","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": need to setup and manage 3+ separate distributed systems. It's also hard to orchestrate with workflow systems due to the interleaving of shuffle with training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Performance overhead","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": intermediate data written to external storage since it needs to cross between distributed systems.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Third Generation Approach","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In contrast, with a 3rd gen architecture we can compose the entire data ingest pipeline using distributed ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"libraries","nodeType":"text"},{"data":{},"marks":[],"value":". In the snippet below ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/examples/train_linear_dataset_example.html#train-linear-dataset-example"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"(see here for a full runnable example)","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we compose a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset-pipeline.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Dataset Pipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with a distributed ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/user_guide.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train Job","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"78pALMO2tFH7mvFI7WL9wK","type":"Entry","createdAt":"2021-11-30T00:24:17.532Z","updatedAt":"2021-11-30T00:24:17.532Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Third Generation Approach","body":"from ray.train import Trainer, get_dataset_shard\n\n# Distributed Preprocessing and Shuffle\npipe = ray.data.read_parquet(path).window(size).repeat()\npipe = pipe.map_batches(preprocess)\npipe = pipe.random_shuffle_each_window()\n\n# Ray Train Function\ndef train_func():\n    model = NeuralNetworkModel(...)\n    model = train.torch.prepare_model(model)\n    for epoch_data in get_dataset_shard().iter_epochs():\n        model.fit(epoch_data.to_torch(...))\n\n# Compose and Run\ntrainer = Trainer(num_workers=3, backend=\"torch\", use_gpu=True)\nresult = trainer.run(train_func, dataset=dataset_pipeline)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The above snippet, while simplified, is able to express the aforementioned ML ingest and training pipeline with just a few lines of Python code--- without any need to wrangle distributed systems. Under the hood, the Dataset and Train libraries leverage Ray Tasks and Actors respectively to execute distributed data preprocessing and ML training. We are able to compose them by just passing a reference to the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"dataset_pipeline","nodeType":"text"},{"data":{},"marks":[],"value":" object to Train:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"n0VLbvBnx8WyFKvQJDDyC","type":"Asset","createdAt":"2021-11-30T00:27:03.418Z","updatedAt":"2021-11-30T21:09:59.864Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"3rdGenTasks andActors","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/n0VLbvBnx8WyFKvQJDDyC/b40de05028d9740c1b3f51af4e20eaeb/3rdGenTasks_andActors.png","details":{"size":148336,"image":{"width":1366,"height":922}},"fileName":"3rdGenTasks andActors.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The above figure illustrates the tasks and actors created by the above code snippet.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Compared to the 2nd gen approach, the 3rd gen approach achieves:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Lower operational and development overheads","nodeType":"text"},{"data":{},"marks":[],"value":": developers can compose and ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"customize","nodeType":"text"},{"data":{},"marks":[],"value":" the entire distributed training system in a single script thanks to the programmability of a 3rd gen architecture.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Better performance","nodeType":"text"},{"data":{},"marks":[],"value":": as we'll see in the case studies, this approach reduces overheads by allowing data to be passed in-memory.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Code Walkthrough","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"So how does it work? Let's walk through the above example starting with the system requirements.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Requirements for the Example","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"For performance, we want data to be passed in-memory between preprocessing, shuffle, and training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"We should support ingestion of a dataset that is larger than memory. In the example below, we'll assume a 2TB dataset, and a cluster with 1TB of memory.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Support for heterogeneous clusters (e.g., a cluster with GPU training nodes and CPU preprocessing nodes).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Part 1: Windowed data loading","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's look at the first part of the code above, which creates a data loading pipeline.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xvSklEPrmcWNId9RfBs5j","type":"Entry","createdAt":"2021-11-30T00:28:20.053Z","updatedAt":"2021-11-30T00:28:20.053Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"pipe = ray.data.read_parquet(path).window(size).repeat()","body":"pipe = ray.data.read_parquet(path).window(size).repeat()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This uses the Ray Dataset library to create a DatasetPipeline reading our parquet data from disk. Since the dataset (2TB) is larger than our cluster memory (1TB), we use the .window() function to process windows of size=200GB at a time, leaving extra memory headroom for execution. Since we want to loop over the dataset indefinitely, we use the .repeat() operator after that.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Part 2: Preprocessing and shuffle pipeline","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The second part of the pipeline is applying the distributed transform and shuffling operations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"K22lS3qMNQAVVakEyi5mO","type":"Entry","createdAt":"2021-11-30T00:29:09.699Z","updatedAt":"2021-11-30T00:29:09.699Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Preprocessing and shuffle pipeline","body":"pipe = pipe.map_batches(preprocess)\npipe = pipe.random_shuffle()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is telling Ray to transform records in the pipeline with a given ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"preprocess","nodeType":"text"},{"data":{},"marks":[],"value":" function, and then shuffling the entire window randomly (e.g., 200GB at a time), to avoid going out of core. So far, nothing has been executed beyond reading the file metadata--- we're building up a logical pipeline.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Part 3: Ray Train Setup","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next we define the code that is run on each GPU worker and implements distributed training. Each worker can read a particular split of the pipeline we defined by calling ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"get_dataset_shard","nodeType":"text"},{"data":{},"marks":[],"value":". It sets up a model using ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"train.torch.prepare_model","nodeType":"text"},{"data":{},"marks":[],"value":" to participate in distributed training. Then, it trains over the data in each epoch (repeat) of the dataset.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4OVxuAmQZAAr2zL5vrYVnO","type":"Entry","createdAt":"2021-11-30T00:29:55.709Z","updatedAt":"2021-11-30T00:29:55.709Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"train_func","body":"def train_func():\n    model = NeuralNetworkModel(...)\n    model = train.torch.prepare_model(model)\n    for epoch_data in get_dataset_shard().iter_epochs():\n        model.fit(epoch_data.to_torch(...))","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To create the training actors, we create a ray.train.Trainer that requires 3 GPU workers:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"BIl5itQv7ZPDqNfgrNZSP","type":"Entry","createdAt":"2021-11-30T00:30:22.961Z","updatedAt":"2021-11-30T00:30:22.961Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"trainer = Trainer(num_workers=3, backend=\"torch\", use_gpu=True)","body":"trainer = Trainer(num_workers=3, backend=\"torch\", use_gpu=True)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"At this point, our pipeline is fully defined, and our training actors have been created and assigned GPUs in the cluster, we just need to run it.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Part 3: Running everything","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"This line of code triggers the execution of the entire pipeline:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"j221k8qfddcLQDzWQVpcA","type":"Entry","createdAt":"2021-11-30T00:31:25.195Z","updatedAt":"2021-11-30T00:31:25.195Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Part 3: Running everything","body":"result = trainer.run(train_func, dataset=dataset_pipeline)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"So what's happening in the cluster?","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train sends actor method calls to each actor to run its given training function.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Each actor pulls data from the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"DatasetPipeline","nodeType":"text"},{"data":{},"marks":[],"value":" shard given to it (each pipeline shard contains a handle to a coordinator actor created by Datasets for this DatasetPipeline instance).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"This triggers actor calls to the coordinator actor.\na. The coordinator schedules execution of the next window of the pipeline, e.g., Ray tasks that use CPU nodes in the cluster to:\n      i. load data for the window (200GB)\n      ii. preprocess the data\n      iii. randomly shuffle the data\n      iv. split up the data and assign splits to trainer actors\nb. The coordinator returns to the trainer actors object references to their assigned Dataset split.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The trainer actors ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.get()","nodeType":"text"},{"data":{},"marks":[],"value":" data blocks from their Dataset split and generate mini-batches to pass to the underlying learning library (i.e., PyTorch).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can visualize the overall dataflow in the following timeline diagram. Once data is loaded, shuffle and execution proceed in a fully pipelined way, leveraging tasks running on CPU nodes to implement shuffling, and actors running on GPUs for training:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"51nJaf8KaCjBBQOzS4W6mU","type":"Asset","createdAt":"2021-11-30T05:53:49.763Z","updatedAt":"2021-11-30T05:53:49.763Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"dataflow","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/51nJaf8KaCjBBQOzS4W6mU/d490293c792df1d7803eb30f13682b1d/dataflow.png","details":{"size":90897,"image":{"width":1210,"height":432}},"fileName":"dataflow.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Try it out yourself with these examples in Ray 1.8:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/examples/train_linear_dataset_example.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://docs.ray.io/en/master/train/examples/train_linear_dataset_example.html","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/examples/big_data_ingestion.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://docs.ray.io/en/master/data/examples/big_data_ingestion.html","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Benchmarks","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a previous blog post, we discussed the ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"performance advantages","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" of passing data in-memory and with pipelining and showed improvements in an ablation study. Since then, Datasets has been used by several open source users to implement large-scale shuffled ML ingest. We present two case studies from our ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/presentation/d/1zANPlmrxQkjPU62I-p92oFO3rJrmjVhs73hL4YbM4C4"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyData Dataset talk","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" showing significant performance improvements:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Case Study 1: high-tech ML platform startup","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Dask-on-Ray → Datasets → Horovod","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Dask-on-Ray and Datasets was ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"8x faster","nodeType":"text"},{"data":{},"marks":[],"value":" than Pandas + S3+ Petastorm, even on a single machine.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Benchmark:","nodeType":"text"},{"data":{},"marks":[],"value":" ","nodeType":"text"},{"data":{"uri":"http://ludwig.ai/"},"content":[{"data":{},"marks":[],"value":"Ludwig AI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" model, NYC Taxi dataset (5 GB subset), single g4dn.4xlarge instance","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UdwUzwL7HNFGOSUgHydvq","type":"Asset","createdAt":"2021-11-30T07:55:45.520Z","updatedAt":"2021-11-30T21:14:24.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Shuffled Data Benchmark","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2UdwUzwL7HNFGOSUgHydvq/3bdea36e01ae221aa6d802001c57a528/shuffledDataBenchmark.png","details":{"size":79641,"image":{"width":1108,"height":656}},"fileName":"shuffledDataBenchmark.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Case Study 2: large transport tech company","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"S3 → Datasets → Horovod","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Datasets from S3 was ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"4x faster","nodeType":"text"},{"data":{},"marks":[],"value":" than Petastorm from S3","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Benchmark:","nodeType":"text"},{"data":{},"marks":[],"value":" 1.5 TB synthetic tabular dataset, 16 nodes (40 vCPUs, 180 GB RAM), 2 shuffle windows ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Gl6shcoyCtt7P1xHuYWO0","type":"Asset","createdAt":"2021-11-30T07:57:18.136Z","updatedAt":"2021-11-30T07:57:18.136Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Petastorm Datasets","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6Gl6shcoyCtt7P1xHuYWO0/63479b61e30fb33db13df73f72e42da4/petastormDatasets.png","details":{"size":28808,"image":{"width":626,"height":290}},"fileName":"petastormDatasets.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We believe 3rd gen ML architectures will help engineers develop and standardize infrastructure for large-scale ML apps. This blog demonstrated that with just a single Python script, we can connect distributed data preprocessing with training in a highly performant way.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Moreover, in true 3rd-gen fashion, we were able to do the above without building a specialized system. We used Ray to interleave execution of two independent distributed libraries--- a key capability not possible in 2nd gen architectures. This composability is possible since both libraries are built on the common and interoperable primitives of Ray tasks, actors, and objects.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While we're just getting started with ML ingest-- look for new examples and performance enhancements as Ray Datasets graduates from beta in the next few months--- this is just one aspect of programmable distributed compute with Ray. Check out other use cases in Tuning, Training, Serving, and more here: ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://www.ray.io/","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7hIMsn7WDpX9zGqRwfZnvu","type":"Asset","createdAt":"2021-11-30T21:13:58.335Z","updatedAt":"2021-11-30T21:13:58.335Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"3rdGenTasks andActors","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7hIMsn7WDpX9zGqRwfZnvu/0f04538b77d8cc926e7021a3be138720/3rdGenTasks_andActors.png","details":{"size":148336,"image":{"width":1366,"height":922}},"fileName":"3rdGenTasks andActors.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}}],"hideIntro":true,"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3rZB4NJNYoF4vsTsX2K2Jm","type":"Entry","createdAt":"2021-10-06T16:00:05.641Z","updatedAt":"2022-06-22T16:20:14.305Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Why Third Generation ML Platforms are More Performant","slug":"why-third-generation-ml-platforms-are-more-performant","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12vKMQ3j3q45idk83ZoPYA","type":"Entry","createdAt":"2021-10-01T20:23:37.378Z","updatedAt":"2021-10-01T20:23:37.378Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Chen Shen","slug":"chen-shen"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2021-10-06","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"In a previous blog post, we defined a \"3rd generation ML platform\" as one that offered full programmability for ML workflows. Key to a 3rd generation platform is the concept of a programmable compute layer. In this blog, we report on emerging patterns of distributed compute we see in advanced ML platform workloads. We show how Ray, a leading programmable compute layer, improves performance by 3-9x in relevant production workloads.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"In a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"previous blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we defined a \"3rd generation ML platform\" as one that offered full programmability for ML workflows. Key to a 3rd generation platform is the","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" ","nodeType":"text"},{"data":{},"marks":[],"value":"concept of a ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"programmable compute layer","nodeType":"text"},{"data":{},"marks":[],"value":". In this blog, we report on emerging patterns of distributed compute we see in advanced ML platform workloads. We show how ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a leading programmable compute layer, improves performance by 3-9x in relevant production workloads.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ecYQnlH7n2Tcv8Ovf1IEZ","type":"Asset","createdAt":"2021-10-01T20:28:28.089Z","updatedAt":"2021-10-05T04:17:24.678Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"2ndVS3rdGeneration","description":"A \"2nd generation\" (left) vs \"3rd generation\" (right) ML platform. Light-blue boxes represent clusters, and light-purple represents libraries. The 3rd generation platform eliminates cluster compute silos and improves performance and programmability.","file":{"url":"//images.ctfassets.net/xjan103pcp94/6ecYQnlH7n2Tcv8Ovf1IEZ/79da29e55ec74c834913c2072c6ebf8d/Fig1.png","details":{"size":76050,"image":{"width":2156,"height":420}},"fileName":"Fig1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Performance Overheads in Second Generation Platforms","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed ML workflows are typically composed from a few types of compute patterns: collective (i.e., a set of processes communicating with each other like in ","nodeType":"text"},{"data":{"uri":"https://github.com/horovod/horovod"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"distributed SGD","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"), chaining (i.e., a ","nodeType":"text"},{"data":{"uri":"https://docs.metaflow.org/metaflow/basics#linear"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"sequential workflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" of tasks run one after the other), and nesting (i.e., tasks that kick off other tasks, commonly seen in ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"hyperparameter tuning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7KYcuMZ98l5DdtDi10vHxp","type":"Asset","createdAt":"2021-10-01T20:43:35.650Z","updatedAt":"2021-10-05T04:21:32.297Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"collectingChainingNesting","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7KYcuMZ98l5DdtDi10vHxp/594784d15b520df3924340af95bc0b86/CollectiveChainingNesting.png","details":{"size":64947,"image":{"width":2180,"height":570}},"fileName":"CollectiveChainingNesting.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"first generation platforms","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ML workflows were implemented as custom-built systems optimized for a specific workflow. In second generation platforms, flexibility was achieved by relying on ","nodeType":"text"},{"data":{"uri":"https://neptune.ai/blog/best-workflow-and-pipeline-orchestration-tools"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"workflow orchestrators","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to provide chaining and nesting, gluing together separate systems that internally implement high-optimized collective operations.","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":" ","nodeType":"text"},{"data":{},"marks":[],"value":"The following figure illustrates a 2nd generation distributed ML platform. Each step is run as a separate cluster by a workflow orchestrator (e.g., ","nodeType":"text"},{"data":{"uri":"https://engineering.fb.com/2016/05/09/core-data/introducing-fblearner-flow-facebook-s-ai-backbone/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FBLearner Flow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker Steps","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://metaflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Metaflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.kubeflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"KubeFlow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"60TSWgITq3zG5xHqP8z3Xi","type":"Asset","createdAt":"2021-10-01T20:44:24.648Z","updatedAt":"2021-10-05T04:24:47.644Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"WorkflowOrchestrator","description":"A \"2nd generation\" distributed ML platform. A workflow orchestrator is needed to connect components implemented as separate distributed systems, limiting the programmability--- and performance--- of the platform.","file":{"url":"//images.ctfassets.net/xjan103pcp94/60TSWgITq3zG5xHqP8z3Xi/badf3cbb8c64666b880e7b28d407e6bf/WorkflowOrchestrator.png","details":{"size":72420,"image":{"width":1676,"height":420}},"fileName":"WorkflowOrchestrator.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, this 2nd generation architecture imposes performance overheads and limits programmability. This is due to:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Scheduling overheads. ","nodeType":"text"},{"data":{},"marks":[],"value":"These platforms rely on separately scheduled VMs or containers per step to distribute the workload. Each step may launch its own distributed framework (e.g., Spark or Distributed PyTorch). This leads to several seconds to minutes of scheduling overhead per step, and prevents optimizations like pipelining.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data movement overheads","nodeType":"text"},{"data":{},"marks":[],"value":". The overhead between steps can be reduced substantially if data is kept in memory between steps when possible and not materialized to storage.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Programmability overheads","nodeType":"text"},{"data":{},"marks":[],"value":". Expressing fine-grained nesting or pipelining can require substantial changes to distributed systems code that are rarely accessible to end-users of existing ML platforms.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Chaining and Nesting can be Performance Bottlenecks","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In many cases, chaining and nesting are ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"not","nodeType":"text"},{"data":{},"marks":[],"value":" performance bottlenecks. This is because the data transferred is small, or scheduling overhead is small compared to execution time. However, there are a growing number of scenarios where bottlenecks do arise. Here we overview several use cases that benefit from optimized chaining and nesting.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BYEdrkn0NDWNOEzPmtSIm","type":"Entry","createdAt":"2021-10-01T21:05:45.584Z","updatedAt":"2021-10-06T20:32:36.631Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":30,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"Compare 2nd vs 3rd Generation","body":"\n|  Use case   | 2nd generation platforms  | 3rd generation platforms | \n| ---------- | ---------- | ---------- |\n| __Data Ingest for Model Training (Chaining):__ Chaining data processing and ML training is simple when the data can fit onto a single machine. It becomes challenging at [large scale](https://arxiv.org/abs/2108.09373), when you want to [shuffle data globally](https://speakerdeck.com/anyscale/per-epoch-shuffling-data-loader-mix-it-up-as-you-train-clark-zinzow-anyscale) during training, or pipeline data preparation with training to [reduce latency](https://docs.ray.io/en/latest/data/dataset-pipeline.html). Although materializing data to cluster storage provides certain benefits, frequent I/O to cluster storage introduces a lot of overhead. | ![01_twineditSimpleChain](//images.contentful.com/xjan103pcp94/45y4oFh98EwSKMRaxzHcJ/93fee7c4c8c217df3a41173f52c6a733/01_twineditSimpleChain.png) | ![01_twineditPipelined](//images.contentful.com/xjan103pcp94/whcbju48HM91FOIIyMHrC/2e2b019b20786f16f84b6479dbeed31b/01_twineditPipelined.png)\n|  __Serving Pipelines (Chaining):__ Similarly, passing data efficiently is critical to the performance of disaggregated [model serving pipelines](https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns). In a disaggregated pipeline, large models are split into multiple parts that can be deployed and scaled separately (e.g., fully-connected networks vs memory-intensive embedding lookups). Predictions can be composed of multiple models in a DAG structure (e.g., [RoboVision](https://robovision.ai/) uses a 5-model stack for vehicle detection pipeline).   | ![02_2nd_4x2_22font_SimpleServing](//images.contentful.com/xjan103pcp94/4UyLFxAEkYrZUIP0AWDXbO/cf95c047d9d4df80066ff766b12ee006/02_2nd_4x2_22font_SimpleServing.png) | ![02_3rd_4x2_22font_DisaggregatedModelServingPipeline](//images.contentful.com/xjan103pcp94/30rncKJXFtnXBK12vcHKyD/7915253df3ff9c68c2faea56e4a45342/02_3rd_4x2_22font_DisaggregatedModelServingPipeline.png)\n|  __Batch Scoring (Nesting):__ Scoring a model on a large dataset can be quite slow without [distributed computation](https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/ai/batch-scoring-python), which naturally generates a nested workload during the course of training. Nesting can be difficult to express, let alone efficiently, in traditional workflow orchestrators.  | ![03_2nd_4x2_22font_SequentuilScoring](//images.contentful.com/xjan103pcp94/5uWbLSWU26EZp5Q9kkEMMh/41c443ee64006e8ee8d88eb20cd65f70/03_2nd_4x2_22font_SequentuilScoring.png)  | ![03_3rd_4x2_22font_DistributedScoring](//images.contentful.com/xjan103pcp94/1IxBk5YiSbdFPRUYneqBKy/8c39b2d0586bfa4af2ab84063a191433/Screen_Shot_2021-10-05_at_6.55.50_PM.png)\n| __Hyperparameter Tuning (Nesting):__ Nesting also occurs naturally during hyperparameter tuning, which seeks to explore many variants of existing workloads. Nesting is needed for [distributed trials](https://docs.ray.io/en/latest/tune/index.html), and efficiency is important for supporting lightweight trials and [population based approaches](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-scheduler-pbt).  | ![04_2nd_4x2_22font_SimpleNesting](//images.contentful.com/xjan103pcp94/6BHEgjIjRWRIymCYfQ1xKc/234d498153325597f75936d9e95cbb96/04_2nd_4x2_22font_SimpleNesting.png)  | ![04_3rd_4x2_22font_DistributedtrialsPBP](//images.contentful.com/xjan103pcp94/2oh4v2FUq0C5RS2FlwBSah/5708b8ccf119b334af0b4584921633a3/04_3rd_4x2_22font_DistributedtrialsPBP.png)\n| __Workflow DAGs (Both Chaining and Nesting):__ Finally, both chaining and nesting occur naturally in higher level workflow DAGs implemented by workflow orchestrators. Here two bottlenecks can occur: (1) if large amounts of data is passed or shared between workflow steps, it is desirable to pass data at memory-speed rather than reading and writing to cluster storage, and (2) if steps are small, scheduling overhead can dominate workflow run time. | ![05_2nd_4x2_22font_DAG](//images.contentful.com/xjan103pcp94/5pHdKYmzsuBM6WAAa2WAur/b45d46823841e0d089d2d80628bb7265/05_2nd_4x2_22font_DAG.png)  | ![05_3rd_4x2_22font_inMemoryLow-overhead](//images.contentful.com/xjan103pcp94/DZmefXHDWCnvBWm0m05wG/aa46b9911cf20c6f4281a8e3763592a9/05_3rd_4x2_22font_inMemoryLow-overhead.png)"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Third Generation Platforms Accelerate Chaining and Nesting","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"A ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"third generation ML platform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" eliminates the above performance bottlenecks. Users and builders of third generation platforms are able to:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Implement chaining and nesting of distributed steps with minimal scheduling and data movement overheads.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Programmatically author ML workflows, weaving together steps like ingest, transform, and training without needing to wrangle separate distributed systems.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is possible with the use of a ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"programmable compute layer ","nodeType":"text"},{"data":{},"marks":[],"value":"such as ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which can serve as a replacement to (or ","nodeType":"text"},{"data":{"uri":"https://www.astronomer.io/blog/airflow-ray-data-science-story"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"accelerator for","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") workflow orchestrators. In a 3rd gen platform, distributed logic such as ","nodeType":"text"},{"data":{"uri":"https://medium.com/distributed-computing-with-ray/data-processing-support-in-ray-ae8da34dce7e"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"data processing","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is implemented as ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/ray-libraries.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"libraries","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" within the compute layer:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5b9o8llgwycHgDtV9c1r8m","type":"Asset","createdAt":"2021-10-04T20:57:30.411Z","updatedAt":"2021-10-04T20:57:30.411Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"3rdGenDistributed","description":"A \"3rd generation\" distributed ML platform. The programmable compute layer allows distributed steps to be tightly woven together in code, eliminating the overheads of separate clusters.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5b9o8llgwycHgDtV9c1r8m/ab3f90039ba3ac7cf2577c1658aad85c/3rdGenDistributed.png","details":{"size":33191,"image":{"width":720,"height":216}},"fileName":"3rdGenDistributed.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Benchmarks","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this section we highlight some of the performance gains users have seen by leveraging Ray's support for efficient chaining and nesting of computations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Uber: Shuffled ML Ingest Pipeline","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a previous blog post, Uber introduced how they were leveraging Ray for ","nodeType":"text"},{"data":{"uri":"https://eng.uber.com/horovod-ray/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"elastic scheduling and training with Horovod","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". In a follow-up project, another team is using Ray's ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/dataset-pipeline.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Dataset Pipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to implement shuffled ML ingest, where data is globally shuffled across cluster CPU workers per iteration of GPU training. Here we analyze the importance of pipelining and in-memory data exchange. We ran a training workload reading 500GB of data on a cluster of 70 CPU nodes and 16 GPU nodes, and find ingest throughput is 3x higher with the pipelining and in-memory data exchange enabled by Ray:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"soBLd3ZStUVqfRH8BCxtR","type":"Asset","createdAt":"2021-10-04T20:59:23.745Z","updatedAt":"2021-10-05T03:52:09.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Shuffled Ingest","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/soBLd3ZStUVqfRH8BCxtR/bafe8e1c993e605a88ae521cdfc99584/Shuffled_Ingest_Throughput.png","details":{"size":88039,"image":{"width":1938,"height":810}},"fileName":"Shuffled Ingest Throughput.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Wildlife Studios: Chained Model Pipelines","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Mobile gaming giant ","nodeType":"text"},{"data":{"uri":"https://wildlifestudios.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Wildlife Studios’","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" legacy system for serving revenue-generating in-game offers was not scaling to meet their latency and cost requirements. After switching to Ray Serve, their Dynamic Offers team was able to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/wildlife-studios-serves-in-game-offers-3x-faster-at-1-10th-the-cost-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"serve offers three times faster","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". The Ray Serve architecture provided support for parallel inference on multiple models in a pipeline, decreasing latency and minimizing idle machines:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1qt32KDRT1K8MzgWLfQ2iq","type":"Asset","createdAt":"2021-10-04T21:00:42.387Z","updatedAt":"2021-10-05T03:52:32.315Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"p95","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1qt32KDRT1K8MzgWLfQ2iq/eaae25d1ae4d466294c568fbe380a0e6/P95_Latency_in_Production_Cluster.png","details":{"size":82299,"image":{"width":1804,"height":784}},"fileName":"P95 Latency in Production Cluster.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Anastasia.ai: Nested Model Evaluation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://anastasia.ai/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Anastasia","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provides a powerful platform that enables organizations to operate AI capacities at scale with a fraction of the resources and effort traditionally required. They were able to accelerate a demand prediction problem using ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and got up to ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-anastasia-implements-ray-and-anyscale-to-speed-up-ml-processes-9x"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"9x performance gains","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" due to fine-grained re-use of resources compared to a coarse grained orchestrator:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4ASH8c3Ia98IGL4o1u1FSS","type":"Asset","createdAt":"2021-10-04T21:02:21.018Z","updatedAt":"2021-10-05T03:58:00.087Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"RelativeTimeEvaluate384CoreCluster","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4ASH8c3Ia98IGL4o1u1FSS/5f9db5896146bc065699d6caacb5739e/Relative_Time_to_Evaluate_on_384-core_Cluster.png","details":{"size":75697,"image":{"width":2020,"height":720}},"fileName":"Relative Time to Evaluate on 384-core Cluster.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog, we showed how a programmable compute layer such as Ray can provide 3-9x performance improvements for production ML workloads, eliminating bottlenecks found in 2nd generation production architectures. This is in addition to the productivity and operational benefits of having a programmable architecture.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"As a final note, the ideas presented here apply generally to distributed programming as well as to ML workloads. If you're interested in the performance and programmability of ML applications and distributed computing, check out the ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray project","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and consider ","nodeType":"text"},{"data":{"uri":"https://jobs.lever.co/anyscale"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"joining us","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fTEska4fx1SFm7wlBB7JF","type":"Asset","createdAt":"2021-10-06T16:49:59.640Z","updatedAt":"2021-10-06T16:49:59.640Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"mlplatformCropped","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5fTEska4fx1SFm7wlBB7JF/cb6fc1348b8a28bc0b91ef9dfb080adb/mlplatformCropped.png","details":{"size":84401,"image":{"width":926,"height":500}},"fileName":"mlplatformCropped.png","contentType":"image/png"}}},"recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3wI6uYJcQZCdFRfR00d2ta","type":"Entry","createdAt":"2022-03-29T00:18:53.787Z","updatedAt":"2022-03-29T00:19:41.793Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Why Third Generation ML Platforms are More Performant","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5YgMgD8vYcB7D6HUNoItHz","type":"Entry","createdAt":"2021-09-15T16:23:49.634Z","updatedAt":"2022-06-22T16:54:14.783Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"The Third Generation of Production ML Architectures","slug":"the-third-generation-of-production-ml-architectures","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2021-09-15","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"As technology has advanced, production ML architectures have evolved. One way to see it is in terms of generations: The first generation involved “fixed function” pipelines, while the second generation involved programmability within the pipeline of particular existing actions.\n\nWhat will the third generation of production ML architectures be like? This post tries to answer that by using history as a guide by looking at the evolution of GPU programming architectures. We’ll then talk about a system called Ray that seems to follow in the same footsteps.","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6S3bvsbfg1wDC80fb1aFXO","type":"Entry","createdAt":"2021-09-14T22:28:07.517Z","updatedAt":"2021-09-15T21:32:52.936Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Third Generation Production ML Architectures","videoUrl":" https://youtu.be/hzW0AKKqew4","caption":"This blog is based on Waleed Kadous’ “Third Generation Production ML Architectures” [talk](https://youtu.be/hzW0AKKqew4) and [slideshow](https://www.slideshare.net/WaleedKadous/third-gen-production-ml-architectures-lessons-from-history-experiences-with-ray) from apply() Conference 2021."}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Where are production ML architectures going? ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Production machine learning architectures are ML systems that are deployed in production environments. These are systems trained on billions of examples, and often serve millions of inferences per second. Because of this mammoth scale, they almost always require distributed training and inference. These systems require careful design to support the massive scale they run at, and so as an industry we have tried to create reusable systems to meet the sometimes difficult requirements. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"As technology has advanced, production ML architectures have evolved. One way to see it is in terms of generations: The ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"first generation","nodeType":"text"},{"data":{},"marks":[],"value":" involved “fixed function” pipelines, while the ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"second generation","nodeType":"text"},{"data":{},"marks":[],"value":" involved programmability within the pipeline of particular existing actions.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What will the third generation of production ML architectures be like? This post tries to answer that by using history as a guide by looking at the evolution of GPU programming architectures. We’ll then talk about a system called ","nodeType":"text"},{"data":{"uri":"https://ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" that seems to follow in the same footsteps. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"GPU Programming History","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let’s quickly look at GPU programming history -- the one that the deep learning revolution is built on. This might give us insight for what the future might hold for production machine learning architectures. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2PPibemMAsHklsf6rbxhYE","type":"Asset","createdAt":"2021-09-14T22:32:10.865Z","updatedAt":"2021-09-14T22:35:25.841Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"1stGenGPU","description":"1st Generation of GPU programming architectures (OpenGL 1.0, Direct 3D)","file":{"url":"//images.ctfassets.net/xjan103pcp94/2PPibemMAsHklsf6rbxhYE/fb3bef34e939d63ea4ec2bb37e8ed465/1stGenGPU.png","details":{"size":28128,"image":{"width":1252,"height":129}},"fileName":"1stGenGPU.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the mid to late 1990’s, we saw the first generation of GPU programming architectures. The input would be things like textures, polygons, meshes, etc. It would go through the pipeline and output be an image shown to the user. The revolutionary aspect of this fixed function pipeline was that it was all hardware accelerated. This was the first time that consumers had hardware accelerated graphics. This enabled the 3D gaming revolution. While this brought many new opportunities, it was also limited in many ways. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"58OjDFJljhp1ev3dCzbowj","type":"Asset","createdAt":"2021-09-15T00:09:37.295Z","updatedAt":"2021-09-15T01:44:35.434Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"ProCons1st","description":"Pros and cons of the 1st GPU programming architecture","file":{"url":"//images.ctfassets.net/xjan103pcp94/58OjDFJljhp1ev3dCzbowj/cbaca85d3dd51414e632774f6014b6a9/ProsCons1st.png","details":{"size":42000,"image":{"width":956,"height":304}},"fileName":"ProsCons1st.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The 2nd generation had the same pipeline, however it was broken up into more programmable sections. This allowed for more flexibility with the vertex and fragment shaders being nothing more than small programs that run in the hardware. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"26fOaclf4se7b8hrvhR3mO","type":"Asset","createdAt":"2021-09-14T22:45:20.741Z","updatedAt":"2021-09-14T22:45:20.741Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2ndGenerationGPU","description":"2nd Generation of GPU programming architectures (OpenGL 2.0, Direct3D 10)","file":{"url":"//images.ctfassets.net/xjan103pcp94/26fOaclf4se7b8hrvhR3mO/b4172c5d5f9cc3d6ada158fbd4a4148a/2ndGeneration.png","details":{"size":56986,"image":{"width":1252,"height":400}},"fileName":"2ndGeneration.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This had some programmability, but it still required conforming to the existing pipeline. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5ixmnmnx5McqhmRwQp3cHY","type":"Asset","createdAt":"2021-09-14T22:46:35.203Z","updatedAt":"2021-09-14T22:46:35.203Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2ndGenerationRigid","description":"The required formats for the different sections were pretty rigid.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5ixmnmnx5McqhmRwQp3cHY/1d250f236acd787eabc4d4d2288c5f50/2ndGenerationRigid.png","details":{"size":78051,"image":{"width":1252,"height":518}},"fileName":"2ndGenerationRigid.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This gave a great deal more flexibility to developers that allowed for amazing effects, and while far more flexible, there were still limits. In particular the inputs and outputs of each stage were well defined, and really could not be changed easily. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The second generation was Turing complete so it was possible to create anything, ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"however possible and easy are not the same thing","nodeType":"text"},{"data":{},"marks":[],"value":". For example, if someone was working on a physics problem (or a machine learning problem), they had to work really hard to make it conform to a combination of vertex and fragment shaders and had to have a deep understanding of how the underlying hardware was designed. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4P5O7R9Cvk5XAhpF68hZv9","type":"Asset","createdAt":"2021-09-14T22:49:44.591Z","updatedAt":"2021-09-14T22:49:44.591Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"thirdGenerationGPU","description":"3rd generation of GPU programming architectures (Cg, OpenCL) are a lot more flexible because there is less of a focus on pipelines and more of a focus on full programmability. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/4P5O7R9Cvk5XAhpF68hZv9/7b0e5c4560c4e779b25aec97b8fb4288/thirdGenerationGPU.png","details":{"size":75364,"image":{"width":1377,"height":508}},"fileName":"thirdGenerationGPU.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The Third generation was the one that brought complete programmability combined with performance to GPU programming architectures. The third generation could do everything the previous generations could do and is a lot more flexible because there is less of a focus on pipelines. It is all about full programmability with a focus on libraries. Indeed the first and second generations literally become nothing more than libraries that can be reused at will (or completely ignored if you want to). ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Originally, games were written on code that runs on Direct3D or OpenGL (first and second generation systems). But with the arrival of the third generation, people started to use engines like Unity or Unreal Engine 4 due to their programmability and flexibility with the GPU being more of an implementation detail. This enabled people without knowledge of pipelines and shaders to use it like a normal code library. This opened up the power of GPUs to a huge number of users and this has led to an explosion of applications. For example, modern deep learning infrastructure is built upon this. For example, CUDA led to cuDNN which led to Caffe then Torch and finally PyTorch. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6rslx90y6tOEPJsr2JXJM4","type":"Asset","createdAt":"2021-09-14T22:53:16.883Z","updatedAt":"2021-09-14T22:53:16.883Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"openGL_Unity","description":"Unity (3rd gen) has become a lot more popular than OpenGL (2nd gen). With Unity, people are worrying less about the architecture and focusing more on the characteristics and the libraries. ","file":{"url":"//images.ctfassets.net/xjan103pcp94/6rslx90y6tOEPJsr2JXJM4/231e5d37ffc5159724827c32e5c33460/openGL_Unity.png","details":{"size":108398,"image":{"width":1047,"height":685}},"fileName":"openGL_Unity.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing GPU and production ML architectures","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"What does any of this have to do with Production ML architectures? There may be a similar evolutionary pattern at play. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To understand some similarities with GPU and ML architectures, let’s first start by looking at the evolution of Uber’s production ML architecture, Michelangelo. Note that this example should also apply very generally to other production ML architectures. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2i3tJAwmYCimATeyB9yLi3","type":"Asset","createdAt":"2021-09-14T22:54:46.537Z","updatedAt":"2021-09-14T22:54:46.537Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"1stGenerationML","description":"Fixed pipeline production ML architecture (image from [2017 Uber blog post](https://eng.uber.com/michelangelo-machine-learning-platform/))","file":{"url":"//images.ctfassets.net/xjan103pcp94/2i3tJAwmYCimATeyB9yLi3/437c63c4ad9ea031d8817f61f39e38a4/1stGenerationML.png","details":{"size":161112,"image":{"width":1183,"height":670}},"fileName":"1stGenerationML.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The diagram above is of a 1st generation fixed pipeline ML production architecture from an ","nodeType":"text"},{"data":{"uri":"https://eng.uber.com/michelangelo-machine-learning-platform/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Uber blog post in 2017","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".  This pipeline has data preparation, data in feature stores (get data), batch training job (train models), repository for the models (eval models), and online as well as offline inference (deploy, predict, and monitor).  If you compare the 1st generation of GPU and ML architectures, they might seem surprisingly familiar. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5oqNtEneu0Y7yJxtYLa8HJ","type":"Asset","createdAt":"2021-09-14T22:55:48.624Z","updatedAt":"2021-09-14T22:55:48.624Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ML_GPU_comparison","description":"The 1st generation of ML and GPU architectures both seem to have fixed function pipelines.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5oqNtEneu0Y7yJxtYLa8HJ/6d8a50bb0c9f6833e120d54d16b49663/ML_GPU_comparison.png","details":{"size":59751,"image":{"width":1502,"height":285}},"fileName":"ML_GPU_comparison.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A ","nodeType":"text"},{"data":{"uri":"https://eng.uber.com/michelangelo-machine-learning-model-representation/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"2019 blog post from Uber","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" discussed the problems with the 1st generation architecture. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"“Michelangelo was initially launched with a monolithic architecture that managed tightly-coupled workflows … made adding support for new Spark transformers difficult and precluded serving of models trained outside of Michelangelo.”","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Essentially, the monolithic architecture limited what they could do and made it difficult to serve models. This motivated the second generation which standardized interfaces and separated out things into components. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UDHcztplSMhsUmnXUBG2G","type":"Asset","createdAt":"2021-09-14T22:58:03.974Z","updatedAt":"2021-09-14T22:58:03.974Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2ndGenerationML","description":"2nd generation Uber Michelangelo architecture (image from [2019 Uber blog post](https://eng.uber.com/michelangelo-machine-learning-model-representation/))","file":{"url":"//images.ctfassets.net/xjan103pcp94/5UDHcztplSMhsUmnXUBG2G/53a36ec9d3d927046134d053963a0525/2ndGenerationMLArchitectures.png","details":{"size":165748,"image":{"width":1500,"height":493}},"fileName":"2ndGenerationMLArchitectures.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The 2nd generation Uber Michelangelo architecture made it easier to replace any of the standard tools that we like to code in (scikit-learn, TensorFlow, Spark) with each other. This is similar to the 2nd generation of GPU architectures which have programmable sections within the pipeline. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray as a 3rd Generation Production ML Architecture","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"If mostly where we are right now is the second generation, let’s ask ourselves what the third generation would look like? We suggest -- using the history of GPU programming architectures as a guide --  that it would need the following qualities: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"It needs to be able to do everything the 1st and 2nd generation can do (they’re just libraries)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The interface for programming the system is just a normal programming language which does not force developers to think about the problem in terms of the underlying architecture","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The focus shifts to libraries: just as people started to talk about Unity and stopped talking about OpenGL itself; in much the same way, people would stop talking about particular architectures for ML and just use libraries.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"With a focus on libraries, the compute engine could just be a detail. This would open the power of ML to a huge number of users and potentially lead to an explosion of applications. That brings us to Ray. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What is Ray","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray can be considered one of these third generation systems. Ray is: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"A simple and flexible framework for distributed computation","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"A cloud-provider independent compute launcher/","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-0.8.2/autoscaling.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"autoscaler","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"An ecosystem of distributed computation libraries built with #1","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"An easy way to “glue” together distributed libraries in code that looks like normal python while still being performant. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What makes Ray simple is that you make simple annotations (i.e., ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"@ray.remote","nodeType":"text"},{"data":{},"marks":[],"value":" decorator in Python) to make functions and classes distributable. What makes Ray flexible is that it is not a batch model. Ray uses the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/actors.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"actor model","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" where tasks and actors can create new tasks and new actors without much cost so you can have a more dynamic creation of functionality.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The image below shows the ray ecosystem. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3UYKLGIOH9JbeSjhPhLM2I","type":"Asset","createdAt":"2021-09-14T23:03:02.844Z","updatedAt":"2021-09-14T23:12:06.362Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"3rd Generation","description":"Ray ecosystem.","file":{"url":"//images.ctfassets.net/xjan103pcp94/3UYKLGIOH9JbeSjhPhLM2I/18823b1c79e7e996913ec6686219cbbf/Screen_Shot_2021-09-14_at_4.10.12_PM.png","details":{"size":494592,"image":{"width":2162,"height":918}},"fileName":"Screen Shot 2021-09-14 at 4.10.12 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"At the bottom layer, you have your favorite compute service provider (AWS, Azure, GCP, K8s, private cluster). At the middle layer, you have Ray which acts as an interface for these machine learning libraries to run on your compute service provider. The top layer contains scalable libraries for machine learning, model serving, data processing, and more. Some are native ray libraries designed from the beginning to run on top of Ray.  There is also a healthy ecosystem of 3rd party libraries like PyTorch, scikit-learn, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/distributed-xgboost-training-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"XGBoost","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/introducing-distributed-lightgbm-training-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LightGBM","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and more with ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-distributed-library-patterns"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"varying levels of Ray integration","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, you can glue together these distributed libraries in a way that feels like a single, normal python script; while underneath it is doing all this distributed computation. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How does Ray “fit” with the historical pattern?  ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Based on the history of GPU architectures, we believe a key goal of 3rd generation production ML architectures is that they are more programmable. Additionally, by moving the focus to libraries, you don’t have to worry about the details of how the distributed computation is happening. You can instead focus on the algorithm. This is similar to the 3rd generation of GPU architectures in which people started to use engines like Unity or Unreal Engine 4 due to their programmability and flexibility with the GPU becoming more of a detail. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How are Customers using Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let’s now go over how Ray is being used by customers to see if it is embodying the characteristics of a 3rd Generation Production ML Architecture. Some ways Ray is being used include: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Being a simpler way to build 1st gen/2nd pipelines","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"A tool to parallelize high performance ML systems","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"A way to build ML applications that make ML accessible to non-specialists","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"A simpler way to build 1st/2nd gen pipelines","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray provides a simpler way to build 1st/2nd gen pipelines because it: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Allows you to implement existing systems more efficiently (programming language to define pipelines)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"More easily allows for shared components (i.e., feature transformation during training vs real time)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"“Out of the box” support for distributed ML","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a recent blog post, Uber shared its use of Ray to show what the 3rd generation Uber Michelangelo architecture would look like. They implemented this architecture in ","nodeType":"text"},{"data":{"uri":"https://medium.com/ludwig-ai/ludwig-ai-v0-4-introducing-declarative-mlops-with-ray-dask-tabnet-and-mlflow-integrations-6509c3875c2e"},"content":[{"data":{},"marks":[],"value":"Ludwig 0.4","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"10cxgivcmEb9igWmIITKoW","type":"Asset","createdAt":"2021-09-14T23:17:42.233Z","updatedAt":"2021-09-17T17:53:35.463Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"UberLeveragingRay","description":"Building 3rd generation Uber Michelangelo architecture (image from [2021 Uber blog post](https://eng.uber.com/horovod-ray/)), implemented in [Ludwig](http://ludwig.ai)","file":{"url":"//images.ctfassets.net/xjan103pcp94/10cxgivcmEb9igWmIITKoW/05ae74c6eece1dc67aeeb4403608bf7e/UberLeveragingRay.png","details":{"size":226672,"image":{"width":1074,"height":586}},"fileName":"UberLeveragingRay.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"They said by leveraging Ray, it allowed them to treat the entire training pipeline as a single script making it far easier to work with and far more intuitive. What is even more interesting is that they mentioned the importance of the machine learning ecosystem/libraries and having a standardized way for people to write machine learning libraries both within Uber and the industry at large. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4Iw4xetfHitr7whm6tj8cN","type":"Asset","createdAt":"2021-09-14T23:22:06.859Z","updatedAt":"2021-09-14T23:22:06.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"UberQuoteRayText","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4Iw4xetfHitr7whm6tj8cN/12dcab7f2107da1760f44e2b57f3dd4e/UberQuoteRayText.png","details":{"size":65906,"image":{"width":1208,"height":220}},"fileName":"UberQuoteRayText.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A Tool for High Performance ML Systems","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The second use case is high performance ML systems. A company using Ray called ","nodeType":"text"},{"data":{"uri":"https://robovision.ai/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Robovision","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" wanted to use vehicle detection using 5 stacked ML models. While they could make their models fit in one GPU or machine, they had to cut their models. They tried to use a vanilla Python implementation to do it which resulted in 5 frames per second. When they used Ray, they got about a 3x performance improvement on the same hardware (16 frames per second). It is important to keep in mind that this is certainly not the first time anyone has worked with 5 stacked ML models. For example, it is definitely possible to do this without Ray by converting each ML model into a microservice, but it is important to keep in mind the complexity. The image below shows the basic pipeline of what Robovision was trying to do. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3Koa7zS4vpRPZ5KWSl5I6j","type":"Asset","createdAt":"2021-09-14T23:23:04.769Z","updatedAt":"2021-09-14T23:23:04.769Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Robovision","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3Koa7zS4vpRPZ5KWSl5I6j/6ab8f1b5e8dfb098e5210b80a6ee271a/Robovision.png","details":{"size":2056026,"image":{"width":2742,"height":1168}},"fileName":"Robovision.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Robovision processes images and outputs classifications of what the vehicles were and what they were doing. When using Ray, they took each of these things and wrapped them in a Ray actor and that gave them a lot more flexibility and an accessible way to the GPU. Code wise implementing a stacked model like this is surprisingly easy to do.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3AgdPEjvrWtxmlq3vJdycT","type":"Entry","createdAt":"2021-09-14T23:23:47.634Z","updatedAt":"2021-09-14T23:23:47.634Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"body":"@ray.remote\nClass Model: \n   def __init__(self, next_actor): \n      self.next = next_actor\n\n   def runmodel(self, inp):\n      out = process(inp); # different for each stage\n      self.next.call.remote(out)\n\n# input_stream -\u003e object_detector -\u003e \n# object_tracker -\u003e speed_calculator -\u003e result_collector\n\nresult_collector = Model.remote()\nspeed_calculator = Model.remote(next_actor=result_collector)\nobject_tracker = Model.remote(next_actor=speed_calculator)\nobject_detector = Model.remote(next_actor=object_tracker)\n\nfor inp in input_stream: \n   object_detector.runmodel.remote(inp)  \n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"At the top of the code, there is an annotation at the top to describe the class called ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Model","nodeType":"text"},{"data":{},"marks":[],"value":" and then you have subclasses that you change the computation for (e.g., ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"speed_calculator","nodeType":"text"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"object_tracker","nodeType":"text"},{"data":{},"marks":[],"value":"). Basically, what you do is for each case is specify who is next in the chain and then you feed the input to the first one and they land at the thing that is actually capturing the data (","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"result_collector","nodeType":"text"},{"data":{},"marks":[],"value":"). The code took a relatively complex stacked model, wrote it in this way, and now it can run on your local machine. By changing the cluster that it runs on, it can easily be run in a distributed manner. Instead of trying to run 5 stacked models on one machine, Ray makes it so you can run 5 stacked models on 5 machines each with their own GPU. Ray takes care of all the coordination and setting up those 5 machines. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Build apps that make ML accessible to non-specialists","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"A third use case is around making machine learning accessible. Say you are looking at images all over the world and you want to find out where piles of garbage are coming into the ocean. ","nodeType":"text"},{"data":{"uri":"https://descarteslabs.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Descartes Labs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" makes this possible by providing an easy to use geospatial data analysis platform with an interface that enables people to analyze petabytes of data at their user’s behest with just a little bit of code. Ray enables this by taking care of the distributed computing details.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"zTVnAfD0itZvJEGd6eg8p","type":"Asset","createdAt":"2021-09-14T23:38:23.262Z","updatedAt":"2021-09-14T23:38:23.262Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"DescartesLabs","description":"If you want to learn more about how Descartes Labs uses Ray, there is an excellent talk on the subject [here](https://www.youtube.com/watch?v=kZCmqnBrUp4) (image courtesy Descartes Labs).","file":{"url":"//images.ctfassets.net/xjan103pcp94/zTVnAfD0itZvJEGd6eg8p/f1eef21bb56ba0dfbed894780dd2b525/DescartesLabsHighResolution.png","details":{"size":4495898,"image":{"width":2860,"height":1554}},"fileName":"DescartesLabsHighResolution.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To undertake ML projects that don’t fit the ML Pipeline","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The final use case are many important and practical ML projects that don’t fit the standard ML pipeline. Some examples of these types of projects include: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Reinforcement Learning: Reinforcement learning mixes the training and testing stages deliberately. We are unaware of any second generation ML system that supports reinforcement learning. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Online Learning: You may want to update your values and behavior online. This means that you have a feedback loop where you are updating your model often on a per minute basis.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Active/Semi-supervised learning.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"For example, ","nodeType":"text"},{"data":{"uri":"https://www.quantumblack.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"QuantumBlack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which is part of McKinsey, used Ray to help build the algorithms that helped that team win the ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/America%27s_Cup"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"America’s Cup ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"- a premiere sailing competition. To learn more about this amazing achievement and how Ray’s ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RLlib","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" library helped enable this, check out the rest of the Tecton talk ","nodeType":"text"},{"data":{"uri":"https://youtu.be/hzW0AKKqew4?t=1564"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". If you would like to learn how other companies are using Ray’s ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RLlib","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" library for reinforcement learning, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"check out this blog","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" highlighting some impressive use cases. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"This post demonstrated how the history of GPU rendering architectures might be giving us hints to where production ML architectures are going. What we saw was the evolution of production ML architectures. This included: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"first generation","nodeType":"text"},{"data":{},"marks":[],"value":" “fixed function” pipelines.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"second generation","nodeType":"text"},{"data":{},"marks":[],"value":" which involves programmability within the pipeline","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"third generation ","nodeType":"text"},{"data":{},"marks":[],"value":"which involves full programmability","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray can be considered an example of a third generation programmable, flexible production ML architecture. It has already led to new and interesting applications like: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Simplifying existing ML architectures ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Parallelizing high performance ML systems","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Making ML accessible to non-specialists","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Highly scalable algorithms for deep reinforcement learning. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you’re interested in learning more about Ray, you can check out the ","nodeType":"text"},{"data":{"uri":"https://ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", join us on ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discourse","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and ","nodeType":"text"},{"data":{"uri":"http://tinyurl.com/ray-white-paper"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"check out the whitepaper!","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" If you're interested in working with us to make it easier to leverage Ray, we're ","nodeType":"text"},{"data":{"uri":"https://jobs.lever.co/anyscale"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"hiring","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"!","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4SA9rDy9K68mQvwo9FwWH7","type":"Asset","createdAt":"2021-09-14T23:42:34.073Z","updatedAt":"2021-09-14T23:42:34.073Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Uber Leveraging Ray","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/4SA9rDy9K68mQvwo9FwWH7/ffb157eaf6488353254319049da24238/UberLeveragingRay.png","details":{"size":226672,"image":{"width":1074,"height":586}},"fileName":"UberLeveragingRay.png","contentType":"image/png"}}},"recommendations":[],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true}},"url":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures","ctaText":"Read more","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mI1ydQdbxC8VN9jq5SE9O","type":"Asset","createdAt":"2022-03-24T22:19:37.854Z","updatedAt":"2022-03-24T22:46:25.932Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"blog-recommended-content-rl-robot-bubble-dark","file":{"url":"//images.ctfassets.net/xjan103pcp94/2mI1ydQdbxC8VN9jq5SE9O/b7509b9f4f23ddf47620aa465a919914/blog-recommended-content-rl-robot-bubble-dark.jpg","details":{"size":40730,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-rl-robot-bubble-dark.jpg","contentType":"image/jpeg"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"SFpqDfxhd1Fkl9YvB2WUM","type":"Entry","createdAt":"2022-03-29T00:09:03.580Z","updatedAt":"2022-03-29T00:09:03.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Deep Dive: Data ingest in a third-generation ML architecture","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5945GdUwJCPlj6wHOPlw7Y","type":"Entry","createdAt":"2021-11-30T16:50:41.093Z","updatedAt":"2022-06-22T16:09:22.594Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Deep Dive: Data Ingest in a Third Generation ML Architecture","slug":"deep-dive-data-ingest-in-a-third-generation-ml-architecture","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12vKMQ3j3q45idk83ZoPYA","type":"Entry","createdAt":"2021-10-01T20:23:37.378Z","updatedAt":"2021-10-01T20:23:37.378Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Chen Shen","slug":"chen-shen"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48lK0gPUqCqrmxWiQi8ve8","type":"Entry","createdAt":"2021-02-16T07:25:36.331Z","updatedAt":"2021-02-16T07:25:36.331Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Clark Zinzow","slug":"clark-zinzow","link":"https://www.linkedin.com/in/clarkzinzow/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2021-11-30","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Distributed libraries allow improved performance by exploiting the full bandwidth of distributed memory, and giving greater programmability. But how does that actually work? What does the code look like?\n\nIn this post, we’ll be looking at a concrete example with code samples: ML ingest with Ray Datasets and Ray Train.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"This is part 3 of our series on ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"third generation ML architectures","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". In the previous post, we talked about how distributed libraries allow improved ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"performance","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" by exploiting the full bandwidth of distributed memory, and giving greater programmability. But how does that ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"actually work","nodeType":"text"},{"data":{},"marks":[],"value":"? What does the code look like?","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this post, we’ll be looking at a concrete example with code samples: ML ingest with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/user_guide.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"We show how these distributed libraries can be woven together with just a few lines of Python--- a key capability not possible in 2nd gen architectures.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"We examine how Datasets and Train use the interoperable primitives of Ray tasks, actors, and objects to enable this composable architecture.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Runnable scripts are available that can be adapted for use on your own Ray cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Small Data Training","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To set the stage, let's consider ML training in the small-data setting. These kinds of pipelines are quite simple since all data fits in memory, and the overhead of shuffling is minimal. You can express it as just a few lines of pseudocode:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3ZZZM2MDEx3oRdMKvIblVb","type":"Entry","createdAt":"2021-11-29T23:28:30.694Z","updatedAt":"2021-11-29T23:28:50.762Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"small data training","body":"data = load_data()\npreprocess(data)\nfor each epoch:\n    random_shuffle(data)\n    train_one_epoch(data)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's review the steps above:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Loading","nodeType":"text"},{"data":{},"marks":[],"value":": Small data is typically read from files on local disk into memory. It may be streamed from files in some cases.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Preprocessing","nodeType":"text"},{"data":{},"marks":[],"value":": Apply simple transformations (i.e., feature engineering).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Shuffling","nodeType":"text"},{"data":{},"marks":[],"value":": Randomly permute the order of items in the dataset. Shuffling randomly for each epoch is ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1709.10432"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"important","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for stochastic gradient descent.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Training","nodeType":"text"},{"data":{},"marks":[],"value":": Fit the model over the data (e.g., using a framework like ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://horovod.readthedocs.io/en/stable/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Horovod","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":").","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Big Data Training Challenges","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Training models over big data adds additional needs around (1) distributed preprocessing, (2) distributed shuffling to improve convergence rates, and (3) pipelined execution with ML training:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed preprocessing","nodeType":"text"},{"data":{},"marks":[],"value":": The data ingestion requirements of large-scale training can be substantial, motivating specialized systems such as ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/2108.09373"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"DPP","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" from Facebook and ","nodeType":"text"},{"data":{"uri":"https://eng.uber.com/petastorm/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Petastorm","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" from Uber. The systems allow for preprocessing to be off-loaded to separate nodes in the cluster distinct from the GPU machines. Some portion of preprocessing can be done offline, but it is desirable for data to be \"minimally preprocessed\" for flexibility.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed shuffling","nodeType":"text"},{"data":{},"marks":[],"value":": It is important for the dataset to be shuffled (randomly re-ordered) for each epoch of training. This can significantly ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1709.10432"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"improve the convergence of SGD","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", but is challenging in the ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/MapReduce"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"distributed setting","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". While ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"global shuffling","nodeType":"text"},{"data":{},"marks":[],"value":" is optimal, typically solutions like TensorFlow/Pytorch data loaders and Petastorm only perform local shuffling due to the engineering complexity of stitching together large-scale data shuffles with ML training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Pipelining data processing and training","nodeType":"text"},{"data":{},"marks":[],"value":": Due to limited cluster memory sizes and the need for random per-epoch shuffles, we see that preprocessing and shuffle computations may need to be interleaved with training. This is only possible today in specialized systems like DPP (e.g., you cannot trivially connect Spark's distributed shuffle with Horovod, since they are separate distributed systems).\nIn other words, our simple pipeline became hard since components necessarily become distributed and pipelined for performance:","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In other words, our simple pipeline became hard since components necessarily become distributed and pipelined for performance:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"10WGpzv003QrG6Mz73dzUo","type":"Entry","createdAt":"2021-11-29T23:32:46.395Z","updatedAt":"2021-11-29T23:32:46.395Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"data = load_data()         # larger than cluster memory :(","body":"data = load_data()         # larger than cluster memory :(\npreprocess(data)           # distributed transforms :(\nfor each epoch:\n    random_shuffle(data)   # distributed shuffle :(\n    train_one_epoch(data)  # pipelined with above distributed steps :(","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Second Generation Approach","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's briefly consider how we could compose existing distributed systems to solve this distributed ingest problem. We need to set up a Spark cluster for data processing, a Horovod cluster for training, a coordinator service for control plane operations, and external storage for data plane communication.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6jWNBb88EkWu12CCvGYDst","type":"Asset","createdAt":"2021-11-30T00:22:56.327Z","updatedAt":"2021-11-30T21:06:31.778Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"2nd Generation - Data Ingest Problem","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6jWNBb88EkWu12CCvGYDst/88488a650b7eb5eb6d88bda7e37ca7c1/dataIngestProblem.png","details":{"size":90348,"image":{"width":1406,"height":666}},"fileName":"dataIngestProblem.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The training pipeline would work in the following steps. First, the coordinator service would (1) submit a shuffle job to the Spark cluster, which (2) reads and writes data out to external storage. Next, the Horovod data reader (e.g., Petastorm) would (3) fetch the written dataset location from the coordinator and (4) read the shuffle data for training. These steps would repeat for each epoch of training, and can run concurrently to optimize execution latencies.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The disadvantages of the 2nd generation approach are:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Lack of programmability","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": need to setup and manage 3+ separate distributed systems. It's also hard to orchestrate with workflow systems due to the interleaving of shuffle with training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Performance overhead","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": intermediate data written to external storage since it needs to cross between distributed systems.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Third Generation Approach","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In contrast, with a 3rd gen architecture we can compose the entire data ingest pipeline using distributed ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"libraries","nodeType":"text"},{"data":{},"marks":[],"value":". In the snippet below ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/examples/train_linear_dataset_example.html#train-linear-dataset-example"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"(see here for a full runnable example)","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we compose a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset-pipeline.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Dataset Pipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with a distributed ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/user_guide.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train Job","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"78pALMO2tFH7mvFI7WL9wK","type":"Entry","createdAt":"2021-11-30T00:24:17.532Z","updatedAt":"2021-11-30T00:24:17.532Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Third Generation Approach","body":"from ray.train import Trainer, get_dataset_shard\n\n# Distributed Preprocessing and Shuffle\npipe = ray.data.read_parquet(path).window(size).repeat()\npipe = pipe.map_batches(preprocess)\npipe = pipe.random_shuffle_each_window()\n\n# Ray Train Function\ndef train_func():\n    model = NeuralNetworkModel(...)\n    model = train.torch.prepare_model(model)\n    for epoch_data in get_dataset_shard().iter_epochs():\n        model.fit(epoch_data.to_torch(...))\n\n# Compose and Run\ntrainer = Trainer(num_workers=3, backend=\"torch\", use_gpu=True)\nresult = trainer.run(train_func, dataset=dataset_pipeline)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The above snippet, while simplified, is able to express the aforementioned ML ingest and training pipeline with just a few lines of Python code--- without any need to wrangle distributed systems. Under the hood, the Dataset and Train libraries leverage Ray Tasks and Actors respectively to execute distributed data preprocessing and ML training. We are able to compose them by just passing a reference to the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"dataset_pipeline","nodeType":"text"},{"data":{},"marks":[],"value":" object to Train:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"n0VLbvBnx8WyFKvQJDDyC","type":"Asset","createdAt":"2021-11-30T00:27:03.418Z","updatedAt":"2021-11-30T21:09:59.864Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"3rdGenTasks andActors","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/n0VLbvBnx8WyFKvQJDDyC/b40de05028d9740c1b3f51af4e20eaeb/3rdGenTasks_andActors.png","details":{"size":148336,"image":{"width":1366,"height":922}},"fileName":"3rdGenTasks andActors.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The above figure illustrates the tasks and actors created by the above code snippet.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Compared to the 2nd gen approach, the 3rd gen approach achieves:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Lower operational and development overheads","nodeType":"text"},{"data":{},"marks":[],"value":": developers can compose and ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"customize","nodeType":"text"},{"data":{},"marks":[],"value":" the entire distributed training system in a single script thanks to the programmability of a 3rd gen architecture.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Better performance","nodeType":"text"},{"data":{},"marks":[],"value":": as we'll see in the case studies, this approach reduces overheads by allowing data to be passed in-memory.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Code Walkthrough","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"So how does it work? Let's walk through the above example starting with the system requirements.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Requirements for the Example","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"For performance, we want data to be passed in-memory between preprocessing, shuffle, and training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"We should support ingestion of a dataset that is larger than memory. In the example below, we'll assume a 2TB dataset, and a cluster with 1TB of memory.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Support for heterogeneous clusters (e.g., a cluster with GPU training nodes and CPU preprocessing nodes).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Part 1: Windowed data loading","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's look at the first part of the code above, which creates a data loading pipeline.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2xvSklEPrmcWNId9RfBs5j","type":"Entry","createdAt":"2021-11-30T00:28:20.053Z","updatedAt":"2021-11-30T00:28:20.053Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"pipe = ray.data.read_parquet(path).window(size).repeat()","body":"pipe = ray.data.read_parquet(path).window(size).repeat()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This uses the Ray Dataset library to create a DatasetPipeline reading our parquet data from disk. Since the dataset (2TB) is larger than our cluster memory (1TB), we use the .window() function to process windows of size=200GB at a time, leaving extra memory headroom for execution. Since we want to loop over the dataset indefinitely, we use the .repeat() operator after that.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Part 2: Preprocessing and shuffle pipeline","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The second part of the pipeline is applying the distributed transform and shuffling operations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"K22lS3qMNQAVVakEyi5mO","type":"Entry","createdAt":"2021-11-30T00:29:09.699Z","updatedAt":"2021-11-30T00:29:09.699Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Preprocessing and shuffle pipeline","body":"pipe = pipe.map_batches(preprocess)\npipe = pipe.random_shuffle()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is telling Ray to transform records in the pipeline with a given ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"preprocess","nodeType":"text"},{"data":{},"marks":[],"value":" function, and then shuffling the entire window randomly (e.g., 200GB at a time), to avoid going out of core. So far, nothing has been executed beyond reading the file metadata--- we're building up a logical pipeline.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Part 3: Ray Train Setup","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next we define the code that is run on each GPU worker and implements distributed training. Each worker can read a particular split of the pipeline we defined by calling ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"get_dataset_shard","nodeType":"text"},{"data":{},"marks":[],"value":". It sets up a model using ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"train.torch.prepare_model","nodeType":"text"},{"data":{},"marks":[],"value":" to participate in distributed training. Then, it trains over the data in each epoch (repeat) of the dataset.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4OVxuAmQZAAr2zL5vrYVnO","type":"Entry","createdAt":"2021-11-30T00:29:55.709Z","updatedAt":"2021-11-30T00:29:55.709Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"train_func","body":"def train_func():\n    model = NeuralNetworkModel(...)\n    model = train.torch.prepare_model(model)\n    for epoch_data in get_dataset_shard().iter_epochs():\n        model.fit(epoch_data.to_torch(...))","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To create the training actors, we create a ray.train.Trainer that requires 3 GPU workers:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"BIl5itQv7ZPDqNfgrNZSP","type":"Entry","createdAt":"2021-11-30T00:30:22.961Z","updatedAt":"2021-11-30T00:30:22.961Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"trainer = Trainer(num_workers=3, backend=\"torch\", use_gpu=True)","body":"trainer = Trainer(num_workers=3, backend=\"torch\", use_gpu=True)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"At this point, our pipeline is fully defined, and our training actors have been created and assigned GPUs in the cluster, we just need to run it.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Part 3: Running everything","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"This line of code triggers the execution of the entire pipeline:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"j221k8qfddcLQDzWQVpcA","type":"Entry","createdAt":"2021-11-30T00:31:25.195Z","updatedAt":"2021-11-30T00:31:25.195Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Part 3: Running everything","body":"result = trainer.run(train_func, dataset=dataset_pipeline)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"So what's happening in the cluster?","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train sends actor method calls to each actor to run its given training function.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Each actor pulls data from the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"DatasetPipeline","nodeType":"text"},{"data":{},"marks":[],"value":" shard given to it (each pipeline shard contains a handle to a coordinator actor created by Datasets for this DatasetPipeline instance).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"This triggers actor calls to the coordinator actor.\na. The coordinator schedules execution of the next window of the pipeline, e.g., Ray tasks that use CPU nodes in the cluster to:\n      i. load data for the window (200GB)\n      ii. preprocess the data\n      iii. randomly shuffle the data\n      iv. split up the data and assign splits to trainer actors\nb. The coordinator returns to the trainer actors object references to their assigned Dataset split.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The trainer actors ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.get()","nodeType":"text"},{"data":{},"marks":[],"value":" data blocks from their Dataset split and generate mini-batches to pass to the underlying learning library (i.e., PyTorch).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can visualize the overall dataflow in the following timeline diagram. Once data is loaded, shuffle and execution proceed in a fully pipelined way, leveraging tasks running on CPU nodes to implement shuffling, and actors running on GPUs for training:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"51nJaf8KaCjBBQOzS4W6mU","type":"Asset","createdAt":"2021-11-30T05:53:49.763Z","updatedAt":"2021-11-30T05:53:49.763Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"dataflow","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/51nJaf8KaCjBBQOzS4W6mU/d490293c792df1d7803eb30f13682b1d/dataflow.png","details":{"size":90897,"image":{"width":1210,"height":432}},"fileName":"dataflow.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Try it out yourself with these examples in Ray 1.8:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/examples/train_linear_dataset_example.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://docs.ray.io/en/master/train/examples/train_linear_dataset_example.html","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/examples/big_data_ingestion.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://docs.ray.io/en/master/data/examples/big_data_ingestion.html","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Benchmarks","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a previous blog post, we discussed the ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/why-third-generation-ml-platforms-are-more-performant"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"performance advantages","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" of passing data in-memory and with pipelining and showed improvements in an ablation study. Since then, Datasets has been used by several open source users to implement large-scale shuffled ML ingest. We present two case studies from our ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/presentation/d/1zANPlmrxQkjPU62I-p92oFO3rJrmjVhs73hL4YbM4C4"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyData Dataset talk","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" showing significant performance improvements:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Case Study 1: high-tech ML platform startup","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Dask-on-Ray → Datasets → Horovod","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Dask-on-Ray and Datasets was ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"8x faster","nodeType":"text"},{"data":{},"marks":[],"value":" than Pandas + S3+ Petastorm, even on a single machine.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Benchmark:","nodeType":"text"},{"data":{},"marks":[],"value":" ","nodeType":"text"},{"data":{"uri":"http://ludwig.ai/"},"content":[{"data":{},"marks":[],"value":"Ludwig AI","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" model, NYC Taxi dataset (5 GB subset), single g4dn.4xlarge instance","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2UdwUzwL7HNFGOSUgHydvq","type":"Asset","createdAt":"2021-11-30T07:55:45.520Z","updatedAt":"2021-11-30T21:14:24.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Shuffled Data Benchmark","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2UdwUzwL7HNFGOSUgHydvq/3bdea36e01ae221aa6d802001c57a528/shuffledDataBenchmark.png","details":{"size":79641,"image":{"width":1108,"height":656}},"fileName":"shuffledDataBenchmark.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Case Study 2: large transport tech company","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"S3 → Datasets → Horovod","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Datasets from S3 was ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"4x faster","nodeType":"text"},{"data":{},"marks":[],"value":" than Petastorm from S3","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Benchmark:","nodeType":"text"},{"data":{},"marks":[],"value":" 1.5 TB synthetic tabular dataset, 16 nodes (40 vCPUs, 180 GB RAM), 2 shuffle windows ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Gl6shcoyCtt7P1xHuYWO0","type":"Asset","createdAt":"2021-11-30T07:57:18.136Z","updatedAt":"2021-11-30T07:57:18.136Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Petastorm Datasets","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6Gl6shcoyCtt7P1xHuYWO0/63479b61e30fb33db13df73f72e42da4/petastormDatasets.png","details":{"size":28808,"image":{"width":626,"height":290}},"fileName":"petastormDatasets.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We believe 3rd gen ML architectures will help engineers develop and standardize infrastructure for large-scale ML apps. This blog demonstrated that with just a single Python script, we can connect distributed data preprocessing with training in a highly performant way.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Moreover, in true 3rd-gen fashion, we were able to do the above without building a specialized system. We used Ray to interleave execution of two independent distributed libraries--- a key capability not possible in 2nd gen architectures. This composability is possible since both libraries are built on the common and interoperable primitives of Ray tasks, actors, and objects.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While we're just getting started with ML ingest-- look for new examples and performance enhancements as Ray Datasets graduates from beta in the next few months--- this is just one aspect of programmable distributed compute with Ray. Check out other use cases in Tuning, Training, Serving, and more here: ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://www.ray.io/","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7hIMsn7WDpX9zGqRwfZnvu","type":"Asset","createdAt":"2021-11-30T21:13:58.335Z","updatedAt":"2021-11-30T21:13:58.335Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"3rdGenTasks andActors","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7hIMsn7WDpX9zGqRwfZnvu/0f04538b77d8cc926e7021a3be138720/3rdGenTasks_andActors.png","details":{"size":148336,"image":{"width":1366,"height":922}},"fileName":"3rdGenTasks andActors.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}}],"hideIntro":true,"recommendations":[]}},"url":"https://www.anyscale.com/blog/deep-dive-data-ingest-in-a-third-generation-ml-architecture","ctaText":"Read more","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"qyo3m0dc8CvMNCfhxDy5e","type":"Asset","createdAt":"2022-03-24T21:30:36.648Z","updatedAt":"2022-03-24T21:30:36.648Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-code-dark","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/qyo3m0dc8CvMNCfhxDy5e/409ed6f79aea5822b10aaa365318a005/blog-recommended-content-code-dark.jpg","details":{"size":39745,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-code-dark.jpg","contentType":"image/jpeg"}}},"recommendations":null}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}}],"hideIntro":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2cRsEGe2jMlctlahwXSXAg","type":"Entry","createdAt":"2021-06-14T17:13:15.535Z","updatedAt":"2022-06-22T17:06:09.329Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":13,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Ray Distributed Library Patterns","slug":"ray-distributed-library-patterns","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6xEJbq1IfaJwNFeM0tPFEy","type":"Entry","createdAt":"2021-01-13T18:21:16.252Z","updatedAt":"2021-01-13T18:32:11.251Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Zhe Zhang","slug":"zhe-zhang","link":"https://www.linkedin.com/in/zhezhang-zhz/"}}],"publishedDate":"2021-06-14","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Ray has many library integrations, from machine learning libraries such as Horovod and Hugging Face to data processing frameworks such as Spark, Modin, and Dask. But what does it mean to be \"integrated with Ray\"? And what benefits does it provide to library developers and users?","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray has many library ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-libraries.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"integrations","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", from machine learning libraries such as ","nodeType":"text"},{"data":{"uri":"https://horovod.readthedocs.io/en/stable/ray_include.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Horovod","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/transformers/master/main_classes/trainer.html#transformers.Trainer.hyperparameter_search"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Hugging Face","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to data processing frameworks such as ","nodeType":"text"},{"data":{"uri":"https://github.com/Intel-bigdata/oap-raydp"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/modin/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Modin","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/dask-on-ray.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Dask","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". But what does it mean to be \"integrated with Ray\"? And what benefits does it provide to library developers and users?","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This blog post answers these questions by looking at ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"three","nodeType":"text"},{"data":{},"marks":[],"value":" ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"common Ray library integration patterns","nodeType":"text"},{"data":{},"marks":[],"value":". To do this, we’ll look at several examples of Ray libraries at each integration level, show how each pattern or “level” of integration solves different problems for library authors and users, and explain how to decide on the right level of integration.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Three Levels of Ray Integration","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Level 1: Scheduling only; Communication done outside Ray","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"17hmpr0c19hFEProZfLnGf","type":"Asset","createdAt":"2021-06-14T15:59:44.516Z","updatedAt":"2021-06-14T17:18:05.589Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Ray Distributed Library Patterns (Figure 1)","file":{"url":"//images.ctfassets.net/xjan103pcp94/17hmpr0c19hFEProZfLnGf/78a09caccd623b53b8757e6d95362037/RayDistributedLibraryPatterns.png","details":{"size":95992,"image":{"width":1130,"height":276}},"fileName":"RayDistributedLibraryPatterns.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Horovod [","nodeType":"text"},{"data":{"uri":"https://horovod.readthedocs.io/en/stable/api.html#horovod-ray-api"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":"]: Horovod uses Ray actors to schedule Horovod worker processes, which enables ","nodeType":"text"},{"data":{"uri":"https://eng.uber.com/horovod-ray/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"dynamic scaling","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" and data pre-processing to be handled within Ray.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Pytorch Lightning [","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray_lightning"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":"]: Similarly, Pytorch Lightning supports distributed data parallel training in Ray, where Ray is used to launch processes.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Spark / RayDP [","nodeType":"text"},{"data":{"uri":"https://github.com/oap-project/raydp"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":"]: RayDP enables Spark to run inside Ray by launching Spark executors as Ray Java actors.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"At this level, libraries are leveraging Ray as a language-integrated actor scheduler. The actual communication between actors is mostly done ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"out-of-band","nodeType":"text"},{"data":{},"marks":[],"value":" (outside Ray). For example, ","nodeType":"text"},{"data":{"uri":"https://horovod.readthedocs.io/en/stable/ray_include.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Horovod-on-Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" uses NCCL or MPI-based collective communications, and ","nodeType":"text"},{"data":{"uri":"https://github.com/oap-project/raydp"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RayDP","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" uses Spark's internal RPC and object manager.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"x4LXVgrAatvEYfRBvV9nJ","type":"Asset","createdAt":"2021-06-14T16:10:56.471Z","updatedAt":"2021-06-14T16:10:56.471Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Distributed Library Patterns (Figure 2)","file":{"url":"//images.ctfassets.net/xjan103pcp94/x4LXVgrAatvEYfRBvV9nJ/b0a29c644acbbeb606a557f31a2d754c/figure2.png","details":{"size":160120,"image":{"width":1472,"height":685}},"fileName":"figure2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"At a first glance, it seems that this integration is superficial. However, this level of integration is often the right choice for libraries that have mature, high-performance internal communication stacks but still want additional features such as:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Fine-grained control over worker scheduling and fault tolerance (e.g., ","nodeType":"text"},{"data":{"uri":"https://eng.uber.com/horovod-ray/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Elastic Horovod","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":").","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The ability to run  subroutines within a larger distributed program (e.g., tuning), or pass data in-memory to downstream stages (e.g., RayDP =\u003e ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/python/ray/util/data/dataset.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLDataset","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" =\u003e Training).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3fyH62AiEfFku6dWgq0hOR","type":"Entry","createdAt":"2021-06-11T22:21:37.487Z","updatedAt":"2021-06-11T22:21:37.487Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Out-of-band Communication Example","body":"# Out-of-band communication example.\n\n# Launch the actors.\nworkers = [MPIActor.remote(...) for _ in range(10)]\nfor i, w in enumerate(workers):\n   w.init_for_rank.remote(i)\n\n# Ray triggers the collective op, but the actual communication\n# happens between the workers out-of-band (e.g., via MPI).\nray.get([w.collective_op.remote() for w in workers])","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We see libraries integrating with Ray in this way often because getting the APIs right to compose and schedule distributed programs is hard. Ray provides the common \"","nodeType":"text"},{"data":{"uri":"https://medium.com/distributed-computing-with-ray/data-processing-support-in-ray-ae8da34dce7e"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"distributed glue","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"\" here so libraries can focus on their core functionality rather than operational aspects.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Level 2: Scheduling and communication","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1pPG7b7UsnlYbRPwRhRh5Y","type":"Asset","createdAt":"2021-06-14T17:08:08.445Z","updatedAt":"2021-06-14T17:21:32.228Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Ray Distributed Library Patterns (Figure 3)","file":{"url":"//images.ctfassets.net/xjan103pcp94/1pPG7b7UsnlYbRPwRhRh5Y/4c18463db0b6e6987656729612c3f7c8/figure3_raydistributed.png","details":{"size":148138,"image":{"width":1314,"height":284}},"fileName":"figure3_raydistributed.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Hugging Face [","nodeType":"text"},{"data":{"uri":"https://huggingface.co/blog/ray-rag"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"blog","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":"]: Hugging Face uses Ray actor calls for faster distributed document retrieval for fine-tuning.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Online resource allocation at Ant [","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/online-resource-allocation-with-ray-at-ant-group"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"blog","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":"]: Ant Group uses actor communication to implement online optimization for ads and recommendations.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Scikit-Learn [","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/joblib.html"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":"]: Ray supports Scikit-Learn through the joblib parallel interface.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Seldon Alibi [","nodeType":"text"},{"data":{"uri":"https://www.seldon.io/how-seldons-alibi-and-ray-make-model-explainability-easy-and-scalable/"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"blog","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":"]: Similarly, Alibi uses actors and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" as methods for scaling out batch prediction.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Many apps and libraries use Ray for parallelizing their programs, leveraging both Ray's scheduling functionality (Level 1 benefits) as well as using task and actor method calls for communication. Under the hood, Ray is ","nodeType":"text"},{"data":{"uri":"https://medium.com/distributed-computing-with-ray/how-ray-uses-grpc-and-arrow-to-outperform-grpc-43ec368cb385?source=post_page-----f34c01b7905c--------------------------------"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"translating task and actor invocations into low-level gRPC calls","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Libraries that are a good fit for this kind of integration require one or more of the following:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Low latency communication and a desire for a simpler programming model than sending raw RPC calls to a pool of compute workers.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Computation that can be parallelized into many individual tasks (e.g., distributed ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/multiprocessing.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"multiprocessing","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/joblib.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"joblib","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":").","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The ability to coordinate a complex topology of actors or tasks (e.g., for implementing ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/rllib.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"reinforcement learning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/online-resource-allocation-with-ray-at-ant-group"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"online decision system","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", or ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"model serving pipeline","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":").","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2bHljoYhj659GGhMTKZv5b","type":"Entry","createdAt":"2021-06-11T22:34:23.976Z","updatedAt":"2021-06-11T22:34:23.976Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Ray Distributed Library Patterns (# Task example)","body":"# Task example.\nresults = [evaluate.remote(latest_params) for arg in work_to_do]\nray.get(results)\n\n# Actor example.\nworkers = [Actor.remote() for _ in range(5)]\nfor w in workers:\n    w.update(latest_params)\nresults = [w.evaluate.remote() for w in workers]\nray.get(results)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"For applications that require specialized communication, we currently still recommend a Level 1 type integration. In the future, ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/document/d/1ySIg04FSrkCWUJGe-XX_mVw2sUH1kP8a8HzZn2UClj8/edit#heading=h.f1hawkpighc0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Collectives","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" will enable more libraries to be implemented natively in Ray with high performance.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Level 3: Scheduling, communication, and distributed memory","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1UC653ghDXauhC6N0apv2H","type":"Asset","createdAt":"2021-06-14T17:10:25.640Z","updatedAt":"2021-06-14T17:10:25.640Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"RayDistributedLibraryPatterns (Figure 4)","file":{"url":"//images.ctfassets.net/xjan103pcp94/1UC653ghDXauhC6N0apv2H/1fd47b1864559dd9ad4d4facec48e1af/figure4.png","details":{"size":136799,"image":{"width":1464,"height":246}},"fileName":"figure4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"XGBoost [","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/xgboost_ray"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":"]: XGBoost on Ray leverages the object store to hold the distributed data matrix used for training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Modin [","nodeType":"text"},{"data":{"uri":"https://github.com/modin-project/modin"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":"]: Modin uses the object store to store partition blocks in shared memory, and tasks to execute compute over these blocks.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Mars [","nodeType":"text"},{"data":{"uri":"https://github.com/mars-project/mars"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":"]: Mars uses actors and the object store to implement distributed execution and data storage.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Apache Airflow [","nodeType":"text"},{"data":{"uri":"https://registry.astronomer.io/providers/ray"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":"]: The Ray Airflow provider allows Airflow operators to be executed in Ray and pass intermediate data using the object store.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Dask [","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/tree/master/python/ray/util/dask"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":"]: The Ray Dask scheduler stores intermediate Dask results in the object store and leverages tasks for execution.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Last but not least, libraries may also seek to leverage Ray's distributed object store, which is seamlessly integrated with Ray's task and actor APIs. Technically speaking, any application that is passing large (\u003e100KB in size) objects around in Ray is already leveraging Ray's distributed memory system for performance.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Level 3 libraries further take advantage of these distributed memory features of Ray:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"First-class ","nodeType":"text"},{"data":{"uri":"https://www.usenix.org/system/files/nsdi21-wang.pdf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"object references","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which can be passed and retrieved freely between tasks and actors in Ray, and are automatically garbage collected if they fall out of scope.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://ray-project.github.io/2017/08/08/plasma-in-memory-object-store.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Shared-memory support","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which means that large objects can be shared by multiple workers on the same machine without any copies, greatly increasing efficiency.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Object spilling, which enables large-scale ","nodeType":"text"},{"data":{"uri":"https://medium.com/distributed-computing-with-ray/executing-a-distributed-shuffle-without-a-mapreduce-system-d5856379426c"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"data processing","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" workloads to be executed by spilling objects to disk or remote storage.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"65JnrAH4sOOKvmezs83FlN","type":"Entry","createdAt":"2021-06-11T22:41:03.202Z","updatedAt":"2021-06-11T22:41:03.202Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Ray distributed library patterns (store a large dataset)","body":"# Store a large dataset in the object store.\ndata_R = [ray.put(block) for block in large_data_blocks]\n\n# Store a small dataset in the object store.\ndata_S = ray.put(small_data)\n\n# Example of implementing broadcast join between R and S using tasks.\njoined_R_S = [join.remote(R_i, data_S) for R_i in data_R]","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray's built-in libraries such as ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/rllib.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RLlib","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" are also Level 3 library examples, leveraging the object store to provide best-in-class performance and flexibility.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deciding on the right level of integration","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"It can be tempting to think \"more integration is better\". However, in accordance with the ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Rule_of_least_power"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"rule of least power","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"we recommend choosing the minimal level of integration needed for the library in question","nodeType":"text"},{"data":{},"marks":[],"value":". Our goal with Ray is to enable users to build complex distributed applications with the simplicity of a single Python file, which is enabled even with Level 1 integration.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Here are a couple rules of thumb:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Libraries that have very specialized communication requirements (i.e., distributed allreduce) should start with Level 1 integration (which can be expanded over time).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"For example, Horovod on Ray uses mostly actors, but is expanding into using objects and tasks as well for data preprocessing.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"It's ok to pick and choose what Ray features to leverage---they all work together, but most applications will use only a subset.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"For example, RayDP uses Ray actors only, whereas Dask-on-Ray uses tasks and objects.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The level of effort to integrate also varies depending on the level of integration and the maturity of the library. As a general rule:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Level 1 integration can start out at \u003c200 lines of code.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"This is typically because libraries already have well-separated routines for launching workers.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Level 2-3 integration can, depending on the library, range from straightforward to very high effort. For example,","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The Dask-on-Ray integration took only a ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/python/ray/util/dask/scheduler.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"few hundred lines of code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", because Dask was designed to have pluggable schedulers.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Similarly, scikit-learn integration took ~","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/python/ray/util/joblib/ray_backend.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"50 lines of code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", leveraging joblib as an integration point.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"On the other hand, fully integrating e.g., Spark to use Ray's memory subsystem would be a large engineering project.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Interoperability between Ray libraries","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"A final question is \"how can Ray libraries work together\"?","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Here are a few common patterns:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Library as a subroutine: in this pattern, one distributed library is run as a subroutine of another. This can be either:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Nested, for example, running a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/tune/tutorials/tune-xgboost.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"XGBoost training job as a trial in Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Interwoven, for example, you may want to use Ray actors or tasks within a map function when using ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/dask-on-ray.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Dask-on-Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Passing data between libraries:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"In Airflow-on-Ray, data is ","nodeType":"text"},{"data":{"uri":"https://www.astronomer.io/blog/airflow-ray-data-science-story"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"passed between workflow steps","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" using Ray's in-memory object store.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"RayDP writes the output of a Spark job into the Ray object store, allowing it to be ","nodeType":"text"},{"data":{"uri":"https://github.com/oap-project/raydp/tree/master/examples"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"easily accessible from downstream application logic","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" via the MLDataset API.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, an app can simply invoke multiple libraries. A Ray application can naturally leverage libraries in separate tasks and actors without the libraries ever needing to know about each other.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We hope this blog makes it easier for future developers to make an informed decision on how to build libraries on top of Ray. If you have any questions, please reach out to us on the ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray forums","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". If you're interested in working with us to make it easier to leverage Ray, we're ","nodeType":"text"},{"data":{"uri":"https://jobs.lever.co/anyscale"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"hiring!","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"x4LXVgrAatvEYfRBvV9nJ","type":"Asset","createdAt":"2021-06-14T16:10:56.471Z","updatedAt":"2021-06-14T16:10:56.471Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Distributed Library Patterns (Figure 2)","file":{"url":"//images.ctfassets.net/xjan103pcp94/x4LXVgrAatvEYfRBvV9nJ/b0a29c644acbbeb606a557f31a2d754c/figure2.png","details":{"size":160120,"image":{"width":1472,"height":685}},"fileName":"figure2.png","contentType":"image/png"}}},"mainImageFit":"contain","tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true,"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1Z1SzUHcFJGqkfO1MyAWPz","type":"Entry","createdAt":"2021-02-16T07:36:10.876Z","updatedAt":"2022-06-22T17:17:34.914Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":11,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Data Processing Support in Ray","slug":"data-processing-support-in-ray","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Dlqb2sA0xjzsxkIPLH2im","type":"Entry","createdAt":"2021-02-16T07:24:21.096Z","updatedAt":"2021-02-16T07:24:21.096Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Sang Cho","slug":"sang-cho","link":"https://github.com/rkooo567"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3MwXLiQy2kRWe7mVi2RjRw","type":"Entry","createdAt":"2021-02-16T07:28:19.034Z","updatedAt":"2021-02-16T07:28:19.034Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Alex Wu","slug":"alex-wu","link":"https://github.com/wuisawesome"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48lK0gPUqCqrmxWiQi8ve8","type":"Entry","createdAt":"2021-02-16T07:25:36.331Z","updatedAt":"2021-02-16T07:25:36.331Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Clark Zinzow","slug":"clark-zinzow","link":"https://www.linkedin.com/in/clarkzinzow/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2aJRcdEj9iAktAnqn0wucE","type":"Entry","createdAt":"2021-02-16T07:30:38.421Z","updatedAt":"2021-02-16T07:30:38.421Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Stephanie Wang","slug":"stephanie-wang","link":"https://github.com/stephanie-wang"}}],"publishedDate":"2021-02-16","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},"intro":"This blog post highlights two features in the latest Ray 1.2 release: native support for spilling to external storage, and support for libraries from the Python data processing ecosystem, including integrations for PySpark and Dask.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Over the past couple years, we’ve heard from many Ray users that they wish to incorporate parallel data processing more directly into their Python applications. These use cases range from processing input CSVs faster to shuffling hundreds of terabytes of ML input data (distributed ETL). The common desire is to stitch together an application with data processing and ML components in a single Python program, without needing to worry about setting up, maintaining, and gluing together separate clusters or services.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Rather than write another DataFrames library for Ray, we're focusing on supporting the integration of other frameworks so that you can","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Break down compute silos: ","nodeType":"text"},{"data":{},"marks":[],"value":"Invoke data processing APIs like Dask DataFrames directly from application code alongside ML libraries like Tune, XGBoost, and Horovod.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Leverage distributed memory: ","nodeType":"text"},{"data":{},"marks":[],"value":"Automatic object spilling enables data library developers and users to take advantage of Ray’s shared-memory object store.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"A single substrate for distributed data processing and machine learning","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The status quo for many teams today is to use ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"workflow orchestrators","nodeType":"text"},{"data":{},"marks":[],"value":" such as ","nodeType":"text"},{"data":{"uri":"https://airflow.apache.org"},"content":[{"data":{},"marks":[],"value":"Airflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"http://kubeflow.org"},"content":[{"data":{},"marks":[],"value":"Kubeflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to stitch distributed programs together. This enables use cases such as:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Running training jobs on the results of batch ETL jobs (e.g., distributed training, XGBoost on Ray).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Composing pipelines together (e.g., connecting Spark =\u003e PyTorch) from separately written operators (tasks).","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, in many cases the use of workflow orchestrators adds costs, due to the compute silo effect, in terms of ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"system efficiency","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"operations","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"System efficiency:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The setup overhead of workflow tasks adds latency and reduces the cost efficiency of the job.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Intermediate data must be materialized to external storage (e.g., HDFS or S3), also adding latency.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Operationally:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"There is high operational overhead maintaining separate distributed systems for data processing.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The pipeline itself must be written and configured in a separate configuration language (vs. a single Python program). This also limits expressivity.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Our goal for supporting data processing in Ray is to enable distributed applications with less glue code, increasing expressivity and reducing system overheads.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"blockquote"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the remainder of this blog we’ll cover the new object spilling feature in Ray and library integrations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Object Spilling","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"We’re happy to announce that Ray now supports ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/memory-management.html#object-spilling"},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"automatic object spilling","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to local disk and Cloud object stores (e.g., S3). This feature is available starting in ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-1.2.0"},"content":[{"data":{},"marks":[],"value":"Ray 1.2","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and will be enabled by default in the next release.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Object spilling enables libraries already using Ray’s object store to work with datasets that may not fit in memory. It also allows Ray programs to operate directly on big datasets. For example, you can now write a simple ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/python/ray/experimental/shuffle.py"},"content":[{"data":{},"marks":[],"value":"out-of-core distributed shuffle","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in just a few dozen lines of Python:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7t3kvc0SRuUHdJXovN33yO","type":"Asset","createdAt":"2021-02-16T07:48:45.338Z","updatedAt":"2021-02-16T07:49:48.039Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Object Spilling 1","description":"Simple shuffle example written in Ray with different object store memory limits on a single 32-core machine (AWS i3.8xlarge). The maximum working set size for this shuffle is 2x the input data size, so a memory limit less than that will require spilling. You can try running this yourself on Ray master with `python -m ray.experimental.shuffle - help`.","file":{"url":"//images.ctfassets.net/xjan103pcp94/7t3kvc0SRuUHdJXovN33yO/39bff881583c8d251e9016f6eaaf75e6/objectSpilling1.png","details":{"size":46180,"image":{"width":1067,"height":435}},"fileName":"objectSpilling1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Here’s an example of using Ray’s object spilling to enable out-of-core workloads for Dask-on-Ray. Note that this code does a full sort of the dataset, so we will not compare it directly to the simple shuffle above.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"XXDVOdoRYTSmBAn2bKldF","type":"Asset","createdAt":"2021-02-16T07:53:36.293Z","updatedAt":"2021-02-16T07:53:36.293Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"objectSpilling2","description":"Dask-on-Ray sort with Ray's object spilling disabled vs. enabled. This is run on a 32-core machine (AWS i3.8xlarge) with a 30GB memory limit for Ray's object store. With object spilling disabled, the application receives an out-of-memory error on 100GB.","file":{"url":"//images.ctfassets.net/xjan103pcp94/XXDVOdoRYTSmBAn2bKldF/41211e942f6ec3d8c5db4b1b8ee66c8d/objectSpilling2.png","details":{"size":36749,"image":{"width":1067,"height":442}},"fileName":"objectSpilling2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Note that standalone systems like Spark already have native support for object spilling. However, this is a significant effort for libraries like Modin and Mars. This is where integration with Ray can reduce burden for library developers and enable new workloads for end users. We’ll cover how this works in the next section.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Note: To reproduce these benchmarks, you will need to run on the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/installation.html#daily-releases-nightlies"},"content":[{"data":{},"marks":[],"value":"nightly wheels","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". You can also do this by installing ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-1.2.0"},"content":[{"data":{},"marks":[],"value":"Ray v1.2","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"pip install -U ray","nodeType":"text"},{"data":{},"marks":[],"value":") and running ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray install-nightly","nodeType":"text"},{"data":{},"marks":[],"value":" from the command line.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Library Integrations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"All of the following examples are runnable with ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-1.2.0"},"content":[{"data":{},"marks":[],"value":"Ray v1.2","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", unless otherwise noted.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"PySpark on Ray","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"With PySpark support in ","nodeType":"text"},{"data":{"uri":"https://github.com/oap-project/raydp"},"content":[{"data":{},"marks":[],"value":"RayDP","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", you can leverage the power of Spark with just a few lines of code. Once RayDP is initialized, you can use any PySpark API you want:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Installation:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DuxTrcpIqRX7bQU4eMU9x","type":"Entry","createdAt":"2021-02-16T07:59:07.630Z","updatedAt":"2021-02-16T07:59:07.630Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"pip install ray raydp","body":"$ pip install ray raydp","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Example:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aKaYE1lucrhahxiAMOh7w","type":"Entry","createdAt":"2021-02-16T08:00:01.029Z","updatedAt":"2021-02-17T20:34:48.017Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"PySparkOnRayDataProcessing","body":"import ray\nimport raydp\n\nray.init()\n\n@ray.remote\nclass PySparkDriver:\n  def __init__(self):\n  self.spark = raydp.init_spark(\n    app_name='RayDP example',\n    num_executors=2,\n    executor_cores=2,\n    executor_memory='4GB')\n\n  def foo(self):\n    return self.spark.range(1000).repartition(10).count()\n\ndriver = PySparkDriver.remote()\nprint(ray.get(driver.foo.remote()))","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Under the hood, ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"raydp.init_spark","nodeType":"text"},{"data":{},"marks":[],"value":" creates num_executors Ray Java ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"actors","nodeType":"text"},{"data":{},"marks":[],"value":" that each launch a Spark executor. The actors communicate between each other using Spark’s internal IO layer. In the above example, we use the PySparkDriver actor class to wrap the Spark session so that it is callable from other parts of a Ray application. Note that this integration doesn’t use Ray’s object store, unlike the other integrations we’ll cover below.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you like to learn more about RayDP, please check out the ","nodeType":"text"},{"data":{"uri":"https://github.com/oap-project/raydp"},"content":[{"data":{},"marks":[],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Dask on Ray","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"We recently released a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/dask-on-ray.html"},"content":[{"data":{},"marks":[],"value":"lightweight plugin","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for ","nodeType":"text"},{"data":{"uri":"https://dask.org/"},"content":[{"data":{},"marks":[],"value":"Dask","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" that lets you use Ray as a backend for executing Dask tasks. This includes Ray-specific ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/issues/13959"},"content":[{"data":{},"marks":[],"value":"optimizations","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for large scale shuffle performance. Here’s how you can try it out:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Installation:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"62g4odN6EnJFtziVVm12Ll","type":"Entry","createdAt":"2021-02-16T08:01:13.715Z","updatedAt":"2021-02-16T08:01:13.715Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"pip install ray 'dask[dataframe]' pandas numpy","body":"$ pip install ray 'dask[dataframe]' pandas numpy","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Note that you do not need to install dask.distributed even if you are using a cluster because Ray will handle the distribution.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Example:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4z1RGZmmOGMEeOdkJWKoaE","type":"Entry","createdAt":"2021-02-16T08:02:42.024Z","updatedAt":"2021-02-16T08:02:42.024Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Dask on Ray Data Processing","body":"import ray\n\nimport dask\n\nimport dask.dataframe as dd \nimport pandas as pd\nimport numpy as np\nfrom ray.util.dask import ray_dask_get\n\ndask.config.set(scheduler=ray_dask_get) # Sets Ray as the default backend.\n\nray.init()\n\ndf = pd.DataFrame(np.random.randint(0, 100, size=(2**10, 2**8)))\ndf = dd.from_pandas(df, npartitions=10)\nprint(df.head(10))","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Under the hood, Dask dispatches tasks to Ray for scheduling and execution. Task inputs and outputs get stored in Ray’s distributed, shared-memory object store. This means that you can seamlessly mix Dask and other Ray library workloads.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you like to learn more about Dask on Ray, please check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/dask-on-ray.html"},"content":[{"data":{},"marks":[],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Modin DataFrames","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://modin.readthedocs.io/en/latest/supported_apis/index.html"},"content":[{"data":{},"marks":[],"value":"Modin DataFrames","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" enables transparent scaling of Pandas on Ray. Try it out:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Installation:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3gOhwCodEhdhXmbifyknLG","type":"Entry","createdAt":"2021-02-16T08:03:34.886Z","updatedAt":"2021-02-16T08:03:34.886Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"pip install 'modin[ray]' pandas numpy","body":"$ pip install 'modin[ray]' pandas numpy","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Example:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3TlvtuOITxG3hvGqgyAeTo","type":"Entry","createdAt":"2021-02-16T08:04:16.043Z","updatedAt":"2021-02-16T08:04:16.043Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Modin DataFrames Data Processing","body":"import ray\nray.init() # Modin defaults to backing Ray's object store with disk. Start Ray before importing modin to use shared memory instead.\n\nimport modin.pandas as pd\nimport numpy as np\n\nframe_data = np.random.randint(0, 100, size=(2**10, 2**8))\ndf = pd.DataFrame(frame_data)\n\nprint(df.head(10))","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you like to learn more about Modin, please check out the ","nodeType":"text"},{"data":{"uri":"https://modin.readthedocs.io/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Mars on Ray","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://modin.readthedocs.io/en/latest/supported_apis/index.html"},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://docs.pymars.org/en/latest/"},"content":[{"data":{},"marks":[],"value":"Mars","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a tensor-based framework that can scale numpy, Pandas, and scikit-learn applications. It recently added ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/mars-on-ray.html"},"content":[{"data":{},"marks":[],"value":"Ray as a backend","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for distributed execution.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Here’s how you can try it out:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Installation:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7mLfdLFvD7ZJniBRXxRBNH","type":"Entry","createdAt":"2021-02-16T08:06:18.555Z","updatedAt":"2021-02-16T08:06:18.555Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"pip install -U pymars ray==1.0","body":"$ pip install -U pymars ray==1.0","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Note that you do not need to install pymars[distributed] even if you are using a cluster because Ray will handle the distribution.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Example","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2XqAbSGoNRiWYmIU9D3pSD","type":"Entry","createdAt":"2021-02-16T08:07:07.539Z","updatedAt":"2021-02-16T08:07:07.539Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Mars on Ray Example","body":"from mars.session import new_session\nray_session = new_session(backend='ray').as_default() # Set Ray as the default backend.\n\nimport mars.dataframe as md\nimport mars.tensor as mt\n\nt = mt.random.randint(100, size=(2**10, 2**8))\ndf = md.DataFrame(t)\nprint(df.head(10).execute())","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The Mars-on-Ray integration is experimental and under active development, but you can learn more from the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/mars-on-ray.html"},"content":[{"data":{},"marks":[],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is just the beginning to provide support in Ray for 3rd party data processing libraries and frameworks. We’re actively developing features like object spilling and memory management to improve stability and performance. Keep an eye out for our next blog post on a large-scale distributed shuffle to better support these data processing frameworks.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the meantime, if you’re interested in learning more, check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/whitepaper.html"},"content":[{"data":{},"marks":[],"value":"whitepaper","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or join us on the ","nodeType":"text"},{"data":{"uri":"http://discuss.ray.io"},"content":[{"data":{},"marks":[],"value":"Ray Discourse","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2GEqK8HdAD6cMXrDR39LrS","type":"Asset","createdAt":"2021-02-16T07:43:37.222Z","updatedAt":"2021-02-16T07:43:37.222Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"AspectDataProcessingImage","file":{"url":"//images.ctfassets.net/xjan103pcp94/2GEqK8HdAD6cMXrDR39LrS/e38fcad35a113f34fbeb3d013ea44239/AspectDataProcessing.png","details":{"size":114962,"image":{"width":1072,"height":537}},"fileName":"AspectDataProcessing.png","contentType":"image/png"}}},"mainImageFit":"cover","tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UTRBgrxnFhyFu7F9N7tAy","type":"Entry","createdAt":"2020-11-05T18:42:48.659Z","updatedAt":"2022-06-22T17:25:30.365Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"The Ideal Foundation for a General Purpose Serverless Platform","slug":"the-ideal-foundation-for-a-general-purpose-serverless-platform","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2ZQcscC4cFydZNkAjQf0Cp","type":"Entry","createdAt":"2020-09-14T19:20:26.098Z","updatedAt":"2020-09-14T19:20:26.098Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Ben Lorica","slug":"ben-lorica","link":"https://twitter.com/bigdata"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4Q7VdHoM3NQcE8qdDmymcJ","type":"Entry","createdAt":"2020-09-14T19:46:36.552Z","updatedAt":"2021-01-05T16:42:23.528Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Ion Stoica","slug":"ion-stoica","link":"https://www.linkedin.com/in/ionstoica/","bio":"Ion Stoica is a Professor in the EECS Department at University of California at Berkeley. He does research on cloud computing and networked computer systems. Past work includes Apache Spark, Apache Mesos, Tachyon, Chord DHT, and Dynamic Packet State (DPS). He is an ACM Fellow and has received numerous awards, including the SIGOPS Hall of Fame Award (2015), the SIGCOMM Test of Time Award (2011), and the ACM doctoral dissertation award (2001). In 2013, he co-founded Databricks a startup to commercialize technologies for Big Data processing, and in 2006 he co-founded Conviva, a startup to commercialize technologies for large scale video distribution.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"753y0wFvxJrLNvhoCbiXXG","type":"Asset","createdAt":"2020-09-21T20:41:24.860Z","updatedAt":"2020-09-21T20:41:24.860Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ion Stoica headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/753y0wFvxJrLNvhoCbiXXG/63f2c90aecd67c48446e512fc307e83c/Ion_Stoica_headshot.jpg","details":{"size":42716,"image":{"width":150,"height":210}},"fileName":"Ion Stoica headshot.jpg","contentType":"image/jpeg"}}}}}],"publishedDate":"2020-11-05","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Why Ray is poised to play a central role in future serverless offerings","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The history of computing infrastructure is one of steady improvements over extended periods. Cloud computing brought several capabilities that we now take for granted including the elimination of upfront investments in hardware, the ability to pay for compute resources on an as-needed basis, and elasticity. The move to the cloud has ","nodeType":"text"},{"data":{"uri":"https://www.networkworld.com/article/3512885/enterprises-now-spend-more-on-cloud-infrastructure-services-than-on-premises-data-center-gear.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"accelerated in recent years","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and many companies now use a mix of on-premise infrastructure alongside ","nodeType":"text"},{"data":{"uri":"https://gradientflow.com/one-simple-chart-most-companies-use-multiple-cloud-providers/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"multiple cloud providers","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In 2015, AWS introduced Lambda, a service that offered ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"cloud functions","nodeType":"text"},{"data":{},"marks":[],"value":" and introduced the concept of ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"serverless computing","nodeType":"text"},{"data":{},"marks":[],"value":". Serverless and cloud functions differ from traditional cloud computing in two critical ways: (1) serverless abstracts away infrastructure (scaling, provisioning servers) freeing developers to focus on writing programs, (2) serverless provides finer-grained increments for billing (sub-second, compared to traditional cloud computing which uses minutes).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Since the introduction of AWS Lambda, interest in serverless and cloud functions ","nodeType":"text"},{"data":{"uri":"https://trends.google.com/trends/explore/TIMESERIES/1597100400?hl=en-US\u0026tz=420\u0026date=2015-01-01+2020-08-10\u0026geo=US\u0026q=serverless\u0026sni=3"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"has grown","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and all major cloud providers now have similar offerings. ","nodeType":"text"},{"data":{"uri":"https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-3.pdf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Experts predict","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" that serverless will continue to grow in importance and more applications will use serverless computing platforms in the years ahead. But we are still in the early stages of serverless computing. A widely read  2019 paper (","nodeType":"text"},{"data":{"uri":"https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-3.pdf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"“A Berkeley View on Serverless Computing”","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") described  “challenges and research opportunities that need to be addressed for serverless computing to fulfill its promise”.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this post we examine some limitations of current cloud functions (also referred to as ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"FaaS","nodeType":"text"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"serverless","nodeType":"text"},{"data":{},"marks":[],"value":"). We note that the distributed computing framework ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" addresses many of these challenges and argue that Ray is the right foundation for a ","nodeType":"text"},{"data":{"uri":"https://www.youtube.com/watch?v=vzMXTpdJSuk"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"general purpose serverless framework","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Serverless: Special Purpose vs General Purpose","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"There have been multiple efforts to expand the scope of serverless. Projects like ","nodeType":"text"},{"data":{"uri":"http://ex.camera/nsdi17/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"ExCamera","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"http://pywren.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyWren","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://www.serverless.com/blog/using-tensorflow-serverless-framework-deep-learning-image-recognition"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"TF-on-serverless","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and ","nodeType":"text"},{"data":{"uri":"https://www.qubole.com/blog/spark-on-aws-lambda/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Spark-on-Lambda","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" try to run more complex workloads on FaaS platforms like Lambda. Several industry platforms provide serverless-like capabilities and user experiences, including ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/rds/aurora/serverless/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Amazon Aurora","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://databricks.com/blog/2017/06/07/databricks-serverless-next-generation-resource-management-for-apache-spark.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Databricks","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://cloud.google.com/serverless/whitepaper"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"BigQuery","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and others. However, these offerings generally target ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"specific workloads","nodeType":"text"},{"data":{},"marks":[],"value":" and do not aim to run arbitrary applications. We define the characteristics of such a framework (Ray) in the following table, along with key tradeoffs:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5bhF7EF9VdFyQtqqqgvz45","type":"Asset","createdAt":"2020-11-05T17:42:42.722Z","updatedAt":"2020-11-05T17:42:42.722Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog table","file":{"url":"//images.ctfassets.net/xjan103pcp94/5bhF7EF9VdFyQtqqqgvz45/c446dcf421ea3c288a6dd087ddba469b/blog_grid.png","details":{"size":109516,"image":{"width":1296,"height":522}},"fileName":"blog grid.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Why Ray for General Purpose Serverless?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray hides servers","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Like existing FaaS platforms, Ray abstracts away servers from the applications. With version 1.0, users do not need to specify the cluster size or the type of instances when launching an application. Instead, they only need to provide Ray with the set of available resources (e.g., a list of instance types in AWS or a list of nodes and their capabilities in an on-prem cluster) and Ray will automatically pick the instance types and dynamically scale up and down to match the application demands.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray supports stateful applications","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Serverless platforms scale and provision storage and compute separately, thus computations on serverless platforms are stateless. Cloud functions do not store previous transactions or knowledge. The typical flow is to run a computation (a function call), write results to a storage service, and if needed, another function can take that output and use it.  ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Actor_model#Fundamental_concepts"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Actor model","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a powerful programming paradigm that focuses on the semantics of message passing, and works seamlessly when local or remote. Ray supports ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Actor_model"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"actors","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which are stateful workers (or services). Having a serverless platform that can support stateful computations dramatically increases the number of possible applications. Stateful operators allow Ray to efficiently support streaming computations, ","nodeType":"text"},{"data":{"uri":"https://anyscale.com/blog/heres-what-you-need-to-look-for-in-a-model-server-to-build-ml-powered-services"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"machine learning model serving","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and web applications. In fact there are already companies who use Ray in applications that combine both streaming and machine learning. For example, Ant Group “built a multi-paradigm fusion engine on top of Ray that combines streaming, graph processing, and machine learning in a single system to perform real-time fraud detection and online promotion.”","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray supports direct communication between tasks","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Almost every distributed application, such as streaming or data processing, exchanges data between tasks. Unfortunately, existing serverless platforms do not allow direct communication between functions. The only way two functions can communicate with each other is via a cloud storage system like S3. Unfortunately, this is slow and expensive. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In contrast, Ray enables arbitrary tasks and actors to communicate with each other. This enables applications to efficiently exchange data and implement arbitrary communication patterns.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray lets developers access hardware accelerators","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Developers know what type of hardware resources best serve their applications so giving them the ability to control resources is an important feature of any compute platform. FaaS services currently let developers specify execution time limits and memory sizes. But in some applications - notably machine learning model training - developers need to access specific accelerators (GPUs, TPUs, etc.). At this time FaaS providers do not offer this level of control over resources. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A serverless platform built on top of Ray would have no limitations in terms of specifying resources. Developers who use Ray can already describe the hardware resources they need (number of CPUs, GPUs, TPUs, and other hardware accelerators). Future versions of Ray will even allow developers to specify the precise type of chip they prefer (e.g., “two V100 GPUs”).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray provides an open source API","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There is no “standard API” for writing serverless applications. We believe that Ray is a strong candidate for such an open, serverless API. Ray allows developers to write general programs, not just functions. It combines support for both stateless and stateful applications and access to widely used programming languages (Python and Java, with more to follow). In fact, Ray is usually described as a distributed computing platform that can be used to scale applications with minimal effort.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Ray supports fine-grained coordination and control, which can lead to better performance","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There are many applications where control over data locality and scheduling are critical. This includes the broad range of applications that need to distribute and share data across compute nodes. Current serverless offerings are a poor fit for this class of applications. For example, a developer who uses serverless to perform a gradient computation in machine learning will not have control over where data and cloud functions are located. Another example comes from reinforcement learning, where developers will want to use the same compute node for training policies and performing rollouts.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray overcomes these limitations by providing data locality and scheduling control. A developer who uses Ray can specify which actors should run on the same machine.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Ray has no runtime limits","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Current offerings (","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/about-aws/whats-new/2018/10/aws-lambda-supports-functions-that-can-run-up-to-15-minutes/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Lambda","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://cloud.google.com/functions/docs/concepts/exec#:~:text=Function%20execution%20time%20is%20limited,period%20up%20to%209%20minutes."},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Cloud Functions","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://build5nines.com/azure-functions-extend-execution-timeout-past-5-minutes/#:~:text=One%20of%20the%20biggest%20benefits,originally%20set%20to%205%20minutes."},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Azure Functions","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") have execution times that are capped at around 15 minutes or less. This limits the types of applications that a FaaS platform can support. In contrast, since Ray runs on top of existing clouds or clusters, there are no time limits.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"But what about costs?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"One of the most popular aspects of serverless is that users are billed based on usage. Serverless platforms use accounting units that are measured in milliseconds, compared to traditional cloud computing which uses minutes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Since cloud providers charge at the level of an instance, as a software layer, (open source) Ray by itself cannot instantiate and release compute resources quickly enough to deliver fine-grained accounting units. For example, if you have an actor running on a server with 32 cores (and you only have one actor running on it), your cloud provider will likely charge you for 32 cores. Ray does not address this discrepancy between utilization and cost. Getting to fine-grained accounting units would require that you build ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/product"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"a serverless platform on top of Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"With that said, Ray can automatically allocate new instances and shutdown existing instances in minutes. Thus, Ray can already provide serverless functionality for coarse grained jobs that run for say 30 minutes. Examples that might fall under the realm of coarse grained jobs include  streaming, model training, and model serving jobs. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, Ray also has the potential to provide substantial cost savings for coarse-grain workloads. The cost of using cloud functions can be 4x-5x higher than a cloud computing instance operating at 100% utilization. Indeed, at the time of writing, a 3GB RAM AWS Lambda ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/lambda/pricing/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"costs $0.0000048958 per 100ms","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or $0.1762488/hour. In comparison, a t3.medium instance with 4GB RAM ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/ec2/pricing/on-demand/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"costs just $0.0416/hour","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" which is 4.2x cheaper, and when considering per GB RAM pricing, it is 5.65x cheaper. This is an underestimate because it does not include the per-request costs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Summary","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As far back as 2016, ","nodeType":"text"},{"data":{"uri":"https://youtu.be/wYCLbLrEoqs?t=95"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"experts were already noting","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" how serverless tools enable teams to quickly build extremely functional and scalable applications. Since then a growing number of developers are turning to serverless technologies to build applications. But for serverless to live up to its promise, current offerings (cloud functions) need to address the limitations listed in this post.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Companies are still in the early stages of exploring serverless technologies. We believe that Ray is the ideal foundation for a general purpose serverless framework. With the rise of AI and other data intensive applications, Ray is poised to play a central role in future serverless offerings.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3fCTTLKGfkisrrjXCw6ocY","type":"Asset","createdAt":"2020-11-05T17:43:06.484Z","updatedAt":"2020-11-05T17:43:06.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Birds","file":{"url":"//images.ctfassets.net/xjan103pcp94/3fCTTLKGfkisrrjXCw6ocY/e69bbf33b8240c55c3c97b534f7df85a/Flock.jpg","details":{"size":8462,"image":{"width":910,"height":607}},"fileName":"Flock.jpg","contentType":"image/jpeg"}}},"mainImageFit":"cover","recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3W6PBizwirIIbmRUDhUiKQ","type":"Entry","createdAt":"2020-09-30T16:13:44.616Z","updatedAt":"2022-06-22T17:27:34.417Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Announcing Ray 1.0","slug":"announcing-ray-1-0","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}}],"publishedDate":"2020-09-30","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},"body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Announcing Ray 1.0","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Today, we’re happy to announce the release of ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[],"value":"Ray 1.0","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Ray 1.0 brings a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/package-ref.html"},"content":[{"data":{},"marks":[],"value":"stable API","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and new ","nodeType":"text"},{"data":{"uri":"https://ray2020.sched.com/event/dhCy/ray-a-general-purpose-serverless-substrate-eric-liang-anyscale"},"content":[{"data":{},"marks":[],"value":"general purpose serverless","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" features, both important steps towards the goal of providing a universal API for distributed computing. This past release has seen 67 contributors and 458 commits, making it the among the ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases"},"content":[{"data":{},"marks":[],"value":"largest yet","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for Ray. In addition, 1.0 brings many new community ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-libraries.html"},"content":[{"data":{},"marks":[],"value":"library integrations","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to the growing Ray ecosystem.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"New Features","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray 1.0 makes it easier than ever to build and compose highly scalable libraries, applications, and services. Here are the highlights:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Resources, Not Machines: ","nodeType":"text"},{"data":{},"marks":[],"value":"Building distributed applications that run portably across different machine types, clusters, and clouds is a challenging task. Ray 1.0 makes this easy with an ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/autoscaling.html"},"content":[{"data":{},"marks":[],"value":"autoscaler","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" that intelligently selects the best ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/autoscaling.html#multiple-node-type-autoscaling"},"content":[{"data":{},"marks":[],"value":"node types","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for an application’s ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/advanced.html#accelerator-types"},"content":[{"data":{},"marks":[],"value":"resource requests","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". In addition, Ray 1.0 introduces ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/placement-group.html"},"content":[{"data":{},"marks":[],"value":"a placement group API","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for fine-grained control over scheduling.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Production Serving: ","nodeType":"text"},{"data":{},"marks":[],"value":"A general purpose serverless framework hosts both offline batch and online serving workloads. Ray 1.0 ships with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/serve/"},"content":[{"data":{},"marks":[],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a production microservice and ML serving library. For custom serving applications, Ray 1.0 also introduces ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/actors.html?highlight=lifetime#actor-lifetimes"},"content":[{"data":{},"marks":[],"value":"detached","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" actor lifetimes, ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/async_api.html"},"content":[{"data":{},"marks":[],"value":"AsyncIO","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" actors, and application-level metrics via ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-metrics.html?highlight=prometheus"},"content":[{"data":{},"marks":[],"value":"Prometheus","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Ray serving applications can be deployed in various ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/cloud.html"},"content":[{"data":{},"marks":[],"value":"cloud providers","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and on ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/kubernetes.html"},"content":[{"data":{},"marks":[],"value":"Kubernetes","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Automatic Memory Management","nodeType":"text"},{"data":{},"marks":[],"value":": Users of Ray 1.0 can say goodbye to “object evicted” errors, thanks to fully automated ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/memory-management.html"},"content":[{"data":{},"marks":[],"value":"memory management","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Application performance and memory usage can be debugged in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-dashboard.html"},"content":[{"data":{},"marks":[],"value":"Ray dashboard","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". To learn more about how Ray implements distributed reference counting with high-performance, reliability, and fault tolerance, check out the new ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/whitepaper.html"},"content":[{"data":{},"marks":[],"value":"Ray 1.0 whitepaper","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Java and Windows Support: ","nodeType":"text"},{"data":{},"marks":[],"value":"Ray 1.0 brings native support for the Java and Windows platforms. This means that you can now use Ray to build ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cross-language.html"},"content":[{"data":{},"marks":[],"value":"cross-language","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://medium.com/distributed-computing-with-ray/build-distributed-java-applications-with-ray-90b381eff564"},"content":[{"data":{},"marks":[],"value":"distributed Java","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" applications, and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/installation.html#windows-support"},"content":[{"data":{},"marks":[],"value":"install Ray on Windows","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Community Update","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Community Integrations","nodeType":"text"},{"data":{},"marks":[],"value":": There are a growing number of ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-libraries.html"},"content":[{"data":{},"marks":[],"value":"community libraries","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" that integrate with Ray 1.0 for distributed execution: ","nodeType":"text"},{"data":{"uri":"https://classyvision.ai/tutorials/ray_aws"},"content":[{"data":{},"marks":[],"value":"ClassyVision","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/dask-on-ray.html"},"content":[{"data":{},"marks":[],"value":"Dask","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://github.com/asappresearch/flambe"},"content":[{"data":{},"marks":[],"value":"Flambe","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://horovod.readthedocs.io/en/stable/ray_include.html"},"content":[{"data":{},"marks":[],"value":"Horovod","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/transformers/master/main_classes/trainer.html#transformers.Trainer.hyperparameter_search"},"content":[{"data":{},"marks":[],"value":"HuggingFace","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://analytics-zoo.github.io/master/#ProgrammingGuide/rayonspark/"},"content":[{"data":{},"marks":[],"value":"Intel Analytics Zoo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/mars-on-ray.html"},"content":[{"data":{},"marks":[],"value":"MARS","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://github.com/modin-project/modin"},"content":[{"data":{},"marks":[],"value":"Modin","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://github.com/Intel-bigdata/oap-raydp"},"content":[{"data":{},"marks":[],"value":"RayDP","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://github.com/SeldonIO/alibi"},"content":[{"data":{},"marks":[],"value":"Seldon Alibi","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and ","nodeType":"text"},{"data":{"uri":"https://pypi.org/project/spacy-ray/"},"content":[{"data":{},"marks":[],"value":"SpaCy","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". This means users of these libraries can now scale their applications with Ray, and Ray users can easily leverage these libraries in their distributed applications.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Open Source","nodeType":"text"},{"data":{},"marks":[],"value":": At Anyscale, we’re proud to develop Ray along with the open source community. Many key Ray contributions are driven by the community — for example, ongoing projects around high availability, multi-tenancy, and placement groups are led by ","nodeType":"text"},{"data":{"uri":"https://www.antgroup.com/en"},"content":[{"data":{},"marks":[],"value":"Ant Group","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and improved autoscaler support for different Clouds has come from ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/"},"content":[{"data":{},"marks":[],"value":"Amazon","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://azure.microsoft.com/en-us/"},"content":[{"data":{},"marks":[],"value":"Microsoft","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"More Information","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"To learn more about Ray, join us at ","nodeType":"text"},{"data":{"uri":"https://events.linuxfoundation.org/ray-summit/"},"content":[{"data":{},"marks":[],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which runs from September 30 to October 1. You can also find out more about Ray 1.0 through the ","nodeType":"text"},{"data":{"uri":"https://forms.gle/9TSdDYUgxYs8SA9e8"},"content":[{"data":{},"marks":[],"value":"Ray Slack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/index.html"},"content":[{"data":{},"marks":[],"value":"Documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"538e8sdc8w0sSkfhbo7MUZ","type":"Asset","createdAt":"2020-09-30T15:34:48.916Z","updatedAt":"2020-09-30T15:34:48.916Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray-1.0 image","file":{"url":"//images.ctfassets.net/xjan103pcp94/538e8sdc8w0sSkfhbo7MUZ/b41d8bbba115f25a5684a4541e15433b/ray1.png","details":{"size":123928,"image":{"width":1024,"height":512}},"fileName":"ray1.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}}],"recommendations":[]}}],"activeTag":null,"activeType":null,"author":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}},"recommendations":[]}},"page":1,"totalPages":1,"allTypes":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KeH0JzrIEpuheazhMAhKf","type":"Entry","createdAt":"2022-06-15T01:31:23.950Z","updatedAt":"2022-06-15T01:31:23.950Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"User Story","slug":"user-story","tintColor":"#6D59C6"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42F9iHs5a05rRrOuqEQwvB","type":"Entry","createdAt":"2022-06-15T01:30:47.322Z","updatedAt":"2022-06-15T01:30:47.322Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Culture","slug":"culture","tintColor":"#39C8E5"}}],"allTags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KwkbI6zRqcE9KD5iKuP8W","type":"Entry","createdAt":"2021-12-05T04:51:33.974Z","updatedAt":"2021-12-05T04:51:33.974Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"RayDP","identifier":"ray_dp"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1OiVmfyPaDqRNavVG5Yp1l","type":"Entry","createdAt":"2021-12-05T04:50:12.541Z","updatedAt":"2021-12-05T04:50:12.541Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Retail","identifier":"retail"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"161LHBXwPkxUmhjt1YbfJH","type":"Entry","createdAt":"2021-12-05T04:49:42.528Z","updatedAt":"2021-12-05T04:49:42.528Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Gaming","identifier":"gaming"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"BDTwwkSW9VtmSkJojGbx8","type":"Entry","createdAt":"2021-12-05T04:49:30.169Z","updatedAt":"2021-12-05T04:49:30.169Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Government","identifier":"government"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4fzKvVOn2R8U6TTi2bcb2R","type":"Entry","createdAt":"2021-12-05T04:49:10.881Z","updatedAt":"2021-12-05T04:49:10.881Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"HLS","identifier":"hls"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"GGg4W2UlqfVP2iPMsc8J1","type":"Entry","createdAt":"2021-12-05T04:44:22.472Z","updatedAt":"2021-12-05T04:44:22.472Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Financial","identifier":"financial"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i6fp6Gfdy5wFeDDOaKcfh","type":"Entry","createdAt":"2021-12-03T22:33:42.248Z","updatedAt":"2021-12-03T22:33:42.248Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Anyscale","identifier":"anyscale"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6chYLcIc6yEB2EtLv2vngw","type":"Entry","createdAt":"2021-11-23T01:06:51.725Z","updatedAt":"2021-11-30T22:20:19.315Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Healthcare","identifier":"healthcare"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BzSmyQ2ojATOq6qOpD4PX","type":"Entry","createdAt":"2021-10-02T19:52:20.790Z","updatedAt":"2021-10-02T19:52:20.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Technology","identifier":"technology"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}}]},"__N_SSP":true},"page":"/blog","query":{"author":"eric-liang"},"buildId":"-oT8_WeW9m_lAxr26cJvY","isFallback":false,"gssp":true,"customServer":true,"scriptLoader":[]}</script><script type="text/javascript" id="hs-script-loader" async="" defer="" src="https://js.hs-scripts.com/20523749.js"></script><script type="text/javascript">
  _linkedin_partner_id = "4210313";
  window._linkedin_data_partner_ids = window._linkedin_data_partner_ids || [];
  window._linkedin_data_partner_ids.push(_linkedin_partner_id);
</script><script type="text/javascript">
  (function(l) { if (!l){window.lintrk = function(a,b){window.lintrk.q.push([a,b])};
  window.lintrk.q=[]} var s = document.getElementsByTagName("script")[0];
  var b = document.createElement("script");
  b.type = "text/javascript";b.async = true;
  b.src = "https://snap.licdn.com/li.lms-analytics/insight.min.js";
  s.parentNode.insertBefore(b, s);})(window.lintrk);
</script><noscript><img height="1" width="1" style="display:none" alt="" src="https://px.ads.linkedin.com/collect/?pid=4210313&amp;fmt=gif"/></noscript></body></html>