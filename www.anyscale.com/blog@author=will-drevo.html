<!DOCTYPE html><html lang="en-US"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Blog | Anyscale</title><meta name="description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta property="og:title" content="Blog | Anyscale"/><meta property="og:description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta property="og:site_name" content="Anyscale"/><meta property="og:image" content=""/><meta property="fb:app_id" content=""/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:description" content="From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab."/><meta name="twitter:site" content=""/><meta name="twitter:title" content="Blog | Anyscale"/><meta name="twitter:image" content=""/><meta name="next-head-count" content="14"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><link rel="icon" href="static/favicon.ico"/><script>
(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-P8H6KQG');
</script><script src="https://tag.clearbitscripts.com/v1/pk_7a96be585f731437b35c7bad25c432b7/tags.js"></script><script src="https://www.googleoptimize.com/optimize.js?id=OPT-NPGB9W3"></script><script>
  !function(f,b,e,v,n,t,s)
  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
  n.queue=[];t=b.createElement(e);t.async=!0;
  t.src=v;s=b.getElementsByTagName(e)[0];
  s.parentNode.insertBefore(t,s)}(window,document,'script',
  'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '168099398554987');
  fbq('track', 'PageView');
</script><link rel="stylesheet" href="static/font-awesome-4.7.0/css/font-awesome.min.css"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="true"/><script src="https://fast.wistia.com/assets/external/E-v1.js" async=""></script><meta name="ahrefs-site-verification" content="4be3c5d78cdad8823d038256ac06d4336ae7a84632dac0ed57975703fc14fe6b"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="_next/static/css/dbba207740458c19.css" as="style"/><link rel="stylesheet" href="_next/static/css/dbba207740458c19.css" data-n-g=""/><link rel="preload" href="_next/static/css/13fbfc51931a4b43.css" as="style"/><link rel="stylesheet" href="_next/static/css/13fbfc51931a4b43.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="_next/static/chunks/webpack-94adde92c5e1c75b.js" defer=""></script><script src="_next/static/chunks/framework-95c3ca6de27174dd.js" defer=""></script><script src="_next/static/chunks/main-774d7304f1973b2a.js" defer=""></script><script src="_next/static/chunks/pages/_app-fd8cb5a9775605f5.js" defer=""></script><script src="_next/static/chunks/0f1ac474-1cd8c801d37caef0.js" defer=""></script><script src="_next/static/chunks/1812-a871a62a8e8aa88c.js" defer=""></script><script src="_next/static/chunks/9065-e24d5adee6cbba35.js" defer=""></script><script src="_next/static/chunks/6139-f3c4647afbd26b94.js" defer=""></script><script src="_next/static/chunks/8423-007cfdc371b27898.js" defer=""></script><script src="_next/static/chunks/3167-65b612e959dd1945.js" defer=""></script><script src="_next/static/chunks/9027-e83e1bb65c284840.js" defer=""></script><script src="_next/static/chunks/pages/blog-1eafbb689a124ac5.js" defer=""></script><script src="_next/static/-oT8_WeW9m_lAxr26cJvY/_buildManifest.js" defer=""></script><script src="_next/static/-oT8_WeW9m_lAxr26cJvY/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Rubik&display=swap">@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4i1Uw.woff) format('woff')}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFWkUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFU0UzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFVUUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFWUUzdYPFkZVOA6w.woff) format('woff');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Rubik';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/rubik/v26/iJWZBXyIfDnIV5PNhY1KTN7Z-Yh-B4iFV0UzdYPFkZVO.woff) format('woff');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href="https://fonts.googleapis.com/css2?family=Outfit:wght@400;500;600;700&display=swap">@font-face{font-family:'Outfit';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4TC1C4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4QK1C4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4e6yC4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYyz_MVcBeNP4NjuGObqx1XmO1I4deyC4I.woff) format('woff')}@font-face{font-family:'Outfit';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Outfit';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/outfit/v10/QGYvz_MVcBeNP4NJtEtqUYLknw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,400;0,700;1,400;1,700&display=swap">@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTeEw.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINscg.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKtdU.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7MIU.woff) format('woff')}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGqZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuE6ZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuFKZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGKZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuGaZJW9XjDlN8.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX-KVElMYYaJe8bpLHnCwDKhdTuF6ZJW9XjDg.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINce_fuJGl18QRY.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINccvfuJGl18QRY.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcdffuJGl18QRY.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINceffuJGl18QRY.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcePfuJGl18QRY.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:italic;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX7KVElMYYaJe8bpLHnCwDKhdTmrINcdvfuJGl18Q.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdzeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdXeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdLeFaxOedfTDw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhd7eFaxOedfTDw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhd_eFaxOedfTDw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYXgKVElMYYaJe8bpLHnCwDKhdHeFaxOedc.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIxsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIVsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIJsdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AI5sdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AI9sdP3pBmtF8A.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'IBM Plex Sans';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/ibmplexsans/v14/zYX9KVElMYYaJe8bpLHnCwDKjWr7AIFsdP3pBms.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body class=""><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-P8H6KQG" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next" data-reactroot=""><svg xmlns="http://www.w3.org/2000/svg" class="sr-only"><symbol id="svg-arrow-right-gradient"><path d="M1.5 7.5h18" stroke="url(#paint0_linear_3927_132230)" stroke-width="1.5" stroke-linecap="round"></path><path d="m16.6562 3.65625 3.8442 3.84412-3.8442 3.84413" stroke="url(#paint1_linear_3927_132230)" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><defs><linearGradient id="paint0_linear_3927_132230" x1="1.06972" y1="8.58929" x2="12.2319" y2="-1.53819" gradientUnits="userSpaceOnUse"><stop stop-color="#8B20BE"></stop><stop offset="1" stop-color="#3579FF" stop-opacity=".97"></stop></linearGradient><linearGradient id="paint1_linear_3927_132230" x1="12.377" y1="7.75171" x2="16.9307" y2="11.8684" gradientUnits="userSpaceOnUse"><stop stop-color="#8B20BE"></stop><stop offset="1" stop-color="#3579FF" stop-opacity=".97"></stop></linearGradient></defs></symbol></svg><div class="Banner_container__zEmFV"><a href="https://raysummit.anyscale.com/?utm_source=anyscale&amp;utm_medium=website&amp;utm_campaign=ray_summit_2023&amp;utm_content=homepage_ticker" rel="noreferrer nofollow" target="_blank"><span class="Banner_banner-text__wr6Xi"><span class="Banner_main-text__cpJ6H">Are you passionate about Ray, scalable AI and distributed computing? Join us at Ray Summit!</span><span class="Banner_cta-button__0JCtt"> Find out more! <!-- --> <svg width="23" height="14" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M1.5 6.75a.75.75 0 0 0 0 1.5v-1.5Zm18 1.5a.75.75 0 0 0 0-1.5v1.5Zm-18 0h18v-1.5h-18v1.5Z" fill="#fff"></path><path d="M17.187 3.126a.75.75 0 0 0-1.061 1.06l1.06-1.06ZM20.5 7.5l.53.53a.75.75 0 0 0 0-1.06l-.53.53Zm-4.374 3.314a.75.75 0 1 0 1.06 1.06l-1.06-1.06Zm0-6.627L19.97 8.03l1.06-1.061-3.843-3.844-1.061 1.06ZM19.97 6.97l-3.844 3.844 1.06 1.06 3.845-3.843L19.97 6.97Z" fill="#fff"></path></svg></span></span></a><button class="Banner_close-button__9ksfY" type="button" aria-label="Close Banner"></button></div><header class="inner-shadow sticky top-0 z-[1000] h-24 transform py-5 transition-transform duration-500 bg-primary translate-y-0"><div class="container mx-auto flex items-center justify-between"><a href="index.html" class="text-white"><svg width="141" height="32" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><g clip-path="url(#anyscale-logo_svg__a)" fill="currentColor"><path d="M10.653 17.46H3.865C1.697 17.46 0 19.16 0 21.332v6.797c0 2.17 1.697 3.87 3.865 3.87h6.788c2.169 0 3.866-1.7 3.866-3.87V21.33c0-2.171-1.792-3.87-3.866-3.87Zm1.509 10.573c0 .85-.66 1.51-1.509 1.51H3.865a1.49 1.49 0 0 1-1.508-1.51v-6.796c0-.85.66-1.51 1.508-1.51h6.788c.849 0 1.509.66 1.509 1.51v6.796Z"></path><path d="M28.095 11.988h-8.108V3.87C19.892 1.7 18.196 0 16.027 0H3.865C1.697 0 0 1.7 0 3.87v6.608c0 2.171 1.697 3.87 3.865 3.87h13.67v13.687c0 2.171 1.698 3.87 3.866 3.87h6.6c2.168 0 3.865-1.699 3.865-3.87V15.858c0-2.076-1.697-3.775-3.771-3.87Zm-24.23 0a1.49 1.49 0 0 1-1.508-1.51V3.87c0-.85.66-1.51 1.508-1.51h12.162c.849 0 1.509.66 1.509 1.51v8.118H3.866Zm25.738 16.047c0 .85-.66 1.51-1.508 1.51h-6.6a1.49 1.49 0 0 1-1.508-1.51V14.348h8.108c.848 0 1.508.66 1.508 1.51v12.177ZM79.194 17.462l-3.017-8.118h-3.3l4.714 11.327-2.546 5.853h3.112l7.165-17.18H82.21l-3.016 8.118ZM93.335 14.819c-.66-.284-1.414-.472-2.075-.661-.66-.189-1.225-.378-1.79-.66-.378-.19-.566-.473-.566-.85 0-.378.188-.661.472-.85.847-.472 1.79-.378 2.639.094.377.284.566.661.566 1.133h2.828c0-1.133-.566-2.171-1.414-2.832-.943-.755-2.169-1.133-3.3-1.038-.849 0-1.697.094-2.45.472-.66.283-1.227.66-1.604 1.227-.377.472-.565 1.133-.565 1.794 0 .66.188 1.321.565 1.887.377.472.849.85 1.414 1.039.66.283 1.414.472 2.168.66.66.19 1.226.378 1.886.661.377.19.566.472.566.85 0 .377-.189.66-.471.85-.377.283-.85.377-1.32.377-.472 0-1.038-.094-1.415-.472-.377-.283-.565-.66-.66-1.133h-2.922c0 .755.282 1.416.754 1.982.472.567 1.131 1.133 1.791 1.416.849.378 1.698.567 2.546.567.849 0 1.697-.095 2.45-.472.661-.283 1.227-.66 1.604-1.227.377-.567.565-1.133.565-1.794 0-.66-.188-1.321-.66-1.888-.565-.66-1.037-.944-1.602-1.132ZM102.575 11.702c.566 0 1.036.095 1.508.472.377.283.66.755.849 1.227h3.205a5.366 5.366 0 0 0-1.886-3.02 5.348 5.348 0 0 0-3.582-1.039c-1.037 0-2.074.284-3.017.755a5.486 5.486 0 0 0-2.074 2.077 7.427 7.427 0 0 0 0 6.325c.472.85 1.131 1.604 2.074 2.076.943.472 1.98.756 3.017.756 1.32 0 2.546-.378 3.582-1.133a5.366 5.366 0 0 0 1.886-3.02h-3.111c-.283 1.037-1.32 1.698-2.357 1.604-.754 0-1.509-.283-1.98-.944a5.119 5.119 0 0 1 0-5.192 1.914 1.914 0 0 1 1.886-.944ZM118.412 10.1a.405.405 0 0 1-.377.378c-.094 0-.188 0-.188-.095a3.382 3.382 0 0 0-.944-.66c-.659-.378-1.507-.567-2.262-.567-.943 0-1.885.283-2.734.755a4.459 4.459 0 0 0-1.885 2.077c-.472.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.376.85 1.037 1.604 1.885 2.17.849.473 1.697.756 2.64.756.754 0 1.603-.189 2.263-.567.659-.283 1.131-.755 1.602-1.321v1.699h2.923V9.345h-2.923v.755Zm-.377 6.891c-.283.472-.66.944-1.132 1.227-.47.283-1.037.378-1.507.378-.567 0-1.037-.189-1.509-.378-.472-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.094-1.228.472-1.794.848-1.51 2.64-1.982 4.147-1.227h.095c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM55.907 10.1a.406.406 0 0 1-.377.378c-.095 0-.19 0-.19-.095a3.384 3.384 0 0 0-.942-.66c-.66-.378-1.508-.567-2.263-.567-.942 0-1.885.283-2.733.755a4.463 4.463 0 0 0-1.886 2.077c-.471.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.377.85 1.037 1.604 1.886 2.17.848.473 1.697.756 2.64.756.753 0 1.602-.189 2.262-.567.66-.283 1.131-.755 1.603-1.321v1.699h2.922V9.345h-2.922v.755Zm-.472 6.891c-.283.472-.66.944-1.131 1.227-.471.283-1.037.378-1.509.378-.565 0-1.037-.189-1.508-.378-.471-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.095-1.228.472-1.794.848-1.51 2.64-1.982 4.148-1.227h.094c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM126.708 5.477h-2.923v15.575h2.923V5.477ZM140.191 14.914c0-1.038-.189-2.077-.755-3.02-.471-.85-1.131-1.605-2.074-1.983a6.78 6.78 0 0 0-6.033 0 5.485 5.485 0 0 0-2.075 2.077c-.565 1.038-.754 2.077-.754 3.21 0 1.132.189 2.17.754 3.114a5.485 5.485 0 0 0 2.075 2.077c.942.472 1.979.755 3.016.755a6.56 6.56 0 0 0 3.489-1.038c.943-.661 1.602-1.7 1.885-2.738h-3.016c-.377.944-1.321 1.51-2.358 1.416-.659 0-1.414-.283-1.885-.755-.566-.472-.848-1.227-.848-1.982h8.484c.095-.378.095-.755.095-1.133Zm-8.579-.85c.093-.755.377-1.415.942-1.887.566-.472 1.226-.661 1.886-.661.754 0 1.414.189 1.979.66.566.473.849 1.133.849 1.794l-5.656.095ZM67.692 9.159a4.7 4.7 0 0 0-1.98.472c-.377.188-.754.377-1.037.66a.286.286 0 0 1-.377 0c-.094-.094-.094-.094-.094-.188v-.756H61.28v11.705h2.923v-6.418c-.094-.755.188-1.51.754-2.171.471-.472 1.226-.756 1.886-.756.66 0 1.414.284 1.885.755.472.567.754 1.322.66 2.172v6.418h2.923v-6.89c.094-1.322-.283-2.644-1.226-3.682-.848-.85-2.168-1.416-3.394-1.321Z"></path></g><defs><clipPath id="anyscale-logo_svg__a"><path fill="currentColor" d="M0 0h140.19v32H0z"></path></clipPath></defs></svg></a><div class="lg:relative lg:flex lg:flex-row hidden"><div class="max-h-[calc(100vh-192px)] items-center overflow-y-auto py-[1px] shadow-[0_-1px_0_#DAE3F2] lg:flex lg:overflow-visible lg:p-0 lg:shadow-none"><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Products</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="platform.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Anyscale Platform</div></a><a href="ray-open-source.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Open Source</div></a></div></div></div></div></div></div><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Solutions</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="container mx-auto mb-4 lg:mb-0 mt-2 lg:mt-0 lg:mr-12"><div class="header-card h-full rounded-[5px] p-6 lg:w-[296px]"><h5 id="scalable-ai-and-python">Scalable AI and Python</h5>
<p>Leverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more.</p></div></div><div class="grid lg:flex grid-cols-1 md:grid-cols-2 gap-x-10 lg:gap-x-12 container mx-auto"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><p class="text-base leading-normal font-normal text-darkGrey-300 px-6 lg:px-4 mb-4">Scalable AI and Python</p><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2"><a href="data-ingestion.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Data Ingestion</div></a><a href="reinforcement-learning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Reinforcement Learning</div></a><a href="ray-air.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray AIR</div></a><a href="model-serving.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Model Serving</div></a><a href="hyperparameter-tuning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Hyperparameter Tuning</div></a></div></div></div><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><p class="text-base leading-normal font-normal text-darkGrey-300 px-6 lg:px-4 mb-4">Use Cases</p><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2"><a href="large-language-models.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Large Language Models</div></a><a href="demand-forecasting.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Demand Forecasting/Pricing</div></a><a href="industrial-automation.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Industrial Automation</div></a><a href="machine-learning.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Scalable ML Platforms</div></a><a href="natural-language-processing.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">NLP</div></a><a href="recommendation-system.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Recommendation System</div></a></div></div></div></div></div></div></div><a class="block font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4 shadow-[0_1px_0_#DAE3F2] lg:shadow-none" href="ray-ecosystem.html"><div class="w-full container mx-auto">Ecosystem</div></a><a class="block font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4 shadow-[0_1px_0_#DAE3F2] lg:shadow-none" href="user-stories.html"><div class="w-full container mx-auto">Success Stories</div></a><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Learn</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="blog/how-ray-solves-common-production-challenges-for-generative-ai.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Blog</div></a><a href="https://raysummit.anyscale.com/?utm_source=anyscale&amp;utm_medium=website&amp;utm_campaign=ray_summit_2023&amp;utm_content=home_nav-pulldown"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Summit 2023</div></a><a href="event-category/rl-summit.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Videos &amp; Webinars</div></a><a href="https://github.com/ray-project/ray-educational-materials"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Ray Training</div></a><a href="https://docs.anyscale.com"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Anyscale Docs</div></a></div></div></div></div></div></div><div class="relative shadow-[0_1px_0_#DAE3F2] lg:shadow-none"><div class="flex items-center cursor-pointer font-heading font-medium leading-normal lg:text-white w-full lg:w-auto px-11 lg:px-5 py-4"><div class="flex items-center w-full container mx-auto"><span class="mr-auto lg:mr-1 font-heading">Company</span><div class="hidden lg:block"><svg width="12" height="16" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path fill-rule="evenodd" clip-rule="evenodd" d="M9.662 7.066a.75.75 0 0 1 0 1.06l-3.535 3.536a.75.75 0 0 1-1.061 0L1.53 8.127a.75.75 0 1 1 1.061-1.061l3.005 3.005 3.005-3.005a.75.75 0 0 1 1.061 0Z" fill="#fff"></path></svg></div><div class="lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M18.364 10.364A1 1 0 0 0 16.95 8.95l1.414 1.414ZM12 15.314l-.707.707a1 1 0 0 0 1.414 0L12 15.314ZM7.05 8.95a1 1 0 1 0-1.414 1.414L7.05 8.95Zm9.9 0-5.657 5.657 1.414 1.414 5.657-5.657L16.95 8.95Zm-4.243 5.657L7.05 8.95l-1.414 1.414 5.657 5.657 1.414-1.414Z" fill="#222"></path></svg></div></div></div><div class="hidden absolute top-full left-1/2 transform -translate-x-1/2"><svg width="54" height="27" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M25.69 1.134a2 2 0 0 1 2.62 0l24.946 21.604c1.4 1.212.542 3.512-1.31 3.512H2.054c-1.852 0-2.71-2.3-1.31-3.512L25.691 1.134Z" fill="#fff"></path></svg></div><div class="lg:absolute lg:z-10 lg:top-full lg:left-1/2 lg:transform lg:-translate-x-1/2 lg:pt-3 hidden"><div style="margin-right:0px" class="relative lg:flex rounded-[5px] bg-white lg:p-12 lg:shadow-custom"><div class="lg:w-[300px] pb-4 pt-2 lg:p-0"><div class="container mx-auto"><div class="grid grid-cols-1 gap-x-10 gap-y-4 lg:gap-y-2 md:grid-cols-2 lg:grid-cols-1"><a href="about.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">About us</div></a><a href="press.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">News</div></a><a href="careers.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Careers</div></a><a href="community.html"><div class="font-semibold leading-normal text-darkGrey-600 rounded-[5px] lg:bg-transparent hover:bg-lightBlue-100 px-6 py-3 lg:p-4 bg-lightBlue-100/20">Community</div></a></div></div></div></div></div></div></div><div class="absolute bottom-0 left-0 right-0 bg-white py-5 shadow-[0_-1px_0_#DAE3F2] lg:relative lg:ml-5 lg:bg-transparent lg:p-0 lg:shadow-none"><div class="container mx-auto flex justify-end"><a class="btn bg-gradient min-w-[320px] justify-center bg-blue-900 text-center lg:min-w-0" href="beta.html">Try It Now</a></div></div></div><div class="cursor-pointer py-4 pl-6 lg:hidden"><svg width="24" height="24" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><path d="M20.05 11H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95ZM20.05 16H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95ZM20.05 6H3.95a.95.95 0 0 0-.95.95v.1c0 .525.425.95.95.95h16.1a.95.95 0 0 0 .95-.95v-.1a.95.95 0 0 0-.95-.95Z" fill="#fff"></path></svg></div></div></header><div class="Layout_page-sections__2__gb overflow-x-hidden"><div class="ArticlesList_container__mBEpW"><div class="ArticlesList_inner__QWc69"><div class="ArticlesList_header__45BKa"><h1>Posts by Will Drevo</h1><div class="ArticlesList_spacer__8l_nL"></div></div><div class="BlogFilters_root__mrUMs"><div class="BlogFilters_inner__87PZK"><div class="BlogFilters_item__1GpID"><div class="wrapper"><select aria-label="Types" class="SelectDropdown_select__hNpf2"><option selected="">All Types</option><option value="news">News</option><option value="culture">Culture</option><option value="engineering">Engineering</option><option value="user-story">User Story</option></select><div class="SelectDropdown_select__hNpf2 SelectDropdown_select-dropdown__Jabke"><button type="button" aria-haspopup="true" aria-expanded="false" style="width:245px">All Types</button><ul role="menu" class="SelectDropdown_menu__3wacL"><li class="SelectDropdown_item__LO5La SelectDropdown_active__YhRi7" tabindex="0" role="menuitem">All Types</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">News</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Culture</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Engineering</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">User Story</li></ul></div></div></div><div class="BlogFilters_item__1GpID"><div class="wrapper"><select aria-label="Tags" class="SelectDropdown_select__hNpf2"><option selected="">All Products / Libraries</option><option value="anyscale">Anyscale</option><option value="ray_core">Ray Core</option><option value="ray-datasets">Ray Datasets</option><option value="ray_train">Ray Train</option><option value="ray-tune">Ray Tune</option><option value="ray_serve">Ray Serve</option><option value="rllib">Ray RLlib</option></select><div class="SelectDropdown_select__hNpf2 SelectDropdown_select-dropdown__Jabke"><button type="button" aria-haspopup="true" aria-expanded="false" style="width:245px">All Products / Libraries</button><ul role="menu" class="SelectDropdown_menu__3wacL"><li class="SelectDropdown_item__LO5La SelectDropdown_active__YhRi7" tabindex="0" role="menuitem">All Products / Libraries</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Anyscale</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Core</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Datasets</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Train</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Tune</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray Serve</li><li class="SelectDropdown_item__LO5La" tabindex="0" role="menuitem">Ray RLlib</li></ul></div></div></div></div></div><div class="empty"><h2>No posts found.</h2><p><a href="blog/how-ray-solves-common-production-challenges-for-generative-ai.html">View all</a></p></div><div class="ArticlesList_list__uP0RC"></div><div class="Pagination_container__FdBHw"></div></div></div></div><footer class="relative bg-primary pt-30 pb-30 md:pt-36 md:pb-[180px] lg:pt-20"><div class="container mx-auto flex flex-col space-y-12 md:space-y-16 lg:flex-row lg:justify-between lg:space-y-0"><div class="flex flex-col gap-16"><div class="flex flex-col gap-6"><div class="h-10 text-white"><svg width="141" height="32" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="origin-top-left scale-125"><g clip-path="url(#anyscale-logo_svg__a)" fill="currentColor"><path d="M10.653 17.46H3.865C1.697 17.46 0 19.16 0 21.332v6.797c0 2.17 1.697 3.87 3.865 3.87h6.788c2.169 0 3.866-1.7 3.866-3.87V21.33c0-2.171-1.792-3.87-3.866-3.87Zm1.509 10.573c0 .85-.66 1.51-1.509 1.51H3.865a1.49 1.49 0 0 1-1.508-1.51v-6.796c0-.85.66-1.51 1.508-1.51h6.788c.849 0 1.509.66 1.509 1.51v6.796Z"></path><path d="M28.095 11.988h-8.108V3.87C19.892 1.7 18.196 0 16.027 0H3.865C1.697 0 0 1.7 0 3.87v6.608c0 2.171 1.697 3.87 3.865 3.87h13.67v13.687c0 2.171 1.698 3.87 3.866 3.87h6.6c2.168 0 3.865-1.699 3.865-3.87V15.858c0-2.076-1.697-3.775-3.771-3.87Zm-24.23 0a1.49 1.49 0 0 1-1.508-1.51V3.87c0-.85.66-1.51 1.508-1.51h12.162c.849 0 1.509.66 1.509 1.51v8.118H3.866Zm25.738 16.047c0 .85-.66 1.51-1.508 1.51h-6.6a1.49 1.49 0 0 1-1.508-1.51V14.348h8.108c.848 0 1.508.66 1.508 1.51v12.177ZM79.194 17.462l-3.017-8.118h-3.3l4.714 11.327-2.546 5.853h3.112l7.165-17.18H82.21l-3.016 8.118ZM93.335 14.819c-.66-.284-1.414-.472-2.075-.661-.66-.189-1.225-.378-1.79-.66-.378-.19-.566-.473-.566-.85 0-.378.188-.661.472-.85.847-.472 1.79-.378 2.639.094.377.284.566.661.566 1.133h2.828c0-1.133-.566-2.171-1.414-2.832-.943-.755-2.169-1.133-3.3-1.038-.849 0-1.697.094-2.45.472-.66.283-1.227.66-1.604 1.227-.377.472-.565 1.133-.565 1.794 0 .66.188 1.321.565 1.887.377.472.849.85 1.414 1.039.66.283 1.414.472 2.168.66.66.19 1.226.378 1.886.661.377.19.566.472.566.85 0 .377-.189.66-.471.85-.377.283-.85.377-1.32.377-.472 0-1.038-.094-1.415-.472-.377-.283-.565-.66-.66-1.133h-2.922c0 .755.282 1.416.754 1.982.472.567 1.131 1.133 1.791 1.416.849.378 1.698.567 2.546.567.849 0 1.697-.095 2.45-.472.661-.283 1.227-.66 1.604-1.227.377-.567.565-1.133.565-1.794 0-.66-.188-1.321-.66-1.888-.565-.66-1.037-.944-1.602-1.132ZM102.575 11.702c.566 0 1.036.095 1.508.472.377.283.66.755.849 1.227h3.205a5.366 5.366 0 0 0-1.886-3.02 5.348 5.348 0 0 0-3.582-1.039c-1.037 0-2.074.284-3.017.755a5.486 5.486 0 0 0-2.074 2.077 7.427 7.427 0 0 0 0 6.325c.472.85 1.131 1.604 2.074 2.076.943.472 1.98.756 3.017.756 1.32 0 2.546-.378 3.582-1.133a5.366 5.366 0 0 0 1.886-3.02h-3.111c-.283 1.037-1.32 1.698-2.357 1.604-.754 0-1.509-.283-1.98-.944a5.119 5.119 0 0 1 0-5.192 1.914 1.914 0 0 1 1.886-.944ZM118.412 10.1a.405.405 0 0 1-.377.378c-.094 0-.188 0-.188-.095a3.382 3.382 0 0 0-.944-.66c-.659-.378-1.507-.567-2.262-.567-.943 0-1.885.283-2.734.755a4.459 4.459 0 0 0-1.885 2.077c-.472.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.376.85 1.037 1.604 1.885 2.17.849.473 1.697.756 2.64.756.754 0 1.603-.189 2.263-.567.659-.283 1.131-.755 1.602-1.321v1.699h2.923V9.345h-2.923v.755Zm-.377 6.891c-.283.472-.66.944-1.132 1.227-.47.283-1.037.378-1.507.378-.567 0-1.037-.189-1.509-.378-.472-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.094-1.228.472-1.794.848-1.51 2.64-1.982 4.147-1.227h.095c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM55.907 10.1a.406.406 0 0 1-.377.378c-.095 0-.19 0-.19-.095a3.384 3.384 0 0 0-.942-.66c-.66-.378-1.508-.567-2.263-.567-.942 0-1.885.283-2.733.755a4.463 4.463 0 0 0-1.886 2.077c-.471.944-.754 2.077-.754 3.115 0 1.133.188 2.171.754 3.21.377.85 1.037 1.604 1.886 2.17.848.473 1.697.756 2.64.756.753 0 1.602-.189 2.262-.567.66-.283 1.131-.755 1.603-1.321v1.699h2.922V9.345h-2.922v.755Zm-.472 6.891c-.283.472-.66.944-1.131 1.227-.471.283-1.037.378-1.509.378-.565 0-1.037-.189-1.508-.378-.471-.283-.849-.755-1.131-1.227-.283-.566-.472-1.227-.472-1.793 0-.661.095-1.228.472-1.794.848-1.51 2.64-1.982 4.148-1.227h.094c.471.283.849.66 1.131 1.227.472 1.133.472 2.454-.094 3.587ZM126.708 5.477h-2.923v15.575h2.923V5.477ZM140.191 14.914c0-1.038-.189-2.077-.755-3.02-.471-.85-1.131-1.605-2.074-1.983a6.78 6.78 0 0 0-6.033 0 5.485 5.485 0 0 0-2.075 2.077c-.565 1.038-.754 2.077-.754 3.21 0 1.132.189 2.17.754 3.114a5.485 5.485 0 0 0 2.075 2.077c.942.472 1.979.755 3.016.755a6.56 6.56 0 0 0 3.489-1.038c.943-.661 1.602-1.7 1.885-2.738h-3.016c-.377.944-1.321 1.51-2.358 1.416-.659 0-1.414-.283-1.885-.755-.566-.472-.848-1.227-.848-1.982h8.484c.095-.378.095-.755.095-1.133Zm-8.579-.85c.093-.755.377-1.415.942-1.887.566-.472 1.226-.661 1.886-.661.754 0 1.414.189 1.979.66.566.473.849 1.133.849 1.794l-5.656.095ZM67.692 9.159a4.7 4.7 0 0 0-1.98.472c-.377.188-.754.377-1.037.66a.286.286 0 0 1-.377 0c-.094-.094-.094-.094-.094-.188v-.756H61.28v11.705h2.923v-6.418c-.094-.755.188-1.51.754-2.171.471-.472 1.226-.756 1.886-.756.66 0 1.414.284 1.885.755.472.567.754 1.322.66 2.172v6.418h2.923v-6.89c.094-1.322-.283-2.644-1.226-3.682-.848-.85-2.168-1.416-3.394-1.321Z"></path></g><defs><clipPath id="anyscale-logo_svg__a"><path fill="currentColor" d="M0 0h140.19v32H0z"></path></clipPath></defs></svg></div><div class="font-light leading-normal text-lightBlue-300 opacity-70 hidden lg:flex"><span>Â© Anyscale, Inc 2022</span></div></div><div class="z-10 hidden flex-col gap-10 lg:flex"><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Anyscale</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://www.linkedin.com/company/joinanyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M15.784 14.168a1.667 1.667 0 1 1-3.334-.002 1.667 1.667 0 0 1 3.334.002Zm.05 2.9H12.5V27.5h3.334V17.067Zm5.266 0h-3.317V27.5h3.284v-5.475c0-3.05 3.975-3.334 3.975 0V27.5h3.291v-6.609c0-5.141-5.883-4.95-7.266-2.424l.033-1.4Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://www.facebook.com/AnyscaleCompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M21.667 21.247h2.083l.834-3.333h-2.917v-1.667c0-.858 0-1.666 1.666-1.666h1.25v-2.8a23.49 23.49 0 0 0-2.38-.117c-2.263 0-3.87 1.38-3.87 3.917v2.333h-2.5v3.333h2.5v7.084h3.334v-7.084Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://twitter.com/anyscalecompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/anyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Ray</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://twitter.com/raydistributed"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/ray-project/ray"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div></div></div><div class="grid grid-cols-2 gap-x-8 gap-y-12 md:grid-cols-3 md:gap-y-10 md:gap-x-16"><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Products</div><div><ul>
<li><a href="platform.html">Anyscale Compute Platform</a></li>
<li><a href="ray-open-source.html">Ray Open Source</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Events</div><div><ul>
<li><a href="event-category/rl-summit.html">Webinars</a></li>
<li><a href="event-category/rl-summit.html">Meetups</a></li>
<li><a href="event-category/rl-summit.html">Summits</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Company</div><div><ul>
<li><a href="about.html">About Us</a></li>
<li><a href="press.html">News</a></li>
<li><a href="careers.html">Careers</a></li>
<li><a href="community.html">Community</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Learn</div><div><ul>
<li><a href="blog/how-ray-solves-common-production-challenges-for-generative-ai.html">Blog</a></li>
<li><a href="event-category/rl-summit.html">Demos &amp; Webinars</a></li>
<li><a href="event-category/rl-summit.html">Anyscale Academy</a></li>
<li><a href="https://docs.anyscale.com">Anyscale Docs</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Scalable AI and Python</div><div><ul>
<li><a href="data-ingestion.html">Data Ingestion</a></li>
<li><a href="reinforcement-learning.html">Reinforcement Learning</a></li>
<li><a href="ray-air.html">Ray AIR</a></li>
<li><a href="model-serving.html">Model Serving</a></li>
<li><a href="hyperparameter-tuning.html">Hyperparameter Tuning</a></li>
</ul>
</div></div><div class="footer-links"><div class="font-heading font-semibold leading-normal text-white">Use Cases</div><div><ul>
<li><a href="demand-forecasting.html">Demand Forecasting/Pricing</a></li>
<li><a href="industrial-automation.html">Industrial Automation</a></li>
<li><a href="machine-learning.html">Scalable ML Platforms</a></li>
<li><a href="natural-language-processing.html">NLP</a></li>
<li><a href="recommendation-system.html">Recommendation System</a></li>
</ul>
</div></div></div><div class="grid-col-1 grid gap-12 md:grid-cols-3 md:gap-16 lg:hidden"><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Anyscale</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://www.linkedin.com/company/joinanyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M15.784 14.168a1.667 1.667 0 1 1-3.334-.002 1.667 1.667 0 0 1 3.334.002Zm.05 2.9H12.5V27.5h3.334V17.067Zm5.266 0h-3.317V27.5h3.284v-5.475c0-3.05 3.975-3.334 3.975 0V27.5h3.291v-6.609c0-5.141-5.883-4.95-7.266-2.424l.033-1.4Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://www.facebook.com/AnyscaleCompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M21.667 21.247h2.083l.834-3.333h-2.917v-1.667c0-.858 0-1.666 1.666-1.666h1.25v-2.8a23.49 23.49 0 0 0-2.38-.117c-2.263 0-3.87 1.38-3.87 3.917v2.333h-2.5v3.333h2.5v7.084h3.334v-7.084Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://twitter.com/anyscalecompute"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/anyscale"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="flex flex-col gap-5 lg:gap-6"><div class="font-heading font-semibold leading-normal text-white">Follow Ray</div><div class="flex flex-row gap-4"><div class="cursor-pointer"><a class="" href="https://twitter.com/raydistributed"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M28.469 14.71a6.986 6.986 0 0 1-2.002.548A3.497 3.497 0 0 0 28 13.33a6.967 6.967 0 0 1-2.213.846 3.485 3.485 0 0 0-5.938 3.178 9.894 9.894 0 0 1-7.184-3.642 3.485 3.485 0 0 0 1.078 4.653 3.474 3.474 0 0 1-1.578-.435v.043a3.488 3.488 0 0 0 2.796 3.418c-.513.139-1.052.16-1.575.06a3.488 3.488 0 0 0 3.256 2.42 6.996 6.996 0 0 1-5.16 1.444 9.858 9.858 0 0 0 5.343 1.566c6.41 0 9.916-5.31 9.916-9.916 0-.15-.004-.302-.01-.45a7.082 7.082 0 0 0 1.739-1.804l-.001-.001Z" fill="current"></path></svg></a></div><div class="cursor-pointer"><a class="" href="https://github.com/ray-project/ray"><svg width="40" height="40" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="rounded-full bg-darkBlue fill-current text-white transition-all hover:bg-[#6EDBF0] hover:text-primary"><path d="M20.335 12A8.334 8.334 0 0 0 12 20.335a8.334 8.334 0 0 0 5.7 7.908c.416.078.569-.18.569-.4 0-.199-.007-.723-.01-1.418-2.32.503-2.808-1.118-2.808-1.118-.38-.962-.928-1.22-.928-1.22-.755-.516.059-.506.059-.506.837.059 1.276.859 1.276.859.744 1.274 1.951.906 2.428.693.075-.539.29-.906.528-1.115-1.851-.208-3.797-.925-3.797-4.119 0-.91.323-1.653.858-2.236-.094-.21-.375-1.058.073-2.206 0 0 .698-.224 2.292.854a7.986 7.986 0 0 1 2.084-.281 7.985 7.985 0 0 1 2.084.281c1.583-1.078 2.281-.854 2.281-.854.448 1.148.167 1.995.084 2.206a3.23 3.23 0 0 1 .854 2.236c0 3.203-1.948 3.907-3.803 4.112.292.25.563.762.563 1.542 0 1.116-.01 2.012-.01 2.283 0 .219.145.48.572.396a8.306 8.306 0 0 0 5.721-7.897A8.335 8.335 0 0 0 20.335 12" fill="current"></path></svg></a></div></div></div><div class="md:self-end md:text-right"><div class="font-light leading-normal text-lightBlue-300 opacity-70"><span>Â© Anyscale, Inc 2022</span></div></div></div></div><div class="pointer-events-none absolute bottom-0 left-0"><svg width="612" height="297" fill="none" xmlns="http://www.w3.org/2000/svg" role="img" class="hidden lg:block"><g opacity="0.5" clip-path="url(#dots_svg__a)"><mask id="dots_svg__c" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="0" y="0" width="612" height="297"><path transform="rotate(90 611.5 .5)" fill="url(#dots_svg__b)" stroke="#F2F3F7" d="M611.5.5h296v611h-296z"></path></mask><g mask="url(#dots_svg__c)" stroke="#A2B7D4"><g opacity="0.2" stroke-linecap="round"><path d="M598 350v-688M582 350v-688M566 350v-688M550 350v-688M534 350v-688M518 350v-688M502 350v-688M486 350v-688M470 350v-688M454 350v-688M438 350v-688M422 350v-688M406 350v-688M390 350v-688M374 350v-688M358 350v-688M342 350v-688M326 350v-688M310 350v-688M294 350v-688M278 350v-688M262 350v-688M246 350v-688M230 350v-688M214 350v-688M198 350v-688M182 350v-688M166 350v-688M150 350v-688M134 350v-688M118 350v-688M102 350v-688M86 350v-688M70 350v-688M54 350v-688M38 350v-688M22 350v-688M6 350v-688"></path></g><g opacity="0.2" stroke-linecap="round"><path d="M1014 14H0M1014 30H0M1014 46H0M1014 62H0M1014 78H0M1014 94H0M1014 110H0M1014 126H0M1014 142H0M1014 158H0M1014 174H0M1014 190H4M1014 206H4M1014 222H4M1014 238H4M1014 254H4M1014 270H4M1014 286H4"></path></g><path fill="#4C6996" d="M615.5 12.5v3h-3v-3zM615.5 28.5v3h-3v-3zM615.5 44.5v3h-3v-3zM615.5 60.5v3h-3v-3zM615.5 76.5v3h-3v-3zM615.5 92.5v3h-3v-3zM615.5 108.5v3h-3v-3zM615.5 124.5v3h-3v-3zM615.5 140.5v3h-3v-3zM615.5 156.5v3h-3v-3zM615.5 172.5v3h-3v-3zM615.5 188.5v3h-3v-3zM615.5 204.5v3h-3v-3zM615.5 220.5v3h-3v-3zM615.5 236.5v3h-3v-3zM615.5 252.5v3h-3v-3zM615.5 268.5v3h-3v-3zM615.5 284.5v3h-3v-3zM599.5 12.5v3h-3v-3zM599.5 28.5v3h-3v-3zM599.5 44.5v3h-3v-3zM599.5 60.5v3h-3v-3zM599.5 76.5v3h-3v-3zM599.5 92.5v3h-3v-3zM599.5 108.5v3h-3v-3zM599.5 124.5v3h-3v-3zM599.5 140.5v3h-3v-3zM599.5 156.5v3h-3v-3zM599.5 172.5v3h-3v-3zM599.5 188.5v3h-3v-3zM599.5 204.5v3h-3v-3zM599.5 220.5v3h-3v-3zM599.5 236.5v3h-3v-3zM599.5 252.5v3h-3v-3zM599.5 268.5v3h-3v-3zM599.5 284.5v3h-3v-3zM583.5 12.5v3h-3v-3zM583.5 28.5v3h-3v-3zM583.5 44.5v3h-3v-3zM583.5 60.5v3h-3v-3zM583.5 76.5v3h-3v-3zM583.5 92.5v3h-3v-3zM583.5 108.5v3h-3v-3zM583.5 124.5v3h-3v-3zM583.5 140.5v3h-3v-3zM583.5 156.5v3h-3v-3zM583.5 172.5v3h-3v-3zM583.5 188.5v3h-3v-3zM583.5 204.5v3h-3v-3zM583.5 220.5v3h-3v-3zM583.5 236.5v3h-3v-3zM583.5 252.5v3h-3v-3zM583.5 268.5v3h-3v-3zM583.5 284.5v3h-3v-3zM567.5 12.5v3h-3v-3zM567.5 28.5v3h-3v-3zM567.5 44.5v3h-3v-3zM567.5 60.5v3h-3v-3zM567.5 76.5v3h-3v-3zM567.5 92.5v3h-3v-3zM567.5 108.5v3h-3v-3zM567.5 124.5v3h-3v-3zM567.5 140.5v3h-3v-3zM567.5 156.5v3h-3v-3zM567.5 172.5v3h-3v-3zM567.5 188.5v3h-3v-3zM567.5 204.5v3h-3v-3zM567.5 220.5v3h-3v-3zM567.5 236.5v3h-3v-3zM567.5 252.5v3h-3v-3zM567.5 268.5v3h-3v-3zM567.5 284.5v3h-3v-3zM551.5 12.5v3h-3v-3zM551.5 28.5v3h-3v-3zM551.5 44.5v3h-3v-3zM551.5 60.5v3h-3v-3zM551.5 76.5v3h-3v-3zM551.5 92.5v3h-3v-3zM551.5 108.5v3h-3v-3zM551.5 124.5v3h-3v-3zM551.5 140.5v3h-3v-3zM551.5 156.5v3h-3v-3zM551.5 172.5v3h-3v-3zM551.5 188.5v3h-3v-3zM551.5 204.5v3h-3v-3zM551.5 220.5v3h-3v-3zM551.5 236.5v3h-3v-3zM551.5 252.5v3h-3v-3zM551.5 268.5v3h-3v-3zM551.5 284.5v3h-3v-3zM535.5 12.5v3h-3v-3zM535.5 28.5v3h-3v-3zM535.5 44.5v3h-3v-3zM535.5 60.5v3h-3v-3zM535.5 76.5v3h-3v-3zM535.5 92.5v3h-3v-3zM535.5 108.5v3h-3v-3zM535.5 124.5v3h-3v-3zM535.5 140.5v3h-3v-3zM535.5 156.5v3h-3v-3zM535.5 172.5v3h-3v-3zM535.5 188.5v3h-3v-3zM535.5 204.5v3h-3v-3zM535.5 220.5v3h-3v-3zM535.5 236.5v3h-3v-3zM535.5 252.5v3h-3v-3zM535.5 268.5v3h-3v-3zM535.5 284.5v3h-3v-3zM519.5 12.5v3h-3v-3zM519.5 28.5v3h-3v-3zM519.5 44.5v3h-3v-3zM519.5 60.5v3h-3v-3zM519.5 76.5v3h-3v-3zM519.5 92.5v3h-3v-3zM519.5 108.5v3h-3v-3zM519.5 124.5v3h-3v-3zM519.5 140.5v3h-3v-3zM519.5 156.5v3h-3v-3zM519.5 172.5v3h-3v-3zM519.5 188.5v3h-3v-3zM519.5 204.5v3h-3v-3zM519.5 220.5v3h-3v-3zM519.5 236.5v3h-3v-3zM519.5 252.5v3h-3v-3zM519.5 268.5v3h-3v-3zM519.5 284.5v3h-3v-3zM503.5 12.5v3h-3v-3zM503.5 28.5v3h-3v-3zM503.5 44.5v3h-3v-3zM503.5 60.5v3h-3v-3zM503.5 76.5v3h-3v-3zM503.5 92.5v3h-3v-3zM503.5 108.5v3h-3v-3zM503.5 124.5v3h-3v-3zM503.5 140.5v3h-3v-3zM503.5 156.5v3h-3v-3zM503.5 172.5v3h-3v-3zM503.5 188.5v3h-3v-3zM503.5 204.5v3h-3v-3zM503.5 220.5v3h-3v-3zM503.5 236.5v3h-3v-3zM503.5 252.5v3h-3v-3zM503.5 268.5v3h-3v-3zM503.5 284.5v3h-3v-3zM487.5 12.5v3h-3v-3zM487.5 28.5v3h-3v-3zM487.5 44.5v3h-3v-3zM487.5 60.5v3h-3v-3zM487.5 76.5v3h-3v-3zM487.5 92.5v3h-3v-3zM487.5 108.5v3h-3v-3zM487.5 124.5v3h-3v-3zM487.5 140.5v3h-3v-3zM487.5 156.5v3h-3v-3zM487.5 172.5v3h-3v-3zM487.5 188.5v3h-3v-3zM487.5 204.5v3h-3v-3zM487.5 220.5v3h-3v-3zM487.5 236.5v3h-3v-3zM487.5 252.5v3h-3v-3zM487.5 268.5v3h-3v-3zM487.5 284.5v3h-3v-3zM471.5 12.5v3h-3v-3zM471.5 28.5v3h-3v-3zM471.5 44.5v3h-3v-3zM471.5 60.5v3h-3v-3zM471.5 76.5v3h-3v-3zM471.5 92.5v3h-3v-3zM471.5 108.5v3h-3v-3zM471.5 124.5v3h-3v-3zM471.5 140.5v3h-3v-3zM471.5 156.5v3h-3v-3zM471.5 172.5v3h-3v-3zM471.5 188.5v3h-3v-3zM471.5 204.5v3h-3v-3zM471.5 220.5v3h-3v-3zM471.5 236.5v3h-3v-3zM471.5 252.5v3h-3v-3zM471.5 268.5v3h-3v-3zM471.5 284.5v3h-3v-3zM455.5 12.5v3h-3v-3zM455.5 28.5v3h-3v-3zM455.5 44.5v3h-3v-3zM455.5 60.5v3h-3v-3zM455.5 76.5v3h-3v-3zM455.5 92.5v3h-3v-3zM455.5 108.5v3h-3v-3zM455.5 124.5v3h-3v-3zM455.5 140.5v3h-3v-3zM455.5 156.5v3h-3v-3zM455.5 172.5v3h-3v-3zM455.5 188.5v3h-3v-3zM455.5 204.5v3h-3v-3zM455.5 220.5v3h-3v-3zM455.5 236.5v3h-3v-3zM455.5 252.5v3h-3v-3zM455.5 268.5v3h-3v-3zM455.5 284.5v3h-3v-3zM439.5 12.5v3h-3v-3zM439.5 28.5v3h-3v-3zM439.5 44.5v3h-3v-3zM439.5 60.5v3h-3v-3zM439.5 76.5v3h-3v-3zM439.5 92.5v3h-3v-3zM439.5 108.5v3h-3v-3zM439.5 124.5v3h-3v-3zM439.5 140.5v3h-3v-3zM439.5 156.5v3h-3v-3zM439.5 172.5v3h-3v-3zM439.5 188.5v3h-3v-3zM439.5 204.5v3h-3v-3zM439.5 220.5v3h-3v-3zM439.5 236.5v3h-3v-3zM439.5 252.5v3h-3v-3zM439.5 268.5v3h-3v-3zM439.5 284.5v3h-3v-3zM423.5 12.5v3h-3v-3zM423.5 28.5v3h-3v-3zM423.5 44.5v3h-3v-3zM423.5 60.5v3h-3v-3zM423.5 76.5v3h-3v-3zM423.5 92.5v3h-3v-3zM423.5 108.5v3h-3v-3zM423.5 124.5v3h-3v-3zM423.5 140.5v3h-3v-3zM423.5 156.5v3h-3v-3zM423.5 172.5v3h-3v-3zM423.5 188.5v3h-3v-3zM423.5 204.5v3h-3v-3zM423.5 220.5v3h-3v-3zM423.5 236.5v3h-3v-3zM423.5 252.5v3h-3v-3zM423.5 268.5v3h-3v-3zM423.5 284.5v3h-3v-3zM407.5 12.5v3h-3v-3zM407.5 28.5v3h-3v-3zM407.5 44.5v3h-3v-3zM407.5 60.5v3h-3v-3zM407.5 76.5v3h-3v-3zM407.5 92.5v3h-3v-3zM407.5 108.5v3h-3v-3zM407.5 124.5v3h-3v-3zM407.5 140.5v3h-3v-3zM407.5 156.5v3h-3v-3zM407.5 172.5v3h-3v-3zM407.5 188.5v3h-3v-3zM407.5 204.5v3h-3v-3zM407.5 220.5v3h-3v-3zM407.5 236.5v3h-3v-3zM407.5 252.5v3h-3v-3zM407.5 268.5v3h-3v-3zM407.5 284.5v3h-3v-3zM391.5 12.5v3h-3v-3zM391.5 28.5v3h-3v-3zM391.5 44.5v3h-3v-3zM391.5 60.5v3h-3v-3zM391.5 76.5v3h-3v-3zM391.5 92.5v3h-3v-3zM391.5 108.5v3h-3v-3zM391.5 124.5v3h-3v-3zM391.5 140.5v3h-3v-3zM391.5 156.5v3h-3v-3zM391.5 172.5v3h-3v-3zM391.5 188.5v3h-3v-3zM391.5 204.5v3h-3v-3zM391.5 220.5v3h-3v-3zM391.5 236.5v3h-3v-3zM391.5 252.5v3h-3v-3zM391.5 268.5v3h-3v-3zM391.5 284.5v3h-3v-3zM375.5 12.5v3h-3v-3zM375.5 28.5v3h-3v-3zM375.5 44.5v3h-3v-3zM375.5 60.5v3h-3v-3zM375.5 76.5v3h-3v-3zM375.5 92.5v3h-3v-3zM375.5 108.5v3h-3v-3zM375.5 124.5v3h-3v-3zM375.5 140.5v3h-3v-3zM375.5 156.5v3h-3v-3zM375.5 172.5v3h-3v-3zM375.5 188.5v3h-3v-3zM375.5 204.5v3h-3v-3zM375.5 220.5v3h-3v-3z"></path><rect x="375.5" y="236.5" width="3" height="3" rx="1.5" transform="rotate(90 375.5 236.5)" fill="#4C6996"></rect><path fill="#4C6996" d="M375.5 252.5v3h-3v-3zM375.5 268.5v3h-3v-3zM375.5 284.5v3h-3v-3zM359.5 12.5v3h-3v-3zM359.5 28.5v3h-3v-3zM359.5 44.5v3h-3v-3zM359.5 60.5v3h-3v-3zM359.5 76.5v3h-3v-3zM359.5 92.5v3h-3v-3zM359.5 108.5v3h-3v-3zM359.5 124.5v3h-3v-3zM359.5 140.5v3h-3v-3zM359.5 156.5v3h-3v-3zM359.5 172.5v3h-3v-3zM359.5 188.5v3h-3v-3zM359.5 204.5v3h-3v-3zM359.5 220.5v3h-3v-3zM359.5 236.5v3h-3v-3zM359.5 252.5v3h-3v-3zM359.5 268.5v3h-3v-3zM359.5 284.5v3h-3v-3zM343.5 12.5v3h-3v-3zM343.5 28.5v3h-3v-3zM343.5 44.5v3h-3v-3zM343.5 60.5v3h-3v-3zM343.5 76.5v3h-3v-3zM343.5 92.5v3h-3v-3zM343.5 108.5v3h-3v-3zM343.5 124.5v3h-3v-3zM343.5 140.5v3h-3v-3zM343.5 156.5v3h-3v-3zM343.5 172.5v3h-3v-3zM343.5 188.5v3h-3v-3zM343.5 204.5v3h-3v-3zM343.5 220.5v3h-3v-3zM343.5 236.5v3h-3v-3zM343.5 252.5v3h-3v-3zM343.5 268.5v3h-3v-3zM343.5 284.5v3h-3v-3zM327.5 12.5v3h-3v-3zM327.5 28.5v3h-3v-3zM327.5 44.5v3h-3v-3zM327.5 60.5v3h-3v-3zM327.5 76.5v3h-3v-3zM327.5 92.5v3h-3v-3zM327.5 108.5v3h-3v-3zM327.5 124.5v3h-3v-3zM327.5 140.5v3h-3v-3zM327.5 156.5v3h-3v-3zM327.5 172.5v3h-3v-3zM327.5 188.5v3h-3v-3zM327.5 204.5v3h-3v-3zM327.5 220.5v3h-3v-3zM327.5 236.5v3h-3v-3zM327.5 252.5v3h-3v-3zM327.5 268.5v3h-3v-3zM327.5 284.5v3h-3v-3zM311.5 12.5v3h-3v-3zM311.5 28.5v3h-3v-3zM311.5 44.5v3h-3v-3zM311.5 60.5v3h-3v-3zM311.5 76.5v3h-3v-3zM311.5 92.5v3h-3v-3zM311.5 108.5v3h-3v-3zM311.5 124.5v3h-3v-3zM311.5 140.5v3h-3v-3zM311.5 156.5v3h-3v-3zM311.5 172.5v3h-3v-3zM311.5 188.5v3h-3v-3zM311.5 204.5v3h-3v-3zM311.5 220.5v3h-3v-3zM311.5 236.5v3h-3v-3zM311.5 252.5v3h-3v-3zM311.5 268.5v3h-3v-3zM311.5 284.5v3h-3v-3zM295.5 12.5v3h-3v-3zM295.5 28.5v3h-3v-3zM295.5 44.5v3h-3v-3zM295.5 60.5v3h-3v-3zM295.5 76.5v3h-3v-3zM295.5 92.5v3h-3v-3zM295.5 108.5v3h-3v-3zM295.5 124.5v3h-3v-3zM295.5 140.5v3h-3v-3zM295.5 156.5v3h-3v-3zM295.5 172.5v3h-3v-3zM295.5 188.5v3h-3v-3zM295.5 204.5v3h-3v-3zM295.5 220.5v3h-3v-3zM295.5 236.5v3h-3v-3zM295.5 252.5v3h-3v-3zM295.5 268.5v3h-3v-3zM295.5 284.5v3h-3v-3zM279.5 12.5v3h-3v-3zM279.5 28.5v3h-3v-3zM279.5 44.5v3h-3v-3zM279.5 60.5v3h-3v-3zM279.5 76.5v3h-3v-3zM279.5 92.5v3h-3v-3zM279.5 108.5v3h-3v-3zM279.5 124.5v3h-3v-3zM279.5 140.5v3h-3v-3zM279.5 156.5v3h-3v-3zM279.5 172.5v3h-3v-3zM279.5 188.5v3h-3v-3zM279.5 204.5v3h-3v-3zM279.5 220.5v3h-3v-3zM279.5 236.5v3h-3v-3zM279.5 252.5v3h-3v-3zM279.5 268.5v3h-3v-3zM279.5 284.5v3h-3v-3zM263.5 12.5v3h-3v-3zM263.5 28.5v3h-3v-3zM263.5 44.5v3h-3v-3zM263.5 60.5v3h-3v-3zM263.5 76.5v3h-3v-3zM263.5 92.5v3h-3v-3zM263.5 108.5v3h-3v-3zM263.5 124.5v3h-3v-3zM263.5 140.5v3h-3v-3zM263.5 156.5v3h-3v-3zM263.5 172.5v3h-3v-3zM263.5 188.5v3h-3v-3zM263.5 204.5v3h-3v-3zM263.5 220.5v3h-3v-3zM263.5 236.5v3h-3v-3zM263.5 252.5v3h-3v-3zM263.5 268.5v3h-3v-3zM263.5 284.5v3h-3v-3zM247.5 12.5v3h-3v-3zM247.5 28.5v3h-3v-3zM247.5 44.5v3h-3v-3zM247.5 60.5v3h-3v-3zM247.5 76.5v3h-3v-3zM247.5 92.5v3h-3v-3zM247.5 108.5v3h-3v-3zM247.5 124.5v3h-3v-3zM247.5 140.5v3h-3v-3zM247.5 156.5v3h-3v-3zM247.5 172.5v3h-3v-3zM247.5 188.5v3h-3v-3zM247.5 204.5v3h-3v-3zM247.5 220.5v3h-3v-3zM247.5 236.5v3h-3v-3zM247.5 252.5v3h-3v-3zM247.5 268.5v3h-3v-3zM247.5 284.5v3h-3v-3zM231.5 12.5v3h-3v-3zM231.5 28.5v3h-3v-3zM231.5 44.5v3h-3v-3zM231.5 60.5v3h-3v-3zM231.5 76.5v3h-3v-3zM231.5 92.5v3h-3v-3zM231.5 108.5v3h-3v-3zM231.5 124.5v3h-3v-3zM231.5 140.5v3h-3v-3zM231.5 156.5v3h-3v-3zM231.5 172.5v3h-3v-3zM231.5 188.5v3h-3v-3zM231.5 204.5v3h-3v-3zM231.5 220.5v3h-3v-3zM231.5 236.5v3h-3v-3zM231.5 252.5v3h-3v-3zM231.5 268.5v3h-3v-3zM231.5 284.5v3h-3v-3zM215.5 12.5v3h-3v-3zM215.5 28.5v3h-3v-3zM215.5 44.5v3h-3v-3zM215.5 60.5v3h-3v-3zM215.5 76.5v3h-3v-3zM215.5 92.5v3h-3v-3zM215.5 108.5v3h-3v-3zM215.5 124.5v3h-3v-3zM215.5 140.5v3h-3v-3zM215.5 156.5v3h-3v-3zM215.5 172.5v3h-3v-3zM215.5 188.5v3h-3v-3zM215.5 204.5v3h-3v-3zM215.5 220.5v3h-3v-3zM215.5 236.5v3h-3v-3zM215.5 252.5v3h-3v-3zM215.5 268.5v3h-3v-3zM215.5 284.5v3h-3v-3zM199.5 12.5v3h-3v-3zM199.5 28.5v3h-3v-3zM199.5 44.5v3h-3v-3zM199.5 60.5v3h-3v-3zM199.5 76.5v3h-3v-3zM199.5 92.5v3h-3v-3zM199.5 108.5v3h-3v-3zM199.5 124.5v3h-3v-3zM199.5 140.5v3h-3v-3zM199.5 156.5v3h-3v-3zM199.5 172.5v3h-3v-3zM199.5 188.5v3h-3v-3zM199.5 204.5v3h-3v-3zM199.5 220.5v3h-3v-3zM199.5 236.5v3h-3v-3zM199.5 252.5v3h-3v-3zM199.5 268.5v3h-3v-3zM199.5 284.5v3h-3v-3zM183.5 12.5v3h-3v-3zM183.5 28.5v3h-3v-3zM183.5 44.5v3h-3v-3zM183.5 60.5v3h-3v-3zM183.5 76.5v3h-3v-3zM183.5 92.5v3h-3v-3zM183.5 108.5v3h-3v-3zM183.5 124.5v3h-3v-3zM183.5 140.5v3h-3v-3zM183.5 156.5v3h-3v-3zM183.5 172.5v3h-3v-3zM183.5 188.5v3h-3v-3zM183.5 204.5v3h-3v-3zM183.5 220.5v3h-3v-3zM183.5 236.5v3h-3v-3zM183.5 252.5v3h-3v-3zM183.5 268.5v3h-3v-3zM183.5 284.5v3h-3v-3zM167.5 12.5v3h-3v-3zM167.5 28.5v3h-3v-3zM167.5 44.5v3h-3v-3zM167.5 60.5v3h-3v-3zM167.5 76.5v3h-3v-3zM167.5 92.5v3h-3v-3zM167.5 108.5v3h-3v-3zM167.5 124.5v3h-3v-3zM167.5 140.5v3h-3v-3zM167.5 156.5v3h-3v-3zM167.5 172.5v3h-3v-3zM167.5 188.5v3h-3v-3zM167.5 204.5v3h-3v-3zM167.5 220.5v3h-3v-3zM167.5 236.5v3h-3v-3zM167.5 252.5v3h-3v-3zM167.5 268.5v3h-3v-3zM167.5 284.5v3h-3v-3zM151.5 12.5v3h-3v-3zM151.5 28.5v3h-3v-3zM151.5 44.5v3h-3v-3zM151.5 60.5v3h-3v-3zM151.5 76.5v3h-3v-3zM151.5 92.5v3h-3v-3zM151.5 108.5v3h-3v-3zM151.5 124.5v3h-3v-3zM151.5 140.5v3h-3v-3zM151.5 156.5v3h-3v-3zM151.5 172.5v3h-3v-3zM151.5 188.5v3h-3v-3zM151.5 204.5v3h-3v-3zM151.5 220.5v3h-3v-3zM151.5 236.5v3h-3v-3zM151.5 252.5v3h-3v-3zM151.5 268.5v3h-3v-3zM151.5 284.5v3h-3v-3zM135.5 12.5v3h-3v-3zM135.5 28.5v3h-3v-3zM135.5 44.5v3h-3v-3zM135.5 60.5v3h-3v-3zM135.5 76.5v3h-3v-3zM135.5 92.5v3h-3v-3zM135.5 108.5v3h-3v-3zM135.5 124.5v3h-3v-3zM135.5 140.5v3h-3v-3zM135.5 156.5v3h-3v-3zM135.5 172.5v3h-3v-3zM135.5 188.5v3h-3v-3zM135.5 204.5v3h-3v-3zM135.5 220.5v3h-3v-3zM135.5 236.5v3h-3v-3zM135.5 252.5v3h-3v-3zM135.5 268.5v3h-3v-3zM135.5 284.5v3h-3v-3zM119.5 12.5v3h-3v-3zM119.5 28.5v3h-3v-3zM119.5 44.5v3h-3v-3zM119.5 60.5v3h-3v-3zM119.5 76.5v3h-3v-3zM119.5 92.5v3h-3v-3zM119.5 108.5v3h-3v-3zM119.5 124.5v3h-3v-3zM119.5 140.5v3h-3v-3zM119.5 156.5v3h-3v-3zM119.5 172.5v3h-3v-3zM119.5 188.5v3h-3v-3zM119.5 204.5v3h-3v-3zM119.5 220.5v3h-3v-3zM119.5 236.5v3h-3v-3zM119.5 252.5v3h-3v-3zM119.5 268.5v3h-3v-3zM119.5 284.5v3h-3v-3zM103.5 12.5v3h-3v-3zM103.5 28.5v3h-3v-3zM103.5 44.5v3h-3v-3zM103.5 60.5v3h-3v-3zM103.5 76.5v3h-3v-3zM103.5 92.5v3h-3v-3zM103.5 108.5v3h-3v-3zM103.5 124.5v3h-3v-3zM103.5 140.5v3h-3v-3zM103.5 156.5v3h-3v-3zM103.5 172.5v3h-3v-3zM103.5 188.5v3h-3v-3zM103.5 204.5v3h-3v-3zM103.5 220.5v3h-3v-3zM103.5 236.5v3h-3v-3zM103.5 252.5v3h-3v-3zM103.5 268.5v3h-3v-3zM103.5 284.5v3h-3v-3zM87.5 12.5v3h-3v-3zM87.5 28.5v3h-3v-3zM87.5 44.5v3h-3v-3zM87.5 60.5v3h-3v-3zM87.5 76.5v3h-3v-3zM87.5 92.5v3h-3v-3zM87.5 108.5v3h-3v-3zM87.5 124.5v3h-3v-3zM87.5 140.5v3h-3v-3zM87.5 156.5v3h-3v-3zM87.5 172.5v3h-3v-3zM87.5 188.5v3h-3v-3zM87.5 204.5v3h-3v-3zM87.5 220.5v3h-3v-3zM87.5 236.5v3h-3v-3zM87.5 252.5v3h-3v-3zM87.5 268.5v3h-3v-3zM87.5 284.5v3h-3v-3zM71.5 12.5v3h-3v-3zM71.5 28.5v3h-3v-3zM71.5 44.5v3h-3v-3zM71.5 60.5v3h-3v-3zM71.5 76.5v3h-3v-3zM71.5 92.5v3h-3v-3zM71.5 108.5v3h-3v-3zM71.5 124.5v3h-3v-3zM71.5 140.5v3h-3v-3zM71.5 156.5v3h-3v-3zM71.5 172.5v3h-3v-3zM71.5 188.5v3h-3v-3zM71.5 204.5v3h-3v-3zM71.5 220.5v3h-3v-3zM71.5 236.5v3h-3v-3zM71.5 252.5v3h-3v-3zM71.5 268.5v3h-3v-3zM71.5 284.5v3h-3v-3zM55.5 12.5v3h-3v-3zM55.5 28.5v3h-3v-3zM55.5 44.5v3h-3v-3zM55.5 60.5v3h-3v-3zM55.5 76.5v3h-3v-3zM55.5 92.5v3h-3v-3zM55.5 108.5v3h-3v-3zM55.5 124.5v3h-3v-3zM55.5 140.5v3h-3v-3zM55.5 156.5v3h-3v-3zM55.5 172.5v3h-3v-3zM55.5 188.5v3h-3v-3zM55.5 204.5v3h-3v-3zM55.5 220.5v3h-3v-3zM55.5 236.5v3h-3v-3zM55.5 252.5v3h-3v-3zM55.5 268.5v3h-3v-3zM55.5 284.5v3h-3v-3zM39.5 12.5v3h-3v-3zM39.5 28.5v3h-3v-3zM39.5 44.5v3h-3v-3zM39.5 60.5v3h-3v-3zM39.5 76.5v3h-3v-3zM39.5 92.5v3h-3v-3zM39.5 108.5v3h-3v-3zM39.5 124.5v3h-3v-3zM39.5 140.5v3h-3v-3zM39.5 156.5v3h-3v-3zM39.5 172.5v3h-3v-3zM39.5 188.5v3h-3v-3zM39.5 204.5v3h-3v-3zM39.5 220.5v3h-3v-3zM39.5 236.5v3h-3v-3zM39.5 252.5v3h-3v-3zM39.5 268.5v3h-3v-3zM39.5 284.5v3h-3v-3zM23.5 12.5v3h-3v-3zM23.5 28.5v3h-3v-3zM23.5 44.5v3h-3v-3zM23.5 60.5v3h-3v-3zM23.5 76.5v3h-3v-3zM23.5 92.5v3h-3v-3zM23.5 108.5v3h-3v-3zM23.5 124.5v3h-3v-3zM23.5 140.5v3h-3v-3zM23.5 156.5v3h-3v-3zM23.5 172.5v3h-3v-3zM23.5 188.5v3h-3v-3zM23.5 204.5v3h-3v-3zM23.5 220.5v3h-3v-3zM23.5 236.5v3h-3v-3zM23.5 252.5v3h-3v-3zM23.5 268.5v3h-3v-3zM23.5 284.5v3h-3v-3zM7.5 12.5v3h-3v-3zM7.5 28.5v3h-3v-3zM7.5 44.5v3h-3v-3zM7.5 60.5v3h-3v-3zM7.5 76.5v3h-3v-3zM7.5 92.5v3h-3v-3zM7.5 108.5v3h-3v-3zM7.5 124.5v3h-3v-3zM7.5 140.5v3h-3v-3zM7.5 156.5v3h-3v-3zM7.5 172.5v3h-3v-3zM7.5 188.5v3h-3v-3zM7.5 204.5v3h-3v-3zM7.5 220.5v3h-3v-3zM7.5 236.5v3h-3v-3zM7.5 252.5v3h-3v-3zM7.5 268.5v3h-3v-3zM7.5 284.5v3h-3v-3z"></path></g></g><defs><radialGradient id="dots_svg__b" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="rotate(-136.519 627.256 92.637) scale(929.292 677.674)"><stop stop-color="#D9D9D9"></stop><stop offset="0.574" stop-color="#D9D9D9" stop-opacity="0"></stop></radialGradient><clipPath id="dots_svg__a"><path fill="#fff" transform="rotate(90 306 306)" d="M0 0h297v612H0z"></path></clipPath></defs></svg></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"site":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ke4xTi5OD9gnn2YtRXyCV","type":"Entry","createdAt":"2022-10-14T08:31:23.161Z","updatedAt":"2023-06-06T20:01:24.167Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"site"}},"locale":"en-US"},"fields":{"title":"Anyscale","description":"From the creators of Ray, Anyscale is a framework for building machine learning applications at any scale originating from the UC Berkeley RISELab.","identifier":"site-master-new","footer":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"16eb9I0yhaIALNOPR8WKTY","type":"Entry","createdAt":"2020-09-27T16:03:59.581Z","updatedAt":"2022-10-27T01:20:31.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer","body":"Â© Anyscale, Inc 2022","images":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"C7AXPFHtleWPOZ9BuT73K","type":"Asset","createdAt":"2020-09-28T14:40:50.925Z","updatedAt":"2020-09-28T14:40:50.925Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"anyscale_logo","file":{"url":"//images.ctfassets.net/xjan103pcp94/C7AXPFHtleWPOZ9BuT73K/927f5130df1357db041d7a7908ab572e/Graphic_only__transparent_background_.png","details":{"size":1415,"image":{"width":331,"height":331}},"fileName":"Graphic only (transparent background).png","contentType":"image/png"}}}],"subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1cDdKq2eUfklc6w52dU6dg","type":"Entry","createdAt":"2020-09-27T16:05:42.307Z","updatedAt":"2022-10-27T01:20:26.714Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-1","header":"Products","body":"- [Anyscale Compute Platform](/platform)\n- [Ray Open Source](/ray-open-source)\n"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5tpXhcAJwSdhX8Wqw9HYTt","type":"Entry","createdAt":"2020-09-27T16:06:37.409Z","updatedAt":"2022-10-27T01:20:21.887Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":10,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-2","header":"Events","body":"- [Webinars](/events)\n- [Meetups](/events)\n- [Summits](/events)\n"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3c3LEomScM93hIg3i2zItU","type":"Entry","createdAt":"2020-09-27T16:07:42.856Z","updatedAt":"2022-10-27T01:20:18.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-3","header":"Company","body":"- [About Us](/about)\n- [News](/press)\n- [Careers](/careers)\n- [Community](/community)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6du4SSXPTxDZKoIVNHVVnT","type":"Entry","createdAt":"2022-10-27T01:20:13.320Z","updatedAt":"2022-10-27T01:20:13.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-4","header":"Learn","body":"- [Blog](/blog)\n- [Demos \u0026 Webinars](/events)\n- [Anyscale Academy](/events)\n- [Anyscale Docs](https://docs.anyscale.com)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1hovCXhGcWlgJmrqNzJ79k","type":"Entry","createdAt":"2022-10-27T01:20:09.895Z","updatedAt":"2022-10-27T01:20:09.895Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-5","header":"Scalable AI and Python","body":"- [Data Ingestion](/data-ingestion)\n- [Reinforcement Learning](/reinforcement-learning)\n- [Ray AIR](/ray-air)\n- [Model Serving](/model-serving)\n- [Hyperparameter Tuning](/hyperparameter-tuning)"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"YFy5fUqwOD5ShdMaYEvzB","type":"Entry","createdAt":"2022-10-27T01:20:06.631Z","updatedAt":"2022-10-27T01:20:06.631Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"footer-links-6","header":"Use Cases","body":"- [Demand Forecasting/Pricing](/demand-forecasting)\n- [Industrial Automation](/industrial-automation)\n- [Scalable ML Platforms](/machine-learning)\n- [NLP](/natural-language-processing)\n- [Recommendation System](/recommendation-system)"}}]}},"navItems":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"KBAZ5OavkdZfNTVjdJkp3","type":"Entry","createdAt":"2022-10-14T10:47:29.682Z","updatedAt":"2022-10-14T10:47:29.682Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Products","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KNN1Kmzo7jxSlxX1k74XQ","type":"Entry","createdAt":"2022-10-07T13:14:07.892Z","updatedAt":"2022-10-07T13:14:07.892Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"52Mmru1brS6ZkmItPeIW86","type":"Entry","createdAt":"2022-10-07T13:13:22.198Z","updatedAt":"2022-11-03T01:33:41.860Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Anyscale Platform","link":"/platform"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6cN6ZwdiFNiPcqIhd7EiQY","type":"Entry","createdAt":"2022-10-07T13:13:49.673Z","updatedAt":"2022-10-07T13:13:49.673Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Open Source","link":"/ray-open-source"}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5R1OTMfuLnPqont0Fjn9Xb","type":"Entry","createdAt":"2022-10-07T13:20:19.578Z","updatedAt":"2022-10-14T10:43:21.550Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Solutions","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1mp3IF3ZJE5JAViu3L9E95","type":"Entry","createdAt":"2022-10-07T13:16:00.346Z","updatedAt":"2022-10-07T13:21:01.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Scalable AI and Python","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4VUyKorAkgQP2V295h8eW3","type":"Entry","createdAt":"2022-10-07T13:15:35.949Z","updatedAt":"2022-10-07T13:15:35.949Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Data Ingestion","link":"/data-ingestion"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"Ouf5WgAzYmLovCZFNWqYm","type":"Entry","createdAt":"2022-10-07T13:17:09.728Z","updatedAt":"2022-10-07T13:17:09.728Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Reinforcement Learning","link":"/reinforcement-learning"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5kVPKqUbXBbKmrkvsImcIi","type":"Entry","createdAt":"2022-10-07T13:17:50.307Z","updatedAt":"2022-10-07T13:17:50.307Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray AIR","link":"/ray-air"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5uUidEvWw1oqjWrRcskEW","type":"Entry","createdAt":"2022-10-07T13:18:21.246Z","updatedAt":"2022-10-07T13:18:21.246Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Model Serving","link":"/model-serving"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Rbw0CrBN1KecPafCPvF4H","type":"Entry","createdAt":"2022-10-07T13:18:45.601Z","updatedAt":"2022-10-07T13:18:45.601Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Hyperparameter Tuning","link":"/hyperparameter-tuning"}}],"card":"##### Scalable AI and Python\n\nLeverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more."}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2RgWCuIiZe2TI6Nx5Ns7sB","type":"Entry","createdAt":"2022-10-07T13:25:03.101Z","updatedAt":"2023-05-31T12:47:25.274Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Use Cases","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3XMbpU1mPdsU7gktltlAzX","type":"Entry","createdAt":"2023-05-31T12:43:54.011Z","updatedAt":"2023-05-31T12:43:54.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Large Language Models","link":"/large-language-models"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6MAYw6HTSmH5JHcOoYjCzC","type":"Entry","createdAt":"2022-10-07T13:22:29.165Z","updatedAt":"2022-10-07T13:22:29.165Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Demand Forecasting/Pricing","link":"/demand-forecasting"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"30BlKhv6h4neaZwjA2jlxO","type":"Entry","createdAt":"2022-10-07T13:23:09.471Z","updatedAt":"2022-10-07T13:23:09.471Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Industrial Automation","link":"/industrial-automation"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2gGwUVxDuBeUso1uQH2gEW","type":"Entry","createdAt":"2022-10-07T13:23:32.145Z","updatedAt":"2022-10-07T13:23:32.145Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Scalable ML Platforms","link":"/machine-learning"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1cbwbmIvP5ZkV9iNEraQGP","type":"Entry","createdAt":"2022-10-07T13:23:59.732Z","updatedAt":"2022-10-07T13:23:59.732Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"NLP","link":"/natural-language-processing"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2A7FMbckpN5bgW1wp5ZuSh","type":"Entry","createdAt":"2022-10-07T13:24:41.861Z","updatedAt":"2022-10-07T13:24:41.861Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Recommendation System","link":"/recommendation-system"}}],"card":"##### Use Cases\n\nLeverage Ray and Anyscale to scale AI and Python applications. Learn more about Ray for reinforcement learning, deep learning, model serving and more."}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QNdrsmqz82oiBJvFTQ26n","type":"Entry","createdAt":"2023-04-13T11:37:45.354Z","updatedAt":"2023-04-13T11:37:45.354Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ecosystem","link":"/ray-ecosystem"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i0nHjF3pmjzGAdEE4V5MG","type":"Entry","createdAt":"2021-12-05T02:08:18.719Z","updatedAt":"2022-03-03T02:01:08.894Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Success Stories","link":"/user-stories"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5mFZvzY56aQCY4xvlBDi9W","type":"Entry","createdAt":"2022-10-14T10:46:35.218Z","updatedAt":"2022-10-14T10:46:35.218Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Learn","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"32tQWjMRAI5qci97W3cuie","type":"Entry","createdAt":"2022-10-07T13:28:46.057Z","updatedAt":"2023-03-22T20:31:55.243Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5JQbOCOkxHfH2ZPGNNQVbt","type":"Entry","createdAt":"2021-12-05T14:57:28.664Z","updatedAt":"2021-12-05T14:57:28.664Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Blog","link":"/blog"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4qEkJgYkXZmiDq3lsxg0xW","type":"Entry","createdAt":"2023-03-22T20:31:21.666Z","updatedAt":"2023-03-22T20:31:21.666Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Summit 2023","link":"https://raysummit.anyscale.com/?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=home_nav-pulldown"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48sdeWXptwt3UgEZzeTt3k","type":"Entry","createdAt":"2022-10-07T13:26:28.331Z","updatedAt":"2022-10-07T13:26:28.331Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Videos \u0026 Webinars","link":"/events"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6hMfRHFDZUXcJH1tIq0xhY","type":"Entry","createdAt":"2021-10-21T20:59:36.918Z","updatedAt":"2023-03-06T20:24:26.811Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Ray Training","link":"https://github.com/ray-project/ray-educational-materials"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"cVWy7hmqbAzJCW6d7cLUx","type":"Entry","createdAt":"2022-10-07T13:28:33.537Z","updatedAt":"2022-10-07T13:28:33.537Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Anyscale Docs","link":"https://docs.anyscale.com"}}]}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6Qjs7zNfhrinHTNnzMpVIO","type":"Entry","createdAt":"2022-10-14T10:44:42.403Z","updatedAt":"2022-10-14T10:44:42.403Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Company","subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ZFDrG8YGgU0oyNgBOId7X","type":"Entry","createdAt":"2022-10-07T13:32:38.746Z","updatedAt":"2022-10-07T13:32:38.746Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"subLinks":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2RD65wmpn4gmL5Bvxu5VOD","type":"Entry","createdAt":"2020-09-08T17:53:32.413Z","updatedAt":"2021-06-20T15:04:11.482Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"About us","link":"/about"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71EHE1t3QrjnQb8hD9O8Ww","type":"Entry","createdAt":"2022-10-07T13:32:07.593Z","updatedAt":"2022-10-07T13:32:07.593Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"News","link":"/press"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3TNUPaz7ViQ6hbiZciBOQV","type":"Entry","createdAt":"2020-09-28T23:19:39.838Z","updatedAt":"2022-05-12T19:18:18.399Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Careers","link":"/careers"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"ubc5oYY5moS7HexIp7AjO","type":"Entry","createdAt":"2022-07-22T20:40:53.600Z","updatedAt":"2022-07-22T20:40:53.600Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"navItem"}},"locale":"en-US"},"fields":{"text":"Community","link":"/community"}}]}}]}}],"bannerText":"Are you passionate about Ray, scalable AI and distributed computing? Join us at Ray Summit!","bannerLink":"https://raysummit.anyscale.com/?utm_source=anyscale\u0026utm_medium=website\u0026utm_campaign=ray_summit_2023\u0026utm_content=homepage_ticker","bannerCta":" Find out more! "}},"title":"Blog","slug":"blog","sections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6nWVrKik3UzKQc0m6QjMQ7","type":"Entry","createdAt":"2020-09-01T18:35:40.585Z","updatedAt":"2023-06-20T17:29:52.368Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":59,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"featured-posts","header":"Blog","subheader":"Featured Posts and News","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7xkHJWpQ3mRxrgAQw3oYFk","type":"Entry","createdAt":"2023-06-15T13:47:45.558Z","updatedAt":"2023-06-16T13:24:48.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Introducing the Anyscale Databricks Connector","seoTitle":"Integrating Anyscale Ray with Databricks","slug":"introducing-the-anyscale-databricks-connector","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3e6JquwKuIz6QvCWO0BAz","type":"Entry","createdAt":"2023-06-15T13:16:27.453Z","updatedAt":"2023-06-15T13:16:27.453Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Greene","slug":"eric-greene","link":"https://www.linkedin.com/in/greene/"}}],"publishedDate":"2023-06-15","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Introducing the Anyscale Databricks connector. Simple Data Transfer and Efficient AI Workflows with Anyscale and Databricks.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The Anyscale Databricks Connector is a new Ray Datasets capability that facilitates easy data transfer between Databricks clusters and Anyscale hosted Ray clusters. The connector makes it easy for Data Scientists using Anyscale to leverage their Databricks data lake during machine learning and discovery.Â  It also enables a simpler way for machine learning engineers to create end-to-end workloads by allowing the entire ML pipeline to be executed within a single Python script. By taking advantage of the highly-scalable nature of Ray and Ray Datasets, machine learning workloads such as training, tuning and batch serving jobs can be executed more quickly and at lower cost, all while taking advantage of the latest advancements in AI through other Ray integrations with Hugging Face, XGBoost, LightGBM and many AI frameworks and libraries.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Key Benefits","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simple access to data","nodeType":"text"},{"data":{},"marks":[],"value":" - Access Databricks data through SQL queries within Anyscale Workspaces Visual Code or JupyterHub environments, empowering data science discovery and development.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Improved data security and governance","nodeType":"text"},{"data":{},"marks":[],"value":" - Ensure data security and governance by directly copying data from Databricks data lakes to Ray clusters, eliminating the need for intermediate steps and maintaining separate controls over sensitive data. All data is encrypted in transit and at rest.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Highly-scalable data exchange","nodeType":"text"},{"data":{},"marks":[],"value":" - Take advantage of the parallel read and write capabilities of Ray Datasets, enabling the exchange of terabytes of data in minutes. This capability significantly speeds up data-intensive operations and enhances scalability while reducing job run times and overall costs.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simplified workload development","nodeType":"text"},{"data":{},"marks":[],"value":" - Simplify machine learning workflows by consolidating all the necessary logic into a single script. This script can query features, train and tune models, and score and materialize results back into the data lake, streamlining the entire process.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Unlock the latest AI Innovations","nodeType":"text"},{"data":{},"marks":[],"value":" - Leverage the power of Databricks for querying and joining data, while benefiting from the scalability and simplicity of Ray AIR for machine learning and AI development. Ray AIR provides integration to the latest AI innovations such as pretrained Hugging Face language models.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Simple, Secure and Scalable Data Access","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing the Anyscale Databricks connector, large datasets can be queried from a Databricks SQL warehouse, and quickly transferred into a Ray Dataset distributed across the Ray cluster. The Databricks connector reads query results in chunks of data, in parallel across the Ray cluster. The size of the dataset and the speed at which it can be transferred scales based on the size of your Ray cluster and the number of simultaneous requests the underlying Databricks SQL warehouse supports.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6taTlZK9ffeivO5RCUJiQ4","type":"Asset","createdAt":"2023-06-15T13:27:43.116Z","updatedAt":"2023-06-15T13:27:43.116Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"1 - Anyscale Databricks Architecture","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6taTlZK9ffeivO5RCUJiQ4/c5fc0bfef768f0fb6e8785bf71ded1e2/1_-_Anyscale_Databricks_Arch.jpg","details":{"size":95989,"image":{"width":761,"height":478}},"fileName":"1 - Anyscale Databricks Arch.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Anyscale Databricks connectors, parallel reads and writes and overall speed will scale with the size of the Ray cluster. The benchmarks for reading and writing demonstrate how the cluster scales out to support larger data sets.  ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2KimYEe845rk6fb2oYaCmz","type":"Asset","createdAt":"2023-06-15T13:35:36.597Z","updatedAt":"2023-06-15T13:35:36.597Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"2 - Anyscale Databricks Table Example","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2KimYEe845rk6fb2oYaCmz/8302017d6535b5d676d3dfb50dc12cbd/2_-_Anyscale_Databricks_Table_Example.jpg","details":{"size":218942,"image":{"width":2178,"height":1002}},"fileName":"2 - Anyscale Databricks Table Example.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Simplified ML Development","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nUsing the Databricks connector from within an Anyscale Workspaces, Data Scientists and Machine Learning Engineers have a unified experience while developing workloads. The data query, feature engineering, training, tuning and inference can all be executed within a single Python script that scales across a Ray cluster. Ray integrates with the latest AI and Machine learning libraries, enabling the most advanced ML workloads that work with 3rd party libs  like Hugging Face, XGBoost, LightGBM, and most PyTorch and TensorFlow model architectures.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7c7IX8TpJKbaXdnLIwoD88","type":"Asset","createdAt":"2023-06-15T13:39:22.298Z","updatedAt":"2023-06-15T13:39:22.298Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"3 - Anyscale Databricks UI","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7c7IX8TpJKbaXdnLIwoD88/f17fb8c8884ce197c9ad35a8ce3f6b4e/3_-_Anyscale_Databricks_Product_UI.jpg","details":{"size":242920,"image":{"width":2226,"height":1230}},"fileName":"3 - Anyscale Databricks Product UI.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2muX02EO70X2MXN8wBRyUJ","type":"Asset","createdAt":"2023-06-15T13:41:08.580Z","updatedAt":"2023-06-15T13:41:08.580Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"4 - Anyscale Databricks Product UI 2","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2muX02EO70X2MXN8wBRyUJ/cc5301c192c66e632cb3141a9e5bde55/4_-_Anyscale_Databricks_Product_UI_2.jpg","details":{"size":383181,"image":{"width":1860,"height":1282}},"fileName":"4 - Anyscale Databricks Product UI 2.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlock new use cases with AI and ML ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nAnyscale and Ray integrate with most open source AI and ML libraries, enabling the latest innovation in AI to be applied to Databricks data. Ray makes working with Hugging face, XGBoost, LightGBM, TensorFlow and PyTorch and SciKit Learn a unified experience, with the added benefits of scaling with distributed training, tuning and serving.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical ML workload to train a LightGBM can be implemented in 20 lines of code.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6rUeSQ88fvbTd68jkhrii0","type":"Asset","createdAt":"2023-06-15T13:42:58.984Z","updatedAt":"2023-06-15T13:42:58.984Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"5 - Anyscale Databricks deployment","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/6rUeSQ88fvbTd68jkhrii0/43de8dc64eaf8792ad73ac6faf8b4551/5_-_Anyscale_Databricks_deployment.jpg","details":{"size":272363,"image":{"width":1470,"height":1402}},"fileName":"5 - Anyscale Databricks deployment.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Want to see for yourself? ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/signup"},"content":[{"data":{},"marks":[],"value":"Request a Trial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" today!\n","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"71IpYQvsxplh7okIL7RJVx","type":"Asset","createdAt":"2023-06-15T13:50:54.646Z","updatedAt":"2023-06-16T13:19:09.178Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"locale":"en-US"},"fields":{"title":"0 -Anyscale Databricks headline image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/71IpYQvsxplh7okIL7RJVx/b193e65fbc2d6500e3ccb2162d323080/0_-_Updated_Databricks_Anyscale_Image.png","details":{"size":90723,"image":{"width":1500,"height":1000}},"fileName":"0 - Updated Databricks Anyscale Image.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2GdxAQ2xXKoNlzKc7DXvLS","type":"Entry","createdAt":"2023-05-31T19:04:09.560Z","updatedAt":"2023-06-01T03:29:01.792Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Announcing Aviary: Open Source Multi-LLM Serving","slug":"announcing-aviary-open-source-multi-llm-serving-solution","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-05-31","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Exec Summary","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": Open source LLMs are getting better every day, so today weâre open sourcing Aviary to make testing, evaluating and deploying Open Source LLMs easier â we found it harder than we thought it should be so we used Ray Serve to fix it.Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Try it out with Aviary Explorer ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://aviary.anyscale.com/"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Github Repo ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/aviary"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Video demo and explanation ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.youtube.com/watch?v=WmqPfQOXJ-4"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Slack support ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Forum support ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/c/llms-generative-ai/27"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Want a managed version of Aviary? Sign up ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://bit.ly/run-aviary"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". \n\nWeâre excited to announce the release of Aviary: a new open source project that simplifies and enables easy self-hosted serving of multiple LLM models efficiently.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Weâre big fans of open source LLMs here at Anyscale. The rate of improvement of open source LLMs has been ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.semianalysis.com/p/google-we-have-no-moat-and-neither"},"content":[{"nodeType":"text","value":"nothing short of phenomenal","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". This has given the AI community many options beyond the big âclosedâ players like OpenAI, Anthropic, Cohere and more.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Why are companies exploring self hosted open source LLMs? There are a few reasons: ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Cost: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"LLMs are very expensive to operate. A single query can cost tens of cents. Using open source can be considerably cheaper.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Latency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"By colocating LLMs and business logic, sometimes even on the same machine, latency â one of the biggest issues with deploying LLMs â can be kept low.Â Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Transparency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Organizations want to understand exactly what is happening in their models. Open source allows them to understand whatâs happening inside them.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Deployment flexibility: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Open Source models can be deployed on premise, in the cloud using the userâs cloud resources, or as part of a SaaS offering.Â Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Data Control: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"Data governance issues are much easier to guarantee if the data does not leave your control.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Customization:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" Customizing/fine-tuning on proprietary data to enable domain specific answers at high quality.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We were writing some tools for ourselves to really understand the answers to the questions above, as well as the advantages of closed models, mainly focused around quality. A funny thing happened on the way, however: we discovered that while the open source models have improved, the infrastructure for serving LLMs has not kept up. We started building our own library for loading the models, autoscaling them efficiently, etc on top of Ray Serve, dealing with variations between the models on things like stop tokens etc.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It was then that we realized, actually the rest of the LLM community might benefit from this library as well. So, today we are releasing Aviary as open source.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary helps leverage the advantages of open source models by building on the solid foundation of ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io"},"content":[{"nodeType":"text","value":"Ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", the popular framework for scalable AI. In particular, it takes advantage of ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"nodeType":"text","value":"Ray Serve","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", the highly flexible serving framework that is part of Ray.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary does this by:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Providing an extensive suite of pre-configured open source LLMs, with reasonable defaults that work out of the box.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Integrating acceleration approaches like DeepSpeed with the packaged LLMs.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Simplifying the deployment of multiple LLMs within a single unified framework.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Simplifying the addition of new LLMs to within minutes in most cases","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Offering unique autoscaling support, including scale-to-zero (a first in open source).Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Weâve also included a demo Gradio frontend that shows off whatâs possible with these capabilities, as well as some command line tools to address the evaluation questions that originally motivated this project.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"You can see these features in the video below:Â ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"jx26kCBQB1GRKEulIDxoi","type":"Entry","createdAt":"2023-05-31T02:10:20.974Z","updatedAt":"2023-05-31T02:10:20.974Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Aviary Announcement Video","videoUrl":"https://www.youtube.com/watch?v=WmqPfQOXJ-4"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"While existing solutions have some of these features individually and we are grateful to build on their foundations (such as Hugging Faceâs text-generation-inference), none of the existing solutions brings these capabilities together in a way that is convenient for users. We are also planning to continue to expand the feature set of Aviary, adding support for features like streaming, continuous batching and others.Â Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Aviary is also open to community contributions â especially for adding new LLMs. Weâll then redeploy these in production so the rest of the world can use those LLMs too. We will also be supporting Aviary through Slack, Discourse, GitHub issues and an LLM-based RayBot.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"At the same time we understand that not everyone wants to take on the additional challenges of maintaining their own aviary of models. For that reason, we will also be offering a hosted version of Aviary that builds on top of our Anyscale managed Ray platform and offers additional features around deployment including:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Using spot instances with on-demand fallback for large potential savings","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Zero downtime upgrades with no dropped requests","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Faster deployment and autoscaling","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Improved observability tools.Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We are also announcing that Aviary will be available at no additional charge for existing Anyscale customers via our workspaces solution, and weâre actively onboarding new Aviary customers now. If youâd like to deploy Aviary, please reach out to us ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://bit.ly/run-aviary"},"content":[{"nodeType":"text","value":"here","marks":[],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1c2aG5pfLWU6YenPBZIHvI","type":"Asset","createdAt":"2023-05-31T19:00:13.845Z","updatedAt":"2023-05-31T19:00:13.845Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Aviary Light Mode","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1c2aG5pfLWU6YenPBZIHvI/742f647b979b4eb4b682e37fc35c71a7/aviary-light-mode.png","details":{"size":712180,"image":{"width":3030,"height":1750}},"fileName":"aviary-light-mode.png","contentType":"image/png"}}},"mainImageFit":"contain","hideIntro":true,"showMainImage":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UsGSYssf1ebf8N5mRbNxT","type":"Entry","createdAt":"2023-05-17T17:26:50.771Z","updatedAt":"2023-05-24T20:17:43.464Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":9,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Numbers every LLM Developer should know","slug":"num-every-llm-developer-should-know","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-05-17","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"When I was at Google, there was a document put together by Jeff Dean, the legendary engineer, called ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://brenocon.com/dean_perf.html"},"content":[{"nodeType":"text","value":"Numbers every Engineer should know","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Itâs really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Prompts","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"40-90%: Amount saved by appending âBe Conciseâ to your prompt","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Itâs important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money [1]. This can be broadened beyond simply appending âbe conciseâ to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"1.3: Average tokens per word","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"LLMs operate on tokens. Tokens are words or sub-parts of words, so âeatingâ might be broken into two tokens âeatâ and âingâ.Â A 750 word document will be about 1000 tokens.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Knowing this ratio is important because most billing is done in tokens, and the LLMâs context window size is also defined in tokens.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Prices","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Prices [2] are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~50: Cost Ratio of GPT-4 to GPT-3.5 Turbo","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"What this means is that for many practical applications, itâs much better to use GPT-4 for things like generation and then use that data to fine tune a smaller model. It is roughly 50 times cheaper to use GPT-3.5-Turbo [3] than GPT-4 (the âroughlyâ is because GPT-4 charges differently for the prompt and the generated output)Â  â so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"5: Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embeddingÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. âWhat is the capital of Delaware?â when looked up in an neural information retrieval system costs about 5x less [4] than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x!Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"10: Cost Ratio of OpenAI embedding to Self-Hosted embeddingÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using HuggingFaceâs SentenceTransformers (which are pretty much as good as OpenAIâs embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees).Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"6: Cost Ratio of OpenAI base vs fine tuned model queries","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"1: Cost Ratio of Self-Hosted base vs fine-tuned model queriesÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If youâre self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Training and Fine Tuning\n","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://arxiv.org/abs/2302.13971"},"content":[{"nodeType":"text","value":"LLaMa paper","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. Thatâs not something most companies can do (shameless plug time: of course, we at Anyscale can â thatâs our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray"},"content":[{"nodeType":"text","value":"bread and butter","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":"! Contact us if youâd like to learn more). The point is that training your own LLM is possible, but itâs not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"\u003c 0.001: Cost ratio of fine tuning vs training from scratch","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using"},"content":[{"nodeType":"text","value":"6B parameter model for about $7","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Even at OpenAIâs rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), youâre looking at $40. However, fine tuning is one thing and training from scratch is another [5].\n","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"GPU Memory","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If youâre self-hosting a model, itâs really important to understand GPU memory because LLMs push your GPUâs memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"It may seem strange, but itâs important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"2x number of parameters: Typical GPU memory requirements of an LLM for serving","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. Thereâs usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy there is too much loss of resolution. Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but thatâs atypical.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~1GB: Typical GPU memory requirements of an embedding model","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/"},"content":[{"nodeType":"text","value":"sentence transformers","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". OpenAI also has its own embeddings that they provide commercially.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"You typically donât have to worry about how much memory embeddings take on the GPU, theyâre fairly small. Weâve even had the embedding and the LLM on the same GPU.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"\u003e10x: Throughput improvement from batching LLM requestsÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second.Â  The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point.Â ","marks":[],"data":{}}]},{"nodeType":"heading-4","data":{},"content":[{"nodeType":"text","value":"~1 MB: GPU Memory required for 1 token of output with a 13B parameter model","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say â I have 24GB to spare, whatâs 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but itâs still a real issue.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"embedded-entry-inline","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1ACoDuIrrm71E0BWViBO4Y","type":"Entry","createdAt":"2023-05-17T17:26:34.593Z","updatedAt":"2023-05-17T17:26:34.593Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"inlineImage"}},"locale":"en-US"},"fields":{"name":"numbers-cheatsheet","image":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1AkaPcJlWoSpqixtqeKcD1","type":"Asset","createdAt":"2023-05-17T16:48:22.989Z","updatedAt":"2023-05-17T20:48:45.978Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"numbers-cheatsheet","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1AkaPcJlWoSpqixtqeKcD1/9505980d855c36120b4818980745fd00/Screenshot_2023-05-17_at_1.46.09_PM.png","details":{"size":550573,"image":{"width":2194,"height":1734}},"fileName":"Screenshot 2023-05-17 at 1.46.09 PM.png","contentType":"image/png"}}},"isRetina2x":false}}},"content":[]},{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Next Steps","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Review the up-to-date metrics referenced in this blog in the following ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/llm-numbers"},"content":[{"nodeType":"text","value":"Github repo","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". \n\nSee our earlier ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"nodeType":"text","value":"blog series on solving Generative AI infrastructure","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" ","marks":[{"type":"underline"}],"data":{}},{"nodeType":"text","value":"and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"nodeType":"text","value":"using LangChain with Ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in learning more about Ray, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io/"},"content":[{"nodeType":"text","value":"Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://docs.ray.io/"},"content":[{"nodeType":"text","value":"Docs.Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nTo connect with the Ray community join #LLM on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"Ray Slack","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" or our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"Discuss forum","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://www.anyscale.com/platform"},"content":[{"nodeType":"text","value":"Anyscale.com/Platform ","marks":[],"data":{}}]},{"nodeType":"text","value":"and click the 'Try it now' button","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Summit 2023:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"nodeType":"text","value":"Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"nodeType":"text","value":"practical training focused on LLMs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"hr","data":{},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[1] Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2022-05-14.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[2] Retrieved from ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://openai.com/pricing"},"content":[{"nodeType":"text","value":"http://openai.com/pricing","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on 2022-05-14.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[3] ","marks":[],"data":{}},{"nodeType":"text","value":"GPT-4","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). ","marks":[],"data":{}},{"nodeType":"text","value":"GPT-3.5 Turbo","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": 0.2c/1k tokens.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[4] This assumes the vector lookup is âfree.â Itâs not, but it uses CPUs (much cheaper) and is fairly fast.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"[5] 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3ZnD84ZZAfH60U5ncBebaO","type":"Asset","createdAt":"2023-05-17T15:56:36.431Z","updatedAt":"2023-05-17T18:04:30.622Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"numbers-cover-image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3ZnD84ZZAfH60U5ncBebaO/227f68207ec7731e789f342e7ec320e8/Screenshot_2023-05-17_at_10.12.01_AM.png","details":{"size":445922,"image":{"width":2348,"height":1616}},"fileName":"Screenshot 2023-05-17 at 10.12.01 AM.png","contentType":"image/png"}}},"mainImageFit":"contain","hideIntro":true,"showMainImage":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3vUo7K4aHmhARGamZX7Ujj","type":"Entry","createdAt":"2023-05-11T15:17:24.618Z","updatedAt":"2023-06-16T21:13:49.520Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Streaming distributed execution across CPUs and GPUs","seoTitle":"Streaming distributed execution across CPUs and GPUs","slug":"streaming-distributed-execution-across-cpus-and-gpus","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2aJRcdEj9iAktAnqn0wucE","type":"Entry","createdAt":"2021-02-16T07:30:38.421Z","updatedAt":"2021-02-16T07:30:38.421Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Stephanie Wang","slug":"stephanie-wang","link":"https://github.com/stephanie-wang"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2edothjNBpyQEu2aX55KRl","type":"Entry","createdAt":"2023-05-10T15:30:16.699Z","updatedAt":"2023-05-10T15:30:16.699Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Cheng Su","slug":"cheng-su"}}],"publishedDate":"2023-05-11","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"In a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/offline-batch-inference-comparing-ray-apache-spark-and-sagemaker"},"content":[{"data":{},"marks":[{"type":"italic"},{"type":"underline"}],"value":"previous blog,","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":" we showed how Ray Data can provide speedups over SageMaker and frameworks like Apache Spark for large-scale batch inference workloads. This blog post delves further into how Ray Data streaming works and how to use it for your own ML pipelines.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Some of the most demanding machine learning (ML) use cases we have encountered involve pipelines that span both CPU and GPU devices in distributed environments. These situations arise in various workloads, including:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Batch inference, which involves a CPU-intensive preprocessing stage (e.g., video decoding or image resizing) before utilizing a GPU-intensive model to make predictions.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed training, where similar CPU-heavy transformations are required to prepare or augment the dataset prior to GPU training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Interestingly, in many of these workloads, the preprocessing steps often prove to be the bottleneck. This can happen when preprocessing requires ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"parallelism across multiple nodes","nodeType":"text"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"significant memory to buffer results","nodeType":"text"},{"data":{},"marks":[],"value":" between the CPU and GPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For instance, consider the decoding of compressed images in memory. A typical JPEG decompression ratio is 10:1, which implies that the output memory size may be ten times that of the input, resulting in substantial memory pressure in addition to CPU load. The challenges become more complex with other data modalities, such as video; for example, H264 can decompress at a 2000:1 ratio, producing 200GB of frame outputs for a 100MB file input. This means that practitioners trying to offload CPU-heavy preprocessing onto multiple CPUs or machines have to deal with intermediate results that are far larger than their already sizable source datasets:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7ntuAY4zsUXtrtIxSofnyg","type":"Asset","createdAt":"2023-05-10T15:33:44.406Z","updatedAt":"2023-05-10T18:55:03.562Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"streaming_figure_1","description":"Figure 1. Decoding of compressed images with large images spilled to disk","file":{"url":"//images.ctfassets.net/xjan103pcp94/7ntuAY4zsUXtrtIxSofnyg/1fdb9631162d6b66777d205d1d3a7e1c/Screenshot_from_2023-05-10_11-00-43.png","details":{"size":64895,"image":{"width":1493,"height":436}},"fileName":"Screenshot from 2023-05-10 11-00-43.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Executed naively, these compute and memory-intensive workloads fail to fully utilize expensive GPU hardware. For example, we could buffer 200GB of frame outputs on disk, but that could add minutes of overheads. Even if this is overlapped with GPU computation, we can end up with long pauses on the GPU. This motivates fully streamed execution in the cluster setting, which avoids such delays by streaming intermediate data through cluster memory:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"40VpN8OKxEM7wHohV802gE","type":"Asset","createdAt":"2023-05-10T15:36:35.326Z","updatedAt":"2023-05-10T18:56:06.767Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_2","description":"Figure 2. Decoding intermediate data streamed through memory","file":{"url":"//images.ctfassets.net/xjan103pcp94/40VpN8OKxEM7wHohV802gE/c07086bfe677dc5e3109496f5a4ec083/Screenshot_from_2023-05-10_11-01-00.png","details":{"size":35974,"image":{"width":1199,"height":268}},"fileName":"Screenshot from 2023-05-10 11-01-00.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we guide you through our development of a versatile streaming backend for Ray Data, and show examples of how to implement the demanding use cases mentioned earlier. We discuss the performance benefits of implementing pipelined execution across CPUs and GPUs compared to bulk processing, and discuss its broader applicability to various workloads, including distributed training.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Pipelined execution in a cluster","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To understand the necessity and advantages of pipelined (or streaming) execution across CPU and GPU devices, let's first examine how a ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Bulk_synchronous_parallel"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"bulk synchronous parallel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (BSP) framework might handle batch inference. Because of its simplicity and generality, BSP is a common way frameworks (e.g., MapReduce, Apache Spark) parallelize distributed computations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Note that pipelining within a single machine is already commonly used in data loading libraries such as ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Torch DataLoader","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://petastorm.readthedocs.io/en/latest/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Petastorm.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" Here, we discuss pipelining computations across an entire cluster of machines.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"A typical batch inference job consists of the following operations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6mkS4HlXNovPW6Dl387jYe","type":"Asset","createdAt":"2023-05-10T15:40:10.724Z","updatedAt":"2023-05-10T19:28:57.091Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"locale":"en-US"},"fields":{"title":"streaming_figure_3","description":"Figure 3. A typical batch inference job sequential operations","file":{"url":"//images.ctfassets.net/xjan103pcp94/6mkS4HlXNovPW6Dl387jYe/1a707558e51ac00b776ae6f5d808b74e/Screenshot_from_2023-05-10_12-23-58.png","details":{"size":33844,"image":{"width":1526,"height":244}},"fileName":"Screenshot from 2023-05-10 12-23-58.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"To implement such an inference job, you'd write the code using Ray Data (or another similar framework):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1kYVOacE6epR4czS5YoOHX","type":"Entry","createdAt":"2023-05-10T15:41:24.239Z","updatedAt":"2023-05-10T15:41:24.239Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_1","body":"import ray\n\n# Define model and preprocessor.\nmodel = ...\npreprocess_fn = ...\n\n# Define the inference function.\ndef model_fn(batch: Dict[str, np.ndarray]):\n    return {\"results\": model(batch)}\n\n# The following snippet implements the pipeline.\nray.data.read_parquet(...) \\           # 1. Load\n    .map_batches(preprocess_fn) \\      # 2. Preprocess\n    .map_batches(model_fn) \\           # 3. Inference\n    .write_parquet(...)                # 4. Save\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a BSP framework, each of these operations can be modeled as a 1:1 transformation, which can be fused together into a single stage. Each task within the stage will execute the mentioned operations locally in sequence:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4oQ40RlSK1AF7Y4ZnVGV8J","type":"Asset","createdAt":"2023-05-10T15:44:10.631Z","updatedAt":"2023-05-10T18:59:16.654Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_4","description":"Figure 4. BSP framework operations modeled as a 1:1 transformation","file":{"url":"//images.ctfassets.net/xjan103pcp94/4oQ40RlSK1AF7Y4ZnVGV8J/a0413e90a55ec69d7d0ba49554a7f2a0/Screenshot_from_2023-05-10_11-07-39.png","details":{"size":103326,"image":{"width":1511,"height":583}},"fileName":"Screenshot from 2023-05-10 11-07-39.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This execution strategy is ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"memory optimal","nodeType":"text"},{"data":{},"marks":[],"value":", as there is no intermediate data and execution is inherently pipelined (since there is only a single stage). However, challenges emerge in a heterogeneous setting, where certain pipeline steps may prefer to run on CPUs or GPUs independently. In Ray Data, such inference pipelines are expressed with transformations that launch actors scheduled onto GPUs ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"(num_gpus=1 and compute=ActorPoolStrategy)","nodeType":"text"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3nLf8c4wMq9KInsI4ca7HB","type":"Entry","createdAt":"2023-05-10T15:46:05.506Z","updatedAt":"2023-05-10T16:00:09.475Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_2","body":"import ray\nfrom ray.data import ActorPoolStrategy\n\nModel = ...\npreprocess_fn = ...\n\n# Define the model as a stateful class with cached setup.\nclass MyModelCallableCls:\n    def __init__(self):\n        self.model = Model(...)\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e\n        return {\"results\": self.model(batch)}\n\n# Modify the pipeline to use GPU actors.\nray.data.read_parquet(...) \\\n    .map_batches(preprocess_fn) \\\n    .map_batches(\n          MyModelCallableCls,\n          num_gpus=1,\n          compute=ActorPoolStrategy(size=N)) \\\n    .write_parquet(...)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the above code snippet, the load and preprocessing steps run on CPU (Stage 1), inference runs on GPUs (Stage 2), and then the result saving runs on CPU again (Stage 3). This configuration leads to spilling of data to remote storage when intermediate results (e.g., decoded video frames) exceed cluster memory sizes. Hence, we see that ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"BSP is not memory optimal for heterogeneous workloads","nodeType":"text"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"66FIu8nzl2kZRB1Ht5xZFL","type":"Asset","createdAt":"2023-05-10T15:49:26.155Z","updatedAt":"2023-05-10T19:00:24.815Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_5","description":"Figure 5. BSP is not memory optimal for heterogeneous workloads","file":{"url":"//images.ctfassets.net/xjan103pcp94/66FIu8nzl2kZRB1Ht5xZFL/295ab9176fab5d04ee47ac53c5e1b418/Screenshot_from_2023-05-10_11-02-18.png","details":{"size":162611,"image":{"width":1750,"height":730}},"fileName":"Screenshot from 2023-05-10 11-02-18.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"These unnecessary overheads can be avoided with end-to-end pipelining (i.e., streaming) across the cluster, which we present in the next section.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data streaming","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In Ray 2.4, we've changed the default execution strategy of Ray Data to streaming from bulk synchronous. These streaming Datasets are fully backwards compatible with the existing API, i.e., they can be transformed lazily with map operations, support shuffle operations, and also caching / materialization in memory:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5CSJWRWgN6pQwxcFMBhMSr","type":"Entry","createdAt":"2023-05-10T15:50:53.654Z","updatedAt":"2023-05-10T15:59:44.870Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_3","body":"# Create a dataset over parquet files\nds: ray.data.Dataset = ray.data.read_parquet(...)\n\n# Transform the dataset\nds = ds.map_batches(my_preprocess_fn)\nds = ds.map_batches(my_model_fn)\n\n# Iterate over dataset batches in streaming fashion\nfor batch in ds.iter_batches():\n   print(batch)\n\n# Materialize all contents of the dataset in memory\nds = ds.materialize()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In other words, the Ray Data API now leverages streaming execution for improved performance on large datasets, with the same simple transformation API as in previous Ray versions.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Video processing example","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"To more fully understand the capabilities of this API, let's walk through what this looks like in an example video processing pipeline. In the pipeline, we'll first decode video frames, annotate each frame with an ML model, apply a classification model to the annotated frames, and then save the results to JSON:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"s1kmF3kQraHomGuDsvOQt","type":"Asset","createdAt":"2023-05-10T15:55:25.682Z","updatedAt":"2023-05-10T19:30:26.377Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"streaming_figure_6","description":"Figure 6. A pipeline decoding video frames, annotating frames, applying classification, and saving results to JSON.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/s1kmF3kQraHomGuDsvOQt/9385b71522afe76eebc6342f54387ffa/Screenshot_from_2023-05-10_12-24-36.png","details":{"size":38452,"image":{"width":1478,"height":230}},"fileName":"Screenshot from 2023-05-10 12-24-36.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Let's start by declaring the components of our pipeline. For brevity, we'll just provide stub classes here, but you can find the ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/ericl/ac086f0a9bdf41b6101dca665045dfec"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"full skeleton here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (and the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference docs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") that can be adapted to your own workloads. Data between these steps is kept in dicts of numpy arrays:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3DoR7DQ01JPe4k0MjPdubR","type":"Entry","createdAt":"2023-05-10T15:57:40.210Z","updatedAt":"2023-05-10T15:59:24.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_5","body":"# Declare a function that decodes frames.\ndef decode_frames(batch: Dict[str, np.ndarray]) -\u003e Dict:\n    video_data = batch[\"bytes\"]\n    ...\n    return {\"frames\": decoded_frames}\n\n# Declare a model that annotates decoded frame data.\nclass FrameAnnotator:\n    def __init__(self):\n        self.model = ...\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e Dict:\n        frames = batch[\"frames\"]\n        return {\"annotated_frames\": self.model(frames)}\n\n# Declare a model that classifies annotated frame data.\nclass FrameClassifier:\n    def __init__(self):\n        self.model = ...\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -\u003e Dict:\n        frames = batch[\"annotated_frames\"]\n        return {\"results\": self.model(frames)}","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next, we'll use Ray Data to connect these steps together. This takes just a few lines of code, and we can customize the resource requirements per step to configure CPUs and GPUs:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1U2wSU4PEADDsQpVQqJJAa","type":"Entry","createdAt":"2023-05-10T15:59:06.340Z","updatedAt":"2023-05-10T15:59:06.340Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_6","body":"# Create a dataset from video binary data.\nds = ray.data.read_binary(...)\n\n# Apply the decode step. We can customize the resources per\n# task. Here each decode task requests 4 CPUs from Ray.\nds = ds.map_batches(decode_frames, num_cpus=4)\n\n# Apply the annotation step, using an actor pool of size 5.\n# Each actor runs on a CPU.\nds = ds.map_batches(\n    FrameAnnotator,\n    compute=ActorPoolStrategy(size=5))\n\n# Apply the classification step, using a pool of 2 GPU actors,\n# and a fixed data batch size of 64 for the actor calls.\nds = ds.map_batches(\n    FrameClassifier,\n    num_gpus=1,\n    batch_size=64,\n    compute=ActorPoolStrategy(size=2))\n\n# Trigger execution and write outputs to json.\nds.write_json(\"/tmp/output\")\n","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The configured streaming topology from above looks like this logically. Since each step has a unique resource requirement / task vs actor strategy, here every step becomes its own stage (Ray Data fuses steps with equivalent resource requirements):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2XaeSiV04QKLILBLAlD0t5","type":"Asset","createdAt":"2023-05-11T15:44:03.608Z","updatedAt":"2023-05-11T15:44:03.608Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"streaming_figure_7","description":"Figure 7: Streaming execution across CPUs and GPUs","file":{"url":"//images.ctfassets.net/xjan103pcp94/2XaeSiV04QKLILBLAlD0t5/d75fceb54323c7587312028b40a4b37c/Screenshot_from_2023-05-10_11-02-51.png","details":{"size":68124,"image":{"width":1445,"height":299}},"fileName":"Screenshot from 2023-05-10 11-02-51.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We run the above script (","nodeType":"text"},{"data":{"uri":"https://gist.github.com/ericl/ac086f0a9bdf41b6101dca665045dfec"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GitHub gist","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":") in an Anyscale workspace with 2 GPU nodes and 5 CPU nodes (you can use the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/getting-started.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray cluster launcher","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to also launch a cluster for this):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3hsRFIiW2RGkFM5Igr2jVT","type":"Entry","createdAt":"2023-05-10T16:04:25.585Z","updatedAt":"2023-05-10T16:06:19.918Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"streaming_code_snippet_7","body":"$ python workload.py\n\n2023-05-02 15:10:01,105 INFO streaming_executor.py:91 -- Executing DAG\nInputDataBuffer[Input] -\u003e TaskPoolMapOperator[MapBatches(decode_frames)] -\u003e \nActorPoolMapOperator[MapBatches(FrameAnnotator)] -\u003e\nActorPoolMapOperator[MapBatches(FrameClassifier)] -\u003e \nTaskPoolMapOperator[Write]\n2023-05-02 15:10:01,128 INFO actor_pool_map_operator.py:114 -- MapBatches(FrameAnnotator): Waiting for 5 pool actors to start...\n2023-05-02 15:10:02,691 INFO actor_pool_map_operator.py:114 -- MapBatches(FrameClassifier): Waiting for 2 pool actors to start...\n\nRunning: 25.0/112.0 CPU, 2.0/2.0 GPU, 33.19 GiB/32.27 GiB object_store_memory:  28%|ââââ        | 285/1000 [01:40\u003c03:12,  3.71it/s]","language":"Shell"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"While the workload is running, we can view the observability graphs in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-core/ray-dashboard.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Dashboard","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". We can see from the actor metrics that our 7 worker actors were active for the entire run (RUNNING_TASK), and that Ray Data kept them busy with MapBatches(FrameClassifier) as well as MapBatches(FrameAnnotator) tasks. Because of the streaming execution strategy, tasks for all stages run concurrently:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5MDgPW8470uVoy6MFBqowq","type":"Asset","createdAt":"2023-05-10T16:09:18.740Z","updatedAt":"2023-05-10T16:09:27.512Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"streaming_figure_8","description":"Figure 8. View active tasks and actors for this job in the Ray Dashboard","file":{"url":"//images.ctfassets.net/xjan103pcp94/5MDgPW8470uVoy6MFBqowq/39f6300387d895145313c0caee8149cf/image3.png","details":{"size":77415,"image":{"width":1216,"height":516}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can also inspect hardware utilization in the Ray dashboard. We'll focus on the network activity. We can see two nodes receiving ~500MiB/s (these are the two GPU nodes hosting the FrameClassifier actors), and a number of other nodes driving significant traffic from 100-200MiB/s, likely sending data to the GPU nodes or between the decode and FrameAnnotator steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6QEOgHncDDrSl3SKl9YzWF","type":"Asset","createdAt":"2023-05-10T16:12:30.030Z","updatedAt":"2023-05-10T19:54:24.559Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"streaming_figure_9","description":"Figure 9. Ray Dashboard showing network activity","file":{"url":"//images.ctfassets.net/xjan103pcp94/6QEOgHncDDrSl3SKl9YzWF/bae6e33fce2749f2ade8afd73681e16b/Screenshot_from_2023-05-10_12-42-27.png","details":{"size":107840,"image":{"width":1635,"height":707}},"fileName":"Screenshot from 2023-05-10 12-42-27.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The overall pipeline finishes in ~5 minutes, successfully processing ~350GB of video frame data total on a heterogeneous cluster of CPUs and GPUs. We see that Ray Data was able to pipeline the entire execution across the cluster to make the best use of memory.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Discussion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Optimizations","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"In order to seamlessly execute such streaming topologies, Ray Data provides a number of optimizations under the hood. This includes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Memory stability","nodeType":"text"},{"data":{},"marks":[],"value":": Ray Data relies on the underlying Ray scheduler to schedule tasks and actors, but still needs to manage back-pressure across the streaming topology to bound memory usage and avoid object store spilling. It does this by only scheduling new tasks if it would keep the streaming execution under ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/data-internals.html#streaming-execution"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"configured resources limits","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Intuitively, enforcing a cap on intermediate result memory usage is needed to avoid degrading to bulk execution.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data locality","nodeType":"text"},{"data":{},"marks":[],"value":": While Ray will already place tasks on nodes where their input arguments are local, Ray Data's streaming backend extends this to optimize the scheduling of actor tasks. For example, in the above example, a lot of network traffic is avoided between the `decode_frames` and `FrameAnnotator` steps by routing decoded frames to actors that are on the same node.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Fault tolerance","nodeType":"text"},{"data":{},"marks":[],"value":": Ray Data leverages Ray's built-in ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/ray-core/fault-tolerance.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"fault tolerance","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in order to handle object loss in large jobs. When objects are lost, they are recomputed based on their task lineage tracked by Ray. If actors were needed to produce these objects, Ray restarts these actors prior to re-submitting the task.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scalability","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"To give an idea of the upper scalability envelope of Ray Data streaming, we ran a number of synthetic benchmarks that stressed Ray's object manager and Ray Data's ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/execution/streaming_executor.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"streaming executor","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" processing a 20TiB array dataset. Under the hood, Ray Data is using ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.remote() / ray.wait() ","nodeType":"text"},{"data":{},"marks":[],"value":"to orchestrate the streaming topology, which means it is similarly scalable as Ray itself:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2vKUP0VZYl0yPNspQmfU05","type":"Asset","createdAt":"2023-05-10T16:17:37.701Z","updatedAt":"2023-05-10T16:17:37.701Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"streaming_figure_10","description":"Figure 10. Synthetic benchmarks stressing Ray's object manager and Ray Data's streaming executor processing a 20TiB array dataset","file":{"url":"//images.ctfassets.net/xjan103pcp94/2vKUP0VZYl0yPNspQmfU05/0566c92beb5ea743d9ecc3404a551705/image9.png","details":{"size":44857,"image":{"width":1636,"height":640}},"fileName":"image9.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"These benchmarks were run on a cluster of 500 machines, and test pipelines with between one to four stages similar to the video streaming example. We see that for the simplest pipeline on this cluster, Ray Data can process input data at a rate exceeding 1TiB/s. For more complex multi-stage pipelines, Ray Data can still sustain about 100-200GiB/s of end-to-end throughput on a large cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed Training","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"A significant portion of ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/train/train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" users today are also using Ray Data as a distributed data backend for ML training. Ray Data streaming allows these users to efficiently work with datasets that are larger than cluster memory for training, leveraging both CPU and GPU nodes to speed up their jobs. We are working on enhancing Ray Train's API to natively work with Ray Data streaming.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"What's next?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Today, early users are taking advantage of the Ray Data streaming backend to create efficient large-scale inference pipelines over unstructured data, including video and audio data. To learn more about how to build your own pipeline, check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"batch inference tutorial","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in our docs. We'll also be following up with a blog on more unstructured application examples. To learn more generally about streaming in Ray Data, check out the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/data.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"library docs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and let us know what ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/issues"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"improvements","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" you'd like to see to Ray Data.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Also, we will be presenting at the ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Register now","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â â early bird registration is open until","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Â June 30, 2023.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5qTVayH4YnKM7pGL8CiSwc","type":"Asset","createdAt":"2023-05-11T15:46:46.691Z","updatedAt":"2023-05-11T15:46:46.691Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_streaming_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5qTVayH4YnKM7pGL8CiSwc/99806986301b595308c6c89a8bad6249/images_and_diagrams__13_.png","details":{"size":83586,"image":{"width":960,"height":540}},"fileName":"images and diagrams (13).png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1n9cmtmuJ3wQPGW1TtXZ4t","type":"Entry","createdAt":"2023-05-08T15:57:32.674Z","updatedAt":"2023-05-09T00:38:09.625Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Building a Self Hosted Question Answering Service using LangChain + Ray in 20 minutes","slug":"building-a-self-hosted-question-answering-service-using-langchain-ray","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-05-08","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 3 of a blog series. In this blog, weâll show you how to build an LLM question and answering service. In future parts, we will optimize the code and measure performance: cost, latency and throughput.","body":{"nodeType":"document","data":{},"content":[{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4laoMTyctlD4QnuM9KzTI6","type":"Entry","createdAt":"2023-05-08T20:47:41.452Z","updatedAt":"2023-05-08T20:47:41.452Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"Building a Question Answering Chatbot","videoUrl":"https://youtu.be/Sy-Xp-sdlh0"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\nThis blog post builds on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"nodeType":"text","value":"Part 1","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" of our LangChain series, where we built a semantic search service in about 100 lines. Still, search is so â¦ 2022. What if we wanted to build a question answering service?Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"One option is we could just ask the LLM directly without any background at all. Unfortunately one of the biggest problems with LLMs is not just ignorance (âI donât knowâ) but hallucination (âI think I know but I actually donât ","marks":[],"data":{}},{"nodeType":"text","value":"at all","marks":[{"type":"italic"}],"data":{}},{"nodeType":"text","value":".â) This is perhaps the biggest issue facing LLMs at the current time. The way we overcome that is by combining factual information from our search engine and the capabilities of an LLM together.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Again, as we demonstrated before, there is a powerful combination in Ray + LangChain. LangChain provides a chain that is well suited to this (Retrieval QA). To give a fuller picture of how the pieces come together, weâre going to implement some parts that could usually just as easily be wrapped.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The Question Answering Service we will build will query the results from our Search Engine, and then use an LLM to summarize the results of the search.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Previously we had shown this diagram for how to serve semantic search results: ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1yspRkXhEt1xIRdiuztbyh","type":"Asset","createdAt":"2023-05-08T15:43:10.951Z","updatedAt":"2023-05-08T15:43:10.951Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"qna-save-search-queries","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/1yspRkXhEt1xIRdiuztbyh/f5a50d1046b085b95cd18742e51d5393/qna-save-search-queries.png","details":{"size":243832,"image":{"width":1600,"height":794}},"fileName":"qna-save-search-queries.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We are going to augment that now by creating a chain that consists of the above stage, then generating a prompt, and feeding that to an LLM to generate the answer.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Hence, the resulting system we are trying to build looks like this:Â ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mD1pRX9Do1SDRCUor1Yex","type":"Asset","createdAt":"2023-05-08T15:44:48.379Z","updatedAt":"2023-05-08T15:44:48.379Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"qna-answering-questions-with","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2mD1pRX9Do1SDRCUor1Yex/d4041f6d0d296c73edd77de26e076a17/qna-answering-questions-with.png","details":{"size":170666,"image":{"width":1556,"height":1024}},"fileName":"qna-answering-questions-with.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In other words, we take the vector search results, we take a prompt template, generate the prompt and then pass that to the LLM. Today we will use ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/Stability-AI/StableLM"},"content":[{"nodeType":"text","value":"StableLM","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" but you can easily swap in whatever model you want to.Â \n\nBefore we get started, itâs worth noting that you can find the source code to this project in our LangChain Ray examples repo at: ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/langchain-ray"},"content":[{"nodeType":"text","value":"https://github.com/ray-project/langchain-ray","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 1: The Prompt Template","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The prompt template is derived from the suggested one from StableLM, but modified for our use case.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In serve.py, we declare the following template:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2MvraLhg4SD47Hq41r5Jd7","type":"Entry","createdAt":"2023-05-08T15:46:38.818Z","updatedAt":"2023-05-08T15:46:38.818Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-1","body":"TEMPLATE = \"\"\"\n\u003c|SYSTEM|\u003e# StableLM Tuned (Alpha version)\n- You are a helpful, polite, fact-based agent for answering questions about Ray. \n- Your answers include enough detail for someone to follow through on your suggestions. \n\u003c|USER|\u003e\n\nPlease answer the following question using the context provided. If you don't know the answer, just say that you don't know. Base your answer on the context below. Say \"I don't know\" if the answer does not appear to be in the context below. \n\nQUESTION: {question} \nCONTEXT: \n{context}\n\nANSWER: \u003c|ASSISTANT|\u003e\n\"\"\"","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Letâs go through the template. The first part is the â\u003c|SYSTEM|\u003eâ tag. You can think of this as setting the âpersonalityâ of the LLM. LLMs are trained to treat the system tag differently. Not all LLMs support this, but OpenAI and StableLM do.Â  The second part is the â\u003c|USER|\u003eâ tag which is the question we want to ask. Note that the question and context are âtemplatizedâ â we will provide them from another source. Finally, since LLMs generate outputs by continuing on from the input, we say to the LLM âOK, hereâs where you take over.âÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The question will come from the userâs query. The context will use what we built last time: the search results from our semantic search.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 2: Setting up the embeddings and the LLM","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now letâs have a look at the __init__ method for our deployment. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3A4WRFfgvE3dNjwHACGYRB","type":"Entry","createdAt":"2023-05-08T15:47:56.437Z","updatedAt":"2023-05-08T15:47:56.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-2","body":"def __init__(self):\n       #... the same code from Part 1 .. \n       self.llm = StableLMPipeline.from_model_id(model_id=\"stabilityai/stablelm-tuned-alpha-7b\",\n                                                    task=\"text-generation\", model_kwargs=\n                                                    {\"device_map\":\"auto\", \"torch_dtype\": torch.float16})\n       WandbTracer.init({\"project\": \"wandb_prompts_2\"})\n       self.chain = load_qa_chain(\n           llm=self.llm,\n           chain_type=\"stuff\",\n           prompt=PROMPT)","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"As you can see, weâve added just 3 lines.Â \n\nThe first line creates a new StableLM LLM. In this case we had to write a little bit of glue code because we wanted to specify using ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://en.wikipedia.org/wiki/Half-precision_floating-point_format"},"content":[{"nodeType":"text","value":"float16","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" (halving the memory consumption of the model). We are working with the authors of Langchain to make this unnecessary. The key line from that file is this one: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"lthNpTGxWTEV31PX65mYb","type":"Entry","createdAt":"2023-05-08T15:50:17.006Z","updatedAt":"2023-05-08T15:50:17.006Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-3","body":" response = self.pipeline(prompt, temperature=0.1, max_new_tokens=256, do_sample=True)\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Here we specify the maximum number of tokens, and that we want it to pretty much answer the question the same way every time, and that we want to do one word at a time.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The second line sets up our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.wandb.ai/ref/python/integrations/wandbtracer"},"content":[{"nodeType":"text","value":"tracing with Weights and Biases","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". This is completely optional, but will allow us to visualize the input. You can find out more about Weights and Biases ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://wandb.ai"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The third thing we do is create a new chain that is specifically designed for answering questions. We specify the LLM to use, the prompt to use and finally the âchain typeâ â for now we set this to âstuffâ but there are other options like âmap_reduceâ, and also pass in the prompt.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 3: Respond to questions","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Weâve now got our Langchain ready, now all we have to do is write the code that uses the chain to answer questions!Â ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4Z72OU7xq4tPFzV8zLks6","type":"Entry","createdAt":"2023-05-08T15:51:41.475Z","updatedAt":"2023-05-08T15:51:41.475Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-4","body":"   def answer_question(self, query):\n       search_results = self.db.similarity_search(query)\n       print(f'Results from db are: {search_results}')\n       result = self.chain({\"input_documents\": search_results, \"question\":query})\n\n       print(f'Result is: {result}')\n       return result[\"output_text\"]","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Youâll notice the first line is identical to our previous version. Now we execute the chain with both our search results and the question being fed into the template.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Step 4: Go!","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now letâs get this started. If youâre using Weights and Biases, donât forget to log in using wandb login. To start, letâs do serve run serve:deployment.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now itâs started, letâs use a simple query script to test it.","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4XXeXIabNrMpB1kEND5KBK","type":"Entry","createdAt":"2023-05-08T15:53:45.904Z","updatedAt":"2023-05-08T15:53:45.904Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-5","body":"$ python query.py 'What are placement groups?'\nPlacement groups are a way for users to group resources together and schedule tasks or actors on those resources. They allow users to reserve resources across multiple nodes and can be used for gang-scheduling actors or for spreading resources apart. Placement groups are represented by a list of bundles, which are used to group resources together and schedule tasks or actors. After the placement group is created, tasks or actors can be scheduled according to the placement group and even on individual bundles.\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Yay! It works (well, mostly, that part at the end is a bit weird)! Letâs also check that it doesnât make stuff up: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12iqa2l4xNWuK18GLTu9Ly","type":"Entry","createdAt":"2023-05-08T15:54:22.876Z","updatedAt":"2023-05-08T15:54:22.876Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-6","body":"$ python query.py 'How do I make fried rice?'\nI don't know.\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Letâs just check the prompt that was sent to StableLM. This is where Weights and Biases comes in. Pulling up our interface we can find the prompt that was sent:Â ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"64oHWc7Rnz0fgCIBOR8gEF","type":"Entry","createdAt":"2023-05-08T15:55:06.135Z","updatedAt":"2023-05-08T15:55:06.135Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"qna-code-7","body":"\u003c|SYSTEM|\u003e# StableLM Tuned (Alpha version)\n- You are a helpful, polite, fact-based agent for answering questions about Ray. \n- Your answers include enough detail for someone to follow through on your suggestions. \n\u003c|USER|\u003e\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\n\nPlease answer the following question using the context provided. \nQUESTION: What are placement groups? \nCONTEXT: \nPlacement Groups#\nPlacement groups allow users to atomically reserve groups of resources across multiple nodes (i.e., gang scheduling).\nThey can be then used to schedule Ray tasks and actors packed as close as possible for locality (PACK), or spread apart\n(SPREAD). Placement groups are generally used for gang-scheduling actors, but also support tasks.\nHere are some real-world use cases:\n\nray list placement-groups provides the metadata and the scheduling state of the placement group.\nray list placement-groups --detail provides statistics and scheduling state in a greater detail.\nNote\nState API is only available when you install Ray is with pip install \"ray[default]\"\nInspect Placement Group Scheduling State#\nWith the above tools, you can see the state of the placement group. The definition of states are specified in the following files:\nHigh level state\nDetails\n\nPlacement groups are represented by a list of bundles. For example, {\"CPU\": 1} * 4 means youâd like to reserve 4 bundles of 1 CPU (i.e., it reserves 4 CPUs).\nBundles are then placed according to the placement strategies across nodes on the cluster.\nAfter the placement group is created, tasks or actors can be then scheduled according to the placement group and even on individual bundles.\nCreate a Placement Group (Reserve Resources)#\n\nSee the User Guide for Objects.\nPlacement Groups#\nPlacement groups allow users to atomically reserve groups of resources across multiple nodes (i.e., gang scheduling). They can be then used to schedule Ray tasks and actors packed as close as possible for locality (PACK), or spread apart (SPREAD). Placement groups are generally used for gang-scheduling actors, but also support tasks.\nSee the User Guide for Placement Groups.\nEnvironment Dependencies#\n\nANSWER: \u003c|ASSISTANT|\u003e\n","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We can see the search results that we made in are being included. StableLM is then using this to synthesize its answer.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Conclusion","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this blog post we showed how we could build on the simple search engine we built in the previous blog in this series and make a retrieval-based question answering service. It didnât need us to do much: we needed to bring up a new LLM (StableLM),Â  we needed to generate a prompt with the search results in it, and then feed that result to the LLM asking it to derive an answer from it. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Next Steps","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Review the code and data used in this blog in the following ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/langchain-ray/tree/main/open_source_LLM_retrieval_qa"},"content":[{"nodeType":"text","value":"Github repo","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". \n\nIf you are interested in learning more about Ray, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io/"},"content":[{"nodeType":"text","value":"Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://docs.ray.io/"},"content":[{"nodeType":"text","value":"Docs.Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\n","marks":[],"data":{}},{"nodeType":"text","value":"See our earlierÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"nodeType":"text","value":"blog series on solving Generative AI infrastructure","marks":[],"data":{}}]},{"nodeType":"text","value":"Â with Ray.","marks":[],"data":{}},{"nodeType":"text","value":"\n\nTo connect with the Ray community join #LLM on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"Ray Slack","marks":[],"data":{}}]},{"nodeType":"text","value":" or our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"Discuss forum","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://www.anyscale.com/platform"},"content":[{"nodeType":"text","value":"Anyscale.com/Platform ","marks":[],"data":{}}]},{"nodeType":"text","value":"and click the 'Try it now' button","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Summit 2023:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"nodeType":"text","value":"Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"nodeType":"text","value":"practical training focused on LLMs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2mD1pRX9Do1SDRCUor1Yex","type":"Asset","createdAt":"2023-05-08T15:44:48.379Z","updatedAt":"2023-05-08T15:44:48.379Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"qna-answering-questions-with","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2mD1pRX9Do1SDRCUor1Yex/d4041f6d0d296c73edd77de26e076a17/qna-answering-questions-with.png","details":{"size":170666,"image":{"width":1556,"height":1024}},"fileName":"qna-answering-questions-with.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2C7QtMnASHfVdag9xYamXh","type":"Entry","createdAt":"2023-05-04T15:03:36.052Z","updatedAt":"2023-06-06T14:20:09.992Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker","seoTitle":"Offline Batch Inference: Comparing Ray, Apache Spark, and SageMaker","slug":"offline-batch-inference-comparing-ray-apache-spark-and-sagemaker","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DwU0wh6AmJoGv8JOy6n0Q","type":"Entry","createdAt":"2020-11-11T23:34:15.165Z","updatedAt":"2021-02-05T22:22:08.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","slug":"amog-kamsetty","link":"https://www.linkedin.com/in/amogkamsetty/","bio":"Amog is a software engineer at Anyscale. He helps maintain and develop two machine learning libraries: Ray Tune for distributed hyperparameter tuning and RaySGD for distributed deep learning training.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J134jX2ybZRLPfcAIGLTI","type":"Asset","createdAt":"2020-11-13T00:05:05.533Z","updatedAt":"2020-11-13T00:05:05.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J134jX2ybZRLPfcAIGLTI/12589bb0c6792051050ebc6e1e83af25/Amog_Kamsetty_.jpg","details":{"size":57660,"image":{"width":480,"height":640}},"fileName":"Amog Kamsetty .jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3aG8Qqy4YblAvVgAj0basm","type":"Entry","createdAt":"2020-09-21T20:16:11.836Z","updatedAt":"2021-02-16T07:23:28.142Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Eric Liang","slug":"eric-liang","link":"http://github.com/ericl","bio":"Eric Liang is a PhD candidate at UC Berkeley studying the intersection of systems and machine learning. Before grad school, he spent several years industry working on distributed systems at Google and Databricks. He currently leads Ray core and RLlib development at Anyscale.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"eZiOVLMNnNdvIH51Jk7oU","type":"Asset","createdAt":"2020-09-21T20:16:09.188Z","updatedAt":"2022-08-09T07:03:38.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"Eric Liang headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/eZiOVLMNnNdvIH51Jk7oU/f2ce3d3e0250aa9693ab86cccd545f01/Eric_Liang_headshot.jpeg","details":{"size":199172,"image":{"width":512,"height":512}},"fileName":"Eric Liang headshot.jpeg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}}],"publishedDate":"2023-05-04","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Update 5/10: ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"Thanks to the Databricks Apache Spark developers for pointing out the prefetching flag ","nodeType":"text"},{"data":{},"marks":[{"type":"code"},{"type":"italic"}],"value":"spark.databricks.execution.pandasUDF.prefetch.maxBatches ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"available in Databricks Runtime that can be used with the Iterator API. It is also possible to implement this manually in Spark open source with background threads. With prefetching set to 4 batches, Spark reaches ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"},{"type":"italic"}],"value":"159.86 images/sec","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":". The code can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/blob/main/spark/code/torch-batch-inference-s3-10G-standard-iterator-databricks-prefetch.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"italic"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"italic"}],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"TL;DR","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As more companies use large scale machine learning (ML) models for training and evaluation, offline batch inference becomes an essential workload. A number of challenges come with it: managing compute infrastructure; optimizing use of all heterogeneous resources; and transferring data from storage to hardware accelerators. Addressing these challenges, Ray performs significantly better as it can coordinate clusters of diverse resources, allowing for better utilization of the specific resource requirements of the workload.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we conduct a comparison of three different solutions for offline batch inference: AWS ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker Batch Transform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Our experiments demonstrate that Ray Data achieves speeds up to ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"17x ","nodeType":"text"},{"data":{},"marks":[],"value":"faster than SageMaker Batch Transform and","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" 2x","nodeType":"text"},{"data":{},"marks":[],"value":" faster than Spark for offline image classification. Ray Data also scales effectively to terabyte-sized datasets. The code for these benchmarks is publicly available and can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introduction","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46bNG3QTa1atN02rsemfe8","type":"Asset","createdAt":"2023-05-03T22:20:44.214Z","updatedAt":"2023-05-03T22:20:44.214Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_1_batch_offline_inference","description":"Figure 1. General parallel batch processing architecture\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/46bNG3QTa1atN02rsemfe8/120bf3aac26ae94257b41ac674f42e94/image3.png","details":{"size":48540,"image":{"width":960,"height":540}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Offline batch inference is a critical workload for many AI products, especially with the growing usage of pre-trained foundation models. At its core, it seems like a simple problem: given a trained model and a dataset, get model predictions for each data point. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"However, there are many challenges to doing this at scale:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 1: ","nodeType":"text"},{"data":{},"marks":[],"value":"Managing compute infrastructure and cloud clusters, especially when needing to use ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"heterogeneous clusters","nodeType":"text"},{"data":{},"marks":[],"value":", consisting of different instance types to maximize throughput.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 2: ","nodeType":"text"},{"data":{},"marks":[],"value":"Parallelizing data processing and utilizing all resources (CPUs \u0026 GPUs) in the cluster at any given point in time.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 3: ","nodeType":"text"},{"data":{},"marks":[],"value":"Efficiently transferring data between cloud storage, CPU RAM for preprocessing, and GPU RAM for model inference, especially for unstructured data like images and text. This has ramifications for both performance and developer velocity.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Challenge 4:","nodeType":"text"},{"data":{},"marks":[],"value":" A user experience that makes it easy to iterate and develop while working at scale.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"What does the industry recommend for offline batch inference?","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"The industry hasnât converged on a standard solution. ","nodeType":"text"},{"data":{"uri":"https://twitter.com/benhamner/status/1605730034450169857"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"It seems like every ML practitioner has their own favorite approach.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5hRMwuEGUS4nJ7eV5riIK9","type":"Asset","createdAt":"2023-05-03T22:23:14.590Z","updatedAt":"2023-05-03T22:23:14.590Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"tweet_image","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5hRMwuEGUS4nJ7eV5riIK9/05b76abcbe4ea58ae3839fd319858594/image4.png","details":{"size":177304,"image":{"width":1798,"height":588}},"fileName":"image4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We see that there are three categories of solutions that attempt to address the above challenges. However, only category three addresses all the challenges.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Batch Services: ","nodeType":"text"},{"data":{},"marks":[],"value":"Cloud providers such as AWS, GCP, and Azure provide batch services to manage compute infrastructure for you. Some newer products like ","nodeType":"text"},{"data":{"uri":"https://modal.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Modal Labs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provide a user experience layer around cloud providers to abstract away even more complexity. Regardless of which service you choose, the process is the same: you provide your code, and the service runs your code on each node in a cluster. However, while infrastructure management is necessary (Challenge #1), it is not enough. These services have limitations, such as a lack of software libraries to address Challenges #2, #3, and #4 which makes them suitable only for experienced users who can write their own optimized batch inference code.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Online Inference solutions: ","nodeType":"text"},{"data":{},"marks":[],"value":"Â Solutions like ","nodeType":"text"},{"data":{"uri":"https://www.bentoml.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Bento ML","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provide APIs to make it easy to write performant inference code and can abstract away infrastructure complexities. But they are designed for online inference rather than offline batch inferenceâtwo different problems with different sets of requirements. These solutions often donât perform well in the offline case, leading inference service providers like ","nodeType":"text"},{"data":{"uri":"https://modelserving.com/blog/unifying-real-time-and-batch-inference-with-bentoml-and-spark"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Bento ML to integrating with Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for offline inference.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Distributed data systems:","nodeType":"text"},{"data":{},"marks":[],"value":" These solutions are designed to handle large amounts of data and are better suited to handle all three challenges listed above. They break large datasets into reasonable batches for processing and can map a given function (i.e., the model) over the dataset efficiently, effectively handling the challenges of offline batch inference at scale. Examples include ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Apache Spark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ultimately, we have observed that Ray Data is the best practical solution for offline batch inference for modern deep learning applications, both in terms of performance and user experience.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We ran the following comparisons and experiments to corroborate the above observations:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing AWS SageMaker Batch Transform, Apache Spark, and Ray Data performance and UX on an image classification use case","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling up Ray Data batch inference to 10 TB, showing 90%+ GPU utilization","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Image Classification: SageMaker Batch Transform vs. Apache Spark vs. Ray Data","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We ran an image classification task from the ","nodeType":"text"},{"data":{"uri":"https://arxiv.org/abs/1911.02549"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"MLPerf Inference Benchmark","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" suite in the offline setting. This benchmark uses images from the ","nodeType":"text"},{"data":{"uri":"https://image-net.org/challenges/LSVRC/2012/2012-downloads.php#Images"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"ImageNet 2012 Dataset","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and a pre-trained ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch ResNet50 model.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We timed a full end-to-end batch inference workload. With images stored in the parquet format in a public S3 bucket, the workload involved the following three steps:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Reading images from S3","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Simple CPU preprocessing (resizing, cropping, normalization)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Model inference on GPU","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"While all Ray experiments were run on Anyscale, Spark experiments were run on Databricks runtime v12.0, with Apache Spark 3.3.1. More details on the full configurations can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"in the repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In particular for Spark, we used two different setups:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Single-cluster","nodeType":"text"},{"data":{},"marks":[],"value":": Both the CPU preprocessing and GPU inference were done on the same cluster. We tried both versions of the PySpark Pandas UDF API: ","nodeType":"text"},{"data":{"uri":"https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.pandas_udf.html#pyspark.sql.functions.pandas_udf"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Series-based and Iterator-based","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Multi-cluster","nodeType":"text"},{"data":{},"marks":[],"value":": CPU preprocessing is done on one cluster, the preprocessed data is saved to a shared file system, and GPU inference is done on a different cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"All the source code for the experiments conducted above is available ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", including additional details on various other configurations that we attempted.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Â Results of throughput from experiments","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"When running with a 10 GB dataset on a single g4dn.12xlarge instance (48 CPUs, 4 GPUs), we observe the following throughput (images/sec) results for ResNet-50 batch inference:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Urc9cqOTvtI8d4w9DjiEm","type":"Asset","createdAt":"2023-05-04T00:09:19.405Z","updatedAt":"2023-05-04T00:09:19.405Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_5_table","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Urc9cqOTvtI8d4w9DjiEm/42718e584dcc4d0d16592749b0782b4f/Screen_Shot_2023-05-03_at_5.01.12_PM.png","details":{"size":91393,"image":{"width":1211,"height":541}},"fileName":"Screen Shot 2023-05-03 at 5.01.12 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The above throughput differences translate directly into cost savings as the batch inference job with Ray can run much fasterâso the faster it finishes the fewer resources are consumed and the cheaper it gets. Additionally, Ray is fully open source.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2dNPqjBTFiDZn2gcXkdir9","type":"Asset","createdAt":"2023-05-03T22:50:03.518Z","updatedAt":"2023-05-03T22:50:03.518Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 1","description":"Figure 1. Throughput difference among three different solutions for 10GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/2dNPqjBTFiDZn2gcXkdir9/f07a675dcb4f124e29aeb2f6618dbf15/image1.png","details":{"size":163062,"image":{"width":1334,"height":948}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"SageMaker Batch Transform for batch inference","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As we can see, SageMaker Batch Transform is not well suited for batch inference, both for throughput and cost.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While SageMaker handles","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" Challenge #1 ","nodeType":"text"},{"data":{},"marks":[],"value":"to some degree and abstracts away compute infrastructure management, it doesnât fully address the remaining challenges","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The low-throughput results are primarily because while SageMaker Batch Transform claims to be a ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"solution for batch inference on large datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", under the hood, it uses the same architecture as for online serving systems. It starts an HTTP server, and deploys your model as an endpoint. And then for each image, a separate request is sent to the server.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"SageMaker also does not provide support for batching across ","nodeType":"text"},{"data":{"uri":"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"multiple files","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and even if you batched your data beforehand, the max payload per request is 100 MB, nowhere near enough to fully saturate your GPUs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In terms of developer velocity, when running these benchmarks, we ran into a few additional challenges with SageMaker. In particular, the need to spin up a new cluster every time we ran a script and the difficult to parse stack traces slowed down developer iterations.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While online inference solutions have their place, the additional overhead they have, as well as the inability to fully utilize cluster resources and saturate GPUs by maximizing batch sizes and effectively pipelining data make them highly inefficient for offline batch inference.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Comparing Spark and Ray for batch inference","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Clearly, as we observed above, shoehorning online inference solutions for offline use cases is not the right approach. Letâs compare the two distributed data system approaches: Spark and Ray Data.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We see that Ray Data has","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":" 2x ","nodeType":"text"},{"data":{},"marks":[],"value":"faster throughput than Spark. Letâs break down how Ray Data and Spark fare for the challenges mentioned earlier:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #2: Hybrid CPU and GPU workload","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deep learning workloads involve ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"both CPU and GPU computation","nodeType":"text"},{"data":{},"marks":[],"value":". For optimal parallelization, all the CPUs and GPUs in the cluster should be fully utilized at any given time. However, this is a limitation of Spark scheduling as all the stages are fused together, regardless of their resource requirements. This means that in the single cluster setup, the total parallelism is limited by the number of GPUs, leading to underutilization of CPUs. This also means that GPUs are completely idle during the reading and preprocessing steps.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"When using a separate CPU and GPU cluster, data needs to be persisted and orchestrated between stages. Overall, for these CPU+GPU workloads, Spark does not fully address ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"challenge #2","nodeType":"text"},{"data":{},"marks":[],"value":" and cannot utilize all resources in the cluster concurrently. We attempted to use Sparkâs stage level scheduling feature to address this issue, but were ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/tree/main/spark#stage-level-scheduling"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"unsuccessful","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6m5c6zQiMVCithTLEIazb8","type":"Asset","createdAt":"2023-05-03T23:02:01.263Z","updatedAt":"2023-05-03T23:02:01.263Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 2","description":"Figure 2. How Ray data efficiently pipelines and optimizes hardware: CPUs/GPUs","file":{"url":"//images.ctfassets.net/xjan103pcp94/6m5c6zQiMVCithTLEIazb8/1be9788a7aa54707a42f38023597ae2f/5mb.gif","details":{"size":4991804,"image":{"width":3260,"height":1182}},"fileName":"5mb.gif","contentType":"image/gif"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data, on the other hand, can independently scale the CPU preprocessing and GPU inference steps. It also streams data ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"through these stages","nodeType":"text"},{"data":{},"marks":[],"value":", increasing CPUs and GPUs utilization and reducing out-of-memory errors (OOMs), and fine-grained resource allocation for ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html#scaling-and-resource-allocation"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"autoscaling and management","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #3: Large, multi-dimensional tensors","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deep learning workloads often involve dealing with complex data types like images, audio, or video, which are represented by multi-dimensional ","nodeType":"text"},{"data":{"uri":"https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"arrays","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Having first-class support for these data types is necessary for better performance and developer experience. Ray Data has Numpy batch format as a first-class citizen, and does not require Pandas, which is suited for deep learning and leads to higher performance and more memory efficiency, ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-2-2-improved-developer-experience-performance-and-stability"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"as shown in previous benchmarks","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"},{"type":"bold"}],"value":"Challenge #4: Ray is Python native","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Another fundamental difference is that Ray is Python native whereas Spark is not, which also impacts performance. We ran a microbenchmark that applies a ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"time.sleep(1)","nodeType":"text"},{"data":{},"marks":[],"value":" UDF. Surprisingly, running the UDF takes ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/tree/main/spark#microbenchmark"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"80 seconds","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"; we believe due to JVM\u003c\u003ePyarrow overhead! On Ray Data, the same microbenchmark takes a little ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks/blob/main/ray/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"under 5 seconds","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Not to mention, this makes Spark development more difficult. In particular when debugging, the recommended way is to include debug information in the ","nodeType":"text"},{"data":{"uri":"https://stackoverflow.com/a/57175768"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"output pandas dataframe rather than using print statements","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Even as we scale to a larger 300 GB dataset using 4 nodes and 16 GPUs, we see that these performance differences still hold:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5wDKGXZNHB2Hlt6JEMMKOt","type":"Asset","createdAt":"2023-05-03T23:04:40.956Z","updatedAt":"2023-05-03T23:04:40.956Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"figure_3_batch_inference","description":"Figure 3. Throughput difference between Spark and Ray solution for 300 GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/5wDKGXZNHB2Hlt6JEMMKOt/da27ee6ea7e5773abef42eb291dd1a97/image5.png","details":{"size":92846,"image":{"width":1102,"height":830}},"fileName":"image5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"While big data processing systems are the best solutions for offline inference, for deep learning workloads, Ray Data outperforms Spark.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scaling up Ray Data and maximizing performance","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we test the scalability of Ray Data and attempt to maximize performance by using a ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"heterogeneous cluster","nodeType":"text"},{"data":{},"marks":[],"value":" consisting of a mix of GPU nodes and CPU-only nodes. Deep learning workloads involving complex data types are often memory-bound, so by using additional CPU-only nodes, we add more total memory to our cluster to better maximize GPU utilization.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Unlike SageMaker or Spark, Ray works natively with heterogeneous clusters consisting of different instance types, fully addressing ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"challenge #1.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We run the same benchmark, except with 10 TB worth of images, using a single cluster of 10 g4dn.12xlarge instances and 10 m5.16xlarge instances.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Throughput: 11,580.958 img/sec","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"As we can see below, Ray Data achieves over 90%+ GPU utilization throughout the workload run:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"16XJHcynegHxIdae1hupAF","type":"Asset","createdAt":"2023-05-03T23:07:22.875Z","updatedAt":"2023-05-03T23:07:22.875Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"batch_inference_figure_4","description":"Figure 4. GPU utilization of all 40 GPUs in the cluster. We are roughly at 90%+ GPU utilization during the entire duration of the workload.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/16XJHcynegHxIdae1hupAF/8eab9567ac78a8d558bcd04218f7e1ca/image2.png","details":{"size":466601,"image":{"width":1999,"height":621}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nThe ability to scale to many GPUs and large data sizes without needing to rewrite code and not sacrificing on throughput is what sets Ray Data apart compared to other solutions for deep learning workloads.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog, we identified four technical challenges for doing batch inference at scale and investigated three commonly cited industry solutions for offline batch inferences. Using the cited approaches, we compared Ray, Spark, and Amazon SageMaker Batch Transform.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We concluded that Ray Data best meets all four challenges:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data abstracts away compute infrastructure management, and can maximize performance for memory-bound workloads as it natively supports heterogeneous clusters.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"streams","nodeType":"text"},{"data":{},"marks":[],"value":" data from cloud storage -\u003e CPU -\u003e GPU, ensuring that all resources in the cluster are utilized at any given point in time. This improves throughput and reduces overall cost.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data has native support for multi-dimensional tensors, which is vital for deep learning workloads.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Data is Python native and programmatic, making it easy to develop and debug.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Overall, Ray Data is the best option for deep learning offline batch inference. In our image classification benchmarks, as shown in the figures above, Ray Data significantly outperforms SageMaker Batch Transform (by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"17x)","nodeType":"text"},{"data":{},"marks":[],"value":" and Spark (by ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"2x ","nodeType":"text"},{"data":{},"marks":[],"value":"and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"3x","nodeType":"text"},{"data":{},"marks":[],"value":") while linearly scaling to TB level data sizes. All code for these benchmarks can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/amogkam/batch-inference-benchmarks"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Lastly, as a result of higher throughput and Ray as an open source solution, Ray lowers the overall costs on top of the base EC2 cost.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Whatâs Next","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stay tuned for a follow up post where we dive into Ray Data streaming's design and how to use it for your own ML pipelines. Meanwhile, you can get started with Ray Data for batch inference ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/batch_inference.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Alternatively, ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfC6rmBeb9pLPSKRf1e7VCy7BtkjSGu20gh0xjQhdo6rcUflQ/viewform"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"fill out this form","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â to connect with us if you are interested in working on a proof of concept (POC) for your offline batch inference workload with Ray.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have our ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit 2023 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"early-bird registration open. Secure your spot, and save some money.\n\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n\n\n\n\n","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2dNPqjBTFiDZn2gcXkdir9","type":"Asset","createdAt":"2023-05-03T22:50:03.518Z","updatedAt":"2023-05-03T22:50:03.518Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Figure 1","description":"Figure 1. Throughput difference among three different solutions for 10GB batch inference\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/2dNPqjBTFiDZn2gcXkdir9/f07a675dcb4f124e29aeb2f6618dbf15/image1.png","details":{"size":163062,"image":{"width":1334,"height":948}},"fileName":"image1.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"51oIQOHymWExFWIRYQRXge","type":"Entry","createdAt":"2023-05-02T17:47:40.421Z","updatedAt":"2023-05-12T07:23:26.789Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Turbocharge LangChain: guide to 20x faster embedding","slug":"turbocharge-langchain-now-guide-to-20x-faster-embedding","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DwU0wh6AmJoGv8JOy6n0Q","type":"Entry","createdAt":"2020-11-11T23:34:15.165Z","updatedAt":"2021-02-05T22:22:08.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","slug":"amog-kamsetty","link":"https://www.linkedin.com/in/amogkamsetty/","bio":"Amog is a software engineer at Anyscale. He helps maintain and develop two machine learning libraries: Ray Tune for distributed hyperparameter tuning and RaySGD for distributed deep learning training.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J134jX2ybZRLPfcAIGLTI","type":"Asset","createdAt":"2020-11-13T00:05:05.533Z","updatedAt":"2020-11-13T00:05:05.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J134jX2ybZRLPfcAIGLTI/12589bb0c6792051050ebc6e1e83af25/Amog_Kamsetty_.jpg","details":{"size":57660,"image":{"width":480,"height":640}},"fileName":"Amog Kamsetty .jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6G9xTnF76ZUKKvgcVOqhtm","type":"Entry","createdAt":"2022-08-23T02:58:28.957Z","updatedAt":"2023-05-02T04:18:43.884Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Philipp Moritz","slug":"philipp-moritz","link":"https://www.linkedin.com/in/philipp-moritz-61419682/","bio":"Co-founder and CTO at Anyscale"}}],"publishedDate":"2023-05-03","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 2([part 1 here](https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray)) of a blog series. In this blog, weâll show you how to turbocharge embeddings. In future parts, we will show you how to combine a vector database and an LLM to create a fact-based question answering service. Additionally, we will optimize the code and measure performance: cost, latency and throughput.","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7nxbgQNlTcYZECfJ0vAabS","type":"Entry","createdAt":"2023-05-02T17:16:29.953Z","updatedAt":"2023-05-02T17:16:29.953Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"LangChain + Ray Tutorial: How to Generate Embeddings For 33,000 Pages for $1","videoUrl":"https://youtu.be/hGnZajytlac"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Generating embeddings from documents is a critical step for LLM workflows. Many LLM apps are being built today through retrieval based similarity search:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Documents are embedded and stored in a vector database.Â Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Incoming queries are used to pull semantically relevant passages from the vector database, and these passages are used as context for LLMs to answer the query.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"In a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"previous blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we showed how to use ","nodeType":"text"},{"data":{"uri":"https://github.com/hwchase17/langchain"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to do step 1, and we also showed how to parallelize this step by using Ray tasks for faster embedding creation.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog post, we take it one step further, scaling out to many more documents. Continue reading to see how to use ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a distributed data processing system thatâs a part of the Ray framework, to generate and store embeddings for 2,000 PDF documents from cloud storage, parallelizing across 20 GPUs, all in under 4 minutes and in less than 100 lines of code.\n\nWhile in this walkthrough we use ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Data","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to read PDF files from S3 cloud storage, it also supports a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/api/input_output.html#input-output"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"wide number of other data formats","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" like text data, parquet, images, and can read data from a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/api/input_output.html#input-output"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"variety of sources","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" like MongoDB and SQL Databases.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Why do I need to parallelize this?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2CHfkngLbr34XsDZ7IKBAW","type":"Asset","createdAt":"2023-05-02T02:38:14.040Z","updatedAt":"2023-05-02T02:38:14.040Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Why do I need to parallelize this?","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2CHfkngLbr34XsDZ7IKBAW/af3491b43c8afcdb307890881b2adf5c/embedding-why-do-I-need-to-parallelize-this.png","details":{"size":153788,"image":{"width":1600,"height":1004}},"fileName":"embedding-why-do-I-need-to-parallelize-this.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"LangChain provides all the tools and the integrations for building LLM applications, including loading, embedding, and storing documents. While LangChain works great for quickly getting started with a handful of documents, when you want to scale your corpus up to thousands or more documents, this can quickly become unwieldy.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Naively using a for loop to do this for each document within a corpus of a 2,000 documents takes 75 minutes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4eHSySPomCQud1Y6m1yiyB","type":"Entry","createdAt":"2023-05-02T02:45:13.859Z","updatedAt":"2023-05-02T04:15:57.187Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Embedded-Code-Snippet-1","body":"import os\nfrom tqdm import tqdm\n\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=100,\n    length_function=len,\n)\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nhf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n\n# Put your directory containing PDFs here\ndirectory = '/tmp/data/'\npdf_documents = [os.path.join(directory, filename) for filename in os.listdir(directory)]\n\nlangchain_documents = []\nfor document in tqdm(pdf_documents):\n    try:\n        loader = PyPDFLoader(document)\n        data = loader.load()\n        langchain_documents.extend(data)\n    except Exception:\n        continue\n\nprint(\"Num pages: \", len(langchain_documents))\nprint(\"Splitting all documents\")\nsplit_docs = text_splitter.split_documents(langchain_documents)\n\nprint(\"Embed and create vector index\")\ndb = FAISS.from_documents(split_docs, embedding=hf)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Clearly, if you want to iterate quickly and try out different multiple document corpuses, splitting techniques, chunk sizes, or embedding models, just doing this in a for loop wonât cut it.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Instead, for faster development, you need to horizontally scale, and for this you need a framework to make this parallelization very easy.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"By using Ray Data, we can define our embedding generation pipeline and execute it in a few lines of code, and it will automatically scale out, leveraging the compute capabilities of all the CPUs and GPUs in our cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stages of our Data Pipeline","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this example, we want to generate embeddings for our document corpus consisting of the top 2,000 arxiv papers on âlarge language modelsâ. There are over 30,000 pages in all these documents. The code for generating this dataset can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/blob/main/embedding_pdf_documents/arxiv_dataset_generation.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J2BxxlXHAsmjVdrjCqLdn","type":"Asset","createdAt":"2023-05-02T02:52:04.314Z","updatedAt":"2023-05-02T02:52:04.314Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Stages of our Data Pipeline","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J2BxxlXHAsmjVdrjCqLdn/2ee02bfb6d2d96c7be6ab848c757d45e/embedded-stages-of-our-pipeline.png","details":{"size":127047,"image":{"width":1512,"height":556}},"fileName":"embedded-stages-of-our-pipeline.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs take a look at the stages of our data pipeline:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Load the PDF documents from our S3 bucket as raw bytes","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use PyPDF to convert those bytes into string text","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use LangChainâs text splitter to split the text into chunks","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use a pre-trained sentence-transformers model to embed each chunk","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Store the embeddings and the original text into a FAISS vector store","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The full data pipeline was run on 5 g4dn.12xlarge instances on AWS EC2, consisting of 20 GPUs in total. The code for the full data pipeline can be found ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/blob/main/embedding_pdf_documents/embedding_ray.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Starting the Ray Cluster","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Follow the steps ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/blob/main/embedding_pdf_documents/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to set up a multi-node Ray cluster on AWS.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray clusters can also be started on GCP, Azure, or other cloud providers. See the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/getting-started.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Cluster documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for full info.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Alternatively, you can use ","nodeType":"text"},{"data":{"uri":"https://docs.anyscale.com/develop/workspaces/get-started"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Workspaces on Anyscale","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to manage your Ray clusters.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now that we have the cluster setup, letâs go through the steps in our script.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Installing Dependencies","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"First, we need to install the necessary dependencies on all the nodes in our Ray cluster. We can do this via Rayâs ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#id1"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"runtime environment","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" feature.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4EbdJbi2f8hfsYWczaTcqZ","type":"Entry","createdAt":"2023-05-02T03:46:04.461Z","updatedAt":"2023-05-02T03:46:04.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Embedding-code-snippet-2","body":"import ray\n\nray.init(runtime_env={\"pip\": [\"langchain\", \"pypdf\", \"sentence_transformers\", \"transformers\"]})","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 1","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Load the 2,143 documents from our S3 bucket as raw bytes.The S3 bucket contains unmodified PDF files that have been downloaded from arxiv.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can easily do this via Ray Dataâs read APIs, which creates a Ray Dataset object. Ray Datasets are lazy. Further operations can be chained and the stages are run only when execution is triggered.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Cuzf28zxzpUhECAe0PxyR","type":"Entry","createdAt":"2023-05-02T03:47:03.011Z","updatedAt":"2023-05-02T17:11:29.904Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-3","body":"from ray.data.datasource import FileExtensionFilter\n\n# Filter out non-PDF files.\nds = ray.data.read_binary_files(\"s3://ray-llm-batch-inference/\", partition_filter=FileExtensionFilter(\"pdf\"))","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 2","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Use PyPDF to load in the raw bytes and parse them as string text. We also skip over any documents or pages that are unparseable. Even after skipping these, we still have over 33,642 pages in our dataset.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We use the pypdf library directly to read PDFs directly from bytes rather than file paths. Once ","nodeType":"text"},{"data":{"uri":"https://github.com/hwchase17/langchain/pull/3915"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://github.com/hwchase17/langchain/pull/3915","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is merged, LangChainâs PyPdfLoader can be used directly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"60msecWkdwI269c9fYmRih","type":"Entry","createdAt":"2023-05-02T03:49:06.119Z","updatedAt":"2023-05-02T04:15:18.317Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-4","body":"def convert_to_text(pdf_bytes: bytes):\n    pdf_bytes_io = io.BytesIO(pdf_bytes)\n\n    try:\n        pdf_doc = PdfReader(pdf_bytes_io)\n    except pypdf.errors.PdfStreamError:\n        # Skip pdfs that are not readable.\n        # We still have over 30,000 pages after skipping these.\n        return []\n\n    text = []\n    for page in pdf_doc.pages:\n        try:\n            text.append(page.extract_text())\n        except binascii.Error:\n            # Skip all pages that are not parseable due to malformed characters.\n            print(\"parsing failed\")\n    return text\n\n# We use `flat_map` as `convert_to_text` has a 1-\u003eN relationship.\n# It produces N strings for each PDF (one string per page).\n# Use `map` for 1-\u003e1 relationship.\nds = ds.flat_map(convert_to_text)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 3","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Split the text into chunks using LangChainâs TextSplitter abstraction. After applying this transformation, the 33,642 pages are split into 144,411 chunks. Each chunk will then be encoded into an embedding in Step 4.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3S1Tcod9rihoVKoyg9RMMz","type":"Entry","createdAt":"2023-05-02T03:50:32.629Z","updatedAt":"2023-05-02T04:14:39.402Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-5","body":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ndef split_text(page_text: str):\n    # Use chunk_size of 1000.\n    # We felt that the answer we would be looking for would be \n    # around 200 words, or around 1000 characters.\n    # This parameter can be modified based on your documents and use case.\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000, chunk_overlap=100, length_function=len\n    )\n    split_text: List[str] = text_splitter.split_text(page_text)\n\n    split_text = [text.replace(\"\\n\", \" \") for text in split_text]\n    return split_text\n\n# We use `flat_map` as `split_text` has a 1-\u003eN relationship.\n# It produces N output chunks for each input string.\n# Use `map` for 1-\u003e1 relationship.\nds = ds.flat_map(split_text)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stage 4","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Then, we can embed each of our chunks using a pre-trained sentence transformer model on GPUs. Here, we leverage Ray Actors for stateful computation, allowing us to initialize a model only once per GPU, rather than for every single batch.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"At the end of this stage, we have 144,411 encodings by running 20 model replicas across 20 GPUs, each processing a batch of 100 chunks at a time to maximize GPU utilization.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Dp9uXgJznvx0VgoNwZt0e","type":"Entry","createdAt":"2023-05-02T03:53:27.990Z","updatedAt":"2023-05-02T04:13:54.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-6","body":"class Embed:\n    def __init__(self):\n        # Specify \"cuda\" to move the model to GPU.\n        self.transformer = SentenceTransformer(model_name, device=\"cuda\")\n\n    def __call__(self, text_batch: List[str]):\n        # We manually encode using sentence_transformer since LangChain\n        # HuggingfaceEmbeddings does not support specifying a batch size yet.\n        embeddings = self.transformer.encode(\n            text_batch,\n            batch_size=100,  # Large batch size to maximize GPU utilization.\n            device=\"cuda\",\n        ).tolist()\n\n        return list(zip(text_batch, embeddings))\n\n# Use `map_batches` since we want to specify a batch size to maximize GPU utilization.\nds = ds.map_batches(\n    Embed,\n    # Large batch size to maximize GPU utilization.\n    # Too large a batch size may result in GPU running out of memory.\n    # If the chunk size is increased, then decrease batch size.\n    # If the chunk size is decreased, then increase batch size.\n    batch_size=100,  # Large batch size to maximize GPU utilization.\n    compute=ray.data.ActorPoolStrategy(min_size=20, max_size=20),  # I have 20 GPUs in my cluster\n    num_gpus=1,  # 1 GPU for each actor.\n)\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We use the `sentence_transformers` library directly so that we can provide a specific batch size. Once ","nodeType":"text"},{"data":{"uri":"https://github.com/hwchase17/langchain/pull/3914"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"https://github.com/hwchase17/langchain/pull/3914","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is merged, LangChainâs `HuggingfaceEmbeddings` can be used instead.\n\n","nodeType":"text"},{"data":{},"marks":[{"type":"underline"}],"value":"Stage 5","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we can execute this Data Pipeline by iterating through it, and we store the results in a persisted FAISS vector database for future querying.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3WstnFWdDedO9wxSKYDGR8","type":"Entry","createdAt":"2023-05-02T03:54:47.846Z","updatedAt":"2023-05-02T04:17:51.737Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-7","body":"from langchain import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\ntext_and_embeddings = []\nfor output in ds.iter_rows():\n    text_and_embeddings.append(output)\n\nvectore_store = FAISS.from_embeddings(\n    text_and_embeddings,\n    # Provide the embedding model to embed the query.\n    # The documents are already embedded.\n    embedding=HuggingFaceEmbeddings(model_name=model_name)\n)\n\n# Persist the vector store.\nvectore_store.save_local(\"faiss_index\")","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Execution","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Executing this code, we see that all 20 GPUs are utilized at near 100% utilization. And what would normally take over an hour to run, can now be done in under 4 minutes! If you use AWS spot instances, this would only cost $0.95 total.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5rAh6B2lrImLTTXvpBMeHG","type":"Asset","createdAt":"2023-05-02T04:00:42.271Z","updatedAt":"2023-05-02T04:00:42.271Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Execution-1","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5rAh6B2lrImLTTXvpBMeHG/70baf2935bfad28c70d5fb310fe15ef1/embedding-execution-1.png","details":{"size":9146,"image":{"width":352,"height":130}},"fileName":"embedding-execution-1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1yk9oF8K2IIuK88xWBTfGr","type":"Asset","createdAt":"2023-05-02T04:01:19.132Z","updatedAt":"2023-05-02T04:43:16.984Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"locale":"en-US"},"fields":{"title":"Embedding - Execution 2","file":{"url":"//images.ctfassets.net/xjan103pcp94/1yk9oF8K2IIuK88xWBTfGr/ee185a15ff08445d14f3e47ed7f0f2b9/embedding-execution-2.jpg","details":{"size":127225,"image":{"width":1097,"height":780}},"fileName":"embedding-execution-2.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Querying the Vector Database","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can now load in our persisted FAISS database, and query it for similarity search. Letâs see the top document thatâs most relevant to the âprompt engineeringâ query:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3DulKBBh4pLynITCBlacTf","type":"Entry","createdAt":"2023-05-02T04:04:27.586Z","updatedAt":"2023-05-02T04:04:27.586Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-8","body":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain import FAISS\n\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nquery_embedding = HuggingFaceEmbeddings(model_name=model_name)\ndb = FAISS.load_local(\"faiss_index\", query_embedding)\ndocuments = db.similarity_search(query=\"prompt engineering\", k=1)\n[doc.page_content for doc in documents]","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"SMSzMi5hzq9x7y2pUs2Ja","type":"Entry","createdAt":"2023-05-02T04:05:20.246Z","updatedAt":"2023-05-02T04:05:35.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"embedding-code-snippet-9","body":"['Prompt Engineering for Job Classiï¬cation 7 5 LLMs \u0026 Prompt Engineering Table '\n '3. Overview of the various prompt modiï¬cations explored in thi s study. '\n 'Short name Description Baseline Provide a a job posting and asking if it is '\n 'ï¬t for a graduate. CoT Give a few examples of accurate classiï¬cation before '\n 'queryi ng. Zero-CoT Ask the model to reason step-by-step before providing '\n 'its an swer. rawinst Give instructions about its role and the task by adding '\n 'to the user msg. sysinst Give instructions about its role and the task as a '\n 'system msg. bothinst Split instructions with role as a system msg and task '\n 'as a user msg. mock Give task instructions by mocking a discussion where it '\n 'ackn owledges them. reit Reinforce key elements in the instructions by '\n 'repeating the m. strict Ask the model to answer by strictly following a '\n 'given templat e. loose Ask for just the ï¬nal answer to be given following a '\n 'given temp late. right Asking the model to reach the right conclusion.']\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next Steps","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"See part 3 ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/building-a-self-hosted-question-answering-service-using-langchain-ray"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Review the code and data used in this blog in the following ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/tree/main/embedding_pdf_documents"},"content":[{"data":{},"marks":[],"value":"Github repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\n","nodeType":"text"},{"data":{},"marks":[],"value":"See our earlierÂ ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[],"value":"blog series on solving Generative AI infrastructure","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â with Ray.","nodeType":"text"},{"data":{},"marks":[],"value":"\n\nIf you are interested in learning more about Ray, see ","nodeType":"text"},{"data":{"uri":"http://ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"http://docs.ray.io/"},"content":[{"data":{},"marks":[],"value":"Docs.Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nTo connect with the Ray community join #LLM on the ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"data":{},"marks":[],"value":"Ray Slack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or our ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[],"value":"Discuss forum","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","nodeType":"text"},{"data":{"uri":"http://www.anyscale.com/platform"},"content":[{"data":{},"marks":[],"value":"Anyscale.com/Platform ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"and click the 'Try it now' button","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray Summit 2023:","nodeType":"text"},{"data":{},"marks":[],"value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"practical training focused on LLMs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J2BxxlXHAsmjVdrjCqLdn","type":"Asset","createdAt":"2023-05-02T02:52:04.314Z","updatedAt":"2023-05-02T02:52:04.314Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Embedding - Stages of our Data Pipeline","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J2BxxlXHAsmjVdrjCqLdn/2ee02bfb6d2d96c7be6ab848c757d45e/embedded-stages-of-our-pipeline.png","details":{"size":127047,"image":{"width":1512,"height":556}},"fileName":"embedded-stages-of-our-pipeline.png","contentType":"image/png"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2rfQivy2LoE7OTc9bGVGvm","type":"Entry","createdAt":"2023-04-27T13:59:53.933Z","updatedAt":"2023-06-06T14:28:19.779Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Announcing Ray 2.4.0: Infrastructure for LLM training, tuning, inference, and serving","seoTitle":"Announcing Ray 2.4.0: Infrastructure for LLM training, tuning, inference, and serving","slug":"announcing-ray-2-4-0-infrastructure-for-llm-training-tuning-inference-and","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3YgxSHNt3KauhagOXzHM8j","type":"Entry","createdAt":"2021-11-05T17:45:15.693Z","updatedAt":"2023-01-12T21:44:33.859Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jules S. Damji","slug":"jules-s-damji","link":"https://www.linkedin.com/in/dmatrix/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6TqWiyUibvC9ETfyYljP8b","type":"Entry","createdAt":"2022-02-11T15:42:52.165Z","updatedAt":"2022-02-11T15:42:52.165Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jiajun Yao","slug":"jiajun-yao"}}],"publishedDate":"2023-04-27","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"The ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.4.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2.4 release ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"features exciting improvements across the Ray ecosystem. In this blog post, we highlight new features and examples for Generative AI workloads and share four overall additions:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements to Ray data for ease of use, stability, and observability","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Improved Serve observability","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Introduce RLlibâs module for custom reinforcement learning","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Improved Ray scalability for large clusters","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Generative AI model examples and new features using Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Over the past few months, we have seen a flurry of innovative activities around ","nodeType":"text"},{"data":{"uri":"https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-generative-ai"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"generative AI models","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Large_language_model"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"large language models (LLMs)","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". To continue our steadfast commitment to make Ray a pivotal compute substrate for ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"generative AI workloads","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and address the challenges (as explained in our ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"blog series","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"), we have added significant enhancements in this release to ensure that these open source LLM models and workloads are accessible to the open source community and performant with Ray.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QDe7dun9hKhLCoGFSXKh9","type":"Asset","createdAt":"2023-04-26T19:23:25.596Z","updatedAt":"2023-04-26T19:23:25.596Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_image","file":{"url":"//images.ctfassets.net/xjan103pcp94/1QDe7dun9hKhLCoGFSXKh9/d9f93233f6ee72608e34735eac545a74/image1.png","details":{"size":154908,"image":{"width":960,"height":540}},"fileName":"image1.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Generative AI Workloads:","nodeType":"text"},{"data":{},"marks":[],"value":" First, with 2.4 we are releasing a set of new working examples showing how to use Ray with ","nodeType":"text"},{"data":{"uri":"https://stablediffusionweb.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stable Diffusion","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and LLMs like ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/transformers/model_doc/gptj"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"GPT-J","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Below we tabulate the workload of each of these models and their respective links. More are underway with respect to Rayâs integration with ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"49AVYyPR2gbRD9P3NOH7My","type":"Entry","createdAt":"2023-04-26T19:44:22.407Z","updatedAt":"2023-04-26T22:30:40.568Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"generative_ai_features_and_examples","header":"Table 1. New LLM features, examples, and tutorials","body":"| Generative AI Workload| Description| Links|\n| ---------- | ---------- | ---------- |\n|Fine-tuning GPT-J|In this example, we showcase how to use [Ray Train](https://docs.ray.io/en/releases-2.4.0/train/train.html) for fine-tuning [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) using [DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed) and Hugging Face. GPT-J is a GPT-2-like causal language mode, with 6B parameters, trained on the PILE dataset.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html)|\n|Fine-tuning Dreambooth|This example shows how to fine-tune a [DreamBooth](https://dreambooth.github.io/) model using [Ray Train](https://docs.ray.io/en/releases-2.4.0/train/train.html). Dreambooth is a technique to fine-tune diffusion models (like Stable Diffusion) by injecting a custom subject to the model.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/dreambooth_finetuning.html)|\n|Scalable Batch Inference  with GPT-J|This example showcases how to use[ Ray Data](https://docs.ray.io/en/releases-2.4.0/data/dataset.html) for scalable batch inference with  GPT-J.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_batch_prediction.html)| Scalable Batch Inference  with Stable Diffusion|In this example, showcase how to use [Ray Data](https://docs.ray.io/en/releases-2.4.0/data/dataset.html) for scalable batch inference with [Stable Diffusion](https://stablediffusionweb.com/). Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from CompVis, Stability AI and LAION.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/stablediffusion_batch_prediction.html)|\n|Serving GPT-J|In this example, we showcase how to use [Ray Serve](https://docs.ray.io/en/releases-2.4.0/serve/index.html) for serving GPT-J.|[Source and documentation](https://docs.ray.io/en/master/ray-air/examples/gptj_serving.html)|\n|Serving Stable Diffusion|In this example, we showcase how to use [Ray Serve](https://docs.ray.io/en/releases-2.4.0/serve/index.html) for serving [Stable Diffusion.](https://stablediffusionweb.com/)|[Source and documentation](https://docs.ray.io/en/master/serve/tutorials/stable-diffusion.html)|\n|Ray integration with LangChain|In this tutorial, we showcase how to build an open source search engine with Ray and [LangChain](https://python.langchain.com/en/latest/index.html)|[Source and blog](https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray)|"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray AIR new trainers:","nodeType":"text"},{"data":{},"marks":[],"value":" Second, to further enable additional large model workloads on Ray, weâre also releasing an ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"AccelerateTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", allowing you to run ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/accelerate"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"HuggingFace Accelerate","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/docs/accelerate/usage_guides/deepspeed"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"DeepSpeed","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on Ray with minimal code changes. This Trainer integrates with the rest of the Ray ecosystemâincluding the ability to run distributed ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"hyperparameter tuning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with each trial dispatched as a distributed training job.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"See how ","nodeType":"text"},{"data":{"uri":"https://github.com/CarperAI/trlx/blob/9bc08369ca9ec83342c4d7755205dab1a7723006/trlx/sweep.py#L325-L340"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"CarperAI/trlx","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" uses this for RLHF hyperparameter sweeps, and take a peek at the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html#ray-train-huggingface-accelerate-acceleratetrainer"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" too.\n","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4CHwg0j8NDYgH399cu2hGC","type":"Entry","createdAt":"2023-04-26T21:23:30.012Z","updatedAt":"2023-04-26T21:23:30.012Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"accelerate_trainer_code_snippet","body":"from ray.train.huggingface.accelerate import AccelerateTrainer\n\ndef train_func():\n    ...\n\ntrainer = AccelerateTrainer(\n    train_func,\n    accelerate_config={}  # you can pass in a config filepath here as well\n    scaling_config=ScalingConfig(\n        trainer_resources={\"CPU\": 0},\n        num_workers=4,\n        use_gpu=True,\n        resources_per_worker={\"CPU\": 2},\n    ),\n)\n\nresult = trainer.fit()\n\n# Integration with Ray Tune\ntuner = tune.Tuner(\n    trainer,\n    param_space={\"lr\": tune.grid_search([0.1, 0.2, 0.3]),\n)\n\nresults = tuner.fit()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can see the full example for AccelerateTrainer ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.huggingface.accelerate.AccelerateTrainer.html#ray-train-huggingface-accelerate-acceleratetrainer"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Updated LightningTrainer:","nodeType":"text"},{"data":{},"marks":[],"value":" Third, in the broader deep learning space, weâre introducing the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.lightning.LightningTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LightningTrainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", allowing you to scale your ","nodeType":"text"},{"data":{"uri":"https://lightning.ai/docs/pytorch/stable//index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch Lightning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on Ray. As part of our continued effort for seamless integration and ease of use, we have enhanced and replaced our existing ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray_lightning","nodeType":"text"},{"data":{},"marks":[],"value":" integration, which was widely adopted, with the latest changes to PyTorch Lighting.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The new ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"LightningTrainer","nodeType":"text"},{"data":{},"marks":[],"value":" provides better compatibility with the other Ray libraries: Ray Tune or Ray Data or Ray Serve directly with Ray connecting APIs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Check out ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/api/doc/ray.train.lightning.LightningTrainer.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"the documentation for this component","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or take a look ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/examples/lightning/lightning_mnist_example.html#lightning-mnist-example"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"at an example","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1p8EMLSFTFDI0RuBceMrPe","type":"Entry","createdAt":"2023-04-26T21:27:50.764Z","updatedAt":"2023-04-26T21:27:50.764Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"updated_lightning_trainder_code_snippet","body":"from pytorch_lightning.loggers.csv_logs import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\nfrom ray.train.lightning import (\n    LightningTrainer,\n    LightningConfigBuilder,\n    LightningCheckpoint,\n)\n\nclass MNISTClassifier:\n\n    â¦\n\nclass MNISTDataModule:\n    â¦\n\nlightning_config = (\n    LightningConfigBuilder()\n    .module(MNISTClassifier, lr=1e-3, feature_dim=128)\n    .trainer(\n        max_epochs=10,\n        accelerator=\"cpu\",\n        log_every_n_steps=100,\n        logger=CSVLogger(\"logs\"),\n    )\n    .fit_params(datamodule=datamodule)\n    .checkpointing(monitor=\"val_accuracy\", mode=\"max\", save_top_k=3)\n    .build()\n)\nscaling_config = ScalingConfig(\n    num_workers=4, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n)\n\nrun_config = RunConfig(\n    name=\"ptl-mnist-example\",\n    local_dir=\"/tmp/ray_results\",\n    checkpoint_config=CheckpointConfig(\n        num_to_keep=3,\n        checkpoint_score_attribute=\"val_accuracy\",\n        checkpoint_score_order=\"max\",\n    ),\n)\ntrainer = LightningTrainer(\n    lightning_config=lightning_config,\n    scaling_config=scaling_config,\n    run_config=run_config,\n)\n\nresult = trainer.fit()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"See the full source code ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/train/examples/lightning/lightning_mnist_example.html#lightning-mnist-example"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements in Ray DataÂ ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The enhancements and features in the release cut across a few dimensions: stability, ease of use, and observability.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Data and ease of use: ","nodeType":"text"},{"data":{},"marks":[],"value":"Data ingestion and preprocessing for machine learning (ML) at scale are central to any end-to-end ML.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To enable ease of use, the default backend execution model for Ray Data is streaming-based under the hood. As such developers need not worry about instructing Ray Data to use the streaming execution backend; itâs the default execution mode. Second, the default streaming backend lends itself well to large scale workloads such as setting up distributed training data streaming and batch inference or preprocessing. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4fRyCVJVYH8NAjkcFhTgDX","type":"Entry","createdAt":"2023-04-26T21:30:40.030Z","updatedAt":"2023-04-26T21:30:40.030Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_data_code_snippet","body":"import ray\nfrom ray.data import ActorPoolStrategy\n\nimport time\n\ndef preprocess_func(x):\n   # Do some work with the data x\n   x = ...\n   time.sleep(0.1)\n   return x\n\nfor _ in (\n   ray.data.range_tensor(5000, shape=(80, 80, 3), parallelism=200)\n   .map_batches(preprocess_func, compute=ActorPoolStrategy(size=4))\n   .map_batches(preprocess_func, compute=ActorPoolStrategy(size=3))\n   .map_batches(preprocess_func, num_cpus=1)\n   .iter_batches()\n\n):\n\n   pass","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This launches a simple 4-stage pipeline. We use different compute args for each stage, which forces them to be run as separate operators instead of getting fused together. You should see a log message indicating streaming execution is being used.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While the above example is using dummy tasks, these sort of operator pipelines are a common pattern Ray users desire to scale:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6T6VTlZroKCZ6eyyII0HC1","type":"Asset","createdAt":"2023-04-26T21:34:22.865Z","updatedAt":"2023-04-26T21:34:22.865Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_data_distributed_image","description":"Figure 1. Ray data streaming data with ActorPool ","file":{"url":"//images.ctfassets.net/xjan103pcp94/6T6VTlZroKCZ6eyyII0HC1/d7202c977c1a0c25eeb4563ab5c50319/image3.png","details":{"size":106561,"image":{"width":1249,"height":515}},"fileName":"image3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray data and SQL","nodeType":"text"},{"data":{},"marks":[],"value":": Additionally, we have extended Ray Dataâs ability to fetch data from a common ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/data/creating-datastreams.html#reading-from-sql-databases"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SQL data source","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", with ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"read_sql(...)","nodeType":"text"},{"data":{},"marks":[],"value":", enabling you to connect to a common SQL data store.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3imkshawnZw7DglAUBNTN3","type":"Entry","createdAt":"2023-04-26T21:36:16.889Z","updatedAt":"2023-04-26T21:36:16.889Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray_data_sql_connector_code_snippet","body":"import mysql.connector\n\nimport ray\n\ndef create_connection():\n    return mysql.connector.connect(\n        user=\"admin\",\n        password=...,\nhost=\"example-mysql-database.c2c2k1yfll7o.us-west-2.rds.amazonaws.com\",\n      connection_timeout=30,\n      database=\"example\",\n    )\n\n# Get all movies\ndatastream = ray.data.read_sql(\"SELECT * FROM movie\", create_connection)\n# Get movies after the year 1980\ndatastream = ray.data.read_sql(\n    \"SELECT title, score FROM movie WHERE year \u003e= 1980\", create_connection\n)\n# Get the number of movies per year\ndatastream = ray.data.read_sql(\n    \"SELECT year, COUNT(*) FROM movie GROUP BY year\", create_connection\n)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Enhancements to Serve observability","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In 2.4, we are adding new functionality for observability and monitoring for Ray Serve applications. You can use the Ray dashboard to get a high-level overview of your Ray cluster and Ray Serve applicationâs states. This includes details such as:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"the number of deployment replicas currently running","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"logs for your Serve controller, deployment replicas, and HTTP proxies","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"the Ray nodes (i.e., machines) running in your Ray cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"You can access the Ray dashboard at port 8265 at your clusterâs URI. For example, if youâre running Ray Serve locally, use ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"http://localhost:8265","nodeType":"text"},{"data":{},"marks":[],"value":" in your browser.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3CtBAZRebvQVRJsbp63rD2","type":"Asset","createdAt":"2023-04-26T21:39:28.459Z","updatedAt":"2023-04-26T21:39:28.459Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ray_serve_observebility","description":"Figure 1. Serve observesability metrics in the Ray Dashboard.","file":{"url":"//images.ctfassets.net/xjan103pcp94/3CtBAZRebvQVRJsbp63rD2/0951a7ad1f4d4ec1ae2a3ba60a3af218/image2.png","details":{"size":146211,"image":{"width":1999,"height":1040}},"fileName":"image2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"For more information on the dashboard, read the documentation for the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/ray-core/ray-dashboard.html#dash-serve-view"},"content":[{"data":{},"marks":[],"value":"Serve page","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"RLlibâs new module for custom reinforcement","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"In RLlib, we are introducing a new ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/rllib-rlmodule.html#rl-modules-alpha"},"content":[{"data":{},"marks":[],"value":"RLModule","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" abstraction (alpha). RLModule API provides a unified way to define custom reinforcement learning models in RLlib. This API enables you to design and implement your own models to suit specific needs.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To maintain consistency and usability, RLlib offers a standardized approach for defining module objects for both single-agent and multi-agent reinforcement learning environments through the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/package_ref/rl_modules.html"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"SingleAgentRLModuleSpec","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/rllib/package_ref/doc/ray.rllib.core.rl_module.marl_module.MultiAgentRLModuleSpec.html#ray.rllib.core.rl_module.marl_module.MultiAgentRLModuleSpec"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"MultiAgentRLModuleSpec","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" classes. The built-in RLModules in RLlib follow this consistent design pattern, making it easier for you to understand and utilize these modules.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1w6lK0sBUAVl4BmOGBQcQ9","type":"Entry","createdAt":"2023-04-26T21:48:25.122Z","updatedAt":"2023-04-26T21:48:25.122Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"rllib_multiagent_code_snippet","body":"import gymnasium as gym\nfrom ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec\nfrom ray.rllib.core.testing.torch.bc_module import DiscreteBCTorchModule\n\nenv = gym.make(\"CartPole-v1\")\n\nspec = SingleAgentRLModuleSpec(\n    module_class=DiscreteBCTorchModule,\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    model_config_dict={\"fcnet_hiddens\": [64]},\n)\n\nmodule = spec.build()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is an experimental module that serves as a general replacement for ModelV2, and is subject to change. It will eventually match the functionality of the previous stack. If you only use high-level RLlib APIs such as ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html#ray.rllib.algorithms.algorithm.Algorithm"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Algorithm","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", you should not experience significant changes, except for a few new parameters to the configuration object. Read more about it in the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/releases-2.4.0/rllib/rllib-rlmodule.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Core scaling to 2000 nodes","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Weâve also invested time in making sure Ray can support larger scale workloads. With this release, we are announcing official support for Ray clusters to up to 2000 nodes. See","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/blob/master/release/benchmarks/README.md"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":" scalability envelope","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for more details.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"First and foremost, we want to thank all contributors for their valuable contributions to this new release of ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/releases/tag/ray-2.4.0"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray 2.4","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Your enduring support continues to foster the wider use of Ray adoption.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This release is a milestone in our efforts to offer Ray as a performant compute infrastructure for LLM training, inference, and serving. We shared a number of examples that speak to these efforts.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Additionally, we enhanced Ray Data, improved Serve observability, introduced a new RLlib customization module, and scaled Ray core to support large workloads, up to 2000 nodes. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Have a go at the latest release with pip install âray[default]â and let us know of your feedback. Weâre always interested to hear from you â feel free to reach out to us on ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Github","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discuss","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Whatâs Next?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Stay tuned for more Generative AI how-to blog series as we continue our efforts to make Ray the platform choice and compute substrate for all your Generative AI workloads. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you missed our four-part blog series on Generative AI, in which lay out our vision, position, and solution on how Ray addresses the challenges associated with OSS Generative AI workloads, peruse the links below. Much of the above Generative AI examples and features stem from this series.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-ray-solves-common-production-challenges-for-generative-ai"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"How Ray solves common production challenges for generative AI infrastructure","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Training 175B Parameter Language Models at 1000 GPU scale with Alpa and Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/faster-stable-diffusion-fine-tuning-with-ray-air"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Faster stable diffusion fine-tuning with Ray AIR","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"How to fine tune and serve LLMs simply, quickly and cost effectively using Ray + DeepSpeed + HuggingFace","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ð ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Building an LLM open source search engine in 100 lines using LangChain and Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Join our ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/community"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Community","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the ","nodeType":"text"},{"data":{"uri":"https://forms.gle/9TSdDYUgxYs8SA9e8"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray #LLM slack channel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have our ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit 2023 ","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"early-bird registration open. Secure your spot, and save some money.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1QDe7dun9hKhLCoGFSXKh9","type":"Asset","createdAt":"2023-04-26T19:23:25.596Z","updatedAt":"2023-04-26T19:23:25.596Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"main_image","file":{"url":"//images.ctfassets.net/xjan103pcp94/1QDe7dun9hKhLCoGFSXKh9/d9f93233f6ee72608e34735eac545a74/image1.png","details":{"size":154908,"image":{"width":960,"height":540}},"fileName":"image1.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"574o0Bh7HzHlCEc6AXphfF","type":"Entry","createdAt":"2023-04-19T16:00:08.661Z","updatedAt":"2023-05-31T18:34:15.880Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":16,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Building an LLM open source search engine in 100 lines using LangChain and Ray","seoTitle":"Building an LLM Open-Source Search Engine in 100 Lines","slug":"llm-open-source-search-engine-langchain-ray","description":"In part 1 of a new blog series, we show how to build a search engine in 100 lines using LLM embeddings and a vector database.","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}}],"publishedDate":"2023-04-18","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 1 of a blog series. In this blog, weâll introduce you to LangChain and Ray Serve and how to use them to build a search engine using LLM embeddings and a vector database. In future parts, we will show you how to turbocharge embeddings and how to combine a vector database and an LLM to create a fact-based question answering service. Additionally, we will optimize the code and measure performance: cost, latency and throughput.\n\n\u003cscript type=\"application/ld+json\"\u003e{\n\"@context\": \"http://schema.org\",\n\"@type\": \"VideoObject\",\n\"name\": \"Open Source LLM Search Engine with LangChain on Ray\",\n\"description\": \"Waleed, Head of Engineering at Anyscale, explains how to use LangChain and Ray Serve to build a search engine using LLM embeddings and a vector database. Blog: https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray\",\n\"thumbnailUrl\": \"https://i.ytimg.com/vi/v7a8SR-sZpI/default.jpg\",\n\"uploadDate\": \"2023-04-19T16:00:41Z\",\n\"duration\": \"PT7M36S\",\n\"embedUrl\": \"https://www.youtube.com/embed/v7a8SR-sZpI\",\n\"interactionCount\": \"4540\"\n}\u003c/script\u003e","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"b9V0HkkRoYEqCnIrMO9kC","type":"Entry","createdAt":"2023-04-19T17:16:15.326Z","updatedAt":"2023-04-19T17:16:15.326Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"embed"}},"locale":"en-US"},"fields":{"title":"LLM Search Engine with Langchain","videoUrl":"https://www.youtube.com/watch?v=v7a8SR-sZpI"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this blog, we'll cover:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"An introduction to ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and show why itâs awesome.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"An explanation of how Ray complements ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" by:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Showing how with a few minor changes, we can speed parts of the process up by a factor of 4x or more","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Making ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"âs capabilities available in the cloud using Ray Serve","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Using self-hosted models by running ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/serve/index.html"},"content":[{"data":{},"marks":[],"value":"Ray Serve","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and the model all in the same Ray cluster without having to worry about maintaining individual machines.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introduction","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a very powerful framework for ML orchestration, but with great power comes voluminous documentation. 120 megabytes in fact. How can we make that documentation more accessible? ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Answer: make it searchable! It used to be that creating your own high quality search results was hard. But by using ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", we can build it in about 100 lines of code.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This is where ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" comes in. ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" provides an amazing suite of tools for everything around LLMs. Itâs kind of like ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/"},"content":[{"data":{},"marks":[],"value":"HuggingFace","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" but specialized for LLMs. There are tools (chains) for prompting, indexing, generating and summarizing text. While an amazing tool, using Ray with it can make ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" even more powerful. In particular, it can:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Simply and quickly help you deploy a ","nodeType":"text"},{"data":{"uri":"https://python.langchain.com/en/latest/index.html"},"content":[{"data":{},"marks":[],"value":"LangChain","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" service.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Rather than relying on remote API calls, allow Chains to run co-located and auto-scalable with the LLMs itself. This brings all the advantages we ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"discussed in a previous blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": lower cost, lower latency, and control over your data.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Building the index","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4OzISThpksdKgjZ0gVJUiB","type":"Asset","createdAt":"2023-04-19T16:59:49.253Z","updatedAt":"2023-05-01T09:40:14.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"doc-index-starter-langchain","description":"Build a document index easily with Ray and Langchain","file":{"url":"//images.ctfassets.net/xjan103pcp94/4OzISThpksdKgjZ0gVJUiB/85bb7fccdfef1df3d061c57e9af1062a/index-langchain.jpg","details":{"size":28918,"image":{"width":900,"height":381}},"fileName":"index-langchain.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"First we will build the index via the following steps.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Download the content we want to index locally.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Read the content and cut it into tiny little pieces (about a sentence each). This is because it is easier to match queries against pieces of a page rather than the whole page.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use the ","nodeType":"text"},{"data":{"uri":"https://huggingface.co/sentence-transformers"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Sentence Transformers","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" library from HuggingFace to generate a vector representation of each sentence.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Embed those vectors in a Vector database (we use ","nodeType":"text"},{"data":{"uri":"https://github.com/facebookresearch/faiss"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"FAISS","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", but you could use whatever you like).Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The amazing thing about this code is how simple it is - ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/d06097768abbea54d59e5d3ed4f045f3"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"See Here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". As you will see, thanks to LangChain, all the heavy lifting is done for us. Letâs pick a few excerpts.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Assuming weâve downloaded the Ray docs, this is all we have to do to read all the docs in: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5XcvTPG6LASLRu3arklXJ0","type":"Entry","createdAt":"2023-04-17T08:22:33.719Z","updatedAt":"2023-04-17T08:22:33.719Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-1","body":"loader = ReadTheDocsLoader(\"docs.ray.io/en/master/\")\ndocs = loader.load() ","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The next step is to break each document into little chunks. LangChain uses splitters to do this. So all we have to do is this: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"72vZr3kpoioEza3WTS1vfT","type":"Entry","createdAt":"2023-04-17T08:23:19.379Z","updatedAt":"2023-04-17T08:23:19.379Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-2","body":"chunks = text_splitter.create_documents(\n    [doc.page_content for doc in docs], \n    metadatas=[doc.metadata for doc in docs])","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We want to preserve the metadata of what the original URL was, so we make sure to retain the metadata along with these documents.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now we have the chunks we can embed them as vectors. LLM providers do offer APIs for doing this remotely (and this is how most people use LangChain). But, with just a little bit of glue we can download Sentence Transformers from HuggingFace and run them locally (inspired by LangChainâs support for llama.cpp). ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/aea1d312d68c9431949442cc562d5f2c"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Hereâs the glue code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"By doing so, we reduce latency, stay on open source technologies, and donât need a HuggingFace key or to pay for API usage.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, we have the embeddings, now we can use a vector database â in this case FAISS â to store the embeddings. Vector databases are optimized for doing quick searches in high dimensional spaces. Again, LangChain makes this effortless. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4YljOvEjiKrfhFRmpA1PDG","type":"Entry","createdAt":"2023-04-17T08:23:35.223Z","updatedAt":"2023-04-17T08:23:35.223Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-3","body":"from langchain.vectorstores import FAISS\n\ndb = FAISS.from_documents(chunks, embeddings)\ndb.save_local(FAISS_INDEX_PATH)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"And thatâs it. The code for this is ","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/d0915e52cbe56dff328f5c00ded21107"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Now we can build the store.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"39FzcaaXnaSmciROgXSD0x","type":"Entry","createdAt":"2023-04-17T08:29:45.440Z","updatedAt":"2023-04-17T08:29:45.440Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-python-build-snippet","body":"% python build_vector_store.py"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This takes about 8 minutes to execute. Most of that time is spent doing the embeddings. Of course, itâs not a big deal in this case, but imagine if you were indexing hundreds of gigabytes instead of hundreds of megabytes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Accelerating indexing using Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"[Note: This is a slightly more advanced topic and can be skipped on first reading. It just shows how we can do it more quickly â 4x to 8x times more quickly]","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How can we speed up indexing? The great thing is that embedding is easy to parallelize. What if we:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Sliced the list of chunks into 8 shards.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Embedded each of the 8 shards separately.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Merge the shards.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7tDpD5Q7nxtRyX9lgDvbkI","type":"Asset","createdAt":"2023-04-17T08:04:57.120Z","updatedAt":"2023-04-17T08:04:57.120Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"langchain-ray-accelerated-indexing","description":"Build a document index 4-8x faster with Ray","file":{"url":"//images.ctfassets.net/xjan103pcp94/7tDpD5Q7nxtRyX9lgDvbkI/6209fbd875c5cd379c2289ef6f6554f0/Screen_Shot_2023-04-16_at_6.20.10_PM.png","details":{"size":277144,"image":{"width":2284,"height":936}},"fileName":"Screen Shot 2023-04-16 at 6.20.10 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"One key thing to realize is that embedding is GPU accelerated, so if we want to do this, we need 8 GPUs. Thanks to Ray, those 8 GPUS donât have to be on the same machine. But even on a single machine, there are significant advantages to using Ray. And one does not have to go to the complexity of setting up a Ray cluster, all you need to do is ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"pip install ray[default]","nodeType":"text"},{"data":{},"marks":[],"value":" and then ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"import ray","nodeType":"text"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This requires some minor surgery to the code. Hereâs what we have to do.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"First, create a task that creates the embedding and then uses it to index a shard. Note the Ray annotation and us telling us each task will need a whole GPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3NmJuC8SstZJpoHLrzrLgg","type":"Entry","createdAt":"2023-04-17T08:23:48.410Z","updatedAt":"2023-04-17T08:23:48.410Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-4","body":"@ray.remote(num_gpus=1)\ndef process_shard(shard): \n    embeddings = LocalHuggingFaceEmbeddings('multi-qa-mpnet-base-dot-v1')\n    result = FAISS.from_documents(shard, embeddings)\n    return result\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next, split the workload in the shards. NumPy to the rescue! This is a single line:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7lotIjqpm0Sm03BPHE4t2n","type":"Entry","createdAt":"2023-04-17T08:24:01.572Z","updatedAt":"2023-04-17T08:24:01.572Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-5","body":"shards = np.array_split(chunks, db_shards)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Then, create one task for each shard and wait forÂ the results.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7m8iFk9DvJOBZc3r4maPTY","type":"Entry","createdAt":"2023-04-17T08:24:13.248Z","updatedAt":"2023-04-17T08:24:13.248Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-6","body":"futures = [process_shard.remote(shards[i]) for i in range(db_shards)]\nresults = ray.get(futures)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Finally, letâs merge the shards together. We do this using simple linear merging.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"21KuagZ35WVzVVJ7Xq4qHy","type":"Entry","createdAt":"2023-04-17T08:24:27.839Z","updatedAt":"2023-04-17T08:29:23.908Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-7","body":"db = results[0]\nfor i in range(1,db_shards):\n    db.merge_from(results[i])","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://gist.github.com/waleedkadous/4c41f3ee66040f57d34c6a40e42b5969"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Hereâs what the sped up code looks like.","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"You might be wondering, does this actually work? We ran some tests on a g4dn.metal instance with 8 GPUs. The original code took 313 seconds to create the embeddings, the new code took 70 seconds, thatâs about a 4.5x improvement. Thereâs still some one-time overheads to creating tasks, setting up the GPUs etc. This reduces as the data increases. For example, we did a simple test with 4 times the data, and it was around 80% of the theoretical maximum performance (ie. 6.5x faster vs theoretical maximum of 8x faster from the 8 GPUs).Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can use the Ray Dashboard to see how hard those GPUs are working. Sure enough theyâre all close to 100% running the process_shard method we just wrote. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"14IoasmHxwEeQdAEGJqlyO","type":"Asset","createdAt":"2023-04-17T08:10:19.509Z","updatedAt":"2023-04-17T21:38:58.348Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"accelerated-index-langchain-dashboard","description":"Dashboard shows that GPU utilization is maxed out across all instances","file":{"url":"//images.ctfassets.net/xjan103pcp94/14IoasmHxwEeQdAEGJqlyO/5ae6e6739e258252a78c889ca7959683/raydash.png","details":{"size":278628,"image":{"width":2442,"height":842}},"fileName":"raydash.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"It turns out merging vector databasesÂ  is pretty fast, taking only 0.3 seconds for all 8 shards to be merged.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Serving","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1g6zBePU72Rmz5MBH2reaB","type":"Asset","createdAt":"2023-04-17T08:08:50.696Z","updatedAt":"2023-04-17T08:08:50.696Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Serving-Queries-Ray-Langchain","description":"Serve search queries with Ray and Langchain","file":{"url":"//images.ctfassets.net/xjan103pcp94/1g6zBePU72Rmz5MBH2reaB/db400e9bbbc445d7214d45658f81992f/Screen_Shot_2023-04-16_at_9.42.46_PM.png","details":{"size":380753,"image":{"width":2718,"height":1348}},"fileName":"Screen Shot 2023-04-16 at 9.42.46 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Serving is another area where the combination of LangChain and Ray Serve shows its power. This is just scratching the surface: weâll explore amazing capabilities like independent auto scaling and request batching in our next blog post in the series. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The steps required to do this are: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Load the FAISS database we created and the instantiate the embedding","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Start using FAISS to do similarity searches. ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Serve makes this magically easy. Ray uses a âdeploymentâ to wrap a simple python class. The __init__ method does the loading and then __call__ is what actually does the work. Ray takes care of connecting it to the internet, bringing up a service, http and so on. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hereâs a simplified version of the code: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7GwywaSkEi9NnwUhk3wMMD","type":"Entry","createdAt":"2023-04-17T08:24:42.996Z","updatedAt":"2023-04-17T08:24:42.996Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-8","body":"@serve.deployment\nclass VectorSearchDeployment:\n    def __init__(self):\n        self.embeddings = â¦ \n        self.db = FAISS.load_local(FAISS_INDEX_PATH, self.embeddings)\n\n    def search(self,query): \n        results = self.db.max_marginal_relevance_search(query)\n        retval = \u003csome string processing of the results\u003e\n        return retval\n\n    async def __call__(self, request: Request) -\u003e List[str]:\n        return self.search(request.query_params[\"query\"])\n\ndeployment = VectorSearchDeployment.bind()","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Thatâs it! ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs now start this service with the command line (of course Serve has more deployment options than this, but this is an easy way):","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"o0cCaJMv4yeC2ikcQuJhf","type":"Entry","createdAt":"2023-04-17T08:29:00.307Z","updatedAt":"2023-04-17T08:29:00.307Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-serve-snippet","body":"% serve run serve_vector_store:deployment"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now we can write a simple python script to query the service to get relevant vectors(itâs just a web server running on port 8000). ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3E0la6cbL8n7UPpvve36nJ","type":"Entry","createdAt":"2023-04-17T08:24:56.873Z","updatedAt":"2023-04-17T08:24:56.873Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-9","body":"import requests\nimport sys\nquery = sys.argv[1]\nresponse = requests.post(f'http://localhost:8000/?query={query}')\nprint(response.content.decode())","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"And now letâs try it out: ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SL7auZuJzHEyaq8fVIifv","type":"Entry","createdAt":"2023-04-17T08:25:16.327Z","updatedAt":"2023-04-17T21:14:41.421Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"langchain-snippet-10","body":"$ python query.py 'Does Ray Serve support batching?'\nFrom http://docs.ray.io/en/master/serve/performance.html\n\nYou can check out our microbenchmark instructions\nto benchmark Ray Serve on your hardware.\nRequest Batching#\n====\n\nFrom http://docs.ray.io/en/master/serve/performance.html\n\nYou can enable batching by using the ray.serve.batch decorator. Letâs take a look at a simple example by modifying the MyModel class to accept a batch.\nfrom ray import serve\nimport ray\n@serve.deployment\nclass Model:\n    def __call__(self, single_sample: int) -\u003e int:\n        return single_sample * 2\n====\n\nFrom http://docs.ray.io/en/master/ray-air/api/doc/ray.train.lightgbm.LightGBMPredictor.preferred_batch_format.html\n\nnative batch format.\nDeveloperAPI: This API may change across minor Ray releases.\n====\n\nFrom http://docs.ray.io/en/master/serve/performance.html\n\nMachine Learning (ML) frameworks such as Tensorflow, PyTorch, and Scikit-Learn support evaluating multiple samples at the same time.\nRay Serve allows you to take advantage of this feature via dynamic request batching.\n===="}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"We showed in the above code just how easy it is to build key components of an LLM-based search engine and serve its responses to the entire world by combining the power of LangChain and Ray Serve. And we didnât have to deal with a single pesky API key!Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Tune in for Part 2, where we will show how to turn this into a chatgpt-like answering system. Weâll use open source LLMs like Dolly 2.0 to do that.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"And finally weâll share Part 3 where weâll talk about scalability and cost. The above is fine for a few hundred queries per second, but what if you need to scale to a lot more? And are the claims about latency correct? ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next Steps","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"See part 2 ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/turbocharge-langchain-now-guide-to-20x-faster-embedding"},"content":[{"data":{},"marks":[],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nReview the code and data used in this blog in the following ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/langchain-ray/tree/main/open_source_LLM_search_engine"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Github repo","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". \n\nSee our earlier ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"},"content":[{"data":{},"marks":[],"value":"blog series on solving Generative AI infrastructure","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" with Ray.\n\nIf you are interested in learning more about Ray, see ","nodeType":"text"},{"data":{"uri":"http://ray.io"},"content":[{"data":{},"marks":[],"value":"Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"http://docs.ray.io"},"content":[{"data":{},"marks":[],"value":"Docs.Ray.io","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To connect with the Ray community join #LLM on the ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"data":{},"marks":[],"value":"Ray Slack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or our ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[],"value":"Discuss forum","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".\n\nIf you are interested in our Ray hosted service for ML Training and Serving, see ","nodeType":"text"},{"data":{"uri":"http://www.anyscale.com/platform"},"content":[{"data":{},"marks":[],"value":"Anyscale.com/Platform","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and click the 'Try it now' button","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Ray Summit 2023:","nodeType":"text"},{"data":{},"marks":[],"value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","nodeType":"text"},{"data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Summit","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"practical training focused on LLMs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4OzISThpksdKgjZ0gVJUiB","type":"Asset","createdAt":"2023-04-19T16:59:49.253Z","updatedAt":"2023-05-01T09:40:14.125Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"doc-index-starter-langchain","description":"Build a document index easily with Ray and Langchain","file":{"url":"//images.ctfassets.net/xjan103pcp94/4OzISThpksdKgjZ0gVJUiB/85bb7fccdfef1df3d061c57e9af1062a/index-langchain.jpg","details":{"size":28918,"image":{"width":900,"height":381}},"fileName":"index-langchain.jpg","contentType":"image/jpeg"}}},"mainImageFit":"contain","showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7bptc6mEv7bhFZvq5AOXqc","type":"Entry","createdAt":"2023-04-18T20:36:43.249Z","updatedAt":"2023-04-19T16:00:08.646Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Why I Joined Anyscale: Solving Cutting-Edge Problems in a Time of Enormous Change","seoTitle":"why i joined anyscale by sidney rabsatt","slug":"why-i-joined-anyscale-solving-cutting-edge-problems-in-a-time-of-enormous","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"461P1q4TML68SzYxOG9sxm","type":"Entry","createdAt":"2023-04-18T20:05:20.525Z","updatedAt":"2023-04-18T20:05:20.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Sidney Rabsatt","slug":"sidney-rabsatt","link":"https://www.linkedin.com/in/sidney-rabsatt/"}}],"publishedDate":"2023-04-19","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42F9iHs5a05rRrOuqEQwvB","type":"Entry","createdAt":"2022-06-15T01:30:47.322Z","updatedAt":"2022-06-15T01:30:47.322Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Culture","slug":"culture","tintColor":"#39C8E5"}},"intro":"Why Sidney Rabsatt joined Anyscale as Head of Product.","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Throughout my career, I've had the opportunity to work on cutting-edge problems across some of the most impactful technological transitions of our time. From the early days of the World Wide Web to the rise of Mobile and Cloud computing, Iâve worked on numerous commercial products across Networking, Observability, and App/Cloud Infrastructure and on some of the most widely-adopted open-source projects including Wireshark, Nginx, and now Ray. I've been deeply involved in resolving the complexities and, at times, unexpected infrastructure obstacles that technologists encounter, while empowering companies to fully benefit from these advancements.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7uPFFDyMQg5GZ5FU5kM8rw","type":"Asset","createdAt":"2023-04-18T20:34:23.318Z","updatedAt":"2023-04-18T20:34:23.318Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Sidney Rabsatt Headshot","description":"Picture of Sidney Rabsatt, Anyscale Head of Product","file":{"url":"//images.ctfassets.net/xjan103pcp94/7uPFFDyMQg5GZ5FU5kM8rw/225783b86665705f9c198768754f8722/Sidney_pic.jpeg","details":{"size":501555,"image":{"width":3540,"height":2832}},"fileName":"Sidney pic.jpeg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"AI","nodeType":"text"},{"data":{},"marks":[],"value":" promises to be the most complex transition across the most dimensions that affect our lives. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"I joined Anyscale to embrace and be part of solving those challenges. I joined to make AI available to all organizations, to give people better tools, and do it responsibly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray and Anyscale work at the core of the best known and most advanced AI applications such as Generative AI with OpenAIâs ChatGPT and Prediction for Uber rides / ETAs, not to mention what Spotify, Instacart and others are doing.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The team is one of the strongest in the industry, comprised of AI/ML PhDâs and experienced AI experts from top schools like UC Berkeley and companies like Uber, Google, Amazon, Meta, and more!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Our technology is proven and continues to evolve rapidly with strong community involvement. And the opportunity is vast to define how best to develop and run AI.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"AI will touch and shape our lives in many ways and joining Anyscale to lead Product is how Iâm getting involved.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\n","nodeType":"text"},{"data":{},"marks":[],"value":"\n","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7uPFFDyMQg5GZ5FU5kM8rw","type":"Asset","createdAt":"2023-04-18T20:34:23.318Z","updatedAt":"2023-04-18T20:34:23.318Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Sidney Rabsatt Headshot","description":"Picture of Sidney Rabsatt, Anyscale Head of Product","file":{"url":"//images.ctfassets.net/xjan103pcp94/7uPFFDyMQg5GZ5FU5kM8rw/225783b86665705f9c198768754f8722/Sidney_pic.jpeg","details":{"size":501555,"image":{"width":3540,"height":2832}},"fileName":"Sidney pic.jpeg","contentType":"image/jpeg"}}},"mainImageFit":"contain","hideIntro":true,"showMainImage":false}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7e5A9XUzg0NfneWobPOKxf","type":"Entry","createdAt":"2023-04-10T23:01:28.472Z","updatedAt":"2023-05-31T18:31:32.938Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":12,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"How to fine tune and serve LLMs simply, quickly and cost effectively using Ray + DeepSpeed + HuggingFace","seoTitle":"How to Fine-Tune a 6 Billion Parameter LLM for Less Than $7","slug":"how-to-fine-tune-and-serve-llms","description":"In part 4 of our Generative AI series, we share how to build a system for fine-tuning \u0026 serving LLMs in 40 minutes or less.","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5k10eJcA1I3PErsfEuJTUb","type":"Entry","createdAt":"2021-01-01T00:14:30.554Z","updatedAt":"2021-01-01T00:14:30.554Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Waleed Kadous","slug":"waleed-kadous","link":"https://www.linkedin.com/in/waleedkadous/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2yKvtARlwNkkIYOYLFgkhB","type":"Entry","createdAt":"2022-06-22T19:54:47.912Z","updatedAt":"2022-06-22T19:54:47.912Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Jun Gong","slug":"jun-gong"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BGRfsYFKUgIpGjaXTUYml","type":"Entry","createdAt":"2021-08-09T15:38:43.390Z","updatedAt":"2021-08-09T15:38:43.390Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Antoni Baum","slug":"antoni-baum","link":"https://www.linkedin.com/in/yard1/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}}],"publishedDate":"2023-04-10","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This is part 4 of our blog series on Generative AI. In the previous blog posts we explained:\n1.[Why Ray is a sound platform for Generative AI](https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure)\n2.[we showed how it can push the performance limits](https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray)\n3.[how you can use Ray for stable diffusion](https://www.anyscale.com/blog/faster-stable-diffusion-fine-tuning-with-ray-air). \n","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"In this blog, we share aÂ  practical approach on how you can use the combination of HuggingFace, DeepSpeed, and Ray to build a system for fine-tuning and serving LLMs, in 40 minutes for less than $7 for a 6 billion parameter model. In particular, we illustrate the following:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Using these three components, you can simply and quickly put together an open-source LLM fine-tuning and serving system.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"By taking advantage of Rayâs distributed capabilities, we show how this can be both more cost-effective and faster than using a single large (and often ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://aiascendant.substack.com/p/taiwan-is-pandora-gpus-are-unobtainium"},"content":[{"nodeType":"text","value":"unobtainable","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":") machine.","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Hereâs what weâll be doing:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Discussing why you might want to run your own LLM instead of using one of the new API providers.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing you the evolving tech stack we are seeing for cost-effective LLM fine-tuning and serving, combining HuggingFace, DeepSpeed, Pytorch, and Ray.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing you 40 lines of Python code that can enable you to serve a 6 billion parameter GPT-J model.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing you, for less than $7, how you can fine-tune the model to sound more medieval using the works of Shakespeare by doing it in a distributed fashion on low-cost machines, which is considerably more cost-effective than using a single large powerful machine.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing how you can serve the fine-tuned 6B LLM compiled model binary.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Showing how the fine-tuned model compares to a prompt engineering approach with large systems. ","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Why would I want to run my own LLM?Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"There are ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://openai.com"},"content":[{"nodeType":"text","value":"many","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://anthropic.com/"},"content":[{"nodeType":"text","value":"many","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://cohere.ai/"},"content":[{"nodeType":"text","value":"providers","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" of LLM APIs online. Why would you want to run your own? There are a few reasons:Â ","marks":[],"data":{}}]},{"nodeType":"ordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Cost, especially for fine-tuned inference","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":": For example, OpenAI charges 12c per 1000 tokens (about 700 words) for a fine-tuned model on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://openai.com/pricing"},"content":[{"nodeType":"text","value":"Davinci","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":". Itâs important to remember that many user interactions require multiple backend calls (e.g. one to help with the prompt generation, post-generation moderation, etc), so itâs very possible that a single interaction with an end user could cost a few dollars. For many applications, this is cost prohibitive.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Latency: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"using these LLMs is especially slow. A GPT-3.5 query for example can take up to 30 seconds. Combine a few round trips from your data center to theirs and it is possible for a query to take minutes. Again, this makes many applications impossible. Bringing the processing in-house allows you to optimize the stack for your application, e.g. by using low-resolution models, tightly packing queries to GPUs, and so on. We have heard from users that optimizing their workflow has often resulted in a 5x or more latency improvement.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Data Security \u0026 Privacy: ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"In order to get the response from these APIs, you have to send them a lot of data for many applications (e.g. send a few snippets of internal documents and ask the system to summarize them). Many of the API providers reserve the right to use those instances for retraining. Given the sensitivity of organizational data and also frequent legal constraints like data residency, this is especially limiting. One, particularly concerning recent development, is the ability to ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://arxiv.org/pdf/2301.13188.pdf"},"content":[{"nodeType":"text","value":"regenerate training data from learned models","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", and people ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.techradar.com/news/samsung-workers-leaked-company-secrets-by-using-chatgpt"},"content":[{"nodeType":"text","value":"unintentionally disclosing secret information","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"How to create and run your own LLMÂ ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The LLM space is an incredibly fast-moving space, and it is currently evolving very rapidly. What we are seeing is a particular technology stack that combines multiple technologies: ","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70GhsrIVrh0Qz0fNDUp4A8","type":"Asset","createdAt":"2023-04-07T14:17:45.223Z","updatedAt":"2023-04-07T14:17:45.223Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"llm-stack","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/70GhsrIVrh0Qz0fNDUp4A8/458e076a02c4745369c683851c378536/Screenshot_2023-04-07_at_10.17.23_AM.png","details":{"size":144021,"image":{"width":1073,"height":663}},"fileName":"Screenshot 2023-04-07 at 10.17.23 AM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"What weâve also seen is a reluctance to go beyond a single machine for training. In part, because there is a perception that moving to multiple machines is seen as complicated. The good news is this is where ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io"},"content":[{"nodeType":"text","value":"Ray.io","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" shines (ba-dum-tish). It simplifies cross-machine coordination and orchestration aspects using not much more than Python and Ray decorators, but also is a great framework for composing this entire stack together.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\nRecent results on ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html"},"content":[{"nodeType":"text","value":"Dolly","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/lm-sys/FastChat"},"content":[{"nodeType":"text","value":"Vicuna","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" (both trained on Ray or trained on models built with Ray like ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://huggingface.co/EleutherAI/gpt-j-6b"},"content":[{"nodeType":"text","value":"GPT-J","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":") are small LLMs (relatively speaking â say the open source model GPT-J-6B with 6 billion parameters) that can be incredibly powerful when fine-tuned on the right data. The key is fine-tuning and the right data parts. So you do not always need to use the latest and greatest model with 150 billion-plus parameters to get useful results. Letâs get started! \n","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Serving a pre-existing model with Ray for text generation","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The detailed steps on how to serve the GPT-J model with Ray can be found ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/gptj_serving.html"},"content":[{"nodeType":"text","value":"here","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":", so letâs highlight some of the aspects of how we do that. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"56imF50uP87DrWXWfjxNqV","type":"Entry","createdAt":"2023-04-07T18:37:36.999Z","updatedAt":"2023-04-07T18:47:21.234Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"serve-llm","body":"Â Â Â @serve.deployment(ray_actor_options={\"num_gpus\":1})\nÂ Â Â classPredictDeployment:\nÂ Â Â Â Â def__init__(self, model_id:str, revision:str=None):\nÂ Â Â Â Â Â Â Â from transformers import AutoModelForCausalLM, AutoTokenizer\nÂ Â Â Â Â Â Â Â import torch\nÂ Â Â Â Â Â Â Â self.model = AutoModelForCausalLM.from_pretrained(\nÂ Â Â Â Â Â Â Â Â Â Â Â \"EleutherAI/gpt-j-6B\",\nÂ Â Â Â Â Â Â Â Â Â Â Â revision=revision,\nÂ Â Â Â Â Â Â Â Â Â Â Â torch_dtype=torch.float16,\nÂ Â Â Â Â Â Â Â Â Â Â Â low_cpu_mem_usage=True,\nÂ Â Â Â Â Â Â Â Â Â Â Â device_map=\"auto\",Â  # automatically makes use of all GPUs available to the Actor\nÂ Â Â Â Â Â Â Â )\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Serving in Ray happens in actors, in this case, one called PredictDeployment. This code shows the __init__ method of the action that downloads the model from Hugging Face. To launch the model on the current node, we simply do: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7jOn5zhnrYvHQZg3fqmwjE","type":"Entry","createdAt":"2023-04-07T18:39:26.501Z","updatedAt":"2023-04-07T18:47:32.494Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"serve-cmd","body":"deployment = PredictDeployment.bind(model_id=model_id, revision=revision)\nserve.run(deployment)\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"That starts a service on port 8000 of the local machine.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We can now query that service using a few lines of Python","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6HQg2dtvaXrNsKuUvSIX3S","type":"Entry","createdAt":"2023-04-07T18:41:24.773Z","updatedAt":"2023-04-07T18:47:42.594Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"serve-query","body":"import requests\nprompt = (\n    âOnce upon a time, there was a horse. â\n)\nsample_input = {\"text\": prompt}\noutput = requests.post(\"http://localhost:8000/\", json=[sample_input]).json()\nprint(output)","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"And sure enough, this prints out a continuation of the above opening. Each time it runs, there is something slightly different.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\"Once upon a time, there was a horse.","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"But this particular horse was too big to be put into a normal stall. Instead, the animal was moved into an indoor pasture, where it could take a few hours at a time out of the stall. The problem was that this pasture was so roomy that the horse would often get a little bored being stuck inside. The pasture also didnât have a roof, and so it was a good place for snow to accumulate.\"","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is certainly an interesting direction and story â¦ but now we want to set it in the medieval era. What can we do? ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Fine Tuning Your LLM","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now that weâve shown how to serve a model, how do we fine-tune it to be more medieval? What about if we train it on 2500 lines from Shakespeare?Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is where ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.deepspeed.ai/"},"content":[{"nodeType":"text","value":"DeepSpeed","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" comes in. DeepSpeed is a set of optimized algorithms for training and fine-tuning networks. The problem is that DeepSpeed doesnât have an orchestration layer. This is not so much of a problem on a single machine, but if you want to use multiple machines, this typically involves a bunch of bespoke ssh commands, complex managed keys, and so on.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is where Ray can help.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html"},"content":[{"nodeType":"text","value":"page","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" in the Ray documentation discusses how to fine-tune it to sound more like something from the 15th century with a bit of flair.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Letâs go through the key parts. First, we load the data from hugging face","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12gknw4CuwTUwzyVwR5MEH","type":"Entry","createdAt":"2023-04-07T18:42:15.242Z","updatedAt":"2023-04-07T18:47:51.606Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hf-load-data","body":"from datasets import load_dataset\nprint(\"Loading tiny_shakespeare dataset\")\ncurrent_dataset = load_dataset(\"tiny_shakespeare\")\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Skipping the tokenization code, hereâs the heart of the code that we will run for each worker.Â ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5633wVi5PVmd43ciiJRLrv","type":"Entry","createdAt":"2023-04-07T18:43:13.431Z","updatedAt":"2023-04-07T18:48:00.199Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hf-trainer","body":"def trainer_init_per_worker(train_dataset, eval_dataset=None,**config):\nÂ Â Â Â # Use the actual number of CPUs assigned by Ray\nÂ Â Â Â model = GPTJForCausalLM.from_pretrained(model_name, use_cache=False)\nÂ Â Â Â model.resize_token_embeddings(len(tokenizer))\nÂ Â Â Â enable_progress_bar()\nÂ Â Â Â metric = evaluate.load(\"accuracy\")\nÂ Â Â Â trainer = Trainer(\nÂ Â Â Â Â Â Â Â model=model,\nÂ Â Â Â Â Â Â Â args=training_args,\nÂ Â Â Â Â Â Â Â train_dataset=train_dataset,\nÂ Â Â Â Â Â Â Â eval_dataset=eval_dataset,\nÂ Â Â Â Â Â Â Â compute_metrics=compute_metrics,\nÂ Â Â Â Â Â Â Â tokenizer=tokenizer,\nÂ Â Â Â Â Â Â Â data_collator=default_data_collator,\nÂ Â Â Â )\nÂ Â Â Â return trainer\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"\nAnd now we create a Ray AIR HuggingFaceTrainer that orchestrates the distributed run and wraps around multiple copies of the training loop above: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"Qo7c20UkHRTIAgy2zvQ4G","type":"Entry","createdAt":"2023-04-07T18:43:58.073Z","updatedAt":"2023-04-07T18:48:09.302Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"hf-trainer2","body":"trainer = HuggingFaceTrainer(\nÂ Â Â Â trainer_init_per_worker=trainer_init_per_worker,\nÂ Â Â Â trainer_init_config={\nÂ Â Â Â Â Â Â Â \"batch_size\":16,Â  # per device\nÂ Â Â Â Â Â Â Â \"epochs\":1,\nÂ Â Â Â },\nÂ Â Â Â scaling_config=ScalingConfig(\nÂ Â Â Â Â Â Â Â num_workers=num_workers,\nÂ Â Â Â Â Â Â Â use_gpu=use_gpu,\nÂ Â Â Â Â Â Â Â resources_per_worker={\"GPU\":1,\"CPU\": cpus_per_worker},\nÂ Â Â Â ),\nÂ Â Â Â datasets={\"train\": ray_datasets[\"train\"],\"evaluation\": ray_datasets[\"validation\"]},\nÂ Â Â Â preprocessor=Chain(splitter, tokenizer),\n)\nresults = trainer.fit()\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"While there is some complexity here, it is not much more complex than the code to get it to run on a simple machine. ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Fine-tuning and Performance","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"One of the most important topics related to LLMs is the question of cost. In this particular case, the costs are small (in part because we ran only one epoch of fine-tuning, depending on the problem 1-10 epochs of fine-tuning are used, and also in part because this dataset is not so large). But running the tests on different configurations shows us that understanding performance is not always easy. The below shows some benchmarking results with different configurations of machines on AWS. ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3k8l58AKMayxIzIhrO7Btr","type":"Entry","createdAt":"2023-04-07T14:39:44.461Z","updatedAt":"2023-04-07T14:39:44.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"fine-tune-performance","body":"\n| Configuration| #instances| Time (mins)| Total Cost (on-demand)|Total Cost (spot)| Cost Ratio|\n| ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n| 16 x g4dn.4xlarge (1 x T4 16GB GPU)|16|48|$15.41|__$6.17__|100%|\n| 32 x g4dn.4xlarge (1 x T4 16GB GPU)|32|__30__|$19.26|$7.71|125%|\n| 1 x p3.16xlarge (8 x V100 16GB GPU)|1|44|$17.95|$9.27|150%|\n| 1 x g5.48xlarge (8 x A10G 24GB GPU)|1|84|$22.81|$10.98|178%|\n"}}},"content":[]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3C7MbWRZ8oYJoE00IW4mIi","type":"Asset","createdAt":"2023-04-11T16:45:53.578Z","updatedAt":"2023-04-11T16:45:53.578Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"llm-graph","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/3C7MbWRZ8oYJoE00IW4mIi/93f686b238a84d2ef64abe3aa7670791/Screenshot_2023-04-11_at_12.44.46_PM.png","details":{"size":76511,"image":{"width":1047,"height":644}},"fileName":"Screenshot 2023-04-11 at 12.44.46 PM.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Note:","marks":[{"type":"underline"}],"data":{}},{"nodeType":"text","value":" we tried to run the same test with A100s, but we were unable to obtain the p4d machines to do so.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Looking at these numbers, we see some surprises:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Perhaps the most obvious machine to use â the g5.48xlarge â the machine with the highest on-paper performance â is both the most expensive and the slowest at almost twice the price when using spot instances.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The p3.16xlarge with its use of NVLink between the GPUs is a considerably better option.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Most surprising of all, using multiple machines is both the ","marks":[],"data":{}},{"nodeType":"text","value":"cheapest ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"and the ","marks":[],"data":{}},{"nodeType":"text","value":"fastest ","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":"option.Â ","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The exact same code is running on all the machines, and aside from tweaking the number of GPU workers, nothing else was changed. Using multiple machines gave us the cheapest (16 machines) and the fastest (32 machines) option of the ones we benchmarked.Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This is the beauty and power of Ray. The code itself was simple enough, and in fact, was able to use a standard library âÂ  DeepSpeed â with no modifications. So it was no more complex in this case than a single machine. Simultaneously, it gave more options and flexibility to optimize to be both cheaper and faster than a single machine.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Closing the loop: Serving the fine-tuned model","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Now that we have a fine-tuned model, letâs try to serve it. The only change we need to make is to (a) copy the model to s3 from the fine-tuning process and (b) load it from there. In other words, the only change from the previous code we started with originally is: ","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"202WfToEkr4LafSdvnynbx","type":"Entry","createdAt":"2023-04-07T18:45:09.318Z","updatedAt":"2023-04-07T18:48:20.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"load-model","body":"        checkpoint = Checkpoint.from_uri(\n             \"s3://demo-pretrained-model/gpt-j-6b-shakespeare\"\n        )\n        with checkpoint.as_directory() as dir:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                dir,\n                torch_dtype=torch.float16,\n                low_cpu_mem_usage=True,\n                device_map=\"auto\")\n            self.tokenizer = AutoTokenizer.from_pretrained(dir)\n","language":"Python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"And now letâs try querying it again:Â ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Once upon a time there was a horse. This horse was in my youth, a little unruly, but yet the best of all. I have, sir; I know every horse in the field, and the best that I have known is the dead. And now I thank the gods, and take my leave.","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"As you can see, it definitely has more of a Shakespearean flavor.Â ","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Conclusion","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"We have shown a new tech stack that combines Ray, HuggingFace, DeepSpeed, and PyTorch to make a system that:Â ","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Makes it simple and quick to deploy as a service.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Can be used to cost-effectively fine-tune and is actually most cost-effective when using multiple machines without the complexity.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"How fine-tuning â even a single epoch â can change the output of a trained model.Â ","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Deploying a fine-tuned model is only marginally harder than deploying a standard one.Â ","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Next Steps","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If you want to use LangChain + Ray to serve LLM's, see our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"},"content":[{"nodeType":"text","value":"LangChain blog series","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in learning more about Ray, see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://ray.io"},"content":[{"nodeType":"text","value":"Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":" and ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://docs.ray.io"},"content":[{"nodeType":"text","value":"Docs.Ray.io","marks":[],"data":{}}]},{"nodeType":"text","value":". ","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"To connect with the Ray community join #LLM on the ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"Ray Slack","marks":[],"data":{}}]},{"nodeType":"text","value":" or our ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"Discuss forum","marks":[],"data":{}}]},{"nodeType":"text","value":".\n\nIf you are interested in our Ray hosted service for ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/model-serving"},"content":[{"nodeType":"text","value":"ML Training and Serving","marks":[],"data":{}}]},{"nodeType":"text","value":", see ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"http://www.anyscale.com/platform"},"content":[{"nodeType":"text","value":"Anyscale.com/Platform","marks":[],"data":{}}]},{"nodeType":"text","value":" and click the 'Try it now' button","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Summit 2023:","marks":[{"type":"bold"}],"data":{}},{"nodeType":"text","value":" If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://raysummit.anyscale.com/"},"content":[{"nodeType":"text","value":"Ray Summit","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":" on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"},"content":[{"nodeType":"text","value":"practical training focused on LLMs","marks":[{"type":"underline"}],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"19zr72hDLSKFt8vQMz3hb6","type":"Asset","createdAt":"2023-04-11T00:38:37.097Z","updatedAt":"2023-04-11T00:38:37.097Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"fine-tune-stack","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/19zr72hDLSKFt8vQMz3hb6/fd9f6b83a9fe5b66456ae54ecf9bb04d/fine-tune-stack.png","details":{"size":344489,"image":{"width":1716,"height":1180}},"fileName":"fine-tune-stack.png","contentType":"image/png"}}}}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"70ZthWUkgA42DqmZ1GVmuM","type":"Entry","createdAt":"2022-06-15T16:41:59.066Z","updatedAt":"2022-06-15T16:43:47.038Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"section"}},"locale":"en-US"},"fields":{"identifier":"blog-types-tags","body":"This section is used to order the \"Types\" and \"Tags\" that show up for filters on the Blog Index","subsections":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"56lORqEsxxZXgpuGzAhJBC","type":"Entry","createdAt":"2022-06-15T16:42:23.797Z","updatedAt":"2022-06-15T16:44:24.730Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"list"}},"locale":"en-US"},"fields":{"title":"Types","identifier":"blog-type-filters","items":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42F9iHs5a05rRrOuqEQwvB","type":"Entry","createdAt":"2022-06-15T01:30:47.322Z","updatedAt":"2022-06-15T01:30:47.322Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Culture","slug":"culture","tintColor":"#39C8E5"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KeH0JzrIEpuheazhMAhKf","type":"Entry","createdAt":"2022-06-15T01:31:23.950Z","updatedAt":"2022-06-15T01:31:23.950Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"User Story","slug":"user-story","tintColor":"#6D59C6"}}]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1fDHWgr5HgjPURy6aaDlnB","type":"Entry","createdAt":"2022-06-15T16:42:41.243Z","updatedAt":"2022-06-22T15:37:31.744Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"list"}},"locale":"en-US"},"fields":{"title":"Products / Libraries","identifier":"blog-tag-filters","items":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i6fp6Gfdy5wFeDDOaKcfh","type":"Entry","createdAt":"2021-12-03T22:33:42.248Z","updatedAt":"2021-12-03T22:33:42.248Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Anyscale","identifier":"anyscale"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}}]}}]}}],"recommendations":[],"articles":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"24GtFGUCPeM44gdlGTnhJ2","type":"Entry","createdAt":"2022-01-25T21:01:15.754Z","updatedAt":"2022-06-22T16:03:03.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":10,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Distributed deep learning with Ray Train is now in Beta","slug":"distributed-deep-learning-with-ray-train-is-now-in-beta","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"9prabYTxr4y4oV4rUkzaP","type":"Entry","createdAt":"2022-01-25T05:14:31.358Z","updatedAt":"2022-01-25T05:14:31.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Matthew Deng","slug":"matthew-deng"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DwU0wh6AmJoGv8JOy6n0Q","type":"Entry","createdAt":"2020-11-11T23:34:15.165Z","updatedAt":"2021-02-05T22:22:08.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","slug":"amog-kamsetty","link":"https://www.linkedin.com/in/amogkamsetty/","bio":"Amog is a software engineer at Anyscale. He helps maintain and develop two machine learning libraries: Ray Tune for distributed hyperparameter tuning and RaySGD for distributed deep learning training.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J134jX2ybZRLPfcAIGLTI","type":"Asset","createdAt":"2020-11-13T00:05:05.533Z","updatedAt":"2020-11-13T00:05:05.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J134jX2ybZRLPfcAIGLTI/12589bb0c6792051050ebc6e1e83af25/Amog_Kamsetty_.jpg","details":{"size":57660,"image":{"width":480,"height":640}},"fileName":"Amog Kamsetty .jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7fm8mbdJEOBiNMuOJKqLJj","type":"Entry","createdAt":"2021-08-09T15:40:10.306Z","updatedAt":"2021-08-09T15:40:10.306Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Will Drevo","slug":"will-drevo","link":"https://www.linkedin.com/in/willdrevo/"}}],"publishedDate":"2022-01-25","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},"intro":"Introducing Ray Train, an easy-to-use library for distributed deep learning. In this post, we show how Ray Train improves developer velocity, is production-ready, and comes with batteries included.","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7kS5Mc1LeZRPqVlItSGMNI","type":"Asset","createdAt":"2022-01-25T05:17:10.580Z","updatedAt":"2022-06-06T23:16:27.147Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"locale":"en-US"},"fields":{"title":"Ray Train","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7kS5Mc1LeZRPqVlItSGMNI/28c234fac2d658dbca32e136b2030396/1382642_BlogImageIllustration-11_2_060322.jpg","details":{"size":545837,"image":{"width":1500,"height":1000}},"fileName":"1382642_BlogImageIllustration-11_2_060322.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"italic"}],"value":"Ray is simplifying the APIs of its ML ecosystem as it heads towards Ray 2.0. This blog announces a core feature, distributed deep learning, as part of a broader series of changes to the Ray ML ecosystem.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Todayâs distributed deep learning tools suffer from a major problem: there exists a wide gap between prototyping and production model training.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Existing solutions such as Kubeflow and Sagemaker force practitioners to make a tradeoff between developer velocity and scalability.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To address this gap, we built ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a library that simplifies distributed training. Currently in its Beta release, it offers the following features:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Scale to multi-GPU and multi-node training with 0 code changes","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Runs seamlessly on any cloud (AWS, GCP, Azure, Kubernetes, or on-prem)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Supports PyTorch, TensorFlow, and HorovodÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed data loading and hyperparameter tuning","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Built-in loggers for TensorBoard and MLflow","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The upcoming roadmap for Ray Train can be found ","nodeType":"text"},{"data":{"uri":"#next-steps"},"content":[{"data":{},"marks":[],"value":"below","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"\nIn this post, we will introduce some of the benefits and values that Ray Train provides for distributed deep learning training today. We will showcase examples of using Ray Train with ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", but they can be adapted to work with ","nodeType":"text"},{"data":{"uri":"https://www.tensorflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"TensorFlow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://github.com/horovod/horovod"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Horovod","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" as well.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Background: Why Ray Train?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Existing solutions for distributed training often fall on either side of the wide gap between prototyping and production model training. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The prototyping side is populated with frameworks and libraries that are lightweight, focusing on development velocity and fast iteration, such as Huggingface Transformers and PyTorch Lightning. These frameworks are targeted towards data scientists -- and leave the burden of cluster management and operations to the MLOps practitioner, who has to manage these frameworks at scale.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"On the other side are heavyweight production frameworks like ","nodeType":"text"},{"data":{"uri":"https://www.kubeflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Kubeflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/sagemaker/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". The Kubeflow Training Operator provides a solution for distributed training on Kubernetes, but it lacks ease of use for development. Debugging a distributed training environment often requires relaunching the entire job and waiting for multiple minutes to see results.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We wanted to build a framework that could bring the best of both worlds together -- extremely fast iteration while making it really easy to scale on different cluster environments.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We built Ray Train with the following requirements in mind:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Developer velocity:","nodeType":"text"},{"data":{},"marks":[],"value":" Reduce the friction to go from training on your laptop to training on any distributed cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Production-ready:","nodeType":"text"},{"data":{},"marks":[],"value":" Run end-to-end distributed model training with first class support for cloud compute and experiment monitoring.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Batteries included: ","nodeType":"text"},{"data":{},"marks":[],"value":"Capable of integrating with third party libraries and comes with powerful built-in integrations for large-scale data loading and distributed hyperparameter optimization.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Developer Velocity","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The number one request we heard among developers and data scientists was to reduce the iterative cycle of development from a local or Jupyter environment to a production environment.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Existing solutions will often make you wait for instances to start up in order to run your training script every time you make a code change or can be very complicated to integrate.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train is fast to integrate, easy to scale, and allows you to iterate very quickly.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train is fast to integrate","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Quickly distribute your existing PyTorch, TensorFlow, or Horovod code with five simple steps.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For example, an existing PyTorch training script can be converted to run across ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"},{"type":"code"}],"value":"num_workers=4","nodeType":"text"},{"data":{},"marks":[],"value":" worker processes with the following changes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5qzveTSs9ZzECtRFB9bJe2","type":"Entry","createdAt":"2022-01-25T05:35:18.795Z","updatedAt":"2022-01-25T20:22:23.215Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"num_workers","body":"def train_func(): # 1. Wrap code in a function.\n    â¦ # Existing model and dataloader setup.\n    model = train.torch.prepare_model(model)  # 2. Sets up data parallelism.\n    dataloader = train.torch.prepare_data_loader(dataloader) # 2. Samples data.\n    for _ in range(num_epochs):\n        â¦ # Existing training loop.\n\nnum_workers = 4 # 3. Define your parallelization factor. \ntrainer = Trainer(backend=\"torch\", num_workers=num_workers) # 3. Initialize Trainer.\ntrainer.start() # 3. Setup worker processes.\nresults = trainer.run(train_func) # 4. Run training function.\ntrainer.shutdown() # 5. Tear down worker processes.","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Whatâs happening with these code changes?Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Wrap your code in a function: ","nodeType":"text"},{"data":{},"marks":[],"value":"Ray Train packages and sends this function to run on each worker process.Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Use Ray Train âprepareâ utility functions: ","nodeType":"text"},{"data":{},"marks":[],"value":"These set up data parallelism when run on multiple workers (under the hood, this uses ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Distributed Data Parallelism","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":").","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Instantiate Ray Train trainer:","nodeType":"text"},{"data":{},"marks":[],"value":" Define your training backend and number of workers. Worker processes are started and the backend communication framework (e.g., Torch process group) is set up.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Run training function: ","nodeType":"text"},{"data":{},"marks":[],"value":"The training function is executed on each worker and results are communicated back to the Trainer.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Tear down work processes: ","nodeType":"text"},{"data":{},"marks":[],"value":"Clean up processes and release resources.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"To see this in action, see ","nodeType":"text"},{"data":{"uri":"https://www.ray.io/ray-sgd"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for a simple example you can run directly on your laptop.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2ItHq3Ulyi5tawW4lpKGwx","type":"Entry","createdAt":"2022-01-25T20:37:50.063Z","updatedAt":"2022-01-25T20:37:50.063Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"inlineImage"}},"locale":"en-US"},"fields":{"name":"Scale to multiple machines","id":"scale-to-multiple-machines"}}},"content":[],"nodeType":"embedded-entry-inline"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train is easy to scale","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train provides simple solutions for three common scaling patterns: distributing across multiple machines, training on GPUs, and running on a remote cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scale to multiple machinesÂ ","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray, scaling Ray Train from your laptop to a multi-node setup is handled entirely by ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/quickstart.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"setting up your Ray cluster","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The same Ray Train script running locally can be run on a Ray cluster with multiple nodes without any additional modifications, just as if it were running on a single machine with more resources. You can further increase ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"num_workers","nodeType":"text"},{"data":{},"marks":[],"value":" to increase your training parallelism and utilize your cluster resources.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Training with multiple GPUs","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"GPU training can be toggled via the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/api.html#trainer"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Trainer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". If your Ray cluster has GPUs, you can turn on GPU training by enabling the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"use_gpu","nodeType":"text"},{"data":{},"marks":[],"value":" flag when initializing the Trainer. Each worker process will be associated with a single GPU (which may be on the same or different machines), and by default it will use ","nodeType":"text"},{"data":{"uri":"https://developer.nvidia.com/nccl"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"NCCL","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for communication.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Tying this together, to scale your workload across 100 GPUs, you would update your Trainer:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3pywnZ0m9jIkIpAnwS2paa","type":"Entry","createdAt":"2022-01-25T05:37:14.980Z","updatedAt":"2022-01-25T20:24:28.828Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"trainer","body":"trainer = Trainer(backend=âtorchâ, num_workers=100, use_gpu=True)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Run on a remote cluster","nodeType":"text"}],"nodeType":"heading-4"},{"data":{},"content":[{"data":{},"marks":[],"value":"Submit Ray Train jobs to a remote cluster from your laptop, another server, or a Jupyter notebook easily usingÂ  ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/ray-client.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Client","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (for interactive runs) or with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/job-submission.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Jobs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" (for production-ready runs).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train is quick to iterateÂ ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"While distributed training infrastructure solutions such as ","nodeType":"text"},{"data":{"uri":"https://aws.amazon.com/sagemaker/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"SageMaker","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://www.kubeflow.org/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Kubeflow","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" offer scaling, what really makes Ray Train stand out is its focus on developer productivity.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"When it comes to iterative development, it is invaluable to run code immediately after writing it.Â  Take a look at how long it takes to re-run a distributed training script after making a code change on two 8-GPU nodes.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2VnsApQWKvDMmo2yCfIkxO","type":"Asset","createdAt":"2022-01-25T05:38:40.412Z","updatedAt":"2022-01-25T17:55:09.509Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"locale":"en-US"},"fields":{"title":"Iteration time","description":"**Image:** Ray Train has minimal overhead for iterative development.","file":{"url":"//images.ctfassets.net/xjan103pcp94/2VnsApQWKvDMmo2yCfIkxO/ba09718eee51a427e4d5abadcfffb7cd/Iteration_time.png","details":{"size":13222,"image":{"width":512,"height":317}},"fileName":"Iteration time.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In this example, we chose to use a ","nodeType":"text"},{"data":{"uri":"https://sagemaker-examples.readthedocs.io/en/latest/training/distributed_training/pytorch/data_parallel/mnist/pytorch_smdataparallel_mnist_demo.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"familiar MNIST training script","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" which has a training loop that executes in one minute. The difference in iteration time occurs during the startup period between writing code and running code -Â  with Ray Train there is ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"zero ","nodeType":"text"},{"data":{},"marks":[],"value":"additional overhead in starting up instances and minimal time spent setting up the distributed processes, so you can start your training function within ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"seconds","nodeType":"text"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The reason for the difference in startup time is that in Ray, you are able to reuse a cluster when iterating. While there still exists a one-time cost of starting up the cluster, subsequent runs are much faster. So not only can you ","nodeType":"text"},{"data":{"uri":"#scale-to-multiple-machines"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"run the same code on your laptop as your cluster","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", but you can also iteratively test your code directly on a distributed cluster!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Production Ready","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"When it comes to moving a training job to production, there are many additional aspects to consider such as the cost of long-running jobs, the ability to run on specific clouds or Kubernetes, and having the right monitoring and experiment tracking functionality. Ray Train was built with these production-level requirements in mind.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Save compute costs with spot instances and fault toleranceÂ ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"When training large or long running jobs, the cost of compute instances can easily grow to significant amounts. Ray Train solves this by providing built-in fault tolerance which allows you to run distributed training workers on spot instances, reducing the cost of your workflow by up to 90%. When a spot instance is interrupted, Ray Train will restore the worker on a new instance and resume training.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7lch3KnediIlze2Qvbn1RL","type":"Asset","createdAt":"2022-01-25T05:41:49.043Z","updatedAt":"2022-01-25T17:55:31.213Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"locale":"en-US"},"fields":{"title":"timeline of fault tolerance ","description":"**Image:** Timeline of fault tolerance handling logic.","file":{"url":"//images.ctfassets.net/xjan103pcp94/7lch3KnediIlze2Qvbn1RL/28c3ae9bf5b21289bba28c9b414a6f85/image_2.jpg","details":{"size":50886,"image":{"width":960,"height":528}},"fileName":"image 2.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Fault tolerance can be enabled by implementing logic to ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/user_guide.html#checkpointing"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"save and load checkpoints","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Deploy anywhere: Multi-cloud and Kubernetes-ready","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Spending tons of time and developer resources to build out a ML training system that is tightly coupled to a proprietary cloud provider can limit flexibility in the future and/or lead to high cost of compute.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Developing on Ray with Ray Train allows teams to avoid cloud lock-in, and deploy their training or ML platforms with or without ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/kuberay"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Kubernetes anywhere,","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" including on-prem.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Monitor with integrated tools","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train has a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/api.html#trainingcallback"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"TrainingCallback","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" interface that can be used to process intermediate results (e.g., at the end of a training epoch). A few out-of-the-box callbacks are available, some of which integrate with your favorite monitoring tools.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1aEUHnH9e48UwVllKtNMFB","type":"Asset","createdAt":"2022-01-25T05:43:44.570Z","updatedAt":"2022-01-25T17:55:54.202Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"locale":"en-US"},"fields":{"title":"Callback","description":"**Image:** Using callbacks to integrate with different monitoring tools.","file":{"url":"//images.ctfassets.net/xjan103pcp94/1aEUHnH9e48UwVllKtNMFB/bd62c7b88410a130c61ac65adfaa8a21/image_3.jpg","details":{"size":24395,"image":{"width":960,"height":384}},"fileName":"image 3.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In the near future, we plan to add additional callbacks for other integrations. In the meantime, the user can also extend the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"TrainingCallback","nodeType":"text"},{"data":{},"marks":[],"value":" API to define their own custom callback logic!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Batteries Included","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"A distributed training framework should allow developers to be flexible in incorporating tools and utilities, but also should not require them to code vital functionality from scratch. We wanted to let developers leverage as much of the open-source data ecosystem as possible.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed Data Loading with Ray Datasets","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Large-scale training jobs simply cannot be cost-effective without efficient data loading. To solve this problem, Ray Train integrates with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dataset.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to perform distributed data loading. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5wnUyp19R8KFHBT0dNwlQ0","type":"Asset","createdAt":"2022-01-25T05:45:46.468Z","updatedAt":"2022-01-25T17:56:06.635Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"locale":"en-US"},"fields":{"title":"Ray dataset","description":"**Image:** Passing a Ray Dataset allows data to be sharded across the workers.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5wnUyp19R8KFHBT0dNwlQ0/4e908d0fa0b7f94d8bec87c8de327691/image_4.jpg","details":{"size":18858,"image":{"width":960,"height":288}},"fileName":"image 4.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This integration unlocks a number of features.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Sharded datasets:","nodeType":"text"},{"data":{},"marks":[],"value":" Only the fraction of the data that is needed by each worker will be transferred to and read by the worker, enabling you to train on large datasets that do not fit into a single nodeâs memory.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Windowed datasets:","nodeType":"text"},{"data":{},"marks":[],"value":" Only apply the chain of Dataset operations to a subset of the data at a time, limiting the working set size. This allows you to efficiently train on large datasets that do not fit into your clusterâs memory at the same time.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Pipelined execution","nodeType":"text"},{"data":{},"marks":[],"value":": When training on one batch of data, the next batch will be processed/loaded in parallel. This can reduce GPU idle time and decrease overall job training time.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Global shuffling: ","nodeType":"text"},{"data":{},"marks":[],"value":"Shuffling of the entire dataset between epochs to optimize training performance over no shuffling or local shuffling.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6e3xRK00DmcmW6LcwEqckr","type":"Asset","createdAt":"2022-01-25T05:47:13.103Z","updatedAt":"2022-01-25T17:56:22.060Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"Ray dataset 2","description":"**Image:** Ray Datasets provides an 8x improvement over other solutions.","file":{"url":"//images.ctfassets.net/xjan103pcp94/6e3xRK00DmcmW6LcwEqckr/906fad0ccdefc6fb75eeb6713686737a/Ray_train_ray_dataset.png","details":{"size":23147,"image":{"width":512,"height":303}},"fileName":"Ray train ray dataset.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"By pairing distributed training with distributed data, we see large improvements in the size, quality, and speed of large-scale ML data ingestion. For a more comprehensive walkthrough, see ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/deep-dive-data-ingest-in-a-third-generation-ml-architecture"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Deep Dive: Data Ingest in a Third Generation ML Architecture","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Hyperparameter Optimization with Ray Tune","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray Train provides an integration with Ray Tune that allows you to perform ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/what-is-hyperparameter-tuning"},"content":[{"data":{},"marks":[],"value":"hyperparameter optimization","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" in just a few lines of code.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2e900UukxFKzACy5Lt1Fsa","type":"Entry","createdAt":"2022-01-25T05:48:11.435Z","updatedAt":"2022-01-25T20:24:47.396Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"trainable","body":"trainable = trainer.to_tune_trainable(train_func)\nanalysis = tune.run(trainable, config=...)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Tune will create one Trial per hyperparameter configuration. In each Trial, a new Trainer will be initializedÂ  and run the training function with its generated configuration.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"SI7vjcdFwQ7RhRqXYHcaH","type":"Asset","createdAt":"2022-01-25T05:50:04.519Z","updatedAt":"2022-01-25T17:56:39.472Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"trainer trainable ","description":"**Image:** Using Ray Tune to conduct a distributed hyperparameter search.","file":{"url":"//images.ctfassets.net/xjan103pcp94/SI7vjcdFwQ7RhRqXYHcaH/8c6a449eff70340479d747835a17353e/image_5.jpg","details":{"size":20619,"image":{"width":960,"height":288}},"fileName":"image 5.jpg","contentType":"image/jpeg"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This native integration provides a few conveniences:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Minimal changes: ","nodeType":"text"},{"data":{},"marks":[],"value":"Training function utilities (","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"train.report()","nodeType":"text"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"train.checkpoint()","nodeType":"text"},{"data":{},"marks":[],"value":") are directly translated to their Tune equivalents (","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"tune.report()","nodeType":"text"},{"data":{},"marks":[],"value":", ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"tune.checkpoint()","nodeType":"text"},{"data":{},"marks":[],"value":").","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Resource management:","nodeType":"text"},{"data":{},"marks":[],"value":" Resource requirements are defined when initializing the Trainer, so you do not need to explicitly define any resource requests in your Tune experiment.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"For examples, see ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/examples.html#ray-tune-integration-examples"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Train Examples - Ray Tune Integration Examples","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Flexibility: use anything in the Python ecosystemÂ ","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Itâs important that a distributed training framework doesnât box you in or limit your flexibility.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ray is and always will be open-source, and will work with anything in the Python ecosystem.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Loading data with Dask? No problem! Run Dask with the performance of Ray with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/dask-on-ray.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Dask on Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"ETLs with Apache Spark? Again, this is common. Run Spark with the speed of Ray with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/data/raydp.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Spark on Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", or simply call out to a Ray cluster from within your PySpark notebook with the interactive ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/ray-client.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Client","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or run a ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/job-submission.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Job","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Ultimately, we feel the ML ecosystem is so broad and multifaceted that the best solution for distributed training will be both open-source and flexible. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next Steps ","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"As of today, Deep Learning on Ray Train is officially in Beta.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"As development continues, Ray Train will be extended with a focus on integrating with both Ray and third party libraries. This includes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Dataset preprocessing and feature transformations to operate on Ray Datasets","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Streamlined model exporting to Ray Serve for model serving","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Expanding beyond deep learning to integrate with XGBoost-Ray, LightGBM-Ray, and HuggingFace Transformers","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Elastic training to support dynamically scaling the number of training workers for improved speed and performance","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Support for advanced deep learning models such as Graph Neural Networks and Embeddings","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Support for alternative distributed training strategies such as asynchronous parameter servers","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Numerous performance optimizations such as FP16 compression","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Improved profiling and monitoring with tools such as PyTorch Profiler","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Integrations with more of your favorite experimentation tracking tools","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"To learn more about Ray Train, you can visit the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/train/train.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". If you are already using Ray Train, weâd love to hear your feedback through the ","nodeType":"text"},{"data":{"uri":"https://forms.gle/PXFcJmHwszCwQhqX7"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"User Survey","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Lastly, If you have any questions about Ray Train, you can reach out to the Ray community on ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/c/raytrain/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Discourse","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Slack","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Contributions: ","nodeType":"text"},{"data":{},"marks":[],"value":"We are actively seeking development partners and open-source committers, so please ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray/issues"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"drop a Github issue","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or get in contact if youâre interested!","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7kS5Mc1LeZRPqVlItSGMNI","type":"Asset","createdAt":"2022-01-25T05:17:10.580Z","updatedAt":"2022-06-06T23:16:27.147Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"locale":"en-US"},"fields":{"title":"Ray Train","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/7kS5Mc1LeZRPqVlItSGMNI/28c234fac2d658dbca32e136b2030396/1382642_BlogImageIllustration-11_2_060322.jpg","details":{"size":545837,"image":{"width":1500,"height":1000}},"fileName":"1382642_BlogImageIllustration-11_2_060322.jpg","contentType":"image/jpeg"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}}],"recommendations":[]}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6uVv34UgJUU0qw4dMa4YJy","type":"Entry","createdAt":"2021-08-19T21:03:21.743Z","updatedAt":"2022-06-22T17:00:15.090Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":10,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Introducing Ray Lightning: Multi-node PyTorch Lightning training made easy","slug":"introducing-ray-lightning-multi-node-gpu-training-for-pytorch-lightning-made","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DwU0wh6AmJoGv8JOy6n0Q","type":"Entry","createdAt":"2020-11-11T23:34:15.165Z","updatedAt":"2021-02-05T22:22:08.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","slug":"amog-kamsetty","link":"https://www.linkedin.com/in/amogkamsetty/","bio":"Amog is a software engineer at Anyscale. He helps maintain and develop two machine learning libraries: Ray Tune for distributed hyperparameter tuning and RaySGD for distributed deep learning training.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J134jX2ybZRLPfcAIGLTI","type":"Asset","createdAt":"2020-11-13T00:05:05.533Z","updatedAt":"2020-11-13T00:05:05.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J134jX2ybZRLPfcAIGLTI/12589bb0c6792051050ebc6e1e83af25/Amog_Kamsetty_.jpg","details":{"size":57660,"image":{"width":480,"height":640}},"fileName":"Amog Kamsetty .jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7fm8mbdJEOBiNMuOJKqLJj","type":"Entry","createdAt":"2021-08-09T15:40:10.306Z","updatedAt":"2021-08-09T15:40:10.306Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Will Drevo","slug":"will-drevo","link":"https://www.linkedin.com/in/willdrevo/"}}],"publishedDate":"2021-08-19","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Ray Lightning is a new plugin that makes running multi-node GPU training with PyTorch Lightning fast and easy. ","body":{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"TL;DR","nodeType":"text"},{"data":{},"marks":[],"value":": Use ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray_lightning"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch Lightning with Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to enable multi-node training and automatic cluster configuration with minimal code changes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5L8p4bcC9ux0PJaxtknAYv","type":"Asset","createdAt":"2021-08-20T16:27:11.303Z","updatedAt":"2021-08-20T16:27:11.303Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"pytorch-lightning-ray","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5L8p4bcC9ux0PJaxtknAYv/42c383b01effa6fedee7d918e8f75600/Screen_Shot_2021-08-20_at_9.26.38_AM.png","details":{"size":47711,"image":{"width":954,"height":208}},"fileName":"Screen Shot 2021-08-20 at 9.26.38 AM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"PyTorch Lightning is a library that provides a high-level interface for PyTorch, and helps you organize your code and reduce boilerplate. By abstracting away engineering code, it makes deep learning experiments easier to reproduce and improves developer productivity.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"PyTorch Lightning also includes plugins to easily parallelize your training across multiple GPUs which you can read more about in ","nodeType":"text"},{"data":{"uri":"https://devblog.pytorchlightning.ai/distributed-deep-learning-with-pytorch-lightning-part-1-8df1d032e6d3"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"this blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". This parallel training, however, depends on a critical assumption: ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"that you already have your GPU(s) set up and networked together in an efficient way for training","nodeType":"text"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"While you may have a managed cluster like SLURM for multi-node training on the cloud, setting up the cluster and its configuration is no easy task.Â  As described in ","nodeType":"text"},{"data":{"uri":"https://devblog.pytorchlightning.ai/how-to-configure-a-gpu-cluster-to-scale-with-pytorch-lightning-part-2-cf69273dde7b"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"this blog post","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" configuring the cluster involves:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Making sure all the nodes in the cluster can communicate with each other","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Making the code accessible to each node","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Setting up the proper PyTorch environment variables on each node","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Running the training script individually on each node.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Multi-node training with PyTorch Lightning has a couple of other limitations as as well:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Setting up a multi-node cluster on any cloud provider (AWS, Azure, GCP, or Kubernetes) requires a significant amount of expertise","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Multi-node training is not possible if you want to use a Jupyter notebook","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Automatically scaling your GPUs up / down to reduce costs will require a lot of infrastructure and custom tooling.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3CpHeRQ6JMSgl8251kgQaE","type":"Asset","createdAt":"2021-08-19T20:58:16.272Z","updatedAt":"2021-08-19T20:58:16.272Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Lightning vs. PyTorch Native comparison","file":{"url":"//images.ctfassets.net/xjan103pcp94/3CpHeRQ6JMSgl8251kgQaE/9677cabfdcb80374bfe50cb06c59dc7d/Screen_Shot_2021-08-19_at_12.09.02_PM.png","details":{"size":48225,"image":{"width":860,"height":368}},"fileName":"Screen Shot 2021-08-19 at 12.09.02 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Wouldnât it be great to be able to leverage multi-node training without needingÂ  extensive infrastructure expertise?","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"And wouldnât it be even better if you could do so with no code changes?","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Introducing Ray Lightning","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray_lightning"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Lightning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a simple plugin for PyTorch Lightning to scale out your training. Here are the main benefits of Ray Lightning:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Simple setup.","nodeType":"text"},{"data":{},"marks":[],"value":" No changes to existing training code.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Easily scale up.","nodeType":"text"},{"data":{},"marks":[],"value":" You can write the same code for 1 GPU, and change 1 parameter to scale to a large cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Works with Jupyter Notebook.","nodeType":"text"},{"data":{},"marks":[],"value":" Simply launch a Jupyter Notebook from the head node and access all the resources on your cluster.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Seamlessly create multi-node clusters","nodeType":"text"},{"data":{},"marks":[],"value":" on AWS/Azure/GCP via the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/cloud.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Cluster Launcher","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Integration with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/tune/index.html"},"content":[{"data":{},"marks":[{"type":"bold"},{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for large-scale distributed hyperparameter search and SOTA algorithms.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"And best of all, it is fully open source and free to use!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Underneath the hood, Ray Lightning leverages ","nodeType":"text"},{"data":{"uri":"https://ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", a simple library for distributed computing in Python.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Ray Lightning, scaling up your PyTorch Lightning training becomes much easier and much more flexible!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How does Ray Lightning work?","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray_lightning"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Lightning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" uses the PyTorch Lightning âpluginâ interface to offer a ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"RayPlugin","nodeType":"text"},{"data":{},"marks":[],"value":" that you can add to your Trainer. It works similar to the built-in ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"DDPSpawn","nodeType":"text"},{"data":{},"marks":[],"value":" Plugin that PyTorch Lightning has, but instead of spawning new processes for training, the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"RayPlugin","nodeType":"text"},{"data":{},"marks":[],"value":" creates new ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/actors.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Actors","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". These actors are just Python processes, except they can be scheduled anywhere on the Ray cluster, allowing you to do multi-node programming without leaving your Python script.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2Wgde0BvcaROipRmsIqdmb","type":"Asset","createdAt":"2021-08-19T20:58:25.504Z","updatedAt":"2021-08-19T20:58:25.504Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Lightning Architecture","description":"Communication between Ray actors on multi-node cluster","file":{"url":"//images.ctfassets.net/xjan103pcp94/2Wgde0BvcaROipRmsIqdmb/132db5a5bb2169ed8d7354b53606ac61/Screen_Shot_2021-08-19_at_12.11.43_PM.png","details":{"size":69452,"image":{"width":806,"height":446}},"fileName":"Screen Shot 2021-08-19 at 12.11.43 PM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Each Ray actor will contain a copy of your ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"LightningModule","nodeType":"text"},{"data":{},"marks":[],"value":" and they will automatically set the proper environment variables and create the PyTorch communication group together. This means that underneath the hood, Ray is just running standard PyTorch ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"DistributedDataParallel","nodeType":"text"},{"data":{},"marks":[],"value":", giving you the same performance, but with Ray you can run your training job programmatically and automatically scale instances up and down as you train.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Managing the Cluster","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Typically,Â  managing clusters can be a pain, especially if you donât have an infra or ML platform team. But with Ray, this becomes very easy âÂ  you can start a Ray cluster with the ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/cloud.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray cluster launcher","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Rayâs cluster launcher supports all the major cloud providers (AWS, GCP, Azure) and also has a Kubernetes operator. So you can run your Ray program wherever you need. And once your code can run on a Ray cluster, migrating or changing clouds is easy.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"To launch a Ray cluster on AWS for example, you need a cluster YAML file specifying configuration details like below:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3bcj23C5uYzBGO0GdGVm5k","type":"Entry","createdAt":"2021-08-19T20:58:34.835Z","updatedAt":"2021-08-19T20:58:34.835Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"ray lightning","body":"cluster_name: ml\n\n# Cloud-provider specific configuration.\nprovider:\n    type: aws\n    region: us-west-2\n    availability_zone: us-west-2a,us-west-2b\n\n# How Ray will authenticate with newly launched nodes.\nauth:\n    ssh_user: ubuntu\n\nhead_node:\n    InstanceType: p3.8xlarge\n    ImageId: latest_dlami\n\n    # You can provision additional disk space with a conf as follows\n    BlockDeviceMappings:\n        - DeviceName: /dev/sda1\n          Ebs:\n              VolumeSize: 100\nworker_nodes:\n    InstanceType: p3.2xlarge\n    ImageId: latest_dlami\n\nfile_mounts: {\n    \"/path1/on/remote/machine\": \"/path1/on/local/machine\",\n}\n\n# List of shell commands to run to set up nodes.\nsetup_commands:\n    - pip install -U ray-lightning","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"The information you put in file_mounts will be synced to all nodes in the cluster, so this is where you can put your training script. For any additional dependencies that you need to install, you can specify them (i.e. ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"pip install foobar","nodeType":"text"},{"data":{},"marks":[],"value":") in the setup_commands. They will be installed on all nodes in the cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Once you have your YAML file, you can simply do ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray up cluster.yaml ","nodeType":"text"},{"data":{},"marks":[],"value":"to launch the nodes and create a Ray cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Then you can do ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray attach cluster.yaml","nodeType":"text"},{"data":{},"marks":[],"value":" to ssh into the head node of your Ray cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The great thing about the cluster launcher is that it will ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"automatically","nodeType":"text"},{"data":{},"marks":[],"value":" add new nodes if more resources are requested than the current cluster has available. Also, if there are idle nodes, Ray will automatically terminate them.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Putting it all together","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now letâs see how we can put everything together and easily train a simple MNIST Classifier on the cloud.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Package Installation","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"First letâs install Ray Lightning using: Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2wabgQZk3yGOs3ibvBBIO9","type":"Entry","createdAt":"2021-08-19T20:58:46.938Z","updatedAt":"2021-08-19T20:58:46.938Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"blog-ray-lightning-install","body":"pip install ray-lightning","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"This will also install PyTorch Lightning and Ray for us.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Vanilla PyTorch Lightning","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"First step is to get our PyTorch Lightning code ready. We first need to create our classifier model which is an instance of ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"LightningModule","nodeType":"text"},{"data":{},"marks":[],"value":". Here is an example of a simple MNIST Classifier adapted from the ","nodeType":"text"},{"data":{"uri":"https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction_guide.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"PyTorch Lightning guide","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":":","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6ZkvuOsyANc4VU1Wuf1Znq","type":"Entry","createdAt":"2021-08-19T20:58:55.133Z","updatedAt":"2021-08-19T20:58:55.133Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"blog-ray-lightning-vanilla-pytorch","body":"import pytorch_lightning as pl\nimport torch\nfrom torch.utils.data import random_split, DataLoader\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms\n\nclass LightningMNISTClassifier(pl.LightningModule):\n    def __init__(self, config, data_dir=None):\n        super(LightningMNISTClassifier, self).__init__()\n\n        self.data_dir = data_dir\n        self.lr = config[\"lr\"]\n        layer_1, layer_2 = config[\"layer_1\"], config[\"layer_2\"]\n        self.batch_size = config[\"batch_size\"]\n\n        # mnist images are (1, 28, 28) (channels, width, height)\n        self.layer_1 = torch.nn.Linear(28 * 28, layer_1)\n        self.layer_2 = torch.nn.Linear(layer_1, layer_2)\n        self.layer_3 = torch.nn.Linear(layer_2, 10)\n        self.accuracy = pl.metrics.Accuracy()\n\n    def forward(self, x):\n        batch_size, channels, width, height = x.size()\n        x = x.view(batch_size, -1)\n        x = self.layer_1(x)\n        x = torch.relu(x)\n        x = self.layer_2(x)\n        x = torch.relu(x)\n        x = self.layer_3(x)\n        x = F.softmax(x, dim=1)\n        return x\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.lr)\n\n    def training_step(self, train_batch, batch_idx):\n        x, y = train_batch\n        logits = self.forward(x)\n        loss = F.nll_loss(logits, y)\n        acc = self.accuracy(logits, y)\n        self.log(\"ptl/train_loss\", loss)\n        self.log(\"ptl/train_accuracy\", acc)\n        return loss\n\n    def validation_step(self, val_batch, batch_idx):\n        x, y = val_batch\n        logits = self.forward(x)\n        loss = F.nll_loss(logits, y)\n        acc = self.accuracy(logits, y)\n        return {\"val_loss\": loss, \"val_accuracy\": acc}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n        avg_acc = torch.stack([x[\"val_accuracy\"] for x in outputs]).mean()\n        self.log(\"ptl/val_loss\", avg_loss)\n        self.log(\"ptl/val_accuracy\", avg_acc)\n\n    def prepare_data(self):\n        self.dataset = MNIST(\n            self.data_dir,\n            train=True,\n            download=True,\n            transform=transforms.ToTensor())\n\n    def train_dataloader(self):\n        dataset = self.dataset\n        train_length = len(dataset)\n        dataset_train, _ = random_split(\n            dataset, [train_length - 5000, 5000],\n            generator=torch.Generator().manual_seed(0))\n        loader = DataLoader(\n            dataset_train,\n            batch_size=self.batch_size,\n            num_workers=1,\n            drop_last=True,\n            pin_memory=True,\n        )\n        return loader\n\n    def val_dataloader(self):\n        dataset = self.dataset\n        train_length = len(dataset)\n        _, dataset_val = random_split(\n            dataset, [train_length - 5000, 5000],\n            generator=torch.Generator().manual_seed(0))\n        loader = DataLoader(\n            dataset_val,\n            batch_size=self.batch_size,\n            num_workers=1,\n            drop_last=True,\n            pin_memory=True,\n        )","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Then we need to instantiate this model, create our ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Trainer","nodeType":"text"},{"data":{},"marks":[],"value":" and start training.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4FrMJWvLONnKC8CGVGotXU","type":"Entry","createdAt":"2021-08-19T20:59:04.989Z","updatedAt":"2021-08-19T20:59:04.989Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"blog-ray-lightning-create-trainer","body":"model = LightningMNISTClassifier(config, data_dir=\"./\")\n\ntrainer = pl.Trainer( max_epochs=10)\ntrainer.fit(model)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"And thatâs it for single threaded execution - you can now train your classifier on your laptop. Now letâs parallelize across a large cluster using GPUs with the Ray Lightning Plugin.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Parallelizing on Laptop with Ray Lightning","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"To use Ray Lightning, we simply need to add the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"RayPlugin","nodeType":"text"},{"data":{},"marks":[],"value":" to our Trainer.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs first see how we can parallelize training across the cores of our laptop by adding the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"RayPlugin","nodeType":"text"},{"data":{},"marks":[],"value":". For now, we will disable GPUs. To go straight to parallel training on a cluster with GPUs, head on over to the next section.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"75BTPD0H3kVNF5n4Aj21CL","type":"Entry","createdAt":"2021-08-19T20:59:13.920Z","updatedAt":"2021-08-19T20:59:13.920Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"blog-ray-lightning-parallelize","body":"from ray_lightning import RayPlugin\n\nclass LightningMNISTClassifier(...):\n   # ... etc\n\n# variables for Ray around parallelism and hardware\nnum_workers = 8\nuse_gpu = False\n\n# Initialize ray.\nray.init()\n\nmodel = LightningMNISTClassifier(config, data_dir)\n\ntrainer = pl.Trainer(\n    max_epochs=10, \n    plugins=[RayPlugin(num_workers=num_workers, use_gpu=use_gpu)])\ntrainer.fit(model)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"And with just those small changes, we can run the script again, except have training parallelized with 8 workers (i.e. 8 processes).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"For this, you will need a Ray cluster though. Letâs see how to do that.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Multi-node Training on a Ray Cluster","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"To leverage multiple GPUs, and possible multiple nodes for training, we just have to use the Ray cluster launcher with the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"RayPlugin","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"First, we start up the Ray Cluster by following the instructions above","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6yaxis4q9At4vX66Hdj4CA","type":"Entry","createdAt":"2021-08-19T20:59:21.990Z","updatedAt":"2021-08-19T20:59:21.990Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"blog-ray-lightning-start-cluster","body":"ray up cluster.yaml","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"For a full step-by-step guide with all the possible configurations you can add to your YAML file you can check out the instructions ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/cloud.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Make sure to add your training script to the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"file_mounts","nodeType":"text"},{"data":{},"marks":[],"value":" section and any pip dependencies as part of the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"setup_commands","nodeType":"text"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Once your cluster has started, then you can ssh into the head node via","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1MLSDjyBQjidysq8C5VTwA","type":"Entry","createdAt":"2021-08-19T20:59:40.043Z","updatedAt":"2021-08-19T20:59:40.043Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"body":"ray attach cluster.yaml\n","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"You should see your training script synced on this head node since you added to the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"file_mounts","nodeType":"text"},{"data":{},"marks":[],"value":" of your cluster.yaml.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now you just take the same code from the previous section, and make 2 changes:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"ray.init() ","nodeType":"text"},{"data":{},"marks":[],"value":"-\u003e ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"ray.init(âautoâ)","nodeType":"text"},{"data":{},"marks":[],"value":" so Ray knows to connect to the cluster instead of just starting a local instance","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Â In the code snippet, set ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"use_gpu","nodeType":"text"},{"data":{},"marks":[],"value":" to True and ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"num_workers","nodeType":"text"},{"data":{},"marks":[],"value":" to be the number of total processes/GPUs you want to use for training.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"And final step is to just run your Python script:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"75fHgvVz4ngl4OdKoUhiVv","type":"Entry","createdAt":"2021-08-19T21:03:05.607Z","updatedAt":"2021-08-19T21:03:05.607Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"body":"python train.py","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"And thatâs it! You should be seeing the GPUs in your cluster being used for training.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Youâve now successfully run a multi-node, multi-GPU distributed training job with very few code changes and no extensive cluster configuration!","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next steps","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Youâre now up and running with multi-GPU training on your cloud of choice.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"But ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray_lightning"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Lightning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" comes with many more features:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"If standard PyTorch DDP is not your cup of tea, try out these alternatives instead:","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray_lightning#horovod-plugin-on-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RayHorovodPlugin","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": utilizes ","nodeType":"text"},{"data":{"uri":"https://github.com/horovod/horovod"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Horovod","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for the underlying distributed training protocol instead of DDP.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray_lightning#model-parallel-sharded-training-on-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RayShardedPlugin","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":": memory efficient model parallel training with ","nodeType":"text"},{"data":{"uri":"https://github.com/facebookresearch/fairscale"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Fairscale","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray_lightning"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Lightning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" also integrates with ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/tune/index.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" allowing you to run distributed hyperparameter tuning experiments with each training run also run in a parallel fashion. Check out the full ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/auto_examples/using-ray-with-pytorch-lightning.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray+PyTorch Lightning E2E guide","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for more details.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Use ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/master/cluster/ray-client.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray Client","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" to do training on the cloud without ","nodeType":"text"},{"data":{},"marks":[{"type":"italic"}],"value":"ever having to leave your laptop","nodeType":"text"},{"data":{},"marks":[],"value":". More details ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/ray_lightning#multi-node-training-from-your-laptop"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"here","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"And if youâre curious to learn more about what the entire Ray ecosystem can offer you can check out these guides:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://medium.com/distributed-computing-with-ray/simple-end-to-end-ml-from-selection-to-serving-with-ray-tune-and-ray-serve-10f5564d33ba"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"End-to-end guide to tune and serve your models","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://medium.com/distributed-computing-with-ray/how-to-speed-up-pandas-with-modin-84aa6a87bcdb"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"How to speed up Pandas with Modin","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://medium.com/distributed-computing-with-ray/autoscaling-clusters-with-ray-36bad4da6b9c"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Autoscaling clusters with Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"Happy training, and may your model's error always be low! :)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Sa19AASCPeGE50HCuR7Ro","type":"Asset","createdAt":"2021-08-19T21:03:17.278Z","updatedAt":"2021-08-19T21:03:17.278Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Lightning","description":"","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Sa19AASCPeGE50HCuR7Ro/92396a47ee91240314b418dee039b50c/Screen_Shot_2021-08-19_at_12.57.37_PM.png","details":{"size":54091,"image":{"width":862,"height":518}},"fileName":"Screen Shot 2021-08-19 at 12.57.37 PM.png","contentType":"image/png"}}},"mainImageFit":"contain","recommendations":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4JygVS7bwENFcboFDrFlm4","type":"Entry","createdAt":"2022-03-29T00:29:24.086Z","updatedAt":"2022-03-29T00:29:24.086Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Time series forecasting using an LSTM version of RNN with PyTorch Forecasting and Torch Lightning","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3pYLTu2A3ZcmQBTViHKiUF","type":"Entry","createdAt":"2021-12-21T19:22:29.656Z","updatedAt":"2022-06-22T16:05:17.448Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":49,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Time Series Forecasting using an LSTM version of RNN with PyTorch Forecasting and Torch Lightning","slug":"scaling-time-series-forecasting-on-pytorch-lightning-ray","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3SFhFEpkZp9zfNpMHbW9Xk","type":"Entry","createdAt":"2021-11-23T02:59:24.594Z","updatedAt":"2022-01-04T17:25:07.883Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Christy Bergman","slug":"christy-bergman","link":"https://www.linkedin.com/in/christybergman/","bio":"Christy is a Developer Advocate at Anyscale. Her work involves figuring out how to parallelize different AI algorithms and creating demos and tutorials on how to use Ray and Anyscale.  Before that, she was a Senior AI/ML Specialist Solutions Architect at AWS and Data Scientist at several other companies. In her spare time, she enjoys hiking and bird watching."}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4DwU0wh6AmJoGv8JOy6n0Q","type":"Entry","createdAt":"2020-11-11T23:34:15.165Z","updatedAt":"2021-02-05T22:22:08.358Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Amog Kamsetty","slug":"amog-kamsetty","link":"https://www.linkedin.com/in/amogkamsetty/","bio":"Amog is a software engineer at Anyscale. He helps maintain and develop two machine learning libraries: Ray Tune for distributed hyperparameter tuning and RaySGD for distributed deep learning training.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2J134jX2ybZRLPfcAIGLTI","type":"Asset","createdAt":"2020-11-13T00:05:05.533Z","updatedAt":"2020-11-13T00:05:05.533Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Amog Kamsetty","file":{"url":"//images.ctfassets.net/xjan103pcp94/2J134jX2ybZRLPfcAIGLTI/12589bb0c6792051050ebc6e1e83af25/Amog_Kamsetty_.jpg","details":{"size":57660,"image":{"width":480,"height":640}},"fileName":"Amog Kamsetty .jpg","contentType":"image/jpeg"}}}}}],"publishedDate":"2021-12-21","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"This blog, Part 2, will explain how to use Ray to speed up Deep Learning forecasting when training one large global model in order to predict many target time series. We will train an LSTM version of RNN with GRN building blocks, Encoder-Decoder, and Attention Mechanism.  Weâll use PyTorch Forecasting APIs on top of PyTorch Lightning APIs on top of PyTorch.\n","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UCuGp7AED7z6HahJXQmyT","type":"Asset","createdAt":"2021-12-21T22:03:07.634Z","updatedAt":"2022-01-18T00:16:17.745Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"forecasting blog 2 of 2 image 1","description":"Displaying [New York City Yellow Taxi](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) ride volumes, with 1 week hourly forecast.  Blue=observed, Orange=predicted, per validation dataset.  Forecast generated using Googleâs Temporal Fusion Transformer algorithm implemented by Pytorch forecasting, and parallelized by Ray for faster runtime, either on a laptop or on any cloud.  Image by Author.  ","file":{"url":"//images.ctfassets.net/xjan103pcp94/5UCuGp7AED7z6HahJXQmyT/e2249629b06020ff13730b7b04e5cc2c/blog_timeseries_part2_image1_try3.png","details":{"size":228804,"image":{"width":842,"height":672}},"fileName":"blog_timeseries_part2_image1_try3.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"In Part 1, my ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/scaling-time-series-forecasting-on-ray-arima-and-prophet-on-ray"},"content":[{"data":{},"marks":[],"value":"previous blog","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" ","nodeType":"text"},{"data":{"uri":"https://towardsdatascience.com/scaling-time-series-forecasting-with-ray-arima-and-prophet-e6c856e605ee"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"explained how to apply the â","nodeType":"text"},{"data":{"uri":"https://en.wikipedia.org/wiki/Embarrassingly_parallel#:~:text=In%20parallel%20computing%2C%20an%20embarrassingly,a%20number%20of%20parallel%20tasks."},"content":[{"data":{},"marks":[],"value":"Embarrassingly Parallel","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"â pattern to speed up forecasting when each model is independently trained, such as with traditional forecasting algorithms ARIMA, Prophet, and Neural Prophet. Data, Training and inference get distributed by the Ray engine across local laptop cores. The concept is similar to Multiprocessing Pool, except Ray can handle distributing more complex classes and functions. Unlike Multiprocessing, the exact same code can run in parallel across any cluster in any cloud too.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This post will explain how to use Ray to speed up Deep Learning forecasting when training one large global model in order to predict many target time series. Why do this? Well, often, things a company wants to forecast are related to each other like sports fan items, washer and dryer that are the same brand and color, supermarket items that are often bought together, etc. Then each target time series is used as input to the same model, and each gets a different output.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Parallelizing code for distributed runtime of global deep learning models requires Distributed Data Parallelism and Model Parallelism. This requires co-ordination between distributed compute nodes to shard the data, share the gradients between nodes each with its own data shard, and combine the gradients into a single global model. Ray handles the Data and Model Parallelism, while keeping a simple API for developers. Further, Ray can train and inference the Deep Learning model in parallel, distributed across a single laptopâs cores or across compute nodes in any cloud.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This blog is organized into the following topics:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Intro Deep Learning AI algorithms used in forecasting","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Using Googleâs Temporal Fusion Transformer in Pytorch Forecasting (uses PyTorch Lightning APIs)","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"How to speed up model training and inference using Ray","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"How to speed up model training and inference in any cloud using AnyscaleÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Intro Deep Learning AI Algorithms used in Forecasting","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"mVupIaXPeceH1j6RSuPYr","type":"Asset","createdAt":"2021-12-21T18:47:00.288Z","updatedAt":"2021-12-22T02:27:14.725Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":6,"locale":"en-US"},"fields":{"title":"image2 of rnn components","description":"Left.  High-level view of an RNN and unfolded time steps.  Image [source](https://www.researchgate.net/figure/The-standard-RNN-and-unfolded-RNN_fig1_318332317).   \nMiddle.  Typical ANN building block, consisting of 1 input layer, 2 hidden dense layers, and an output layer.  Image [source](https://www.researchgate.net/figure/An-Artificial-Neural-Network-ANN-with-two-hidden-layers-and-six-nodes-in-each-hidden_fig1_335855384).  Right.  Example GRN consisting of 2 dense layers, with 2 activation functions (ELU exponential linear unit and GLU gated linear unit), and dropout.  Image from Temporal Fusion Transformer [paper](https://arxiv.org/pdf/1912.09363.pdf).","file":{"url":"//images.ctfassets.net/xjan103pcp94/mVupIaXPeceH1j6RSuPYr/0cf6ed7e1afb556632e30c570b13cf01/blog_timeseries_part2_image2_nocaption.png","details":{"size":368405,"image":{"width":1796,"height":422}},"fileName":"blog_timeseries_part2_image2_nocaption.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"A ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Recurrent Neural Network","nodeType":"text"},{"data":{},"marks":[],"value":" ( ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"RNN","nodeType":"text"},{"data":{},"marks":[],"value":") is a type of neural network that is often used for time series since it processes data sequentially. RNN consists of a sequence of ANNs (artificial neural network) per fixed time step. Each ANN building block is a set of neurons divided into input layer, hidden layers and output layer, where each neuron is connected to other neurons and each connection has a trainable weight. RNNs were first used for text translation.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Following are some related concept terms.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"LSTM (Long Short-Term Memory) ","nodeType":"text"},{"data":{},"marks":[],"value":"is a type of recurrent neural network architecture, designed to overcome the vanishing gradient problem (where things way in the past might get close to 0-value weights). LSTM has 3 memory gates which together allows a network to remember and forget.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"GRN ","nodeType":"text"},{"data":{},"marks":[],"value":"or Gated Residual Network can replace a basic ANN building block. It consists of specifically: 2 dense layers and 2 activation functions (ELU exponential linear unit and GLU gated linear unit). This allows the network to understand which input transformations are simple, which require more complex modeling, and which to skip entirely.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Encoder-Decoder Model ","nodeType":"text"},{"data":{},"marks":[],"value":"is a type of RNN where the input sequence of data (training data) can have a different length than the output sequence (validation or test data, otherwise called the forecast horizon). Positional encodings are added to the input embeddings to indicate the position of the input with respect to the entire time sequence.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Self-Attention Mechanism","nodeType":"text"},{"data":{},"marks":[],"value":" is an evolution developed to solve the long-range dependency problem of LSTMs (because of LSTMâs forget gate, important information can be lost). By adding a transformer, certain inputs can get more âattentionâ that feed-forward through the RNN network. At each time step, learnable weights are calculated as a function of the ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Q","nodeType":"text"},{"data":{},"marks":[],"value":"uery (ith particular input vector w.r.t. other inputs), ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"K","nodeType":"text"},{"data":{},"marks":[],"value":"ey (input embeddings that also contain that query), and ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"V","nodeType":"text"},{"data":{},"marks":[],"value":"alue (output vector calculated usually by dot-product from Q,K learned weights). The outputs are feed-forwarded through the RNN network. Since the Q,K are all calculated from the same input, which is in turn applied to the same input, the process is called âself-attentionâ.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Multi-headed Attention ","nodeType":"text"},{"data":{},"marks":[],"value":"uses multiple Q,K transforms at each time step. Pure self-attention uses all historical data at each time step. For example, if h=4 attention heads, input data is split into 4 chunks, then self-attention is applied to each chunk using Q,K matrices to get 4 different V-score vectors. This means a single input gets projected onto 4 different ârepresentation subspacesâ and those feed-forward through the RNN network. The result is more nuanced self-attention. From a distributed-computing point of view this is ideal, since each chunk, h, from multi-headed attention can be run asynchronously on a separate node.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Backtesting. ","nodeType":"text"},{"data":{},"marks":[],"value":"Training and validation data is split into batches of sliding windows (each \nbatch is the previous batch shifted by 1 value in the future). This technique is called âback testingâ, since you canât take an 80/20 train/test random sample like usual. The sequential data order must be kept intact. Time series typically take a context_length size window of \ndata for training, then a different prediction_length size window for validation.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Example using Googleâs Temporal Fusion Transformer implementation in Pytorch Forecasting","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"The dataset used in this tutorial is 8 months of historical ","nodeType":"text"},{"data":{"uri":"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"New York City Yellow Taxi","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" ride volumes.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Our data coding object will be a generator to repeatedly fold the sequential data using the backtesting technique. To take care of de-trending, we will use ","nodeType":"text"},{"data":{"uri":"https://pytorch-forecasting.readthedocs.io/en/latest/api/pytorch_forecasting.data.encoders.GroupNormalizer.html#pytorch_forecasting.data.encoders.GroupNormalizer"},"content":[{"data":{},"marks":[],"value":"PyTorch Forecastingâs","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" ","nodeType":"text"},{"data":{"uri":"https://pytorch-forecasting.readthedocs.io/en/latest/api/pytorch_forecasting.data.encoders.GroupNormalizer.html#pytorch_forecasting.data.encoders.GroupNormalizer"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" Group Normalizer, or batch norm per item_id. Each batch is split between 63-hours training inputs and 168-hour or 1-week prediction targets.Â  That is, the data is train/valid sampled using 63/168 window lengths in order to keep the sequential ordering of the data intact.Â Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The network design will be an LSTM version of RNN with GRN building blocks, Encoder-Decoder, and Attention Mechanism.Â  Weâll use PyTorch Forecasting's implementation of Googleâs ","nodeType":"text"},{"data":{"uri":"https://github.com/google-research/google-research/tree/master/tft"},"content":[{"data":{},"marks":[],"value":"Temporal Fusion Transformer","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â  PyTorch Forecasting is a set of convenience APIs for ","nodeType":"text"},{"data":{"uri":"https://www.pytorchlightning.ai/"},"content":[{"data":{},"marks":[],"value":"PyTorch Lightning","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â  PyTorch Lightning in turn is a set of convenience APIs on top of ","nodeType":"text"},{"data":{"uri":"https://pytorch.org/"},"content":[{"data":{},"marks":[],"value":"PyTorch","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".  This is a similar concept to how Keras is a set of convenience APIs on top of TensorFlow.Â Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Code for the demo is ","nodeType":"text"},{"data":{"uri":"https://github.com/christy/AnyscaleDemos/blob/main/forecasting_demos/pytorch_forecasting_ray_local.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"on github","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Example how to speed up model training and inference using Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6UjrjWwD4i87SXsoQiqfd9","type":"Asset","createdAt":"2021-12-21T22:08:26.200Z","updatedAt":"2021-12-21T22:08:26.200Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"forecasting blog ray image","description":"Ray Ecosystem, from [Ion Stoicaâs keynote at Ray Summit 2021](https://youtu.be/dn8hu2sgRWU?t=561).","file":{"url":"//images.ctfassets.net/xjan103pcp94/6UjrjWwD4i87SXsoQiqfd9/80accc5a3b4d776d1e275eec7a465a51/forecasting_blog_ray_image.png","details":{"size":988792,"image":{"width":1656,"height":826}},"fileName":"forecasting_blog_ray_image.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Ray is an open-source library","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"underline"}],"value":" ","nodeType":"text"},{"data":{},"marks":[],"value":"developed at RISELab from UC Berkeley, which also developed Apache Spark. Ray makes it easy to parallelize and distribute Python code.Â  The code can then run across any type of cores: a) your own laptop cores, b) cluster in AWS, GCP, or any common cloud.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This rest of the post assumes you already have a PyTorch Lightning model defined, either through vanilla PyTorch Lightning or through PyTorch Forecasting. ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"The parts of code you need to change to make it run on Ray are shown in bold below.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Step 1. Install and import Ray, Ray Plugin for PyTorch Lightning, and Anyscale.Â  Make sure your PyTorch Lightning version is 1.4.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7MTqfzxOG9d5YfYCyZ38MZ","type":"Entry","createdAt":"2022-01-09T18:31:14.339Z","updatedAt":"2022-01-09T18:31:14.339Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"forecast blog2 of 2, code step 1","body":"# Install these libraries in your conda environmentÂ \nconda install pytorchÂ \npip install pytorch_lightning==1.4 Â  #required version for rayÂ \npip install git+https://github.com/jdb78/pytorch-forecasting@maintenance/pip-installÂ  #used at time of writing this blog, check for updatesÂ \npip install rayÂ \npip install anyscaleÂ \npip install ray_lightning\n# Import these libraries in your .py or .ipynb codeÂ Â Â Â \nimport torchÂ Â \nimport pytorch_lightning as plÂ Â Â \nimport pytorch_forecasting as ptfÂ Â \nimport rayÂ \nfrom ray_lightning import RayPlugin","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Step 2. Initialize Ray","nodeType":"text"},{"data":{},"marks":[],"value":" for the number of cores on your laptop (this is default behavior). Mine had 8 cores.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"},{"type":"code"}],"value":"ray.init()","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Step 3.","nodeType":"text"},{"data":{},"marks":[],"value":" ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"}],"value":"Initialize the Ray Lightning plugin","nodeType":"text"},{"data":{},"marks":[],"value":", also for the number of cores on your laptop.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5SPbLaPdfbkMkTJQHObAOX","type":"Entry","createdAt":"2022-01-09T18:30:20.130Z","updatedAt":"2022-01-09T18:38:50.005Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"forecast blog2 of 2, code step 3","body":"plugin = RayPlugin(\n          num_workers=8,Â  #fixed num CPU\n          num_cpus_per_worker=1,\n          use_gpu=False,Â  #True or False\n          find_unused_parameters=False, # skip warnings\n          )","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Step 4.","nodeType":"text"},{"data":{},"marks":[],"value":" Read sample data which is located in the same github repo as the code. Data is already aggregated into hourly taxi rides per location in NYC.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5w5BTUKTvcKMlCcLx8suv0","type":"Entry","createdAt":"2022-01-09T18:29:16.378Z","updatedAt":"2022-01-09T18:48:55.404Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"forecast blog2 of 2, code step 4","body":"# read data into pandas dataframe\nfilename = \"data/clean_taxi_hourly.parquet\"\ndf = pd.read_parquet(filename)\n\n# keep only certain columns\ndf = df[[\"time_idx\", \"pulocationid\", \"day_hour\",\n         \"trip_quantity\", \"mean_item_loc_weekday\",\n         \"binned_max_item\"]].copy()","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Step 5. ","nodeType":"text"},{"data":{},"marks":[],"value":" Convert your data to PyTorch tensors and define PyTorch Forecasting data loaders, like usual. The PyTorch Forecasting data loaders API conveniently folds tensors into train/test backtest windows automatically. Next, in the PyTorch Lightning Trainer, pass in the Ray Plugin. Add ","nodeType":"text"},{"data":{},"marks":[{"type":"code"},{"type":"bold"}],"value":"plugins=[ray_plugin]","nodeType":"text"},{"data":{},"marks":[],"value":" parameter below.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7L8HlTp53YWiGQzkZWkeRq","type":"Entry","createdAt":"2022-01-09T18:24:03.381Z","updatedAt":"2022-01-09T18:42:44.882Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"forecast blog2 of 2, code step 5","body":"# convert data to PyTorch tensors and PyTorch Forecasting loaders\n# PyTorch Forecasting folds tensors into backtest windows\ntrain_dataset, train_loader, val_loader = \\\n     convert_pandas_pytorch_timeseriesdata(df)\n\n# define the pytorch lightning trainer\ntrainer = pl.Trainer(      \n      max_epochs=EPOCHS,      \n      gpus=NUM_GPU,      \n      gradient_clip_val=0.1,        \n      limit_train_batches=30,       \n      callbacks=[lr_logger, \n                 early_stop_callback],      \n      # how often to log, default=50      \n      logger=logger,      \n      # To go back to regular python - just comment out below      \n      # Plugin allows Ray engine to distribute objects     \n      plugins=[ray_plugin]\n      )","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Step 6. ","nodeType":"text"},{"data":{},"marks":[],"value":"Run your code like usual.  ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"50prMNlgOdCScyiguUesNz","type":"Entry","createdAt":"2022-01-09T18:22:13.006Z","updatedAt":"2022-01-09T18:50:17.543Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"forecasting blog2 of 2, code step 6","body":"# define a pytorch forecasting model\nmodel = ptf.models.TemporalFusionTransformer.from_dataset(\n          train_dataset,   \n          learning_rate=LR,      \n          hidden_size=HIDDEN_SIZE,      \n          attention_head_size=ATTENTION_HEAD_SIZE,      \n          dropout=DROPOUT,    \n          hidden_continuous_size=HIDDEN_CONTINUOUS_SIZE,\n          loss=ptf.metrics.QuantileLoss(),      \n          log_interval=10,      \n          reduce_on_plateau_patience=4, \n          )\n\n# fit the model on training data\ntrainer.fit(      \n    model,      \n    train_dataloaders=train_loader,         \n     val_dataloaders=val_loader, \n)\n\n# get best model from the trainer\nbest_model_path = trainer.checkpoint_callback.best_model_path\nbest_model = \\\nptf.models.TemporalFusionTransformer.load_from_checkpoint(\n      best_model_path\n)","language":"Python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Thatâs it! Now your PyTorch Lightning model will run distributed.  Behind the scenes, the Ray Lightning Plugin APIs together with Ray are distributing both the data and the models, automatically.  The input data is automatically fully sharded, data shards and training functions placed on every distributed node, gradients shared between nodes, one global model produced, and the resulting model is returned as requested type (PyTorch Lightning or PyTorch Forecasting model type).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6UjeqkQVQTkWLTFyb9xZ7m","type":"Asset","createdAt":"2022-01-09T02:17:15.169Z","updatedAt":"2022-01-12T19:02:30.511Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"Image same data, same function, DL parallel pattern","description":"What Ray does behind the scenes to distribute learning of global models across N compute nodes. The input data is fully sharded, each node gets the same training functions, gradients are shared between parallel nodes, and one global model is produced. Image by Author.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/6UjeqkQVQTkWLTFyb9xZ7m/f72181521556085fc39c34c900ddb78d/blog_timeseries_part2_image5.png","details":{"size":95446,"image":{"width":854,"height":558}},"fileName":"blog_timeseries_part2_image5.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Previously, I tried to train this model on my laptop, but interrupted the runtime after several hours, since the first epoch still had not finished. After distributing the code with Ray, the same code runs in about 1 hour.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"These small tweaks made it possible to train a very accurate DL global forecasting model in about 1 hour on a fairly small compute resource (my laptop).Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Another benefit of Ray is now that the code runs in parallel on my laptop, I can run the SAME code on any cloud using Anyscale, which Iâll show next.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"fVI4OY3NfgUXB7RKbTQQq","type":"Asset","createdAt":"2021-12-22T03:41:12.687Z","updatedAt":"2022-01-09T02:29:59.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"forecast blog 2 of 2 output ray local","description":"Model training output running on a laptop with 8 cores. iPython %% time output shows it took about 1 hour to train a very accurate forecast.","file":{"url":"//images.ctfassets.net/xjan103pcp94/fVI4OY3NfgUXB7RKbTQQq/bb634331c68e44849fb4831a8ab330b3/blog_timeseries_output1_ray_local.png","details":{"size":64043,"image":{"width":1302,"height":200}},"fileName":"blog_timeseries_output1_ray_local.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1Mc0lWUtqx67DWA7qtCdP8","type":"Asset","createdAt":"2021-12-22T03:44:58.165Z","updatedAt":"2022-01-18T01:02:08.378Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"forecasting blog part 2 of 2 ray local output accuracy","description":"Model training output running on a laptop with 8 cores. Accuracy calculated using predictions on hold-out validation data 1 week.","file":{"url":"//images.ctfassets.net/xjan103pcp94/1Mc0lWUtqx67DWA7qtCdP8/5db05dc398164ac50c13594d23e3e37c/blog_timeseries_part2_output2_ray_local.png","details":{"size":322804,"image":{"width":1520,"height":1098}},"fileName":"blog_timeseries_part2_output2_ray_local.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"How to speed up model training and inference in any cloud using Anyscale","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Next, to train or do inference faster, you probably want to run that same code on a cloud on bigger instances or across a cluster. In order to use the exact same Ray code on a cloud, (AWS, GCP,Â â¦), you need to use either ","nodeType":"text"},{"data":{"uri":"https://docs.ray.io/en/latest/cluster/index.html"},"content":[{"data":{},"marks":[],"value":"Ray open source cluster","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" or ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/"},"content":[{"data":{},"marks":[],"value":"Anyscale","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" which simplifies any cloud setup.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"With Anyscale, you have a choice to either a) do pip installs and github clone on a cluster config, or b) do them at runtime. See ","nodeType":"text"},{"data":{"uri":"https://docs.anyscale.com/user-guide/configure/dependency-management/anyscale-environments"},"content":[{"data":{},"marks":[],"value":"cluster or runtime environments","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for more information. The cluster config is used first, then the runtime config, if specified, will override the cluster config.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"The steps to run Ray code on any cloud using Anyscale are:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Step 1. ","nodeType":"text"},{"data":{"uri":"http://anyscale-staging.herokuapp.com/signup"},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/signup"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Sign up for Anyscale (see this link)","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"bold"}],"value":" and ","nodeType":"text"},{"data":{"uri":"https://docs.anyscale.com/get-started"},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"set up your account (see this link)","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[{"type":"bold"}],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Step 2. Create a cluster configuration","nodeType":"text"},{"data":{},"marks":[],"value":".Â I did this for convenience, since I had a number of atypical, newer ML libraries  to install with dependencies. Open your browser to the ","nodeType":"text"},{"data":{"uri":"https://console.anyscale.com/"},"content":[{"data":{},"marks":[],"value":"Anyscale console","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", and under the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Configurations","nodeType":"text"},{"data":{},"marks":[],"value":" left menu, click the ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Create new environment","nodeType":"text"},{"data":{},"marks":[],"value":" button. See picture of Anyscale console below.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"Cluster environment name","nodeType":"text"},{"data":{},"marks":[],"value":".Â  Give your environment configuration any name.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Select a Python version","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Select a base docker image. I chose ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"anyscale/ray-ml:1.9.0-python38-gpu","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Under ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Pip packages","nodeType":"text"},{"data":{},"marks":[],"value":", see picture below for packages to install and what order.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Under ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Post build commands","nodeType":"text"},{"data":{},"marks":[],"value":", see picture below if you would like to install ","nodeType":"text"},{"data":{"uri":"https://github.com/christy/AnyscaleDemos/tree/main/forecasting_demos"},"content":[{"data":{},"marks":[],"value":"this demo code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and data automatically.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Click ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Create","nodeType":"text"},{"data":{},"marks":[],"value":" button.\n\nMake note of your ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"cluster-config-name:version_number","nodeType":"text"},{"data":{},"marks":[],"value":". In the screenshot below, mine is ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"christy-forecast-pytorch:13","nodeType":"text"},{"data":{},"marks":[],"value":". Youâll need this for the next step.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"ordered-list"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5Imi06trBrbXj6itaXUfem","type":"Asset","createdAt":"2021-12-21T23:23:21.762Z","updatedAt":"2022-01-09T18:12:07.542Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"forecasting part 2 image anyscale config","description":"Anyscale console, showing Configuration left menu. The main screen shows an example configuration with additional pip installs and github clone on top of the base docker image.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5Imi06trBrbXj6itaXUfem/4e0da4948ec268585ff15ccf6b01c4ed/blog_timeseries_part2_image4.png","details":{"size":696385,"image":{"width":1388,"height":600}},"fileName":"blog_timeseries_part2_image4.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Step 3. Initialize Ray","nodeType":"text"},{"data":{},"marks":[],"value":" with the name of the cloud cluster and cluster config.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"import anyscale\n\n# initialize ray on Anyscale to run on any cloud \n# name your cluster on the fly \n# set cluster_env parameter = your preconfigured cluster config name\nray.init(\nÂ Â Â Â Â ","nodeType":"text"},{"data":{},"marks":[{"type":"bold"},{"type":"code"}],"value":"anyscale://my-cool-cluster","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":", #give your cluster any name\nÂ Â Â Â Â  ","nodeType":"text"},{"data":{},"marks":[{"type":"code"},{"type":"bold"}],"value":"cluster_env=christy-forecast-pytorch:13,","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"\n)","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Step 4. Initialize the Ray Lightning plugin","nodeType":"text"},{"data":{},"marks":[],"value":" with num_workers=N, where N \u003e num cpu on the head node of your cloud cluster. If you specify any number \u003c= N, Anyscale will not scale out. With any number \u003e N, Anyscale autoscaling will trigger automatically, up to limit configured in your account. Set GPU if you have access to a GPU.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"code"}],"value":"plugin = \\\nÂ Â Â Â Â Â Â RayPlugin(num_workers=","nodeType":"text"},{"data":{},"marks":[{"type":"bold"},{"type":"code"}],"value":"10","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":",Â \nÂ Â Â Â Â Â Â Â Â Â Â Â num_cpus_per_worker=1,Â \nÂ Â Â Â Â Â Â Â Â Â Â Â use_gpu=","nodeType":"text"},{"data":{},"marks":[{"type":"bold"},{"type":"code"}],"value":"True","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":",\nÂ Â Â Â Â Â Â )","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now, run your python code (or notebook) the way you would normally.Â  It will automatically run in parallel in any cloud! While your application is running, you can monitor your Cloud Cluster usage in the ","nodeType":"text"},{"data":{"uri":"https://console.anyscale.com/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Anyscale console","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" under ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"Clusters","nodeType":"text"},{"data":{},"marks":[],"value":".Â  ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"This blog demonstrated how easy it is to enable both data and model parallelism for PyTorch Lightning models used for time series forecasting.Â  Only minimal code changes were required.Â  Once modified for Ray, the same code can run in parallel on your laptop or in parallel on any cloud through Anyscale.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Full code for the demo is ","nodeType":"text"},{"data":{"uri":"https://github.com/christy/AnyscaleDemos/blob/main/forecasting_demos/pytorch_forecasting_ray_local.ipynb"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"on github","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Thanks to Jan Beitner, author of ","nodeType":"text"},{"data":{"uri":"https://pytorch-forecasting.readthedocs.io/en/stable/"},"content":[{"data":{},"marks":[],"value":"PyTorch Forecasting","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", for accepting my Pull Requests and creating a maintenance release, used in this demo.","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5UCuGp7AED7z6HahJXQmyT","type":"Asset","createdAt":"2021-12-21T22:03:07.634Z","updatedAt":"2022-01-18T00:16:17.745Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"forecasting blog 2 of 2 image 1","description":"Displaying [New York City Yellow Taxi](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) ride volumes, with 1 week hourly forecast.  Blue=observed, Orange=predicted, per validation dataset.  Forecast generated using Googleâs Temporal Fusion Transformer algorithm implemented by Pytorch forecasting, and parallelized by Ray for faster runtime, either on a laptop or on any cloud.  Image by Author.  ","file":{"url":"//images.ctfassets.net/xjan103pcp94/5UCuGp7AED7z6HahJXQmyT/e2249629b06020ff13730b7b04e5cc2c/blog_timeseries_part2_image1_try3.png","details":{"size":228804,"image":{"width":842,"height":672}},"fileName":"blog_timeseries_part2_image1_try3.png","contentType":"image/png"}}},"recommendations":[],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true}},"url":"https://www.anyscale.com/blog/scaling-time-series-forecasting-on-pytorch-lightning-ray","ctaText":"Read more","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5gmz6c7uen1QngBP1vo9Eb","type":"Asset","createdAt":"2022-03-24T22:19:37.899Z","updatedAt":"2022-03-24T22:19:37.899Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-clock-dark","file":{"url":"//images.ctfassets.net/xjan103pcp94/5gmz6c7uen1QngBP1vo9Eb/1182c95be2c6caaab7ae3b0478551f34/blog-recommended-content-clock-dark.jpg","details":{"size":36818,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-clock-dark.jpg","contentType":"image/jpeg"}}},"recommendations":null}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"b9EGYWhpIS8hJSZ4WHo5F","type":"Entry","createdAt":"2022-03-29T00:31:48.304Z","updatedAt":"2022-03-29T00:31:48.304Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"contentLink"}},"locale":"en-US"},"fields":{"title":"Getting started with distributed machine Learning with PyTorch and Ray","content":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"FDhTjqC8OfY6GaZeVASC5","type":"Entry","createdAt":"2021-03-02T17:30:34.235Z","updatedAt":"2022-06-22T17:16:52.860Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Getting Started with Distributed Machine Learning with PyTorch and Ray","slug":"getting-started-with-distributed-machine-learning-with-pytorch-and-ray","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"38FnKzZSVXRjdb7jina31C","type":"Entry","createdAt":"2021-01-01T00:11:08.469Z","updatedAt":"2021-01-01T00:11:08.469Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Michael Galarnyk","slug":"michael-galarnyk","link":"https://twitter.com/GalarnykMichael"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"12Tr9ugpd52qSq7KDPaVwt","type":"Entry","createdAt":"2020-09-21T20:30:51.160Z","updatedAt":"2021-05-26T16:48:57.527Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Richard Liaw","slug":"richard-liaw","link":"https://twitter.com/richliaw?lang=en","bio":"Richard Liaw is a Software Engineer at Anyscale and works on RayTune.","photo":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4At2pjWw8OizUTP5bmK7BV","type":"Asset","createdAt":"2020-09-21T20:30:01.585Z","updatedAt":"2020-09-21T20:30:01.585Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Richard Liaw headshot","file":{"url":"//images.ctfassets.net/xjan103pcp94/4At2pjWw8OizUTP5bmK7BV/0a46b76296b34714f31fc963d279e82f/Richard_Liaw_headshot.jpg","details":{"size":469680,"image":{"width":2170,"height":2560}},"fileName":"Richard Liaw headshot.jpg","contentType":"image/jpeg"}}}}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4QtW45NTCt8MQYWOaE1TX2","type":"Entry","createdAt":"2020-09-14T18:41:27.491Z","updatedAt":"2022-08-23T02:59:22.436Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Robert Nishihara","slug":"robert-nishihara","link":"https://www.linkedin.com/in/robert-nishihara-b6465444/","bio":"Co-founder and CEO at Anyscale"}}],"publishedDate":"2021-03-02","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},"intro":"Ray is a popular framework for distributed Python that can be paired with PyTorch to rapidly scale machine learning applications.","body":{"nodeType":"document","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Machine learning todayÂ ","marks":[],"data":{}},{"nodeType":"text","value":"requires","marks":[{"type":"italic"}],"data":{}},{"nodeType":"text","value":"Â distributed computing. Whether youâreÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.youtube.com/watch?v=rEB3NPUoxMM"},"content":[{"nodeType":"text","value":"training networks","marks":[],"data":{}}]},{"nodeType":"text","value":",Â ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/tune/"},"content":[{"nodeType":"text","value":"tuning hyperparameters","marks":[],"data":{}}]},{"nodeType":"text","value":",Â ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/serve/"},"content":[{"nodeType":"text","value":"serving models","marks":[],"data":{}}]},{"nodeType":"text","value":", orÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://medium.com/distributed-computing-with-ray/data-processing-support-in-ray-ae8da34dce7e"},"content":[{"nodeType":"text","value":"processing data","marks":[],"data":{}}]},{"nodeType":"text","value":", machine learning is computationally intensive and can be prohibitively slow without access to a cluster.Â ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://ray.io/"},"content":[{"nodeType":"text","value":"Ray","marks":[],"data":{}}]},{"nodeType":"text","value":"Â is a popular framework for distributed Python that can be paired with PyTorch to rapidly scale machine learning applications.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This post covers various elements of the Ray ecosystem and how it can be used with PyTorch!","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"What is Ray","marks":[],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"QGnrgOJx9rGd8EfSnVehx","type":"Asset","createdAt":"2021-03-02T07:35:23.665Z","updatedAt":"2021-03-02T07:35:23.665Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray Stack","file":{"url":"//images.ctfassets.net/xjan103pcp94/QGnrgOJx9rGd8EfSnVehx/e8080f8a43268238ff3557fdbbbadb4a/RayStack.png","details":{"size":283040,"image":{"width":1600,"height":611}},"fileName":"RayStack.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray is an open source library for parallel and distributed Python. The diagram above shows that at a high level, the Ray ecosystem consists of three parts: the core Ray system, scalable libraries for machine learning (both native and third party), and tools forÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://medium.com/distributed-computing-with-ray/how-to-scale-python-on-every-major-cloud-provider-12b3bde01208"},"content":[{"nodeType":"text","value":"launching clusters on any cluster or cloud provider","marks":[],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"The Core Ray System","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray"},"content":[{"nodeType":"text","value":"Ray","marks":[],"data":{}}]},{"nodeType":"text","value":"Â can be used toÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://towardsdatascience.com/modern-parallel-and-distributed-python-a-quick-tutorial-on-ray-99f8d70369b8"},"content":[{"nodeType":"text","value":"scale Python applications","marks":[],"data":{}}]},{"nodeType":"text","value":"Â across multiple cores or machines. It has a couple major advantages including:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Simplicity: you can scale your Python applications without rewriting them, and the same code can run on one machine or multiple machines.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Robustness: applications gracefully handle machine failures and preemption.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://towardsdatascience.com/10x-faster-parallel-python-without-python-multiprocessing-e5017c93cce1"},"content":[{"nodeType":"text","value":"Performance","marks":[],"data":{}}]},{"nodeType":"text","value":": tasks run with millisecond latencies, scale to tens of thousands of cores, and handle numerical data with minimal serialization overhead.","marks":[],"data":{}}]}]}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Library Ecosystem","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Because Ray is a general-purpose framework, the community has built many libraries and frameworks on top of it to accomplish different tasks. The vast majority of these support PyTorch, require minimal modifications to your code, and integrate seamlessly with each other. Below are just a few of theÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://www.anyscale.com/blog/understanding-the-ray-ecosystem-and-community"},"content":[{"nodeType":"text","value":"many libraries","marks":[],"data":{}}]},{"nodeType":"text","value":"Â in the ecosystem.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"RaySGD","marks":[{"type":"bold"}],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"Q6imUHLrgZxNCogiojmSh","type":"Asset","createdAt":"2021-03-02T07:37:27.295Z","updatedAt":"2021-03-02T07:38:12.352Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"locale":"en-US"},"fields":{"title":"RaySGDPyTorchDataParallel","description":"Comparison of PyTorchâs DataParallel vs Ray (which uses PyTorchâs Distributed DataParallel underneath the hood) on p3dn.24xlarge instances.\n","file":{"url":"//images.ctfassets.net/xjan103pcp94/Q6imUHLrgZxNCogiojmSh/ce3753b76bd8ebd0cb550aac566e0c0d/RaySGDPyTorchDataParallel.png","details":{"size":17765,"image":{"width":600,"height":371}},"fileName":"RaySGDPyTorchDataParallel.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"RaySGD is a library that provides distributed training wrappers for data parallel training. For example, theÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/raysgd/raysgd_pytorch.html"},"content":[{"nodeType":"text","value":"RaySGD TorchTrainer","marks":[],"data":{}}]},{"nodeType":"text","value":"Â is a wrapper around torch.distributed.launch. It provides a Python API to easily incorporate distributed training into a larger Python application, as opposed to needing to wrap your training code in bash scripts.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Some other advantages of the library are:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ease of use: You can scale PyTorchâs native DistributedDataParallel without needing to monitor individual nodes.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Scalability: You can scale up and down. Start on a single CPU. Scale up to multi-node, multi-CPU, or multi-GPU clusters by changing 2 lines of code.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Accelerated Training: There is built-in support for mixed precision training with NVIDIA Apex.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Fault Tolerance: There is support for automatic recovery when cloud machines are preempted.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Compatibility: There is seamless integration with other libraries likeÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/tune/index.html"},"content":[{"nodeType":"text","value":"Ray Tune","marks":[],"data":{}}]},{"nodeType":"text","value":"Â andÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/serve/"},"content":[{"nodeType":"text","value":"Ray Serve","marks":[],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"You can get started with TorchTrainer by installing Ray (pip install -U ray torch) and running the code below:","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"30HYvUjeSTXplk0SZU4ELH","type":"Entry","createdAt":"2021-03-02T07:52:04.158Z","updatedAt":"2021-03-02T07:56:59.961Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"mini_cifar_example.py","body":"import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nimport torchvision.transforms as transforms\n\nimport ray\nfrom ray.util.sgd.torch import TorchTrainer\n# https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\nfrom ray.util.sgd.torch.resnet import ResNet18\n\ndef cifar_creator(config):\n    \"\"\"Returns dataloaders to be used in `train` and `validate`.\"\"\"\n    tfms = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465),\n                             (0.2023, 0.1994, 0.2010)),\n    ])  # meanstd transformation\n    train_loader = DataLoader(\n        CIFAR10(root=\"~/data\", download=True, transform=tfms), batch_size=config[\"batch\"])\n    validation_loader = DataLoader(\n        CIFAR10(root=\"~/data\", download=True, transform=tfms), batch_size=config[\"batch\"])\n    return train_loader, validation_loader\n\ndef optimizer_creator(model, config):\n    \"\"\"Returns an optimizer (or multiple)\"\"\"\n    return torch.optim.SGD(model.parameters(), lr=config[\"lr\"])\n\nray.init()\n\ntrainer = TorchTrainer(\n    model_creator=ResNet18,  # A function that returns a nn.Module\n    data_creator=cifar_creator,  # A function that returns dataloaders\n    optimizer_creator=optimizer_creator,  # A function that returns an optimizer\n    loss_creator=torch.nn.CrossEntropyLoss,  # A loss function\n    config={\"lr\": 0.01, \"batch\": 64},  # parameters\n    num_workers=2,  # amount of parallelism\n    use_gpu=torch.cuda.is_available(),\n    use_tqdm=True)\n\nstats = trainer.train()\nprint(trainer.validate())\n\ntorch.save(trainer.state_dict(), \"checkpoint.pt\")\ntrainer.shutdown()\nprint(\"success!\")","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The script will download CIFAR10 and use a ResNet18 model to do image classification. With a single parameter change (num_workers=N), you can utilize multiple GPUs.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If you would like to learn more about RaySGD and how to scale PyTorch training across a cluster, you should check out thisÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://medium.com/distributed-computing-with-ray/faster-and-cheaper-pytorch-with-raysgd-a5a44d4fd220"},"content":[{"nodeType":"text","value":"blog post","marks":[],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Tune","marks":[{"type":"bold"}],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"VhlE1ooVZbQoFRUw79NH0","type":"Asset","createdAt":"2021-03-02T08:41:07.223Z","updatedAt":"2021-03-02T08:41:07.223Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"RayTuneImageDeepmind","description":"Ray Tuneâs implementation of optimization algorithms like Population Based Training (shown above) can be used with PyTorch for more performant models. Image from Deepmind.","file":{"url":"//images.ctfassets.net/xjan103pcp94/VhlE1ooVZbQoFRUw79NH0/a92081f63ac38251a668bb659f1b47f2/RayTuneImageDeepmind.png","details":{"size":52939,"image":{"width":800,"height":497}},"fileName":"RayTuneImageDeepmind.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/tune/index.html"},"content":[{"nodeType":"text","value":"Ray Tune","marks":[],"data":{}}]},{"nodeType":"text","value":"Â is a Python library for experiment execution and hyperparameter tuning at any scale. Some advantages of the library are:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The ability to launch a multi-node ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/tune/tutorials/tune-distributed.html#tune-distributed"},"content":[{"nodeType":"text","value":"distributed hyperparameter sweep","marks":[],"data":{}}]},{"nodeType":"text","value":" in fewer than 10 lines of code.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Support for every major machine learning framework ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html"},"content":[{"nodeType":"text","value":"including PyTorch","marks":[],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"First-class support for GPUs.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Automatic management of checkpoints and logging to ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/tune/user-guide.html#tune-logging"},"content":[{"nodeType":"text","value":"TensorBoard","marks":[],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Access to state of the art algorithms such as ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/tune/api_docs/schedulers.html#tune-scheduler-pbt"},"content":[{"nodeType":"text","value":"Population Based Training (PBT)","marks":[],"data":{}}]},{"nodeType":"text","value":",Â ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/tune/api_docs/suggestion.html#bayesopt"},"content":[{"nodeType":"text","value":"BayesOptSearch","marks":[],"data":{}}]},{"nodeType":"text","value":",Â ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/tune/api_docs/schedulers.html#tune-scheduler-hyperband"},"content":[{"nodeType":"text","value":"HyperBand/ASHA","marks":[],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"You can get started with Ray Tune by installing Ray (pip install ray torch torchvision) and running the code below.","marks":[],"data":{}}]},{"nodeType":"embedded-entry-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"nNtI5j7Kz10TPyBKp0ZK5","type":"Entry","createdAt":"2021-03-02T08:15:22.437Z","updatedAt":"2021-03-02T08:15:22.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"fullscript.py","body":"import numpy as np\nimport torch\nimport torch.optim as optim\n\nfrom ray import tune\nfrom ray.tune.examples.mnist_pytorch import get_data_loaders, train, test\nimport ray\nimport sys\n\nif len(sys.argv) \u003e 1:\n    ray.init(redis_address=sys.argv[1])\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n        self.fc = nn.Linear(192, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n        x = x.view(-1, 192)\n        x = self.fc(x)\n        return F.log_softmax(x, dim=1)\n\ndef train_mnist(config):\n    model = ConvNet()\n    train_loader, test_loader = get_data_loaders()\n    optimizer = optim.SGD(\n        model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n    for i in range(10):\n        train(model, optimizer, train_loader, torch.device(\"cpu\"))\n        acc = test(model, test_loader, torch.device(\"cpu\"))\n        tune.track.log(mean_accuracy=acc)\n        if i % 5 == 0:\n            # This saves the model to the trial directory\n            torch.save(model.state_dict(), \"./model.pth\")\n\nfrom ray.tune.schedulers import ASHAScheduler\n\nsearch_space = {\n    \"lr\": tune.choice([0.001, 0.01, 0.1]),\n    \"momentum\": tune.uniform(0.1, 0.9)\n}\n\nanalysis = tune.run(\n    train_mnist,\n    num_samples=30,\n    scheduler=ASHAScheduler(metric=\"mean_accuracy\", mode=\"max\", grace_period=1),\n    config=search_space)","language":"python"}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The script shows you how to leverage a state-of-the-art early stopping algorithm AHSA which terminates trials that are less promising and allocates more time and resources to more promising trials.Â If you would like to learn about how to incorporate Ray Tune into your PyTorch workflow, you should check out thisÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://towardsdatascience.com/fast-hyperparameter-tuning-at-scale-d428223b081c"},"content":[{"nodeType":"text","value":"blog post","marks":[],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Ray Serve","marks":[{"type":"bold"}],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4XPqEH0SpcR70YOeOygTHb","type":"Asset","createdAt":"2021-03-02T08:19:00.327Z","updatedAt":"2021-03-02T08:19:00.327Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"FastAPI_PyTorch","description":"Ray Serve can not only be used to serve models on its own, but also to scale other serving tools like FastAPI.","file":{"url":"//images.ctfassets.net/xjan103pcp94/4XPqEH0SpcR70YOeOygTHb/68401bf0045e8e3f680aadc514b7970c/fastAPI.png","details":{"size":22121,"image":{"width":851,"height":214}},"fileName":"fastAPI.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/serve/"},"content":[{"nodeType":"text","value":"Ray Serve","marks":[],"data":{}}]},{"nodeType":"text","value":"Â is a library for easy-to-use scalable model serving. Some advantages of the library are:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"The ability to use a single toolkit to serve everything from deep learning models (PyTorch, TensorFlow, etc) to scikit-learn models, to arbitrary Python business logic.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Scale to many machines, both in your datacenter and in the cloud.","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Compatibility with many other libraries like ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://medium.com/distributed-computing-with-ray/how-to-scale-up-your-fastapi-application-using-ray-serve-c9a7b69e786"},"content":[{"nodeType":"text","value":"Ray TuneÂ andÂ FastAPI","marks":[],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"If you would like to learn how to incorporate Ray Serve and Ray Tune together into your PyTorch workflow, you should check out theÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/tune/tutorials/tune-serve-integration-mnist.html#tune-trainable-for-model-selection"},"content":[{"nodeType":"text","value":"documentation","marks":[],"data":{}}]},{"nodeType":"text","value":"Â for aÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/tune/tutorials/tune-serve-integration-mnist.html#sphx-glr-download-tune-tutorials-tune-serve-integration-mnist-py"},"content":[{"nodeType":"text","value":"full code example","marks":[],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"RLlib","marks":[{"type":"bold"}],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6WhBZxLou5CQvNqXwIgz2g","type":"Asset","createdAt":"2021-03-02T08:22:04.827Z","updatedAt":"2021-03-02T08:22:04.827Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"rllibPyTorchBlog","description":"RLlib provides ways to customize almost all aspects of training, including neural network models, action distributions, policy definitions, environments, and the sample collection process.","file":{"url":"//images.ctfassets.net/xjan103pcp94/6WhBZxLou5CQvNqXwIgz2g/4975d674eb44596345b98e45815787d1/rllibPyTorchBlog.png","details":{"size":71209,"image":{"width":933,"height":398}},"fileName":"rllibPyTorchBlog.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/rllib.html"},"content":[{"nodeType":"text","value":"RLlib","marks":[],"data":{}}]},{"nodeType":"text","value":"Â is a library for reinforcement learning that offers both high scalability and a unified API for a variety of applications. Some advantages include:","marks":[],"data":{}}]},{"nodeType":"unordered-list","data":{},"content":[{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Native support for PyTorch, TensorFlow Eager, and TensorFlow (1.x and 2.x)","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Support for model-free, model-based, evolutionary, planning, and multi-agent algorithms","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Support for complex model types, such as attention nets and LSTM stacks via simple config flags and auto-wrappers","marks":[],"data":{}}]}]},{"nodeType":"list-item","data":{},"content":[{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Compatibility with other libraries like ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/rllib-training.html"},"content":[{"nodeType":"text","value":"Ray Tune","marks":[],"data":{}}]},{"nodeType":"text","value":"","marks":[],"data":{}}]}]}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Cluster Launcher","marks":[{"type":"bold"}],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5ZNVSnDSiL3xFlMD0anHs4","type":"Asset","createdAt":"2021-03-02T08:26:24.100Z","updatedAt":"2021-03-02T08:26:24.100Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"ClusterLauncherPyTorch","description":"The Ray Cluster Launcher simplifies the process of launching and scaling across any cluster or cloud provider.","file":{"url":"//images.ctfassets.net/xjan103pcp94/5ZNVSnDSiL3xFlMD0anHs4/2752e1b95fb61f92c68478e3e63b047d/ClusterLauncherPyTorch.png","details":{"size":193302,"image":{"width":924,"height":379}},"fileName":"ClusterLauncherPyTorch.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Once you have developed an application on your laptop and want to scale it up to the cloud (perhaps with more data or more GPUs), the next steps arenât always clear. The process is either to have an infrastructure team set it up for you or to go through the following steps.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"1. Choose a cloud provider (AWS, GCP, or Azure).","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"2. Navigate the management console to set instance types, security groups, spot prices, instance limits, and more.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"3. Figure out how to distribute your Python script across a cluster.","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"An easier approach is to use theÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://medium.com/distributed-computing-with-ray/how-to-scale-python-on-every-major-cloud-provider-12b3bde01208"},"content":[{"nodeType":"text","value":"Ray Cluster Launcher to launch and scale machines across any cluster or cloud provider","marks":[],"data":{}}]},{"nodeType":"text","value":". Cluster Launcher allows you autoscale, sync files, submit scripts, port forward, and more. This means that you can run your Ray clusters on Kubernetes, AWS, GCP, Azure, or a private cluster without needing to understand the low-level details of cluster management.","marks":[],"data":{}}]},{"nodeType":"heading-2","data":{},"content":[{"nodeType":"text","value":"Conclusion","marks":[{"type":"bold"}],"data":{}}]},{"nodeType":"embedded-asset-block","data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7ephbbyHPz1MiLKTwX7vmS","type":"Asset","createdAt":"2021-03-02T08:28:52.827Z","updatedAt":"2021-03-02T08:28:52.827Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"AntGroupFusionEngine","description":"Ray provides a distributed computing foundation for Ant Groupâs Fusion Engine.","file":{"url":"//images.ctfassets.net/xjan103pcp94/7ephbbyHPz1MiLKTwX7vmS/2ed1aaebb92aa51fa130a906f6aa48dc/AntGroupFusionEngine.png","details":{"size":71227,"image":{"width":933,"height":320}},"fileName":"AntGroupFusionEngine.png","contentType":"image/png"}}}},"content":[]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"This article contained some of the benefits of Ray in the PyTorch ecosystem. Ray is being used for a wide variety of applications fromÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://youtu.be/Wwv9YNlXx0Q"},"content":[{"nodeType":"text","value":"Ant Group using Ray to support its financial business","marks":[],"data":{}}]},{"nodeType":"text","value":", toÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://youtu.be/0Z0Th9ySIfs"},"content":[{"nodeType":"text","value":"LinkedIn running Ray on Yarn","marks":[],"data":{}}]},{"nodeType":"text","value":", toÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://youtu.be/U9deXfKYDSs"},"content":[{"nodeType":"text","value":"Pathmind using Ray to connect reinforcement learning to simulation software","marks":[],"data":{}}]},{"nodeType":"text","value":", and more. If you have any questions or thoughts about Ray or want to learn more aboutÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.ray.io/en/master/"},"content":[{"nodeType":"text","value":"parallel and distributed Python","marks":[],"data":{}}]},{"nodeType":"text","value":", please join our community throughÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://discuss.ray.io/"},"content":[{"nodeType":"text","value":"Discourse","marks":[],"data":{}}]},{"nodeType":"text","value":",Â ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"},"content":[{"nodeType":"text","value":"Slack","marks":[],"data":{}}]},{"nodeType":"text","value":", orÂ ","marks":[],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://github.com/ray-project/ray"},"content":[{"nodeType":"text","value":"GitHub","marks":[],"data":{}}]},{"nodeType":"text","value":".","marks":[],"data":{}}]},{"nodeType":"paragraph","data":{},"content":[{"nodeType":"text","value":"Originally published on ","marks":[{"type":"italic"}],"data":{}},{"nodeType":"hyperlink","data":{"uri":"https://medium.com/pytorch/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-fd83c98fdead"},"content":[{"nodeType":"text","value":"PyTorchâs Blog","marks":[{"type":"italic"}],"data":{}}]},{"nodeType":"text","value":".","marks":[{"type":"italic"}],"data":{}}]}]},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2m0uHe9HF1RJzTebDLqifF","type":"Asset","createdAt":"2021-03-02T07:34:00.264Z","updatedAt":"2021-03-02T07:34:00.264Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"PyTorch + Ray","file":{"url":"//images.ctfassets.net/xjan103pcp94/2m0uHe9HF1RJzTebDLqifF/9ceeeab257e88f7e6bd16e4ad0f0a144/PyTorch_Ray.png","details":{"size":54786,"image":{"width":977,"height":489}},"fileName":"PyTorch_Ray.png","contentType":"image/png"}}},"mainImageFit":"cover","tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}}],"recommendations":[]}},"url":"https://www.anyscale.com/blog/getting-started-with-distributed-machine-learning-with-pytorch-and-ray","ctaText":"Read more","thumbnail":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"wSQM6naQrXBOvxNfv5iYe","type":"Asset","createdAt":"2022-03-24T22:19:37.934Z","updatedAt":"2022-03-24T22:19:37.934Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"blog-recommended-content-nodes-dark","file":{"url":"//images.ctfassets.net/xjan103pcp94/wSQM6naQrXBOvxNfv5iYe/ed3965226bd4e0dc9d39da26d95d6e14/blog-recommended-content-nodes-dark.jpg","details":{"size":41506,"image":{"width":140,"height":140}},"fileName":"blog-recommended-content-nodes-dark.jpg","contentType":"image/jpeg"}}},"recommendations":null}}],"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}}],"hideIntro":true}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"9sNdiy0FTvOnKdJmgEOT4","type":"Entry","createdAt":"2021-08-09T20:38:34.238Z","updatedAt":"2022-06-22T17:01:22.347Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPost"}},"locale":"en-US"},"fields":{"title":"Introducing Distributed LightGBM Training with Ray","slug":"introducing-distributed-lightgbm-training-with-ray","authors":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BGRfsYFKUgIpGjaXTUYml","type":"Entry","createdAt":"2021-08-09T15:38:43.390Z","updatedAt":"2021-08-09T15:38:43.390Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Antoni Baum","slug":"antoni-baum","link":"https://www.linkedin.com/in/yard1/"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7fm8mbdJEOBiNMuOJKqLJj","type":"Entry","createdAt":"2021-08-09T15:40:10.306Z","updatedAt":"2021-08-09T15:40:10.306Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Will Drevo","slug":"will-drevo","link":"https://www.linkedin.com/in/willdrevo/"}}],"publishedDate":"2021-08-10","type":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},"intro":"LightGBM is a gradient boosting framework based on tree-based learning algorithms. Compared to XGBoost, it is a relatively new framework, but one that is quickly becoming popular in both academic and production use cases.Â \nWeâre excited to announce a beta release of LightGBM on Ray, which lets you easily distribute your training on your Ray cluster.","body":{"data":{},"content":[{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"pNeBWppPVsjmrsHYinvEa","type":"Asset","createdAt":"2021-08-09T16:33:50.896Z","updatedAt":"2021-08-09T16:33:50.896Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"LightGBM Ray","file":{"url":"//images.ctfassets.net/xjan103pcp94/pNeBWppPVsjmrsHYinvEa/1cf2172636dee8c070cd2faf0562e95a/RAYlightGBM.png","details":{"size":50225,"image":{"width":1222,"height":360}},"fileName":"RAYlightGBM.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://lightgbm.readthedocs.io/en/latest/"},"content":[{"data":{},"marks":[{"type":"bold"},{"type":"underline"}],"value":"LightGBM","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" is a gradient boosting framework based on tree-based learning algorithms. Compared to XGBoost, it is a relatively new framework, but one that is quickly becoming popular in both academic and production use cases.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Today, weâre excited to announce a beta release of ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/lightgbm_ray/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LightGBM on Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", which lets you easily distribute your training on your Ray cluster.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Features of LightGBM-Ray include:","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/lightgbm_ray/#usage"},"content":[{"data":{},"marks":[],"value":"Multi-node","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/lightgbm_ray/#multi-gpu-training"},"content":[{"data":{},"marks":[],"value":"multi-GPU","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" training support","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Seamless integration with ","nodeType":"text"},{"data":{"uri":"http://tune.io/"},"content":[{"data":{},"marks":[],"value":"Ray Tune","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" for distributed hyperparameter searchÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/lightgbm_ray/#fault-tolerance"},"content":[{"data":{},"marks":[],"value":"Fault tolerance handling","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"Â ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/lightgbm_ray/#distributed-data-loading"},"content":[{"data":{},"marks":[],"value":"Distributed dataframes and distributed data loading support","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Why yet another gradient boosting algorithm?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"If you are familiar with gradient boosting methods, you are likely well-versed with XGBoost, which has established itself as the standard for training ML models on tabular data. XGBoost is a terrific library that is used extensively in production by ML-heavyweights like ","nodeType":"text"},{"data":{"uri":"https://eng.uber.com/elastic-xgboost-ray/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Uber","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://stripe.com/blog/railyard-training-models"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Stripe","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"So you might be wondering, why yet another gradient boosting library? LightGBM is designed to be fast and efficient, and offers several advantages:Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Faster training","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Built-in support for categorical variablesÂ ","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Optimizations for ","nodeType":"text"},{"data":{"uri":"https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-distributed-learning"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"training on larger datasets","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":"","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"},{"data":{},"content":[{"data":{},"content":[{"data":{},"marks":[],"value":"Better accuracy in certain situations","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"list-item"}],"nodeType":"unordered-list"},{"data":{},"content":[{"data":{},"marks":[],"value":"The good news is that you donât have to choose. Since it is based on ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/xgboost_ray"},"content":[{"data":{},"marks":[],"value":"XGBoost-Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":", the LightGBM on Ray integration lets you easily switch between XGBoost on Ray and LightGBM on Ray for your classification or regression problems (support for ranking coming soon), compare the results, and choose the one that works best for your usecase. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Getting started with LightGBM on Ray","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs take a look at a few examples that show you can easily port your existing LightGBM code to use this new integration, and run in distributed mode. In order to run all the code in this section, ","nodeType":"text"},{"data":{"uri":"https://scikit-learn.org/stable/install.html"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"scikit-learn","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/lightgbm_ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"lightgbm_ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" need to be installed. This can be done with ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"72zCeLKTmp4yHbQA9YgSKX","type":"Entry","createdAt":"2021-08-09T15:46:34.163Z","updatedAt":"2021-08-09T15:46:34.163Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"pip install sklearn lightgbm_ray","body":"pip install sklearn lightgbm_ray","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Training","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Letâs start with a simple, non-distributed example, running on a single node with core LightGBM.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3pgrVZtVeA0o7Eygn6BKKZ","type":"Entry","createdAt":"2021-08-09T15:47:59.157Z","updatedAt":"2021-08-10T02:37:29.890Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"train LightGBM","body":"from lightgbm import Dataset, train\nfrom sklearn.datasets import load_breast_cancer\n\ntrain_x, train_y = load_breast_cancer(return_X_y=True)\ntrain_set = Dataset(train_x, train_y)\n\nevals_result = {}\nbst = train(\n    {\n      \"objective\": \"binary\",\n      \"metric\": [\"binary_logloss\", \"binary_error\"],\n    },\n    train_set,\n    num_boost_round=10,\n    evals_result=evals_result,\n    valid_sets=[train_set],\n    valid_names=[\"train\"],\n    verbose_eval=False,)\n\nbst.save_model(\"model.lgbm\")","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"If your run is successful, you should see a number of ","nodeType":"text"},{"data":{},"marks":[{"type":"code"}],"value":"[LightGBM] [Warning] No further splits with positive gain, best gain: -inf","nodeType":"text"},{"data":{},"marks":[],"value":" lines as LightGBM finishes training and decides no more splits are needed.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Now letâs scale out. Just by changing four lines of code, we can turn this into a distributed run. The changes have been highlighted below.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7sL1hOa7bOmBIDQoQ1Olfd","type":"Entry","createdAt":"2021-08-09T16:45:12.278Z","updatedAt":"2021-08-10T02:43:15.507Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"Highlighted LightGBM run","body":"from lightgbm_ray import RayDMatrix, RayParams, train\nfrom sklearn.datasets import load_breast_cancer\n\ntrain_x, train_y = load_breast_cancer(return_X_y=True)\ntrain_set = RayDMatrix(train_x, train_y)\n\nevals_result = {}\nbst = train(\n    {\n      \"objective\": \"binary\",\n      \"metric\": [\"binary_logloss\", \"binary_error\"],\n    },\n    train_set,\n    num_boost_round=10,\n    evals_result=evals_result,\n    valid_sets=[train_set],\n    valid_names=[\"train\"],\n    verbose_eval=False,\n    ray_params=RayParams(num_actors=2, cpus_per_actor=2))\n\nbst.booster_.save_model(\"model.lgbm\")","highlightRows":["1","5","19","21"],"language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Like XGBoost-Ray, LightGBM-Ray also uses the ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/xgboost_ray/blob/master/xgboost_ray/matrix.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"RayDMatrix","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" class for datasets. This lets you easily switch between LightGBM-Ray and XGBoost-Ray just by changing the import statement. ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Prediction","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"Distributed inference can be run in a similar way to XGBoost-Ray.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5l7l7CUmmaNZPKpYOgz3RN","type":"Entry","createdAt":"2021-08-09T16:52:31.892Z","updatedAt":"2021-08-09T16:52:44.118Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"LightGBM Prediction","body":"from sklearn import datasets\nimport lightgbm as lgbm\nfrom lightgbm_ray import RayDMatrix, predict\n\ndata, labels = datasets.load_breast_cancer(return_X_y=True)\n\ndpred = RayDMatrix(data, labels)\n\nbst = lgbm.Booster(model_file=\"model.lgbm\")\npredictions = predict(bst, dpred)\n\nprint(predictions)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"Scikit-learn API","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"LightGBM-Ray also provides a fully functional scikit-learn API for both training and prediction. You can either use NumPy arrays or pandas dataframes, which will be converted internally, or you can pass a RayDMatrix object for greater control.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"669S69JGOF8psOQWWUL9HO","type":"Entry","createdAt":"2021-08-09T17:07:20.329Z","updatedAt":"2021-08-10T02:44:19.422Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"code"}},"locale":"en-US"},"fields":{"identifier":"lightGBM Scikit-Learn API","body":"from lightgbm_ray import RayLGBMClassifier, RayParams\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\nseed = 42\n\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=0.25, random_state=42)\n\nclf = RayLGBMClassifier(\n    n_jobs=2,  # In LightGBM-Ray, n_jobs sets the number of actors\n    random_state=seed)\n\n# scikit-learn API will automatically convert the data to RayDMatrix format as needed.\n# You can also pass X as a RayDMatrix, in which case y will be ignored.\n\nclf.fit(X_train, y_train)\n\npred_ray = clf.predict(X_test)\nprint(pred_ray)\n\npred_proba_ray = clf.predict_proba(X_test)\nprint(pred_proba_ray)\n\n# It is also possible to pass a RayParams object to fit/predict/predict_proba methods - will override\n# n_jobs set during initialization\n\nclf.fit(X_train, y_train, ray_params=RayParams(num_actors=2))\n\npred_ray = clf.predict(X_test, ray_params=RayParams(num_actors=2))\nprint(pred_ray)","language":"python"}}},"content":[],"nodeType":"embedded-entry-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"OK, but how do I load real-world data into LightGBM-Ray?","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"See the ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/lightgbm_ray/blob/main/lightgbm_ray/examples/higgs.py"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"Higgs LightGBM-Ray example code","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" from the Github repository.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"There youâll find a full example that reads from a CSV and trains a classification model.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Benchmarking","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"We have run two simple benchmarks comparing LightGBM-Ray to XGBoost-Ray and non-distributed LightGBM.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Comparison to XGBoost depending on the number of workers","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tkqGQqfQ1ii3tQRyo8UKJ","type":"Asset","createdAt":"2021-08-09T20:09:57.746Z","updatedAt":"2021-08-09T20:18:11.620Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":4,"locale":"en-US"},"fields":{"title":"LightGBM Table 1","file":{"url":"//images.ctfassets.net/xjan103pcp94/4tkqGQqfQ1ii3tQRyo8UKJ/c6495faf6dc14db97ed4c4575f30f1d9/Presentation2.png","details":{"size":327671,"image":{"width":3840,"height":2160}},"fileName":"Presentation2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"We can see that with a large synthetic dataset, distributing LightGBM using Ray can reduce training time by over 66%. Furthermore, LightGBM-Ray consistently outperforms XGBoost-Ray on training time, but does lose out on accuracy (for this particular dataset).","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[{"type":"bold"}],"value":"Comparison with XGBoost-Ray during hyperparameter tuning with Ray Tune","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"This experiment was conducted using a million row dataset and a 75-25 train-test split. Both XGBoost-Ray and LightGBM-Ray were distributed over 8 actors per trial, each using 2 threads. There were 4 trials running concurrently with a deadline of 5 minutes. The search spaces were identical.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{"target":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"23JZn2C8bJSNYe1hdrovQ0","type":"Asset","createdAt":"2021-08-09T20:34:47.777Z","updatedAt":"2021-08-09T20:43:36.506Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"locale":"en-US"},"fields":{"title":"LightGBM Chart 2","file":{"url":"//images.ctfassets.net/xjan103pcp94/23JZn2C8bJSNYe1hdrovQ0/0a9202b1f483ccfb44e8e9bc85b0abb9/LightGBMChart2.png","details":{"size":189442,"image":{"width":2999,"height":1687}},"fileName":"LightGBMChart2.png","contentType":"image/png"}}}},"content":[],"nodeType":"embedded-asset-block"},{"data":{},"content":[{"data":{},"marks":[],"value":"By using Ray Tune for hyperparameter optimization, LightGBM is able to narrow the gap between XGBoost-Ray in terms of accuracy, even slightly outperforming it in the same time budget. As LightGBM trains faster than XGBoost, it is possible to evaluate more hyperparameter combinations, increasing the chances of finding those that increase accuracy.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Architecture of LightGBM-Ray","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"LightGBM-Ray does not change how LightGBM works. Instead, it manages the data sharding and actors through Ray. It distributes LightGBM training and prediction by dividing up the data among several Ray Actors, running either on your laptop or in a multi-node Ray cluster. Each of those Actors then uses built-in LightGBM socket-based communication to share information about the training state.Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Furthermore, LightGBM-Rayâs fault tolerance mechanisms ensure that training will be automatically restarted (without the need to read data again) should an Actor die for any reason.Â \n\nThis is a ","nodeType":"text"},{"data":{"uri":"https://www.anyscale.com/blog/distributed-xgboost-training-with-ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"very similar setup to XGBoost-Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":".","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Known Issues","nodeType":"text"}],"nodeType":"heading-3"},{"data":{},"content":[{"data":{},"marks":[],"value":"As of now, due to many common internals, ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/lightgbm_ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"LightGBM-Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" uses ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/xgboost_ray"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"XGBoost-Ray","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" as a hard dependency, necessitating XGBoost to be installed. We are working on removing that requirement. Furthermore, elastic training (present in XGBoost-Ray) is not yet supported, but will be added in a future release.","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Using too many actors (controlled by the num_actors parameter in a RayParams object) may result in a drop of accuracy, if each of them receives too small of a portion of a dataset. The same issue may occur if the data isnât well-shuffled. This is due to how LightGBM conducts distributed training. Additionally, in order to ensure efficient training, each actor requires at least two CPUs, so that the communication thread can run without blocking (","nodeType":"text"},{"data":{"uri":"https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html#configuring-the-dask-cluster"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"as per LightGBM documentation","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":").","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"Conclusion","nodeType":"text"}],"nodeType":"heading-2"},{"data":{},"content":[{"data":{},"marks":[],"value":"LightGBM is a fast training and accurate alternative to XGBoost that offers many advantages. With LightGBM on Ray, itâs now possible to scale your LightGBM code on any cloud provider with just a few code changes .Â ","nodeType":"text"}],"nodeType":"paragraph"},{"data":{},"content":[{"data":{},"marks":[],"value":"We welcome feedback and issues ","nodeType":"text"},{"data":{"uri":"https://github.com/ray-project/lightgbm_ray/issues"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"on GitHub","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":" and ","nodeType":"text"},{"data":{"uri":"https://discuss.ray.io/"},"content":[{"data":{},"marks":[{"type":"underline"}],"value":"on the Ray Discourse","nodeType":"text"}],"nodeType":"hyperlink"},{"data":{},"marks":[],"value":". Let us know if you run into problems!","nodeType":"text"}],"nodeType":"paragraph"}],"nodeType":"document"},"mainImage":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3XJGF7ybBwEz214fbWQgFa","type":"Asset","createdAt":"2021-08-09T20:39:24.849Z","updatedAt":"2021-08-09T20:39:24.849Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"locale":"en-US"},"fields":{"title":"Ray + LightGBM","file":{"url":"//images.ctfassets.net/xjan103pcp94/3XJGF7ybBwEz214fbWQgFa/55edc6d070e091d97e6fed78f7629005/RAYlightGBM.png","details":{"size":50225,"image":{"width":1222,"height":360}},"fileName":"RAYlightGBM.png","contentType":"image/png"}}},"tags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}}],"hideIntro":true,"recommendations":[]}}],"activeTag":null,"activeType":null,"author":{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"7fm8mbdJEOBiNMuOJKqLJj","type":"Entry","createdAt":"2021-08-09T15:40:10.306Z","updatedAt":"2021-08-09T15:40:10.306Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"author"}},"locale":"en-US"},"fields":{"name":"Will Drevo","slug":"will-drevo","link":"https://www.linkedin.com/in/willdrevo/","recommendations":[]}},"page":1,"totalPages":1,"allTypes":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"46OK1jYn9uj0d6TqHQuA9h","type":"Entry","createdAt":"2022-06-15T01:31:45.525Z","updatedAt":"2022-06-15T01:31:45.525Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"News","slug":"news","tintColor":"#FF9600"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5KeH0JzrIEpuheazhMAhKf","type":"Entry","createdAt":"2022-06-15T01:31:23.950Z","updatedAt":"2022-06-15T01:31:23.950Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"User Story","slug":"user-story","tintColor":"#6D59C6"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5fIT04OR3VoZwhSSxorKjL","type":"Entry","createdAt":"2022-06-15T01:31:00.437Z","updatedAt":"2022-06-15T01:31:00.437Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Engineering","slug":"engineering","tintColor":"#234999"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"42F9iHs5a05rRrOuqEQwvB","type":"Entry","createdAt":"2022-06-15T01:30:47.322Z","updatedAt":"2022-06-15T01:30:47.322Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"blogPostType"}},"locale":"en-US"},"fields":{"title":"Culture","slug":"culture","tintColor":"#39C8E5"}}],"allTags":[{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"48DCxKSHFPbblq3jTbrAbh","type":"Entry","createdAt":"2021-11-23T01:08:18.782Z","updatedAt":"2022-06-22T15:37:09.011Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray RLlib","identifier":"rllib"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"3RrNIHV8OsKPvqBCwrIp1n","type":"Entry","createdAt":"2021-12-05T04:55:33.551Z","updatedAt":"2022-06-22T15:36:44.320Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Train","identifier":"ray_train"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"2pJdkO0i5e1L5nY97kKmJ8","type":"Entry","createdAt":"2022-06-22T15:36:12.577Z","updatedAt":"2022-06-22T15:36:12.577Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Datasets","identifier":"ray-datasets"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4KwkbI6zRqcE9KD5iKuP8W","type":"Entry","createdAt":"2021-12-05T04:51:33.974Z","updatedAt":"2021-12-05T04:51:33.974Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"RayDP","identifier":"ray_dp"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"1OiVmfyPaDqRNavVG5Yp1l","type":"Entry","createdAt":"2021-12-05T04:50:12.541Z","updatedAt":"2021-12-05T04:50:12.541Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Retail","identifier":"retail"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"161LHBXwPkxUmhjt1YbfJH","type":"Entry","createdAt":"2021-12-05T04:49:42.528Z","updatedAt":"2021-12-05T04:49:42.528Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Gaming","identifier":"gaming"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"BDTwwkSW9VtmSkJojGbx8","type":"Entry","createdAt":"2021-12-05T04:49:30.169Z","updatedAt":"2021-12-05T04:49:30.169Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Government","identifier":"government"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4fzKvVOn2R8U6TTi2bcb2R","type":"Entry","createdAt":"2021-12-05T04:49:10.881Z","updatedAt":"2021-12-05T04:49:10.881Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"HLS","identifier":"hls"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"GGg4W2UlqfVP2iPMsc8J1","type":"Entry","createdAt":"2021-12-05T04:44:22.472Z","updatedAt":"2021-12-05T04:44:22.472Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Financial","identifier":"financial"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"DB5qQfNQvn6rz3ff2IQUq","type":"Entry","createdAt":"2021-12-03T22:34:01.808Z","updatedAt":"2021-12-03T22:34:01.808Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Serve","identifier":"ray_serve"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"i6fp6Gfdy5wFeDDOaKcfh","type":"Entry","createdAt":"2021-12-03T22:33:42.248Z","updatedAt":"2021-12-03T22:33:42.248Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Anyscale","identifier":"anyscale"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"4tNMUjWZl2OPNdjqn19Agd","type":"Entry","createdAt":"2021-12-03T22:28:54.795Z","updatedAt":"2021-12-03T22:28:54.795Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Core","identifier":"ray_core"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"6chYLcIc6yEB2EtLv2vngw","type":"Entry","createdAt":"2021-11-23T01:06:51.725Z","updatedAt":"2021-11-30T22:20:19.315Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Healthcare","identifier":"healthcare"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5BzSmyQ2ojATOq6qOpD4PX","type":"Entry","createdAt":"2021-10-02T19:52:20.790Z","updatedAt":"2021-10-02T19:52:20.790Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Technology","identifier":"technology"}},{"metadata":{"tags":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"xjan103pcp94"}},"id":"5dMC68RISCoahmEKj8nX7H","type":"Entry","createdAt":"2020-11-02T05:16:15.386Z","updatedAt":"2020-11-02T05:16:15.386Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"revision":1,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"tag"}},"locale":"en-US"},"fields":{"name":"Ray Tune","identifier":"ray-tune"}}]},"__N_SSP":true},"page":"/blog","query":{"author":"will-drevo"},"buildId":"-oT8_WeW9m_lAxr26cJvY","isFallback":false,"gssp":true,"customServer":true,"scriptLoader":[]}</script><script type="text/javascript" id="hs-script-loader" async="" defer="" src="https://js.hs-scripts.com/20523749.js"></script><script type="text/javascript">
  _linkedin_partner_id = "4210313";
  window._linkedin_data_partner_ids = window._linkedin_data_partner_ids || [];
  window._linkedin_data_partner_ids.push(_linkedin_partner_id);
</script><script type="text/javascript">
  (function(l) { if (!l){window.lintrk = function(a,b){window.lintrk.q.push([a,b])};
  window.lintrk.q=[]} var s = document.getElementsByTagName("script")[0];
  var b = document.createElement("script");
  b.type = "text/javascript";b.async = true;
  b.src = "https://snap.licdn.com/li.lms-analytics/insight.min.js";
  s.parentNode.insertBefore(b, s);})(window.lintrk);
</script><noscript><img height="1" width="1" style="display:none" alt="" src="https://px.ads.linkedin.com/collect/?pid=4210313&amp;fmt=gif"/></noscript></body></html>