
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Batch Prediction &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/js/versionwarning.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../_static/js/docsearch.js"></script>
    <script src="../../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../_static/js/custom.js"></script>
    <script defer="defer" src="../../_static/js/top-navigation.js"></script>
    <script src="../../_static/js/tags.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script async="async" src="../../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/ray-core/examples/batch_prediction.html" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Batch Training with Ray Core" href="batch_training.html" />
    <link rel="prev" title="Using Ray for Highly Parallelizable Tasks" href="highly_parallel.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "ray-core/examples/batch_prediction", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".ipynb", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../walkthrough.html">
   Ray Core
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="../key-concepts.html">
     Key Concepts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../user-guide.html">
     User Guides
    </a>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="overview.html">
     Examples
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3">
      <a class="reference internal" href="gentle_walkthrough.html">
       A Gentle Introduction to Ray Core by Example
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="monte_carlo_pi.html">
       Monte Carlo Estimation of π
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="plot_example-a3c.html">
       Asynchronous Advantage Actor Critic (A3C)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="plot_example-lm.html">
       Fault-Tolerant Fairseq Training
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="plot_hyperparameter.html">
       Simple Parallel Model Selection
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="plot_parameter_server.html">
       Parameter Server
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="plot_pong_example.html">
       Learning to Play Pong
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="highly_parallel.html">
       Using Ray for Highly Parallelizable Tasks
      </a>
     </li>
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="batch_prediction.html#">
       Batch Prediction
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="batch_training.html">
       Batch Training with Ray Core
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="automl_for_time_series.html">
       Simple AutoML for time series with Ray Core
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="web-crawler.html">
       Speed up your web crawler by parallelizing it with Ray
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="map_reduce.html">
       A Simple MapReduce Example with Ray Core
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/index.html">
     Ray Core API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Fray-core/examples/batch_prediction.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/ray-core/examples/batch_prediction.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/ray-core/examples/batch_prediction.ipynb.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="batch_prediction.html#actor-based-batch-prediction">
   Actor-based batch prediction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="batch_prediction.html#batch-prediction-with-gpus">
   Batch prediction with GPUs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="batch_prediction.html#faqs">
   FAQs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="batch_prediction.html#how-to-load-and-pass-model-efficiently-in-ray-cluster-if-the-model-is-large">
     How to load and pass model efficiently in Ray cluster if the model is large?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="batch_prediction.html#how-to-improve-the-gpu-utilization-rate">
     How to improve the GPU utilization rate?
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Batch Prediction</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="batch_prediction.html#actor-based-batch-prediction">
   Actor-based batch prediction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="batch_prediction.html#batch-prediction-with-gpus">
   Batch prediction with GPUs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="batch_prediction.html#faqs">
   FAQs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="batch_prediction.html#how-to-load-and-pass-model-efficiently-in-ray-cluster-if-the-model-is-large">
     How to load and pass model efficiently in Ray cluster if the model is large?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="batch_prediction.html#how-to-improve-the-gpu-utilization-rate">
     How to improve the GPU utilization rate?
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="batch-prediction">
<h1>Batch Prediction<a class="headerlink" href="batch_prediction.html#batch-prediction" title="Permalink to this headline">#</a></h1>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Refer to the blog on <a class="reference external" href="https://www.anyscale.com/blog/model-batch-inference-in-ray-actors-actorpool-and-datasets">Model Batch Inference in Ray</a> for an overview of batch inference strategies in Ray and additional examples.</p>
</div>
<p>The batch prediction is the process of using a trained model to generate predictions for a collection of observations. It has the following elements:</p>
<ul class="simple">
<li><p>Input dataset: this is a collection of observations to generate predictions for. The data is usually stored in an external storage system like S3, HDFS or database, and can be large.</p></li>
<li><p>ML model: this is a trained ML model which is usually also stored in an external storage system.</p></li>
<li><p>Predictions: these are the outputs when applying the ML model on observations. The predictions are usually written back to the storage system.</p></li>
</ul>
<p>With Ray, you can build scalable batch prediction for large datasets at high prediction throughput. This is achieved by splitting the dataset into disjoint shards and executing them in parallel, with either Ray tasks or Ray actors across a Ray cluster.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
## Task-based batch prediction

With Ray tasks, you can build a batch prediction program in this way:
1. Loads the model
2. Launches Ray tasks, with each taking in the model and a shard of input dataset
3. Each worker executes predictions on the assigned shard, and writes out results

Let’s take NYC taxi data in 2009 for example. Suppose we have this simple model:
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">load_model</span><span class="p">():</span>
    <span class="c1"># A dummy model.</span>
    <span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">batch</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
        <span class="c1"># Dummy payload so copying the model will actually copy some data</span>
        <span class="c1"># across nodes.</span>
        <span class="n">model</span><span class="o">.</span><span class="n">payload</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100_000_000</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;score&quot;</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;passenger_count&quot;</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">})</span>
    
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<p>The dataset has 12 files (one for each month) so we can naturally have each Ray task to take one file. By taking the model and a shard of input dataset (i.e. a single file), we can define a Ray remote task for prediction:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyarrow.parquet</span> <span class="k">as</span> <span class="nn">pq</span>
<span class="kn">import</span> <span class="nn">ray</span>

<span class="nd">@ray</span><span class="o">.</span><span class="n">remote</span>
<span class="k">def</span> <span class="nf">make_prediction</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">shard_path</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">shard_path</span><span class="p">)</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

    <span class="c1"># Write out the prediction result.</span>
    <span class="c1"># NOTE: unless the driver will have to further process the</span>
    <span class="c1"># result (other than simply writing out to storage system),</span>
    <span class="c1"># writing out at remote task is recommended, as it can avoid</span>
    <span class="c1"># congesting or overloading the driver.</span>
    <span class="c1"># ...</span>

    <span class="c1"># Here we just return the size about the result in this example.</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The driver launches all tasks for the entire input dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 12 files, one for each remote task.</span>
<span class="n">input_files</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sa">f</span><span class="s2">&quot;s3://anonymous@air-example-data/ursa-labs-taxi-data/downsampled_2009_full_year_data.parquet&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;/fe41422b01c04169af2a65a83b753e0f_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">06d</span><span class="si">}</span><span class="s2">.parquet&quot;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
<span class="p">]</span>

<span class="c1"># ray.put() the model just once to local object store, and then pass the</span>
<span class="c1"># reference to the remote tasks.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">()</span>
<span class="n">model_ref</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">result_refs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Launch all prediction tasks.</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">input_files</span><span class="p">:</span>
    <span class="c1"># Launch a prediction task by passing model reference and shard file to it.</span>
    <span class="c1"># NOTE: it would be highly inefficient if you are passing the model itself</span>
    <span class="c1"># like make_prediction.remote(model, file), which in order to pass the model</span>
    <span class="c1"># to remote node will ray.put(model) for each task, potentially overwhelming</span>
    <span class="c1"># the local object store and causing out-of-disk error.</span>
    <span class="n">result_refs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">make_prediction</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">model_ref</span><span class="p">,</span> <span class="n">file</span><span class="p">))</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">result_refs</span><span class="p">)</span>

<span class="c1"># Let&#39;s check prediction output size.</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prediction output size:&quot;</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In order to not overload the cluster and cause OOM, we can control the parallelism by setting the proper resource requirement for tasks, see details about this design pattern in <a class="reference internal" href="../patterns/limit-running-tasks.html"><span class="doc">Pattern: Using resources to limit the number of concurrently running tasks</span></a>.
For example, if it’s easy for your to get a good estimate of the in-memory size for data loaded from external storage, you can control the parallelism by specifying the amount of memory needed for each task, e.g. launching tasks with <code class="docutils literal notranslate"><span class="pre">make_prediction.options(memory=100*1023*1025).remote(model_ref,</span> <span class="pre">file)</span></code>. Ray will then do the right thing and make sure tasks scheduled to a node will not exceed its total memory.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To avoid repeatedly storing the same model into object store (this can cause Out-of-disk for driver node), use ray.put() to store the model once, and then pass the reference around.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To avoid congest or overload the driver node, it’s preferable to have each task to write out the predictions (instead of returning results back to driver which actualy does nothing but write out to storage system).</p>
</div>
<section id="actor-based-batch-prediction">
<h2>Actor-based batch prediction<a class="headerlink" href="batch_prediction.html#actor-based-batch-prediction" title="Permalink to this headline">#</a></h2>
<p>In the above solution, each Ray task will have to fetch the model from the driver node before it can start performing the prediction. This is an overhead cost that can be significant if the model size is large. We can optimize it by using Ray actors, which will fetch the model just once and reuse it for all tasks assigned to the actor.</p>
<p>First, we define a callable class that’s structured with an interface (i.e. constructor) to load/cache the model, and the other to take in a file and perform prediction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">pyarrow.parquet</span> <span class="k">as</span> <span class="nn">pq</span>
<span class="kn">import</span> <span class="nn">ray</span>

<span class="nd">@ray</span><span class="o">.</span><span class="n">remote</span>
<span class="k">class</span> <span class="nc">BatchPredictor</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shard_path</span><span class="p">):</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">shard_path</span><span class="p">)</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span>
        <span class="n">result</span> <span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

        <span class="c1"># Write out the prediction result.</span>
        <span class="c1"># NOTE: unless the driver will have to further process the</span>
        <span class="c1"># result (other than simply writing out to storage system),</span>
        <span class="c1"># writing out at remote task is recommended, as it can avoid</span>
        <span class="c1"># congesting or overloading the driver.</span>
        <span class="c1"># ...</span>

        <span class="c1"># Here we just return the size about the result in this example.</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The constructor is called only once per actor worker. We use ActorPool to manage a set of actors that can receive prediction requests.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.util.actor_pool</span> <span class="kn">import</span> <span class="n">ActorPool</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">()</span>
<span class="n">model_ref</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">num_actors</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">actors</span> <span class="o">=</span> <span class="p">[</span><span class="n">BatchPredictor</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">model_ref</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_actors</span><span class="p">)]</span>
<span class="n">pool</span> <span class="o">=</span> <span class="n">ActorPool</span><span class="p">(</span><span class="n">actors</span><span class="p">)</span>
<span class="n">input_files</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sa">f</span><span class="s2">&quot;s3://anonymous@air-example-data/ursa-labs-taxi-data/downsampled_2009_full_year_data.parquet&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;/fe41422b01c04169af2a65a83b753e0f_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">06d</span><span class="si">}</span><span class="s2">.parquet&quot;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
<span class="p">]</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">input_files</span><span class="p">:</span>
    <span class="n">pool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">a</span><span class="o">.</span><span class="n">predict</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">file</span><span class="p">)</span>
<span class="k">while</span> <span class="n">pool</span><span class="o">.</span><span class="n">has_next</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prediction output size:&quot;</span><span class="p">,</span> <span class="n">pool</span><span class="o">.</span><span class="n">get_next</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>Note that the ActorPool is fixed in size, unlike task-based approach where the number of parallel tasks can be dynamic (as long as it’s not exceeding max_in_flight_tasks). To have autoscaling actor pool, you will need to use the <a class="reference internal" href="../../data/examples/nyc_taxi_basic_processing.html"><span class="doc">Ray Data batch prediction</span></a>.</p>
</section>
<section id="batch-prediction-with-gpus">
<h2>Batch prediction with GPUs<a class="headerlink" href="batch_prediction.html#batch-prediction-with-gpus" title="Permalink to this headline">#</a></h2>
<p>If your cluster has GPU nodes and your predictor can utilize the GPUs, you can direct the tasks or actors to those GPU nodes by specifying num_gpus. Ray will schedule them onto GPU nodes accordingly. On the node, you will need to move the model to GPU. The following is an example for Torch model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@ray</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">make_torch_prediction</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">shard_path</span><span class="p">):</span>
    <span class="c1"># Move model to GPU.</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">shard_path</span><span class="p">)</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># for each tensor in inputs:</span>
    <span class="c1">#   results.append(model(tensor))</span>
    <span class="c1">#</span>
    <span class="c1"># Write out the results right in task instead of returning back</span>
    <span class="c1"># to the driver node (unless you have to), to avoid congest/overload</span>
    <span class="c1"># driver node.</span>
    <span class="c1"># ...</span>

    <span class="c1"># Here we just return simple/light meta information.</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="faqs">
<h2>FAQs<a class="headerlink" href="batch_prediction.html#faqs" title="Permalink to this headline">#</a></h2>
<section id="how-to-load-and-pass-model-efficiently-in-ray-cluster-if-the-model-is-large">
<h3>How to load and pass model efficiently in Ray cluster if the model is large?<a class="headerlink" href="batch_prediction.html#how-to-load-and-pass-model-efficiently-in-ray-cluster-if-the-model-is-large" title="Permalink to this headline">#</a></h3>
<p>The recommended way is to (taking task-based batch prediction for example, the actor-based is the same):</p>
<ol class="simple">
<li><p>let the driver load the model (e.g. from storage system)</p></li>
<li><p>let the driver ray.put(model) to store the model into object store; and</p></li>
<li><p>pass the same reference of the model to each remote tasks when launching them.
The remote task will fetch the model (from driver’s object store) to its local object store before start performing prediction.</p></li>
</ol>
<p>Note it’s highly inefficient if you skip the step 2 and pass the model (instead of reference) to remote tasks. If the model is large and there are many tasks, it’ll likely cause out-of-disk crash for the driver node.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GOOD: the model will be stored to driver&#39;s object store only once</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">()</span>
<span class="n">model_ref</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">input_files</span><span class="p">:</span>
    <span class="n">make_prediction</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">model_ref</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>

<span class="c1"># BAD: the same model will be stored to driver&#39;s object store repeatedly for each task</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">()</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">input_files</span><span class="p">:</span>
    <span class="n">make_prediction</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>For more details, check out <a class="reference internal" href="../patterns/pass-large-arg-by-value.html"><span class="doc">Anti-pattern: Passing the same large argument by value repeatedly harms performance</span></a>.</p>
</section>
<section id="how-to-improve-the-gpu-utilization-rate">
<h3>How to improve the GPU utilization rate?<a class="headerlink" href="batch_prediction.html#how-to-improve-the-gpu-utilization-rate" title="Permalink to this headline">#</a></h3>
<p>To keep GPUs busy, there are following things to look at:</p>
<ul class="simple">
<li><p><strong>Schedule multiple tasks on the same GPU node if it has multiple GPUs</strong>: If there are multiple GPUs on same node and a single task cannot use them all, you can direct multiple tasks to the node. This is automatically handled by Ray, e.g. if you specify num_gpus=1 and there are 4 GPUs, Ray will schedule 4 tasks to the node, provided there are enough tasks and no other resource constraints.</p></li>
<li><p><strong>Use actor-based approach</strong>: as mentioned above, actor-based approach is more efficient because it reuses model initialization for many tasks, so the node will spend more time on the actual workload.</p></li>
</ul>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="highly_parallel.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Using Ray for Highly Parallelizable Tasks</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="batch_training.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Batch Training with Ray Core</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>