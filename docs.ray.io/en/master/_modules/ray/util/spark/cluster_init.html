
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ray.util.spark.cluster_init &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../../../../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script src="../../../../_static/js/versionwarning.js"></script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../../../_static/js/docsearch.js"></script>
    <script src="../../../../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../../../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../../../_static/js/custom.js"></script>
    <script defer="defer" src="../../../../_static/js/top-navigation.js"></script>
    <script src="../../../../_static/js/tags.js"></script>
    <script src="../../../../_static/tabs.js"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../../_static/design-tabs.js"></script>
    <script async="async" src="../../../../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/_modules/ray/util/spark/cluster_init.html" />
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "_modules/ray/util/spark/cluster_init", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2F_modules/ray/util/spark/cluster_init.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for ray.util.spark.cluster_init</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">socket</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">uuid</span>
<span class="kn">from</span> <span class="nn">packaging.version</span> <span class="kn">import</span> <span class="n">Version</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Type</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray.util.annotations</span> <span class="kn">import</span> <span class="n">PublicAPI</span>
<span class="kn">from</span> <span class="nn">ray._private.storage</span> <span class="kn">import</span> <span class="n">_load_class</span>

<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">exec_cmd</span><span class="p">,</span>
    <span class="n">check_port_open</span><span class="p">,</span>
    <span class="n">get_random_unused_port</span><span class="p">,</span>
    <span class="n">get_spark_session</span><span class="p">,</span>
    <span class="n">get_spark_application_driver_host</span><span class="p">,</span>
    <span class="n">is_in_databricks_runtime</span><span class="p">,</span>
    <span class="n">get_spark_task_assigned_physical_gpus</span><span class="p">,</span>
    <span class="n">get_avail_mem_per_ray_worker_node</span><span class="p">,</span>
    <span class="n">get_max_num_concurrent_tasks</span><span class="p">,</span>
    <span class="n">gen_cmd_exec_failure_msg</span><span class="p">,</span>
    <span class="n">setup_sigterm_on_parent_death</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.start_hook_base</span> <span class="kn">import</span> <span class="n">RayOnSparkStartHook</span>
<span class="kn">from</span> <span class="nn">.databricks_hook</span> <span class="kn">import</span> <span class="n">DefaultDatabricksRayOnSparkStartHook</span>


<span class="n">_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;ray.util.spark&quot;</span><span class="p">)</span>
<span class="n">_logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>


<span class="n">RAY_ON_SPARK_START_HOOK</span> <span class="o">=</span> <span class="s2">&quot;RAY_ON_SPARK_START_HOOK&quot;</span>

<span class="n">MAX_NUM_WORKER_NODES</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

<span class="n">RAY_ON_SPARK_COLLECT_LOG_TO_PATH</span> <span class="o">=</span> <span class="s2">&quot;RAY_ON_SPARK_COLLECT_LOG_TO_PATH&quot;</span>


<span class="k">def</span> <span class="nf">_check_system_environment</span><span class="p">():</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">sys</span><span class="o">.</span><span class="n">platform</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;linux&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Ray on spark only supports running on Linux.&quot;</span><span class="p">)</span>

    <span class="n">spark_dependency_error</span> <span class="o">=</span> <span class="s2">&quot;ray.util.spark module requires pyspark &gt;= 3.3&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">pyspark</span>

        <span class="k">if</span> <span class="n">Version</span><span class="p">(</span><span class="n">pyspark</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span><span class="o">.</span><span class="n">release</span> <span class="o">&lt;</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">spark_dependency_error</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">spark_dependency_error</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">RayClusterOnSpark</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class is the type of instance returned by the `_setup_ray_cluster` interface.</span>
<span class="sd">    Its main functionality is to:</span>
<span class="sd">    Connect to, disconnect from, and shutdown the Ray cluster running on Apache Spark.</span>
<span class="sd">    Serve as a Python context manager for the `RayClusterOnSpark` instance.</span>

<span class="sd">    Args</span>
<span class="sd">        address: The url for the ray head node (defined as the hostname and unused</span>
<span class="sd">                 port on Spark driver node)</span>
<span class="sd">        head_proc: Ray head process</span>
<span class="sd">        spark_job_group_id: The Spark job id for a submitted ray job</span>
<span class="sd">        num_workers_node: The number of workers in the ray cluster.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">address</span><span class="p">,</span>
        <span class="n">head_proc</span><span class="p">,</span>
        <span class="n">spark_job_group_id</span><span class="p">,</span>
        <span class="n">num_workers_node</span><span class="p">,</span>
        <span class="n">temp_dir</span><span class="p">,</span>
        <span class="n">cluster_unique_id</span><span class="p">,</span>
        <span class="n">start_hook</span><span class="p">,</span>
        <span class="n">ray_dashboard_port</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">address</span> <span class="o">=</span> <span class="n">address</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_proc</span> <span class="o">=</span> <span class="n">head_proc</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spark_job_group_id</span> <span class="o">=</span> <span class="n">spark_job_group_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_worker_nodes</span> <span class="o">=</span> <span class="n">num_workers_node</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temp_dir</span> <span class="o">=</span> <span class="n">temp_dir</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cluster_unique_id</span> <span class="o">=</span> <span class="n">cluster_unique_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_hook</span> <span class="o">=</span> <span class="n">start_hook</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ray_dashboard_port</span> <span class="o">=</span> <span class="n">ray_dashboard_port</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_shutdown</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spark_job_is_canceled</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">background_job_exception</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_cancel_background_spark_job</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spark_job_is_canceled</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">get_spark_session</span><span class="p">()</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">cancelJobGroup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spark_job_group_id</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">wait_until_ready</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="kn">import</span> <span class="nn">ray</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_shutdown</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;The ray cluster has been shut down or it failed to start.&quot;</span>
            <span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># connect to the ray cluster.</span>
            <span class="n">ray_ctx</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">address</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">address</span><span class="p">)</span>
            <span class="n">webui_url</span> <span class="o">=</span> <span class="n">ray_ctx</span><span class="o">.</span><span class="n">address_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;webui_url&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">webui_url</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">start_hook</span><span class="o">.</span><span class="n">on_ray_dashboard_created</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ray_dashboard_port</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="nb">__import__</span><span class="p">(</span><span class="s2">&quot;ray.dashboard.optional_deps&quot;</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">ModuleNotFoundError</span><span class="p">:</span>
                    <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;Dependencies to launch the optional dashboard API &quot;</span>
                        <span class="s2">&quot;server cannot be found. They can be installed with &quot;</span>
                        <span class="s2">&quot;pip install ray[default].&quot;</span>
                    <span class="p">)</span>

        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
            <span class="k">raise</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">last_alive_worker_count</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">last_progress_move_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">_RAY_CLUSTER_STARTUP_PROGRESS_CHECKING_INTERVAL</span><span class="p">)</span>

                <span class="c1"># Inside the waiting ready loop,</span>
                <span class="c1"># checking `self.background_job_exception`, if it is not None,</span>
                <span class="c1"># it means the background spark job has failed,</span>
                <span class="c1"># in this case, raise error directly.</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">background_job_exception</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;Ray workers failed to start.&quot;</span>
                    <span class="p">)</span> <span class="kn">from</span> <span class="nn">self.background_job_exception</span>

                <span class="n">cur_alive_worker_count</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="nb">len</span><span class="p">([</span><span class="n">node</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">ray</span><span class="o">.</span><span class="n">nodes</span><span class="p">()</span> <span class="k">if</span> <span class="n">node</span><span class="p">[</span><span class="s2">&quot;Alive&quot;</span><span class="p">]])</span> <span class="o">-</span> <span class="mi">1</span>
                <span class="p">)</span>  <span class="c1"># Minus 1 means excluding the head node.</span>

                <span class="k">if</span> <span class="n">cur_alive_worker_count</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_worker_nodes</span><span class="p">:</span>
                    <span class="k">return</span>

                <span class="k">if</span> <span class="n">cur_alive_worker_count</span> <span class="o">&gt;</span> <span class="n">last_alive_worker_count</span><span class="p">:</span>
                    <span class="n">last_alive_worker_count</span> <span class="o">=</span> <span class="n">cur_alive_worker_count</span>
                    <span class="n">last_progress_move_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
                    <span class="n">_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="s2">&quot;Ray worker nodes are starting. Progress: &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">cur_alive_worker_count</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_worker_nodes</span><span class="si">}</span><span class="s2">)&quot;</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">last_progress_move_time</span>
                        <span class="o">&gt;</span> <span class="n">_RAY_CONNECT_CLUSTER_POLL_PROGRESS_TIMEOUT</span>
                    <span class="p">):</span>
                        <span class="k">if</span> <span class="n">cur_alive_worker_count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                                <span class="s2">&quot;Current spark cluster has no resources to launch &quot;</span>
                                <span class="s2">&quot;Ray worker nodes.&quot;</span>
                            <span class="p">)</span>
                        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                            <span class="s2">&quot;Timeout in waiting for all ray workers to start. &quot;</span>
                            <span class="s2">&quot;Started / Total requested: &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">cur_alive_worker_count</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_worker_nodes</span><span class="si">}</span><span class="s2">). &quot;</span>
                            <span class="s2">&quot;Current spark cluster does not have sufficient resources &quot;</span>
                            <span class="s2">&quot;to launch requested number of Ray worker nodes.&quot;</span>
                        <span class="p">)</span>
                        <span class="k">return</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">ray</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">connect</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">ray</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Already connected to Ray cluster.&quot;</span><span class="p">)</span>
        <span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">address</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">address</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">disconnect</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">ray</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">shutdown</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cancel_background_job</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Shutdown the ray cluster created by the `setup_ray_cluster` API.</span>
<span class="sd">        NB: In the background thread that runs the background spark job, if spark job</span>
<span class="sd">        raise unexpected error, its exception handler will also call this method, in</span>
<span class="sd">        the case, it will set cancel_background_job=False to avoid recursive call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_shutdown</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">disconnect</span><span class="p">()</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;RAY_ADDRESS&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">cancel_background_job</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_cancel_background_spark_job</span><span class="p">()</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="c1"># swallow exception.</span>
                    <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;An error occurred while cancelling the ray cluster &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;background spark job: </span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head_proc</span><span class="o">.</span><span class="n">terminate</span><span class="p">()</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="c1"># swallow exception.</span>
                <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;An Error occurred during shutdown of ray head node: &quot;</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_shutdown</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_val</span><span class="p">,</span> <span class="n">exc_tb</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_convert_ray_node_option_key</span><span class="p">(</span><span class="n">key</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;--</span><span class="si">{</span><span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">,</span> <span class="s1">&#39;-&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="k">def</span> <span class="nf">_convert_ray_node_options</span><span class="p">(</span><span class="n">options</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">_convert_ray_node_option_key</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">_convert_ray_node_option_key</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">options</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="p">]</span>


<span class="n">_RAY_HEAD_STARTUP_TIMEOUT</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">_BACKGROUND_JOB_STARTUP_WAIT</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;RAY_ON_SPARK_BACKGROUND_JOB_STARTUP_WAIT&quot;</span><span class="p">,</span> <span class="s2">&quot;30&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">_RAY_CLUSTER_STARTUP_PROGRESS_CHECKING_INTERVAL</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">_RAY_WORKER_NODE_STARTUP_INTERVAL</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;RAY_ON_SPARK_RAY_WORKER_NODE_STARTUP_INTERVAL&quot;</span><span class="p">,</span> <span class="s2">&quot;10&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">_RAY_CONNECT_CLUSTER_POLL_PROGRESS_TIMEOUT</span> <span class="o">=</span> <span class="mi">120</span>


<span class="k">def</span> <span class="nf">_prepare_for_ray_worker_node_startup</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    If we start multiple ray workers on a machine concurrently, some ray worker</span>
<span class="sd">    processes might fail due to ray port conflicts, this is because race condition</span>
<span class="sd">    on getting free port and opening the free port.</span>
<span class="sd">    To address the issue, this function use an exclusive file lock to delay the</span>
<span class="sd">    worker processes to ensure that port acquisition does not create a resource</span>
<span class="sd">    contention issue due to a race condition.</span>

<span class="sd">    After acquiring lock, it will allocate port range for worker ports</span>
<span class="sd">    (for ray node config --min-worker-port and --max-worker-port).</span>
<span class="sd">    Because on a spark cluster, multiple ray cluster might be created, so on one spark</span>
<span class="sd">    worker machine, there might be multiple ray worker nodes running, these worker</span>
<span class="sd">    nodes might belong to different ray cluster, and we must ensure these ray nodes on</span>
<span class="sd">    the same machine using non-overlapping worker port range, to achieve this, in this</span>
<span class="sd">    function, it creates a file `/tmp/ray_on_spark_worker_port_allocation.txt` file,</span>
<span class="sd">    the file format is composed of multiple lines, each line contains 2 number: `pid`</span>
<span class="sd">    and `port_range_slot_index`, each port range slot allocates 1000 ports, and</span>
<span class="sd">    corresponding port range is:</span>
<span class="sd">     - range_begin (inclusive): 20000 + port_range_slot_index * 1000</span>
<span class="sd">     - range_end (exclusive): range_begin + 1000</span>
<span class="sd">    In this function, it first scans `/tmp/ray_on_spark_worker_port_allocation.txt`</span>
<span class="sd">    file, removing lines that containing dead process pid, then find the first unused</span>
<span class="sd">    port_range_slot_index, then regenerate this file, and return the allocated port</span>
<span class="sd">    range.</span>

<span class="sd">    Returns: Allocated port range for current worker ports</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">psutil</span>
    <span class="kn">import</span> <span class="nn">fcntl</span>

    <span class="k">def</span> <span class="nf">acquire_lock</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">O_RDWR</span> <span class="o">|</span> <span class="n">os</span><span class="o">.</span><span class="n">O_CREAT</span> <span class="o">|</span> <span class="n">os</span><span class="o">.</span><span class="n">O_TRUNC</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">fd</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>
            <span class="c1"># The lock file must be readable / writable to all users.</span>
            <span class="n">os</span><span class="o">.</span><span class="n">chmod</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="mo">0o0777</span><span class="p">)</span>
            <span class="c1"># Allow for retrying getting a file lock a maximum number of seconds</span>
            <span class="n">max_lock_iter</span> <span class="o">=</span> <span class="mi">600</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_lock_iter</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">fcntl</span><span class="o">.</span><span class="n">flock</span><span class="p">(</span><span class="n">fd</span><span class="p">,</span> <span class="n">fcntl</span><span class="o">.</span><span class="n">LOCK_EX</span> <span class="o">|</span> <span class="n">fcntl</span><span class="o">.</span><span class="n">LOCK_NB</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">BlockingIOError</span><span class="p">:</span>
                    <span class="c1"># Lock is used by other processes, continue loop to wait for lock</span>
                    <span class="c1"># available</span>
                    <span class="k">pass</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Acquire lock successfully.</span>
                    <span class="k">return</span> <span class="n">fd</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">TimeoutError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Acquiring lock on file </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2"> timeout.&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">fd</span><span class="p">)</span>

    <span class="n">lock_file_path</span> <span class="o">=</span> <span class="s2">&quot;/tmp/ray_on_spark_worker_startup_barrier_lock.lock&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">lock_fd</span> <span class="o">=</span> <span class="n">acquire_lock</span><span class="p">(</span><span class="n">lock_file_path</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">TimeoutError</span><span class="p">:</span>
        <span class="c1"># If timeout happens, the file lock might be hold by another process and that</span>
        <span class="c1"># process does not release the lock in time by some unexpected reason.</span>
        <span class="c1"># In this case, remove the existing lock file and create the file again, and</span>
        <span class="c1"># then acquire file lock on the new file.</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">lock_file_path</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="n">lock_fd</span> <span class="o">=</span> <span class="n">acquire_lock</span><span class="p">(</span><span class="n">lock_file_path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">release_lock</span><span class="p">():</span>
        <span class="n">fcntl</span><span class="o">.</span><span class="n">flock</span><span class="p">(</span><span class="n">lock_fd</span><span class="p">,</span> <span class="n">fcntl</span><span class="o">.</span><span class="n">LOCK_UN</span><span class="p">)</span>
        <span class="n">os</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">lock_fd</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">port_alloc_file</span> <span class="o">=</span> <span class="s2">&quot;/tmp/ray_on_spark_worker_port_allocation.txt&quot;</span>

        <span class="c1"># NB: reading / writing `port_alloc_file` is protected by exclusive lock</span>
        <span class="c1"># on file `lock_file_path`</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">port_alloc_file</span><span class="p">):</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">port_alloc_file</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
                <span class="n">port_alloc_data</span> <span class="o">=</span> <span class="n">fp</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
            <span class="n">port_alloc_table</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">port_alloc_data</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="p">]</span>
            <span class="n">port_alloc_table</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">pid_str</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">slot_index_str</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">pid_str</span><span class="p">,</span> <span class="n">slot_index_str</span> <span class="ow">in</span> <span class="n">port_alloc_table</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">port_alloc_table</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">port_alloc_file</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;w&quot;</span><span class="p">):</span>
                <span class="k">pass</span>
            <span class="c1"># The port range allocation file must be readable / writable to all users.</span>
            <span class="n">os</span><span class="o">.</span><span class="n">chmod</span><span class="p">(</span><span class="n">port_alloc_file</span><span class="p">,</span> <span class="mo">0o0777</span><span class="p">)</span>

        <span class="n">port_alloc_map</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">pid</span><span class="p">:</span> <span class="n">slot_index</span>
            <span class="k">for</span> <span class="n">pid</span><span class="p">,</span> <span class="n">slot_index</span> <span class="ow">in</span> <span class="n">port_alloc_table</span>
            <span class="k">if</span> <span class="n">psutil</span><span class="o">.</span><span class="n">pid_exists</span><span class="p">(</span><span class="n">pid</span><span class="p">)</span>  <span class="c1"># remove slot used by dead process</span>
        <span class="p">}</span>

        <span class="n">allocated_slot_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">port_alloc_map</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">allocated_slot_set</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">new_slot_index</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_slot_index</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">allocated_slot_set</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">new_slot_index</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">index</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">allocated_slot_set</span><span class="p">:</span>
                    <span class="n">new_slot_index</span> <span class="o">=</span> <span class="n">index</span>
                    <span class="k">break</span>

        <span class="n">port_alloc_map</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">()]</span> <span class="o">=</span> <span class="n">new_slot_index</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">port_alloc_file</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">pid</span><span class="p">,</span> <span class="n">slot_index</span> <span class="ow">in</span> <span class="n">port_alloc_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">fp</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pid</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">slot_index</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">worker_port_range_begin</span> <span class="o">=</span> <span class="mi">20000</span> <span class="o">+</span> <span class="n">new_slot_index</span> <span class="o">*</span> <span class="mi">1000</span>
        <span class="n">worker_port_range_end</span> <span class="o">=</span> <span class="n">worker_port_range_begin</span> <span class="o">+</span> <span class="mi">1000</span>

        <span class="k">if</span> <span class="n">worker_port_range_end</span> <span class="o">&gt;</span> <span class="mi">65536</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Too many ray worker nodes are running on this machine, cannot &quot;</span>
                <span class="s2">&quot;allocate worker port range for new ray worker node.&quot;</span>
            <span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
        <span class="n">release_lock</span><span class="p">()</span>
        <span class="k">raise</span>

    <span class="k">def</span> <span class="nf">hold_lock</span><span class="p">():</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">_RAY_WORKER_NODE_STARTUP_INTERVAL</span><span class="p">)</span>
        <span class="n">release_lock</span><span class="p">()</span>

    <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">hold_lock</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">())</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">worker_port_range_begin</span><span class="p">,</span> <span class="n">worker_port_range_end</span>


<span class="k">def</span> <span class="nf">_setup_ray_cluster</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">num_worker_nodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_cpus_per_node</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_gpus_per_node</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">using_stage_scheduling</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">heap_memory_per_node</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">object_store_memory_per_node</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">head_node_options</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
    <span class="n">worker_node_options</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
    <span class="n">ray_temp_root_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">collect_log_to_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Type</span><span class="p">[</span><span class="n">RayClusterOnSpark</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The public API `ray.util.spark.setup_ray_cluster` does some argument</span>
<span class="sd">    validation and then pass validated arguments to this interface.</span>
<span class="sd">    and it returns a `RayClusterOnSpark` instance.</span>

<span class="sd">    The returned instance can be used to connect to, disconnect from and shutdown the</span>
<span class="sd">    ray cluster. This instance can also be used as a context manager (used by</span>
<span class="sd">    encapsulating operations within `with _setup_ray_cluster(...):`). Upon entering the</span>
<span class="sd">    managed scope, the ray cluster is initiated and connected to. When exiting the</span>
<span class="sd">    scope, the ray cluster is disconnected and shut down.</span>

<span class="sd">    Note: This function interface is stable and can be used for</span>
<span class="sd">    instrumentation logging patching.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">pyspark.util</span> <span class="kn">import</span> <span class="n">inheritable_thread_target</span>

    <span class="k">if</span> <span class="n">RAY_ON_SPARK_START_HOOK</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
        <span class="n">start_hook</span> <span class="o">=</span> <span class="n">_load_class</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">RAY_ON_SPARK_START_HOOK</span><span class="p">])()</span>
    <span class="k">elif</span> <span class="n">is_in_databricks_runtime</span><span class="p">():</span>
        <span class="n">start_hook</span> <span class="o">=</span> <span class="n">DefaultDatabricksRayOnSparkStartHook</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">start_hook</span> <span class="o">=</span> <span class="n">RayOnSparkStartHook</span><span class="p">()</span>

    <span class="n">spark</span> <span class="o">=</span> <span class="n">get_spark_session</span><span class="p">()</span>

    <span class="n">ray_head_ip</span> <span class="o">=</span> <span class="n">socket</span><span class="o">.</span><span class="n">gethostbyname</span><span class="p">(</span><span class="n">get_spark_application_driver_host</span><span class="p">(</span><span class="n">spark</span><span class="p">))</span>
    <span class="n">ray_head_port</span> <span class="o">=</span> <span class="n">get_random_unused_port</span><span class="p">(</span><span class="n">ray_head_ip</span><span class="p">,</span> <span class="n">min_port</span><span class="o">=</span><span class="mi">9000</span><span class="p">,</span> <span class="n">max_port</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>

    <span class="c1"># Make a copy for head_node_options to avoid changing original dict in user code.</span>
    <span class="n">head_node_options</span> <span class="o">=</span> <span class="n">head_node_options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">include_dashboard</span> <span class="o">=</span> <span class="n">head_node_options</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;include_dashboard&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">ray_dashboard_port</span> <span class="o">=</span> <span class="n">head_node_options</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;dashboard_port&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">include_dashboard</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">include_dashboard</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">ray_dashboard_port</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ray_dashboard_port</span> <span class="o">=</span> <span class="n">get_random_unused_port</span><span class="p">(</span>
                <span class="n">ray_head_ip</span><span class="p">,</span> <span class="n">min_port</span><span class="o">=</span><span class="mi">9000</span><span class="p">,</span> <span class="n">max_port</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">exclude_list</span><span class="o">=</span><span class="p">[</span><span class="n">ray_head_port</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="n">ray_dashboard_agent_port</span> <span class="o">=</span> <span class="n">get_random_unused_port</span><span class="p">(</span>
            <span class="n">ray_head_ip</span><span class="p">,</span>
            <span class="n">min_port</span><span class="o">=</span><span class="mi">9000</span><span class="p">,</span>
            <span class="n">max_port</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
            <span class="n">exclude_list</span><span class="o">=</span><span class="p">[</span><span class="n">ray_head_port</span><span class="p">,</span> <span class="n">ray_dashboard_port</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="n">dashboard_options</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">&quot;--dashboard-host=0.0.0.0&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--dashboard-port=</span><span class="si">{</span><span class="n">ray_dashboard_port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--dashboard-agent-listen-port=</span><span class="si">{</span><span class="n">ray_dashboard_agent_port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="c1"># If include_dashboard is None, we don&#39;t set `--include-dashboard` option,</span>
        <span class="c1"># in this case Ray will decide whether dashboard can be started</span>
        <span class="c1"># (e.g. checking any missing dependencies).</span>
        <span class="k">if</span> <span class="n">include_dashboard</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">dashboard_options</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;--include-dashboard=true&quot;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dashboard_options</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">&quot;--include-dashboard=false&quot;</span><span class="p">,</span>
        <span class="p">]</span>

    <span class="n">_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ray head hostname </span><span class="si">{</span><span class="n">ray_head_ip</span><span class="si">}</span><span class="s2">, port </span><span class="si">{</span><span class="n">ray_head_port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">cluster_unique_id</span> <span class="o">=</span> <span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">()</span><span class="o">.</span><span class="n">hex</span><span class="p">[:</span><span class="mi">8</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">ray_temp_root_dir</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ray_temp_root_dir</span> <span class="o">=</span> <span class="n">start_hook</span><span class="o">.</span><span class="n">get_default_temp_dir</span><span class="p">()</span>
    <span class="n">ray_temp_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="n">ray_temp_root_dir</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;ray-</span><span class="si">{</span><span class="n">ray_head_port</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">cluster_unique_id</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">ray_temp_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">ray_head_node_cmd</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">executable</span><span class="p">,</span>
        <span class="s2">&quot;-m&quot;</span><span class="p">,</span>
        <span class="s2">&quot;ray.util.spark.start_ray_node&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;--temp-dir=</span><span class="si">{</span><span class="n">ray_temp_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="s2">&quot;--block&quot;</span><span class="p">,</span>
        <span class="s2">&quot;--head&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;--node-ip-address=</span><span class="si">{</span><span class="n">ray_head_ip</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;--port=</span><span class="si">{</span><span class="n">ray_head_port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="c1"># disallow ray tasks with cpu/gpu requirements from being scheduled on the head</span>
        <span class="c1"># node.</span>
        <span class="s2">&quot;--num-cpus=0&quot;</span><span class="p">,</span>
        <span class="s2">&quot;--num-gpus=0&quot;</span><span class="p">,</span>
        <span class="c1"># limit the memory allocation to the head node (actual usage may increase</span>
        <span class="c1"># beyond this for processing of tasks and actors).</span>
        <span class="sa">f</span><span class="s2">&quot;--memory=</span><span class="si">{</span><span class="mi">128</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="c1"># limit the object store memory allocation to the head node (actual usage</span>
        <span class="c1"># may increase beyond this for processing of tasks and actors).</span>
        <span class="sa">f</span><span class="s2">&quot;--object-store-memory=</span><span class="si">{</span><span class="mi">128</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="o">*</span><span class="n">dashboard_options</span><span class="p">,</span>
        <span class="o">*</span><span class="n">_convert_ray_node_options</span><span class="p">(</span><span class="n">head_node_options</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="n">_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Starting Ray head, command: </span><span class="si">{</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ray_head_node_cmd</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># `preexec_fn=setup_sigterm_on_parent_death` ensures the ray head node being</span>
    <span class="c1"># killed if parent process died unexpectedly.</span>
    <span class="n">ray_head_proc</span><span class="p">,</span> <span class="n">tail_output_deque</span> <span class="o">=</span> <span class="n">exec_cmd</span><span class="p">(</span>
        <span class="n">ray_head_node_cmd</span><span class="p">,</span>
        <span class="n">synchronous</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">preexec_fn</span><span class="o">=</span><span class="n">setup_sigterm_on_parent_death</span><span class="p">,</span>
        <span class="n">extra_env</span><span class="o">=</span><span class="p">{</span><span class="n">RAY_ON_SPARK_COLLECT_LOG_TO_PATH</span><span class="p">:</span> <span class="n">collect_log_to_path</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span><span class="p">},</span>
    <span class="p">)</span>

    <span class="c1"># wait ray head node spin up.</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">_RAY_HEAD_STARTUP_TIMEOUT</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">check_port_open</span><span class="p">(</span><span class="n">ray_head_ip</span><span class="p">,</span> <span class="n">ray_head_port</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">ray_head_proc</span><span class="o">.</span><span class="n">poll</span><span class="p">()</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Ray head GCS service is down. Kill ray head node.</span>
            <span class="n">ray_head_proc</span><span class="o">.</span><span class="n">terminate</span><span class="p">()</span>
            <span class="c1"># wait killing complete.</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="n">cmd_exec_failure_msg</span> <span class="o">=</span> <span class="n">gen_cmd_exec_failure_msg</span><span class="p">(</span>
            <span class="n">ray_head_node_cmd</span><span class="p">,</span> <span class="n">ray_head_proc</span><span class="o">.</span><span class="n">returncode</span><span class="p">,</span> <span class="n">tail_output_deque</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Start Ray head node failed!</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">cmd_exec_failure_msg</span><span class="p">)</span>

    <span class="n">_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Ray head node started.&quot;</span><span class="p">)</span>

    <span class="c1"># NB:</span>
    <span class="c1"># In order to start ray worker nodes on spark cluster worker machines,</span>
    <span class="c1"># We launch a background spark job:</span>
    <span class="c1">#  1. Each spark task launches one ray worker node. This design ensures all ray</span>
    <span class="c1">#     worker nodes have the same shape (same cpus / gpus / memory configuration).</span>
    <span class="c1">#     If ray worker nodes have a non-uniform shape, the Ray cluster setup will</span>
    <span class="c1">#     be non-deterministic and could create issues with node sizing.</span>
    <span class="c1">#  2. A ray worker node is started via the `ray start` CLI. In each spark task,</span>
    <span class="c1">#     a child process is started and will execute a `ray start ...` command in</span>
    <span class="c1">#     blocking mode.</span>
    <span class="c1">#  3. Each task will acquire a file lock for 10s to ensure that the ray worker</span>
    <span class="c1">#     init will acquire a port connection to the ray head node that does not</span>
    <span class="c1">#     contend with other worker processes on the same Spark worker node.</span>
    <span class="c1">#  4. When the ray cluster is shutdown, killing ray worker nodes is implemented by:</span>
    <span class="c1">#     Installing a PR_SET_PDEATHSIG signal for the `ray start ...` child processes</span>
    <span class="c1">#     so that when parent process (pyspark task) is killed, the child processes</span>
    <span class="c1">#     (`ray start ...` processes) will receive a SIGTERM signal, killing it.</span>
    <span class="c1">#     Shutting down the ray cluster is performed by calling</span>
    <span class="c1">#     `sparkContext.cancelJobGroup` to cancel the background spark job, sending a</span>
    <span class="c1">#     SIGKILL signal to all spark tasks. Once the spark tasks are killed, this</span>
    <span class="c1">#     triggers the sending of a SIGTERM to the child processes spawned by the</span>
    <span class="c1">#     `ray_start ...` process.</span>

    <span class="k">def</span> <span class="nf">ray_cluster_job_mapper</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">pyspark.taskcontext</span> <span class="kn">import</span> <span class="n">TaskContext</span>

        <span class="n">_worker_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;ray.util.spark.worker&quot;</span><span class="p">)</span>

        <span class="n">context</span> <span class="o">=</span> <span class="n">TaskContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>

        <span class="p">(</span>
            <span class="n">worker_port_range_begin</span><span class="p">,</span>
            <span class="n">worker_port_range_end</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">_prepare_for_ray_worker_node_startup</span><span class="p">()</span>

        <span class="c1"># Ray worker might run on a machine different with the head node, so create the</span>
        <span class="c1"># local log dir and temp dir again.</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">ray_temp_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">ray_worker_node_dashboard_agent_port</span> <span class="o">=</span> <span class="n">get_random_unused_port</span><span class="p">(</span>
            <span class="n">ray_head_ip</span><span class="p">,</span> <span class="n">min_port</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">max_port</span><span class="o">=</span><span class="mi">20000</span>
        <span class="p">)</span>
        <span class="n">ray_worker_node_cmd</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">executable</span><span class="p">,</span>
            <span class="s2">&quot;-m&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ray.util.spark.start_ray_node&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--temp-dir=</span><span class="si">{</span><span class="n">ray_temp_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--num-cpus=</span><span class="si">{</span><span class="n">num_cpus_per_node</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="s2">&quot;--block&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--address=</span><span class="si">{</span><span class="n">ray_head_ip</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">ray_head_port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--memory=</span><span class="si">{</span><span class="n">heap_memory_per_node</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--object-store-memory=</span><span class="si">{</span><span class="n">object_store_memory_per_node</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--min-worker-port=</span><span class="si">{</span><span class="n">worker_port_range_begin</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--max-worker-port=</span><span class="si">{</span><span class="n">worker_port_range_end</span> <span class="o">-</span> <span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--dashboard-agent-listen-port=</span><span class="si">{</span><span class="n">ray_worker_node_dashboard_agent_port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="o">*</span><span class="n">_convert_ray_node_options</span><span class="p">(</span><span class="n">worker_node_options</span><span class="p">),</span>
        <span class="p">]</span>

        <span class="n">ray_worker_node_extra_envs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">RAY_ON_SPARK_COLLECT_LOG_TO_PATH</span><span class="p">:</span> <span class="n">collect_log_to_path</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="n">num_gpus_per_node</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">task_resources</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">resources</span><span class="p">()</span>

            <span class="k">if</span> <span class="s2">&quot;gpu&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">task_resources</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Couldn&#39;t get the gpu id, Please check the GPU resource &quot;</span>
                    <span class="s2">&quot;configuration&quot;</span>
                <span class="p">)</span>
            <span class="n">gpu_addr_list</span> <span class="o">=</span> <span class="p">[</span>
                <span class="nb">int</span><span class="p">(</span><span class="n">addr</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span> <span class="k">for</span> <span class="n">addr</span> <span class="ow">in</span> <span class="n">task_resources</span><span class="p">[</span><span class="s2">&quot;gpu&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">addresses</span>
            <span class="p">]</span>

            <span class="n">available_physical_gpus</span> <span class="o">=</span> <span class="n">get_spark_task_assigned_physical_gpus</span><span class="p">(</span>
                <span class="n">gpu_addr_list</span>
            <span class="p">)</span>
            <span class="n">ray_worker_node_cmd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;--num-gpus=</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">available_physical_gpus</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">ray_worker_node_extra_envs</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">gpu_id</span><span class="p">)</span> <span class="k">for</span> <span class="n">gpu_id</span> <span class="ow">in</span> <span class="n">available_physical_gpus</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="n">_worker_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Start Ray worker, command: </span><span class="si">{</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ray_worker_node_cmd</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="c1"># `preexec_fn=setup_sigterm_on_parent_death` handles the case:</span>
        <span class="c1"># If a user cancels the PySpark job, the worker process gets killed, regardless</span>
        <span class="c1"># of PySpark daemon and worker reuse settings.</span>
        <span class="c1"># We use prctl to ensure the command process receives SIGTERM after spark job</span>
        <span class="c1"># cancellation.</span>
        <span class="c1"># Note:</span>
        <span class="c1"># When a pyspark job cancelled, the UDF python process are killed by signal</span>
        <span class="c1"># &quot;SIGKILL&quot;, This case neither &quot;atexit&quot; nor signal handler can capture SIGKILL</span>
        <span class="c1"># signal. prctl is the only way to capture SIGKILL signal.</span>
        <span class="n">exec_cmd</span><span class="p">(</span>
            <span class="n">ray_worker_node_cmd</span><span class="p">,</span>
            <span class="n">synchronous</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">extra_env</span><span class="o">=</span><span class="n">ray_worker_node_extra_envs</span><span class="p">,</span>
            <span class="n">preexec_fn</span><span class="o">=</span><span class="n">setup_sigterm_on_parent_death</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># NB: Not reachable.</span>
        <span class="k">yield</span> <span class="mi">0</span>

    <span class="n">spark_job_group_id</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;ray-cluster-</span><span class="si">{</span><span class="n">ray_head_port</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">cluster_unique_id</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="n">cluster_address</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ray_head_ip</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">ray_head_port</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="c1"># Set RAY_ADDRESS environment variable to the cluster address.</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RAY_ADDRESS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_address</span>

    <span class="n">ray_cluster_handler</span> <span class="o">=</span> <span class="n">RayClusterOnSpark</span><span class="p">(</span>
        <span class="n">address</span><span class="o">=</span><span class="n">cluster_address</span><span class="p">,</span>
        <span class="n">head_proc</span><span class="o">=</span><span class="n">ray_head_proc</span><span class="p">,</span>
        <span class="n">spark_job_group_id</span><span class="o">=</span><span class="n">spark_job_group_id</span><span class="p">,</span>
        <span class="n">num_workers_node</span><span class="o">=</span><span class="n">num_worker_nodes</span><span class="p">,</span>
        <span class="n">temp_dir</span><span class="o">=</span><span class="n">ray_temp_dir</span><span class="p">,</span>
        <span class="n">cluster_unique_id</span><span class="o">=</span><span class="n">cluster_unique_id</span><span class="p">,</span>
        <span class="n">start_hook</span><span class="o">=</span><span class="n">start_hook</span><span class="p">,</span>
        <span class="n">ray_dashboard_port</span><span class="o">=</span><span class="n">ray_dashboard_port</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">background_job_thread_fn</span><span class="p">():</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">setJobGroup</span><span class="p">(</span>
                <span class="n">spark_job_group_id</span><span class="p">,</span>
                <span class="s2">&quot;This job group is for spark job which runs the Ray cluster with ray &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;head node </span><span class="si">{</span><span class="n">ray_head_ip</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">ray_head_port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Starting a normal spark job (not barrier spark job) to run ray worker</span>
            <span class="c1"># nodes, the design purpose is:</span>
            <span class="c1"># 1. Using normal spark job, spark tasks can automatically retry</span>
            <span class="c1"># individually, we don&#39;t need to write additional retry logic, But, in</span>
            <span class="c1"># barrier mode, if one spark task fails, it will cause all other spark</span>
            <span class="c1"># tasks killed.</span>
            <span class="c1"># 2. Using normal spark job, we can support failover when a spark worker</span>
            <span class="c1"># physical machine crashes. (spark will try to re-schedule the spark task</span>
            <span class="c1"># to other spark worker nodes)</span>
            <span class="c1"># 3. Using barrier mode job, if the cluster resources does not satisfy</span>
            <span class="c1"># &quot;idle spark task slots &gt;= argument num_spark_task&quot;, then the barrier</span>
            <span class="c1"># job gets stuck and waits until enough idle task slots available, this</span>
            <span class="c1"># behavior is not user-friendly, on a shared spark cluster, user is hard</span>
            <span class="c1"># to estimate how many idle tasks available at a time, But, if using normal</span>
            <span class="c1"># spark job, it can launch job with less spark tasks (i.e. user will see a</span>
            <span class="c1"># ray cluster setup with less worker number initially), and when more task</span>
            <span class="c1"># slots become available, it continues to launch tasks on new available</span>
            <span class="c1"># slots, and user can see the ray cluster worker number increases when more</span>
            <span class="c1"># slots available.</span>
            <span class="n">job_rdd</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span>
                <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_worker_nodes</span><span class="p">)),</span> <span class="n">num_worker_nodes</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">using_stage_scheduling</span><span class="p">:</span>
                <span class="n">resource_profile</span> <span class="o">=</span> <span class="n">_create_resource_profile</span><span class="p">(</span>
                    <span class="n">num_cpus_per_node</span><span class="p">,</span>
                    <span class="n">num_gpus_per_node</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">job_rdd</span> <span class="o">=</span> <span class="n">job_rdd</span><span class="o">.</span><span class="n">withResources</span><span class="p">(</span><span class="n">resource_profile</span><span class="p">)</span>

            <span class="n">job_rdd</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="n">ray_cluster_job_mapper</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="c1"># NB:</span>
            <span class="c1"># The background spark job is designed to running forever until it is</span>
            <span class="c1"># killed, The exception might be raised in following cases:</span>
            <span class="c1">#  1. The background job raises unexpected exception (i.e. ray cluster dies</span>
            <span class="c1">#    unexpectedly)</span>
            <span class="c1">#  2. User explicitly orders shutting down the ray cluster.</span>
            <span class="c1">#  3. On Databricks runtime, when a notebook is detached, it triggers</span>
            <span class="c1">#     python REPL `onCancel` event, cancelling the background running spark</span>
            <span class="c1">#     job.</span>
            <span class="c1">#  For case 1 and 3, only ray workers are killed, but driver side ray head</span>
            <span class="c1">#  might still be running and the ray context might be in connected status.</span>
            <span class="c1">#  In order to disconnect and kill the ray head node, a call to</span>
            <span class="c1">#  `ray_cluster_handler.shutdown()` is performed.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">ray_cluster_handler</span><span class="o">.</span><span class="n">spark_job_is_canceled</span><span class="p">:</span>
                <span class="c1"># Set `background_job_exception` attribute before calling `shutdown`</span>
                <span class="c1"># so inside `shutdown` we can get exception information easily.</span>
                <span class="n">ray_cluster_handler</span><span class="o">.</span><span class="n">background_job_exception</span> <span class="o">=</span> <span class="n">e</span>
                <span class="n">ray_cluster_handler</span><span class="o">.</span><span class="n">shutdown</span><span class="p">(</span><span class="n">cancel_background_job</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span>
            <span class="n">target</span><span class="o">=</span><span class="n">inheritable_thread_target</span><span class="p">(</span><span class="n">background_job_thread_fn</span><span class="p">),</span> <span class="n">args</span><span class="o">=</span><span class="p">()</span>
        <span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

        <span class="c1"># Call hook immediately after spark job started.</span>
        <span class="n">start_hook</span><span class="o">.</span><span class="n">on_cluster_created</span><span class="p">(</span><span class="n">ray_cluster_handler</span><span class="p">)</span>

        <span class="c1"># wait background spark task starting.</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">_BACKGROUND_JOB_STARTUP_WAIT</span><span class="p">):</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">ray_cluster_handler</span><span class="o">.</span><span class="n">background_job_exception</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Ray workers failed to start.&quot;</span>
                <span class="p">)</span> <span class="kn">from</span> <span class="nn">ray_cluster_handler.background_job_exception</span>

        <span class="k">return</span> <span class="n">ray_cluster_handler</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
        <span class="c1"># If driver side setup ray-cluster routine raises exception, it might result</span>
        <span class="c1"># in part of ray processes has been launched (e.g. ray head or some ray workers</span>
        <span class="c1"># have been launched), calling `ray_cluster_handler.shutdown()` to kill them</span>
        <span class="c1"># and clean status.</span>
        <span class="n">ray_cluster_handler</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
        <span class="k">raise</span>


<span class="n">_active_ray_cluster</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">_active_ray_cluster_rwlock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">RLock</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_create_resource_profile</span><span class="p">(</span><span class="n">num_cpus_per_node</span><span class="p">,</span> <span class="n">num_gpus_per_node</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">pyspark.resource.profile</span> <span class="kn">import</span> <span class="n">ResourceProfileBuilder</span>
    <span class="kn">from</span> <span class="nn">pyspark.resource.requests</span> <span class="kn">import</span> <span class="n">TaskResourceRequests</span>

    <span class="n">task_res_req</span> <span class="o">=</span> <span class="n">TaskResourceRequests</span><span class="p">()</span><span class="o">.</span><span class="n">cpus</span><span class="p">(</span><span class="n">num_cpus_per_node</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">num_gpus_per_node</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">task_res_req</span> <span class="o">=</span> <span class="n">task_res_req</span><span class="o">.</span><span class="n">resource</span><span class="p">(</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">num_gpus_per_node</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ResourceProfileBuilder</span><span class="p">()</span><span class="o">.</span><span class="n">require</span><span class="p">(</span><span class="n">task_res_req</span><span class="p">)</span><span class="o">.</span><span class="n">build</span>


<span class="c1"># A dict storing blocked key to replacement argument you should use.</span>
<span class="n">_head_node_option_block_keys</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;temp_dir&quot;</span><span class="p">:</span> <span class="s2">&quot;ray_temp_root_dir&quot;</span><span class="p">,</span>
    <span class="s2">&quot;block&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;head&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;node_ip_address&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;port&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;num_cpus&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;num_gpus&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;dashboard_host&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;dashboard_agent_listen_port&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">_worker_node_option_block_keys</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;temp_dir&quot;</span><span class="p">:</span> <span class="s2">&quot;ray_temp_root_dir&quot;</span><span class="p">,</span>
    <span class="s2">&quot;block&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;head&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;address&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;num_cpus&quot;</span><span class="p">:</span> <span class="s2">&quot;num_cpus_per_node&quot;</span><span class="p">,</span>
    <span class="s2">&quot;num_gpus&quot;</span><span class="p">:</span> <span class="s2">&quot;num_gpus_per_node&quot;</span><span class="p">,</span>
    <span class="s2">&quot;memory&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;object_store_memory&quot;</span><span class="p">:</span> <span class="s2">&quot;object_store_memory_per_node&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dashboard_agent_listen_port&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;min_worker_port&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;max_worker_port&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>


<span class="k">def</span> <span class="nf">_verify_node_options</span><span class="p">(</span><span class="n">node_options</span><span class="p">,</span> <span class="n">block_keys</span><span class="p">,</span> <span class="n">node_type</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">node_options</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;--&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">&quot;-&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;For a ray node option like &#39;--foo-bar&#39;, you should convert it to &quot;</span>
                <span class="s2">&quot;following format &#39;foo_bar&#39; in &#39;head_node_options&#39; / &quot;</span>
                <span class="s2">&quot;&#39;worker_node_options&#39; arguments.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">block_keys</span><span class="p">:</span>
            <span class="n">common_err_msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Setting the option &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39; for </span><span class="si">{</span><span class="n">node_type</span><span class="si">}</span><span class="s2"> nodes is not allowed.&quot;</span>
            <span class="p">)</span>
            <span class="n">replacement_arg</span> <span class="o">=</span> <span class="n">block_keys</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">replacement_arg</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">common_err_msg</span><span class="si">}</span><span class="s2"> You should set the &#39;</span><span class="si">{</span><span class="n">replacement_arg</span><span class="si">}</span><span class="s2">&#39; option &quot;</span>
                    <span class="s2">&quot;instead.&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">common_err_msg</span><span class="si">}</span><span class="s2"> This option is controlled by Ray on Spark.&quot;</span>
                <span class="p">)</span>


<div class="viewcode-block" id="setup_ray_cluster"><a class="viewcode-back" href="../../../../cluster/vms/user-guides/community/spark.html#ray.util.spark.setup_ray_cluster">[docs]</a><span class="nd">@PublicAPI</span><span class="p">(</span><span class="n">stability</span><span class="o">=</span><span class="s2">&quot;alpha&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">setup_ray_cluster</span><span class="p">(</span>
    <span class="n">num_worker_nodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_cpus_per_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_gpus_per_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">object_store_memory_per_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_node_options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">worker_node_options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ray_temp_root_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">strict_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">collect_log_to_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set up a ray cluster on the spark cluster by starting a ray head node in the</span>
<span class="sd">    spark application&#39;s driver side node.</span>
<span class="sd">    After creating the head node, a background spark job is created that</span>
<span class="sd">    generates an instance of `RayClusterOnSpark` that contains configuration for the</span>
<span class="sd">    ray cluster that will run on the Spark cluster&#39;s worker nodes.</span>
<span class="sd">    After a ray cluster is set up, &quot;RAY_ADDRESS&quot; environment variable is set to</span>
<span class="sd">    the cluster address, so you can call `ray.init()` without specifying ray cluster</span>
<span class="sd">    address to connect to the cluster. To shut down the cluster you can call</span>
<span class="sd">    `ray.util.spark.shutdown_ray_cluster()`.</span>
<span class="sd">    Note: If the active ray cluster haven&#39;t shut down, you cannot create a new ray</span>
<span class="sd">    cluster.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_worker_nodes: This argument represents how many ray worker nodes to start</span>
<span class="sd">            for the ray cluster.</span>
<span class="sd">            Specifying the `num_worker_nodes` as `ray.util.spark.MAX_NUM_WORKER_NODES`</span>
<span class="sd">            represents a ray cluster</span>
<span class="sd">            configuration that will use all available resources configured for the</span>
<span class="sd">            spark application.</span>
<span class="sd">            To create a spark application that is intended to exclusively run a</span>
<span class="sd">            shared ray cluster, it is recommended to set this argument to</span>
<span class="sd">            `ray.util.spark.MAX_NUM_WORKER_NODES`.</span>
<span class="sd">        num_cpus_per_node: Number of cpus available to per-ray worker node, if not</span>
<span class="sd">            provided, use spark application configuration &#39;spark.task.cpus&#39; instead.</span>
<span class="sd">            **Limitation** Only spark version &gt;= 3.4 or Databricks Runtime 12.x</span>
<span class="sd">            supports setting this argument.</span>
<span class="sd">        num_gpus_per_node: Number of gpus available to per-ray worker node, if not</span>
<span class="sd">            provided, use spark application configuration</span>
<span class="sd">            &#39;spark.task.resource.gpu.amount&#39; instead.</span>
<span class="sd">            This argument is only available on spark cluster that is configured with</span>
<span class="sd">            &#39;gpu&#39; resources.</span>
<span class="sd">            **Limitation** Only spark version &gt;= 3.4 or Databricks Runtime 12.x</span>
<span class="sd">            supports setting this argument.</span>
<span class="sd">        object_store_memory_per_node: Object store memory available to per-ray worker</span>
<span class="sd">            node, but it is capped by</span>
<span class="sd">            &quot;dev_shm_available_size * 0.8 / num_tasks_per_spark_worker&quot;.</span>
<span class="sd">            The default value equals to</span>
<span class="sd">            &quot;0.3 * spark_worker_physical_memory * 0.8 / num_tasks_per_spark_worker&quot;.</span>
<span class="sd">        head_node_options: A dict representing Ray head node extra options, these</span>
<span class="sd">            options will be passed to `ray start` script. Note you need to convert</span>
<span class="sd">            `ray start` options key from `--foo-bar` format to `foo_bar` format.</span>
<span class="sd">            For flag options (e.g. &#39;--disable-usage-stats&#39;), you should set the value</span>
<span class="sd">            to None in the option dict, like `{&quot;disable_usage_stats&quot;: None}`.</span>
<span class="sd">            Note: Short name options (e.g. &#39;-v&#39;) are not supported.</span>
<span class="sd">        worker_node_options: A dict representing Ray worker node extra options,</span>
<span class="sd">            these options will be passed to `ray start` script. Note you need to</span>
<span class="sd">            convert `ray start` options key from `--foo-bar` format to `foo_bar`</span>
<span class="sd">            format.</span>
<span class="sd">            For flag options (e.g. &#39;--disable-usage-stats&#39;), you should set the value</span>
<span class="sd">            to None in the option dict, like `{&quot;disable_usage_stats&quot;: None}`.</span>
<span class="sd">            Note: Short name options (e.g. &#39;-v&#39;) are not supported.</span>
<span class="sd">        ray_temp_root_dir: A local disk path to store the ray temporary data. The</span>
<span class="sd">            created cluster will create a subdirectory</span>
<span class="sd">            &quot;ray-{head_port}-{random_suffix}&quot; beneath this path.</span>
<span class="sd">        strict_mode: Boolean flag to fast-fail initialization of the ray cluster if</span>
<span class="sd">            the available spark cluster does not have sufficient resources to fulfill</span>
<span class="sd">            the resource allocation for memory, cpu and gpu. When set to true, if the</span>
<span class="sd">            requested resources are not available for recommended minimum recommended</span>
<span class="sd">            functionality, an exception will be raised that details the inadequate</span>
<span class="sd">            spark cluster configuration settings. If overridden as `False`,</span>
<span class="sd">            a warning is raised.</span>
<span class="sd">        collect_log_to_path: If specified, after ray head / worker nodes terminated,</span>
<span class="sd">            collect their logs to the specified path. On Databricks Runtime, we</span>
<span class="sd">            recommend you to specify a local path starts with &#39;/dbfs/&#39;, because the</span>
<span class="sd">            path mounts with a centralized storage device and stored data is persisted</span>
<span class="sd">            after Databricks spark cluster terminated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The address of the initiated Ray cluster on spark.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_active_ray_cluster</span>

    <span class="n">_check_system_environment</span><span class="p">()</span>

    <span class="n">head_node_options</span> <span class="o">=</span> <span class="n">head_node_options</span> <span class="ow">or</span> <span class="p">{}</span>
    <span class="n">worker_node_options</span> <span class="o">=</span> <span class="n">worker_node_options</span> <span class="ow">or</span> <span class="p">{}</span>

    <span class="n">_verify_node_options</span><span class="p">(</span>
        <span class="n">head_node_options</span><span class="p">,</span>
        <span class="n">_head_node_option_block_keys</span><span class="p">,</span>
        <span class="s2">&quot;Ray head node on spark&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">_verify_node_options</span><span class="p">(</span>
        <span class="n">worker_node_options</span><span class="p">,</span>
        <span class="n">_worker_node_option_block_keys</span><span class="p">,</span>
        <span class="s2">&quot;Ray worker node on spark&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">_active_ray_cluster</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Current active ray cluster on spark haven&#39;t shut down. Please call &quot;</span>
            <span class="s2">&quot;`ray.util.spark.shutdown_ray_cluster()` before initiating a new Ray &quot;</span>
            <span class="s2">&quot;cluster on spark.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">ray</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Current python process already initialized Ray, Please shut down it &quot;</span>
            <span class="s2">&quot;by `ray.shutdown()` before initiating a Ray cluster on spark.&quot;</span>
        <span class="p">)</span>

    <span class="n">spark</span> <span class="o">=</span> <span class="n">get_spark_session</span><span class="p">()</span>

    <span class="n">spark_master</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">master</span>

    <span class="n">is_spark_local_mode</span> <span class="o">=</span> <span class="n">spark_master</span> <span class="o">==</span> <span class="s2">&quot;local&quot;</span> <span class="ow">or</span> <span class="n">spark_master</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;local[&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
        <span class="n">spark_master</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;spark://&quot;</span><span class="p">)</span>
        <span class="ow">or</span> <span class="n">spark_master</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;local-cluster[&quot;</span><span class="p">)</span>
        <span class="ow">or</span> <span class="n">is_spark_local_mode</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Ray on Spark only supports spark cluster in standalone mode, &quot;</span>
            <span class="s2">&quot;local-cluster mode or spark local mode.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">is_spark_local_mode</span><span class="p">:</span>
        <span class="n">support_stage_scheduling</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">elif</span> <span class="p">(</span>
        <span class="n">is_in_databricks_runtime</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">Version</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;DATABRICKS_RUNTIME_VERSION&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">major</span> <span class="o">&gt;=</span> <span class="mi">12</span>
    <span class="p">):</span>
        <span class="n">support_stage_scheduling</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">pyspark</span>

        <span class="k">if</span> <span class="n">Version</span><span class="p">(</span><span class="n">pyspark</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span><span class="o">.</span><span class="n">release</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">support_stage_scheduling</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">support_stage_scheduling</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># Environment configurations within the Spark Session that dictate how many cpus</span>
    <span class="c1"># and gpus to use for each submitted spark task.</span>
    <span class="n">num_spark_task_cpus</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">getConf</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;spark.task.cpus&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">num_cpus_per_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">num_cpus_per_node</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Argument `num_cpus_per_node` value must be &gt; 0.&quot;</span><span class="p">)</span>

    <span class="n">num_spark_task_gpus</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
        <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">getConf</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;spark.task.resource.gpu.amount&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">num_gpus_per_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">num_spark_task_gpus</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;The spark cluster is not configured with &#39;gpu&#39; resources, so that &quot;</span>
            <span class="s2">&quot;you cannot specify the `num_gpus_per_node` argument.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">num_gpus_per_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">num_gpus_per_node</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Argument `num_gpus_per_node` value must be &gt;= 0.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">num_cpus_per_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">num_gpus_per_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">support_stage_scheduling</span><span class="p">:</span>
            <span class="n">num_cpus_per_node</span> <span class="o">=</span> <span class="n">num_cpus_per_node</span> <span class="ow">or</span> <span class="n">num_spark_task_cpus</span>
            <span class="n">num_gpus_per_node</span> <span class="o">=</span> <span class="n">num_gpus_per_node</span> <span class="ow">or</span> <span class="n">num_spark_task_gpus</span>

            <span class="n">using_stage_scheduling</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">res_profile</span> <span class="o">=</span> <span class="n">_create_resource_profile</span><span class="p">(</span><span class="n">num_cpus_per_node</span><span class="p">,</span> <span class="n">num_gpus_per_node</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Current spark version does not support stage scheduling, so that &quot;</span>
                <span class="s2">&quot;you cannot set the argument `num_cpus_per_node` and &quot;</span>
                <span class="s2">&quot;`num_gpus_per_node` values. Without setting the 2 arguments, &quot;</span>
                <span class="s2">&quot;per-Ray worker node will be assigned with number of &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;&#39;spark.task.cpus&#39; (equals to </span><span class="si">{</span><span class="n">num_spark_task_cpus</span><span class="si">}</span><span class="s2">) cpu cores &quot;</span>
                <span class="s2">&quot;and number of &#39;spark.task.resource.gpu.amount&#39; &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;(equals to </span><span class="si">{</span><span class="n">num_spark_task_gpus</span><span class="si">}</span><span class="s2">) GPUs. To enable spark stage &quot;</span>
                <span class="s2">&quot;scheduling, you need to upgrade spark to 3.4 version or use &quot;</span>
                <span class="s2">&quot;Databricks Runtime 12.x, and you cannot use spark local mode.&quot;</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">using_stage_scheduling</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">res_profile</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">num_cpus_per_node</span> <span class="o">=</span> <span class="n">num_spark_task_cpus</span>
        <span class="n">num_gpus_per_node</span> <span class="o">=</span> <span class="n">num_spark_task_gpus</span>

    <span class="p">(</span>
        <span class="n">ray_worker_node_heap_mem_bytes</span><span class="p">,</span>
        <span class="n">ray_worker_node_object_store_mem_bytes</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">=</span> <span class="n">get_avail_mem_per_ray_worker_node</span><span class="p">(</span>
        <span class="n">spark</span><span class="p">,</span>
        <span class="n">object_store_memory_per_node</span><span class="p">,</span>
        <span class="n">num_cpus_per_node</span><span class="p">,</span>
        <span class="n">num_gpus_per_node</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">num_worker_nodes</span> <span class="o">==</span> <span class="n">MAX_NUM_WORKER_NODES</span><span class="p">:</span>
        <span class="c1"># num_worker_nodes=MAX_NUM_WORKER_NODES represents using all available</span>
        <span class="c1"># spark task slots</span>
        <span class="n">num_worker_nodes</span> <span class="o">=</span> <span class="n">get_max_num_concurrent_tasks</span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="p">,</span> <span class="n">res_profile</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">num_worker_nodes</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;The value of &#39;num_worker_nodes&#39; argument must be either a positive &quot;</span>
            <span class="s2">&quot;integer or &#39;ray.util.spark.MAX_NUM_WORKER_NODES&#39;.&quot;</span>
        <span class="p">)</span>

    <span class="n">insufficient_resources</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="n">num_cpus_per_node</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">insufficient_resources</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="s2">&quot;The provided CPU resources for each ray worker are inadequate to start &quot;</span>
            <span class="s2">&quot;a ray cluster. Based on the total cpu resources available and the &quot;</span>
            <span class="s2">&quot;configured task sizing, each ray worker node would start with &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">num_cpus_per_node</span><span class="si">}</span><span class="s2"> CPU cores. This is less than the recommended &quot;</span>
            <span class="s2">&quot;value of `4` CPUs per worker. On spark version &gt;= 3.4 or Databricks &quot;</span>
            <span class="s2">&quot;Runtime 12.x, you can set the argument `num_cpus_per_node` to &quot;</span>
            <span class="s2">&quot;a value &gt;= 4 to address it, otherwise you need to increase the spark &quot;</span>
            <span class="s2">&quot;application configuration &#39;spark.task.cpus&#39; to a minimum of `4` to &quot;</span>
            <span class="s2">&quot;address it.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">ray_worker_node_heap_mem_bytes</span> <span class="o">&lt;</span> <span class="mi">10</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">:</span>
        <span class="n">insufficient_resources</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="s2">&quot;The provided memory resources for each ray worker node are inadequate. &quot;</span>
            <span class="s2">&quot;Based on the total memory available on the spark cluster and the &quot;</span>
            <span class="s2">&quot;configured task sizing, each ray worker would start with &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ray_worker_node_heap_mem_bytes</span><span class="si">}</span><span class="s2"> bytes heap memory. This is less than &quot;</span>
            <span class="s2">&quot;the recommended value of 10GB. The ray worker node heap memory size is &quot;</span>
            <span class="s2">&quot;calculated by &quot;</span>
            <span class="s2">&quot;(SPARK_WORKER_NODE_PHYSICAL_MEMORY / num_local_spark_task_slots * 0.8) - &quot;</span>
            <span class="s2">&quot;object_store_memory_per_node. To increase the heap space available, &quot;</span>
            <span class="s2">&quot;increase the memory in the spark cluster by changing instance types or &quot;</span>
            <span class="s2">&quot;worker count, reduce the target `num_worker_nodes`, or apply a lower &quot;</span>
            <span class="s2">&quot;`object_store_memory_per_node`.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">insufficient_resources</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">strict_mode</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You are creating ray cluster on spark with strict mode (it can be &quot;</span>
                <span class="s2">&quot;disabled by setting argument &#39;strict_mode=False&#39; when calling API &quot;</span>
                <span class="s2">&quot;&#39;setup_ray_cluster&#39;), strict mode requires the spark cluster config &quot;</span>
                <span class="s2">&quot;satisfying following criterion: &quot;</span>
                <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">insufficient_resources</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">insufficient_resources</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">_active_ray_cluster_rwlock</span><span class="p">:</span>
        <span class="n">cluster</span> <span class="o">=</span> <span class="n">_setup_ray_cluster</span><span class="p">(</span>
            <span class="n">num_worker_nodes</span><span class="o">=</span><span class="n">num_worker_nodes</span><span class="p">,</span>
            <span class="n">num_cpus_per_node</span><span class="o">=</span><span class="n">num_cpus_per_node</span><span class="p">,</span>
            <span class="n">num_gpus_per_node</span><span class="o">=</span><span class="n">num_gpus_per_node</span><span class="p">,</span>
            <span class="n">using_stage_scheduling</span><span class="o">=</span><span class="n">using_stage_scheduling</span><span class="p">,</span>
            <span class="n">heap_memory_per_node</span><span class="o">=</span><span class="n">ray_worker_node_heap_mem_bytes</span><span class="p">,</span>
            <span class="n">object_store_memory_per_node</span><span class="o">=</span><span class="n">ray_worker_node_object_store_mem_bytes</span><span class="p">,</span>
            <span class="n">head_node_options</span><span class="o">=</span><span class="n">head_node_options</span><span class="p">,</span>
            <span class="n">worker_node_options</span><span class="o">=</span><span class="n">worker_node_options</span><span class="p">,</span>
            <span class="n">ray_temp_root_dir</span><span class="o">=</span><span class="n">ray_temp_root_dir</span><span class="p">,</span>
            <span class="n">collect_log_to_path</span><span class="o">=</span><span class="n">collect_log_to_path</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">cluster</span><span class="o">.</span><span class="n">wait_until_ready</span><span class="p">()</span>  <span class="c1"># NB: this line might raise error.</span>

        <span class="c1"># If connect cluster successfully, set global _active_ray_cluster to be the</span>
        <span class="c1"># started cluster.</span>
        <span class="n">_active_ray_cluster</span> <span class="o">=</span> <span class="n">cluster</span>
    <span class="k">return</span> <span class="n">cluster</span><span class="o">.</span><span class="n">address</span></div>


<div class="viewcode-block" id="shutdown_ray_cluster"><a class="viewcode-back" href="../../../../cluster/vms/user-guides/community/spark.html#ray.util.spark.shutdown_ray_cluster">[docs]</a><span class="nd">@PublicAPI</span><span class="p">(</span><span class="n">stability</span><span class="o">=</span><span class="s2">&quot;alpha&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">shutdown_ray_cluster</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Shut down the active ray cluster.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_active_ray_cluster</span>

    <span class="k">with</span> <span class="n">_active_ray_cluster_rwlock</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">_active_ray_cluster</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;No active ray cluster to shut down.&quot;</span><span class="p">)</span>

        <span class="n">_active_ray_cluster</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
        <span class="n">_active_ray_cluster</span> <span class="o">=</span> <span class="kc">None</span></div>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>