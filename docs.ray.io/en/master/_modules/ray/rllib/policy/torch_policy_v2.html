
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ray.rllib.policy.torch_policy_v2 &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../../../../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script src="../../../../_static/js/versionwarning.js"></script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../../../_static/js/docsearch.js"></script>
    <script src="../../../../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../../../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../../../_static/js/custom.js"></script>
    <script defer="defer" src="../../../../_static/js/top-navigation.js"></script>
    <script src="../../../../_static/js/tags.js"></script>
    <script src="../../../../_static/tabs.js"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../../_static/design-tabs.js"></script>
    <script async="async" src="../../../../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/_modules/ray/rllib/policy/torch_policy_v2.html" />
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "_modules/ray/rllib/policy/torch_policy_v2", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2F_modules/ray/rllib/policy/torch_policy_v2.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for ray.rllib.policy.torch_policy_v2</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tree</span>  <span class="c1"># pip install dm_tree</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.models.base</span> <span class="kn">import</span> <span class="n">STATE_OUT</span>
<span class="kn">from</span> <span class="nn">ray.rllib.models.catalog</span> <span class="kn">import</span> <span class="n">ModelCatalog</span>
<span class="kn">from</span> <span class="nn">ray.rllib.models.modelv2</span> <span class="kn">import</span> <span class="n">ModelV2</span>
<span class="kn">from</span> <span class="nn">ray.rllib.models.torch.torch_action_dist</span> <span class="kn">import</span> <span class="n">TorchDistributionWrapper</span>
<span class="kn">from</span> <span class="nn">ray.rllib.models.torch.torch_modelv2</span> <span class="kn">import</span> <span class="n">TorchModelV2</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.policy</span> <span class="kn">import</span> <span class="n">Policy</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.rnn_sequencing</span> <span class="kn">import</span> <span class="n">pad_batch_to_sequences_of_same_size</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.sample_batch</span> <span class="kn">import</span> <span class="n">SampleBatch</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.torch_policy</span> <span class="kn">import</span> <span class="n">_directStepOptimizerSingleton</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils</span> <span class="kn">import</span> <span class="n">NullContextManager</span><span class="p">,</span> <span class="n">force_list</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module</span> <span class="kn">import</span> <span class="n">RLModule</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.annotations</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DeveloperAPI</span><span class="p">,</span>
    <span class="n">OverrideToImplementCustomLogic</span><span class="p">,</span>
    <span class="n">OverrideToImplementCustomLogic_CallToSuperRecommended</span><span class="p">,</span>
    <span class="n">is_overridden</span><span class="p">,</span>
    <span class="n">override</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.error</span> <span class="kn">import</span> <span class="n">ERR_MSG_TORCH_POLICY_CANNOT_SAVE_MODEL</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.framework</span> <span class="kn">import</span> <span class="n">try_import_torch</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.metrics</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DIFF_NUM_GRAD_UPDATES_VS_SAMPLER_POLICY</span><span class="p">,</span>
    <span class="n">NUM_AGENT_STEPS_TRAINED</span><span class="p">,</span>
    <span class="n">NUM_GRAD_UPDATES_LIFETIME</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.metrics.learner_info</span> <span class="kn">import</span> <span class="n">LEARNER_STATS_KEY</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.numpy</span> <span class="kn">import</span> <span class="n">convert_to_numpy</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.spaces.space_utils</span> <span class="kn">import</span> <span class="n">normalize_action</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.threading</span> <span class="kn">import</span> <span class="n">with_lock</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.torch_utils</span> <span class="kn">import</span> <span class="n">convert_to_torch_tensor</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AlgorithmConfigDict</span><span class="p">,</span>
    <span class="n">GradInfoDict</span><span class="p">,</span>
    <span class="n">ModelGradients</span><span class="p">,</span>
    <span class="n">ModelWeights</span><span class="p">,</span>
    <span class="n">PolicyState</span><span class="p">,</span>
    <span class="n">TensorStructType</span><span class="p">,</span>
    <span class="n">TensorType</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">ray.rllib.evaluation</span> <span class="kn">import</span> <span class="n">Episode</span>  <span class="c1"># noqa</span>

<span class="n">torch</span><span class="p">,</span> <span class="n">nn</span> <span class="o">=</span> <span class="n">try_import_torch</span><span class="p">()</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="TorchPolicyV2"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2">[docs]</a><span class="nd">@DeveloperAPI</span>
<span class="k">class</span> <span class="nc">TorchPolicyV2</span><span class="p">(</span><span class="n">Policy</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;PyTorch specific Policy class to use with RLlib.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="TorchPolicyV2.__init__"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.__init__.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2.__init__">[docs]</a>    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">observation_space</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">AlgorithmConfigDict</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initializes a TorchPolicy instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            observation_space: Observation space of the policy.</span>
<span class="sd">            action_space: Action space of the policy.</span>
<span class="sd">            config: The Policy&#39;s config dict.</span>
<span class="sd">            max_seq_len: Max sequence length for LSTM training.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">framework</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;framework&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;torch&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_loss_initialized</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">observation_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

        <span class="c1"># Create model.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_enable_rl_module_api&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_rl_module</span><span class="p">()</span>

            <span class="n">dist_class</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">dist_class</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_model_and_dist_class</span><span class="p">()</span>

        <span class="c1"># Create multi-GPU model towers, if necessary.</span>
        <span class="c1"># - The central main model will be stored under self.model, residing</span>
        <span class="c1">#   on self.device (normally, a CPU).</span>
        <span class="c1"># - Each GPU will have a copy of that model under</span>
        <span class="c1">#   self.model_gpu_towers, matching the devices in self.devices.</span>
        <span class="c1"># - Parallelization is done by splitting the train batch and passing</span>
        <span class="c1">#   it through the model copies in parallel, then averaging over the</span>
        <span class="c1">#   resulting gradients, applying these averages on the main model and</span>
        <span class="c1">#   updating all towers&#39; weights from the main model.</span>
        <span class="c1"># - In case of just one device (1 (fake or real) GPU or 1 CPU), no</span>
        <span class="c1">#   parallelization will be done.</span>

        <span class="c1"># Get devices to build the graph on.</span>
        <span class="n">num_gpus</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_num_gpus_for_policy</span><span class="p">()</span>
        <span class="n">gpu_ids</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()))</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">gpu_ids</span><span class="p">)</span><span class="si">}</span><span class="s2"> visible cuda devices.&quot;</span><span class="p">)</span>

        <span class="c1"># Place on one or more CPU(s) when either:</span>
        <span class="c1"># - Fake GPU mode.</span>
        <span class="c1"># - num_gpus=0 (either set by user or we are in local_mode=True).</span>
        <span class="c1"># - No GPUs available.</span>
        <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;_fake_gpus&quot;</span><span class="p">]</span> <span class="ow">or</span> <span class="n">num_gpus</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">gpu_ids</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">devices</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">num_gpus</span><span class="p">))</span> <span class="ow">or</span> <span class="mi">1</span><span class="p">)]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">model</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">num_gpus</span><span class="p">))</span> <span class="ow">or</span> <span class="mi">1</span><span class="p">)</span>
            <span class="p">]</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;target_model&quot;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">target_models</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">m</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_model</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span>
                <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="c1"># Place on one or more actual GPU(s), when:</span>
        <span class="c1"># - num_gpus &gt; 0 (set by user) AND</span>
        <span class="c1"># - local_mode=False AND</span>
        <span class="c1"># - actual GPUs available AND</span>
        <span class="c1"># - non-fake GPU mode.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># We are a remote worker (WORKER_MODE=1):</span>
            <span class="c1"># GPUs should be assigned to us by ray.</span>
            <span class="k">if</span> <span class="n">ray</span><span class="o">.</span><span class="n">_private</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">_mode</span><span class="p">()</span> <span class="o">==</span> <span class="n">ray</span><span class="o">.</span><span class="n">_private</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">WORKER_MODE</span><span class="p">:</span>
                <span class="n">gpu_ids</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get_gpu_ids</span><span class="p">()</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">gpu_ids</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">num_gpus</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;TorchPolicy was not able to find enough GPU IDs! Found &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">gpu_ids</span><span class="si">}</span><span class="s2">, but num_gpus=</span><span class="si">{</span><span class="n">num_gpus</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">devices</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">id_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">gpu_ids</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_gpus</span>
            <span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">id_</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">id_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">gpu_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_gpus</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ids</span><span class="p">):</span>
                <span class="n">model_copy</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model_copy</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;target_model&quot;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">target_models</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">m</span><span class="p">:</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_model</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span><span class="p">)</span>
                <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dist_class</span> <span class="o">=</span> <span class="n">dist_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unwrapped_model</span> <span class="o">=</span> <span class="n">model</span>  <span class="c1"># used to support DistributedDataParallel</span>

        <span class="c1"># Lock used for locking some methods on the object-level.</span>
        <span class="c1"># This prevents possible race conditions when calling the model</span>
        <span class="c1"># first, then its value function (e.g. in a loss function), in</span>
        <span class="c1"># between of which another model call is made (e.g. to compute an</span>
        <span class="c1"># action).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">RLock</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_state_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_initial_state</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_recurrent</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_state_inputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="c1"># Auto-update model&#39;s inference view requirements, if recurrent.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_view_requirements_from_init_state</span><span class="p">()</span>
        <span class="c1"># Combine view_requirements for Model and Policy.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">view_requirements</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">view_requirements</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_enable_rl_module_api&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="c1"># We don&#39;t need an exploration object with RLModules</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_exploration</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_enable_learner_api&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizers</span> <span class="o">=</span> <span class="n">force_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">())</span>

            <span class="c1"># Backward compatibility workaround so Policy will call self.loss()</span>
            <span class="c1"># directly.</span>
            <span class="c1"># TODO (jungong): clean up after all policies are migrated to new sub-class</span>
            <span class="c1">#  implementation.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="c1"># Store, which params (by index within the model&#39;s list of</span>
            <span class="c1"># parameters) should be updated per optimizer.</span>
            <span class="c1"># Maps optimizer idx to set or param indices.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu_param_groups</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">main_params</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())}</span>
            <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizers</span><span class="p">:</span>
                <span class="n">param_indices</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">pg_idx</span><span class="p">,</span> <span class="n">pg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                        <span class="n">param_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">main_params</span><span class="p">[</span><span class="n">p</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu_param_groups</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">param_indices</span><span class="p">))</span>

            <span class="c1"># Create n sample-batch buffers (num_multi_gpu_tower_stacks), each</span>
            <span class="c1"># one with m towers (num_gpus).</span>
            <span class="n">num_buffers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;num_multi_gpu_tower_stacks&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_batches</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_buffers</span><span class="p">)]</span>

        <span class="c1"># If set, means we are using distributed allreduce during learning.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distributed_world_size</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">batch_divisibility_req</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_batch_divisibility_req</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">=</span> <span class="n">max_seq_len</span>

        <span class="c1"># If model is an RLModule it won&#39;t have tower_stats instead there will be a</span>
        <span class="c1"># self.tower_state[model] -&gt; dict for each tower.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tower_stats</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;tower_stats&quot;</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tower_stats</span><span class="p">[</span><span class="n">model</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span></div>

    <span class="k">def</span> <span class="nf">loss_initialized</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_initialized</span>

<div class="viewcode-block" id="TorchPolicyV2.loss"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.loss.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2.loss">[docs]</a>    <span class="nd">@DeveloperAPI</span>
    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">ModelV2</span><span class="p">,</span>
        <span class="n">dist_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">TorchDistributionWrapper</span><span class="p">],</span>
        <span class="n">train_batch</span><span class="p">:</span> <span class="n">SampleBatch</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorType</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TensorType</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Constructs the loss function.</span>

<span class="sd">        Args:</span>
<span class="sd">            model: The Model to calculate the loss for.</span>
<span class="sd">            dist_class: The action distr. class.</span>
<span class="sd">            train_batch: The training data.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Loss tensor given the input batch.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Under the new _enable_learner_api the loss function still gets called in order</span>
        <span class="c1"># to initialize the view requirements of the sample batches that are returned by</span>
        <span class="c1"># the sampler. In this case, we don&#39;t actually want to compute any loss, however</span>
        <span class="c1"># if we access the keys that are needed for a forward_train pass, then the</span>
        <span class="c1"># sampler will include those keys in the sample batches it returns. This means</span>
        <span class="c1"># that the correct sample batch keys will be available when using the learner</span>
        <span class="c1"># group API.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_enable_learner_api</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">input_specs_train</span><span class="p">():</span>
                <span class="n">train_batch</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="TorchPolicyV2.action_sampler_fn"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.action_sampler_fn.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2.action_sampler_fn">[docs]</a>    <span class="nd">@DeveloperAPI</span>
    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span> <span class="nf">action_sampler_fn</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">ModelV2</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">obs_batch</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span>
        <span class="n">state_batches</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TensorType</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TensorType</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Custom function for sampling new actions given policy.</span>

<span class="sd">        Args:</span>
<span class="sd">            model: Underlying model.</span>
<span class="sd">            obs_batch: Observation tensor batch.</span>
<span class="sd">            state_batches: Action sampling state batch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Sampled action</span>
<span class="sd">            Log-likelihood</span>
<span class="sd">            Action distribution inputs</span>
<span class="sd">            Updated state</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="TorchPolicyV2.action_distribution_fn"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.action_distribution_fn.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2.action_distribution_fn">[docs]</a>    <span class="nd">@DeveloperAPI</span>
    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span> <span class="nf">action_distribution_fn</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">ModelV2</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">obs_batch</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span>
        <span class="n">state_batches</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TensorType</span><span class="p">,</span> <span class="nb">type</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TensorType</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Action distribution function for this Policy.</span>

<span class="sd">        Args:</span>
<span class="sd">            model: Underlying model.</span>
<span class="sd">            obs_batch: Observation tensor batch.</span>
<span class="sd">            state_batches: Action sampling state batch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Distribution input.</span>
<span class="sd">            ActionDistribution class.</span>
<span class="sd">            State outs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="TorchPolicyV2.make_model"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.make_model.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2.make_model">[docs]</a>    <span class="nd">@DeveloperAPI</span>
    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span> <span class="nf">make_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModelV2</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Create model.</span>

<span class="sd">        Note: only one of make_model or make_model_and_action_dist</span>
<span class="sd">        can be overridden.</span>

<span class="sd">        Returns:</span>
<span class="sd">            ModelV2 model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="TorchPolicyV2.make_model_and_action_dist"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.make_model_and_action_dist.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2.make_model_and_action_dist">[docs]</a>    <span class="nd">@DeveloperAPI</span>
    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span> <span class="nf">make_model_and_action_dist</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ModelV2</span><span class="p">,</span> <span class="n">Type</span><span class="p">[</span><span class="n">TorchDistributionWrapper</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Create model and action distribution function.</span>

<span class="sd">        Returns:</span>
<span class="sd">            ModelV2 model.</span>
<span class="sd">            ActionDistribution class.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="TorchPolicyV2.get_batch_divisibility_req"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.get_batch_divisibility_req.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2.get_batch_divisibility_req">[docs]</a>    <span class="nd">@DeveloperAPI</span>
    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span> <span class="nf">get_batch_divisibility_req</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Get batch divisibility request.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Size N. A sample batch must be of size K*N.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># By default, any sized batch is ok, so simply return 1.</span>
        <span class="k">return</span> <span class="mi">1</span></div>

<div class="viewcode-block" id="TorchPolicyV2.stats_fn"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.stats_fn.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2.stats_fn">[docs]</a>    <span class="nd">@DeveloperAPI</span>
    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span> <span class="nf">stats_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_batch</span><span class="p">:</span> <span class="n">SampleBatch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Stats function. Returns a dict of statistics.</span>

<span class="sd">        Args:</span>
<span class="sd">            train_batch: The SampleBatch (already) used for training.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The stats dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{}</span></div>

<div class="viewcode-block" id="TorchPolicyV2.extra_grad_process"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.extra_grad_process.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2.extra_grad_process">[docs]</a>    <span class="nd">@DeveloperAPI</span>
    <span class="nd">@OverrideToImplementCustomLogic_CallToSuperRecommended</span>
    <span class="k">def</span> <span class="nf">extra_grad_process</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="s2">&quot;torch.optim.Optimizer&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">TensorType</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Called after each optimizer.zero_grad() + loss.backward() call.</span>

<span class="sd">        Called for each self._optimizers/loss-value pair.</span>
<span class="sd">        Allows for gradient processing before optimizer.step() is called.</span>
<span class="sd">        E.g. for gradient clipping.</span>

<span class="sd">        Args:</span>
<span class="sd">            optimizer: A torch optimizer object.</span>
<span class="sd">            loss: The loss tensor associated with the optimizer.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An dict with information on the gradient processing step.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{}</span></div>

<div class="viewcode-block" id="TorchPolicyV2.extra_compute_grad_fetches"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.extra_compute_grad_fetches.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2.extra_compute_grad_fetches">[docs]</a>    <span class="nd">@DeveloperAPI</span>
    <span class="nd">@OverrideToImplementCustomLogic_CallToSuperRecommended</span>
    <span class="k">def</span> <span class="nf">extra_compute_grad_fetches</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Extra values to fetch and return from compute_gradients().</span>

<span class="sd">        Returns:</span>
<span class="sd">            Extra fetch dict to be added to the fetch dict of the</span>
<span class="sd">            `compute_gradients` call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">LEARNER_STATS_KEY</span><span class="p">:</span> <span class="p">{}}</span>  <span class="c1"># e.g, stats, td error, etc.</span></div>

<div class="viewcode-block" id="TorchPolicyV2.extra_action_out"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.extra_action_out.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2.extra_action_out">[docs]</a>    <span class="nd">@DeveloperAPI</span>
    <span class="nd">@OverrideToImplementCustomLogic_CallToSuperRecommended</span>
    <span class="k">def</span> <span class="nf">extra_action_out</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">],</span>
        <span class="n">state_batches</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">TensorType</span><span class="p">],</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">TorchModelV2</span><span class="p">,</span>
        <span class="n">action_dist</span><span class="p">:</span> <span class="n">TorchDistributionWrapper</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Returns dict of extra info to include in experience batch.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_dict: Dict of model input tensors.</span>
<span class="sd">            state_batches: List of state tensors.</span>
<span class="sd">            model: Reference to the model object.</span>
<span class="sd">            action_dist: Torch action dist object</span>
<span class="sd">                to get log-probs (e.g. for already sampled actions).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Extra outputs to return in a `compute_actions_from_input_dict()`</span>
<span class="sd">            call (3rd return value).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{}</span></div>

<div class="viewcode-block" id="TorchPolicyV2.postprocess_trajectory"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.postprocess_trajectory.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2.postprocess_trajectory">[docs]</a>    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="nd">@OverrideToImplementCustomLogic_CallToSuperRecommended</span>
    <span class="k">def</span> <span class="nf">postprocess_trajectory</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_batch</span><span class="p">:</span> <span class="n">SampleBatch</span><span class="p">,</span>
        <span class="n">other_agent_batches</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">SampleBatch</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">episode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;Episode&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SampleBatch</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Postprocesses a trajectory and returns the processed trajectory.</span>

<span class="sd">        The trajectory contains only data from one episode and from one agent.</span>
<span class="sd">        - If  `config.batch_mode=truncate_episodes` (default), sample_batch may</span>
<span class="sd">        contain a truncated (at-the-end) episode, in case the</span>
<span class="sd">        `config.rollout_fragment_length` was reached by the sampler.</span>
<span class="sd">        - If `config.batch_mode=complete_episodes`, sample_batch will contain</span>
<span class="sd">        exactly one episode (no matter how long).</span>
<span class="sd">        New columns can be added to sample_batch and existing ones may be altered.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_batch: The SampleBatch to postprocess.</span>
<span class="sd">            other_agent_batches (Optional[Dict[PolicyID, SampleBatch]]): Optional</span>
<span class="sd">                dict of AgentIDs mapping to other agents&#39; trajectory data (from the</span>
<span class="sd">                same episode). NOTE: The other agents use the same policy.</span>
<span class="sd">            episode (Optional[Episode]): Optional multi-agent episode</span>
<span class="sd">                object in which the agents operated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            SampleBatch: The postprocessed, modified SampleBatch (or a new one).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">sample_batch</span></div>

<div class="viewcode-block" id="TorchPolicyV2.optimizer"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.optimizer.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2.optimizer">[docs]</a>    <span class="nd">@DeveloperAPI</span>
    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span> <span class="nf">optimizer</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="s2">&quot;torch.optim.Optimizer&quot;</span><span class="p">],</span> <span class="s2">&quot;torch.optim.Optimizer&quot;</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Custom the local PyTorch optimizer(s) to use.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The local PyTorch optimizer(s) to use for this Policy.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">):</span>
            <span class="n">optimizers</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">])</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">optimizers</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span><span class="p">:</span>
            <span class="n">optimizers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span><span class="o">.</span><span class="n">get_exploration_optimizer</span><span class="p">(</span><span class="n">optimizers</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">optimizers</span></div>

    <span class="k">def</span> <span class="nf">_init_model_and_dist_class</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">is_overridden</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">make_model</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_overridden</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_model_and_action_dist</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Only one of make_model or make_model_and_action_dist &quot;</span>
                <span class="s2">&quot;can be overridden.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">is_overridden</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">make_model</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_model</span><span class="p">()</span>
            <span class="n">dist_class</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ModelCatalog</span><span class="o">.</span><span class="n">get_action_dist</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">],</span> <span class="n">framework</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">framework</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">is_overridden</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">make_model_and_action_dist</span><span class="p">):</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">dist_class</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_model_and_action_dist</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dist_class</span><span class="p">,</span> <span class="n">logit_dim</span> <span class="o">=</span> <span class="n">ModelCatalog</span><span class="o">.</span><span class="n">get_action_dist</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">],</span> <span class="n">framework</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">framework</span>
            <span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">ModelCatalog</span><span class="o">.</span><span class="n">get_model_v2</span><span class="p">(</span>
                <span class="n">obs_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
                <span class="n">action_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
                <span class="n">num_outputs</span><span class="o">=</span><span class="n">logit_dim</span><span class="p">,</span>
                <span class="n">model_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">],</span>
                <span class="n">framework</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">framework</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">dist_class</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">compute_actions_from_input_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">],</span>
        <span class="n">explore</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timestep</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TensorType</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TensorType</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]:</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># Pass lazy (torch) tensor dict to Model as `input_dict`.</span>
            <span class="n">input_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_tensor_dict</span><span class="p">(</span><span class="n">input_dict</span><span class="p">)</span>
            <span class="n">input_dict</span><span class="o">.</span><span class="n">set_training</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
            <span class="c1"># Pack internal state inputs into (separate) list.</span>
            <span class="n">state_batches</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">input_dict</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">input_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;state_in&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">[:</span><span class="mi">8</span><span class="p">]</span>
            <span class="p">]</span>
            <span class="c1"># Calculate RNN sequence lengths.</span>
            <span class="n">seq_lens</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                    <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">state_batches</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="n">state_batches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">state_batches</span>
                <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_action_helper</span><span class="p">(</span>
                <span class="n">input_dict</span><span class="p">,</span> <span class="n">state_batches</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">,</span> <span class="n">explore</span><span class="p">,</span> <span class="n">timestep</span>
            <span class="p">)</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">compute_actions</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs_batch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">],</span> <span class="n">TensorStructType</span><span class="p">],</span>
        <span class="n">state_batches</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prev_action_batch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">],</span> <span class="n">TensorStructType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prev_reward_batch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">],</span> <span class="n">TensorStructType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">info_batch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">episodes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="s2">&quot;Episode&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">explore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timestep</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TensorType</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]:</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">seq_lens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">obs_batch</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="n">input_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_tensor_dict</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="n">SampleBatch</span><span class="o">.</span><span class="n">CUR_OBS</span><span class="p">:</span> <span class="n">obs_batch</span><span class="p">,</span>
                    <span class="s2">&quot;is_training&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">prev_action_batch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">input_dict</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">PREV_ACTIONS</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">prev_action_batch</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">prev_reward_batch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">input_dict</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">PREV_REWARDS</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">prev_reward_batch</span><span class="p">)</span>
            <span class="n">state_batches</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">convert_to_torch_tensor</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="p">(</span><span class="n">state_batches</span> <span class="ow">or</span> <span class="p">[])</span>
            <span class="p">]</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_action_helper</span><span class="p">(</span>
                <span class="n">input_dict</span><span class="p">,</span> <span class="n">state_batches</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">,</span> <span class="n">explore</span><span class="p">,</span> <span class="n">timestep</span>
            <span class="p">)</span>

    <span class="nd">@with_lock</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">compute_log_likelihoods</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">actions</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">],</span> <span class="n">TensorStructType</span><span class="p">],</span>
        <span class="n">obs_batch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">],</span> <span class="n">TensorStructType</span><span class="p">],</span>
        <span class="n">state_batches</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prev_action_batch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">],</span> <span class="n">TensorStructType</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prev_reward_batch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">],</span> <span class="n">TensorStructType</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">actions_normalized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">in_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>

        <span class="k">if</span> <span class="n">is_overridden</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_sampler_fn</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_overridden</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">action_distribution_fn</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot compute log-prob/likelihood w/o an &quot;</span>
                <span class="s2">&quot;`action_distribution_fn` and a provided &quot;</span>
                <span class="s2">&quot;`action_sampler_fn`!&quot;</span>
            <span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">input_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_tensor_dict</span><span class="p">(</span>
                <span class="p">{</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">CUR_OBS</span><span class="p">:</span> <span class="n">obs_batch</span><span class="p">,</span> <span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">:</span> <span class="n">actions</span><span class="p">}</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">prev_action_batch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">input_dict</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">PREV_ACTIONS</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev_action_batch</span>
            <span class="k">if</span> <span class="n">prev_reward_batch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">input_dict</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">PREV_REWARDS</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev_reward_batch</span>
            <span class="n">seq_lens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">obs_batch</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="n">state_batches</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">convert_to_torch_tensor</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="p">(</span><span class="n">state_batches</span> <span class="ow">or</span> <span class="p">[])</span>
            <span class="p">]</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span><span class="p">:</span>
                <span class="c1"># Exploration hook before each forward pass.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span><span class="o">.</span><span class="n">before_compute_actions</span><span class="p">(</span><span class="n">explore</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="c1"># Action dist class and inputs are generated via custom function.</span>
            <span class="k">if</span> <span class="n">is_overridden</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_distribution_fn</span><span class="p">):</span>
                <span class="n">dist_inputs</span><span class="p">,</span> <span class="n">dist_class</span><span class="p">,</span> <span class="n">state_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_distribution_fn</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">obs_batch</span><span class="o">=</span><span class="n">input_dict</span><span class="p">,</span>
                    <span class="n">state_batches</span><span class="o">=</span><span class="n">state_batches</span><span class="p">,</span>
                    <span class="n">seq_lens</span><span class="o">=</span><span class="n">seq_lens</span><span class="p">,</span>
                    <span class="n">explore</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">action_dist</span> <span class="o">=</span> <span class="n">dist_class</span><span class="p">(</span><span class="n">dist_inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
            <span class="c1"># Default action-dist inputs calculation.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_enable_rl_module_api&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">in_training</span><span class="p">:</span>
                        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward_train</span><span class="p">(</span><span class="n">input_dict</span><span class="p">)</span>
                        <span class="n">action_dist_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_train_action_dist_cls</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">action_dist_cls</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                                <span class="s2">&quot;The RLModules must provide an appropriate action &quot;</span>
                                <span class="s2">&quot;distribution class for training if is_eval_mode is &quot;</span>
                                <span class="s2">&quot;False.&quot;</span>
                            <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward_exploration</span><span class="p">(</span><span class="n">input_dict</span><span class="p">)</span>
                        <span class="n">action_dist_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_exploration_action_dist_cls</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">action_dist_cls</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                                <span class="s2">&quot;The RLModules must provide an appropriate action &quot;</span>
                                <span class="s2">&quot;distribution class for exploration if is_eval_mode is &quot;</span>
                                <span class="s2">&quot;True.&quot;</span>
                            <span class="p">)</span>

                    <span class="n">action_dist_inputs</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                        <span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTION_DIST_INPUTS</span><span class="p">,</span> <span class="kc">None</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="n">action_dist_inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;The RLModules must provide inputs to create the action &quot;</span>
                            <span class="s2">&quot;distribution. These should be part of the output of the &quot;</span>
                            <span class="s2">&quot;appropriate forward method under the key &quot;</span>
                            <span class="s2">&quot;SampleBatch.ACTION_DIST_INPUTS.&quot;</span>
                        <span class="p">)</span>

                    <span class="n">action_dist</span> <span class="o">=</span> <span class="n">action_dist_cls</span><span class="o">.</span><span class="n">from_logits</span><span class="p">(</span><span class="n">action_dist_inputs</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">dist_class</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_class</span>
                    <span class="n">dist_inputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="n">state_batches</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">)</span>

                    <span class="n">action_dist</span> <span class="o">=</span> <span class="n">dist_class</span><span class="p">(</span><span class="n">dist_inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>

            <span class="c1"># Normalize actions if necessary.</span>
            <span class="n">actions</span> <span class="o">=</span> <span class="n">input_dict</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">actions_normalized</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;normalize_actions&quot;</span><span class="p">]:</span>
                <span class="n">actions</span> <span class="o">=</span> <span class="n">normalize_action</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space_struct</span><span class="p">)</span>

            <span class="n">log_likelihoods</span> <span class="o">=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">log_likelihoods</span>

    <span class="nd">@with_lock</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">learn_on_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">postprocessed_batch</span><span class="p">:</span> <span class="n">SampleBatch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]:</span>

        <span class="c1"># Set Model to train mode.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="c1"># Callback handling.</span>
        <span class="n">learn_stats</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">on_learn_on_batch</span><span class="p">(</span>
            <span class="n">policy</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_batch</span><span class="o">=</span><span class="n">postprocessed_batch</span><span class="p">,</span> <span class="n">result</span><span class="o">=</span><span class="n">learn_stats</span>
        <span class="p">)</span>

        <span class="c1"># Compute gradients (will calculate all losses and `backward()`</span>
        <span class="c1"># them to get the grads).</span>
        <span class="n">grads</span><span class="p">,</span> <span class="n">fetches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">postprocessed_batch</span><span class="p">)</span>

        <span class="c1"># Step the optimizers.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">_directStepOptimizerSingleton</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_grad_updates</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;metrics&quot;</span><span class="p">):</span>
            <span class="n">fetches</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">metrics</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">fetches</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="n">fetches</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;custom_metrics&quot;</span><span class="p">:</span> <span class="n">learn_stats</span><span class="p">,</span>
                <span class="n">NUM_AGENT_STEPS_TRAINED</span><span class="p">:</span> <span class="n">postprocessed_batch</span><span class="o">.</span><span class="n">count</span><span class="p">,</span>
                <span class="n">NUM_GRAD_UPDATES_LIFETIME</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_grad_updates</span><span class="p">,</span>
                <span class="c1"># -1, b/c we have to measure this diff before we do the update above.</span>
                <span class="n">DIFF_NUM_GRAD_UPDATES_VS_SAMPLER_POLICY</span><span class="p">:</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">num_grad_updates</span>
                    <span class="o">-</span> <span class="mi">1</span>
                    <span class="o">-</span> <span class="p">(</span><span class="n">postprocessed_batch</span><span class="o">.</span><span class="n">num_grad_updates</span> <span class="ow">or</span> <span class="mi">0</span><span class="p">)</span>
                <span class="p">),</span>
            <span class="p">}</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">fetches</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">load_batch_into_buffer</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch</span><span class="p">:</span> <span class="n">SampleBatch</span><span class="p">,</span>
        <span class="n">buffer_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="c1"># Set the is_training flag of the batch.</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">set_training</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Shortcut for 1 CPU only: Store batch in `self._loaded_batches`.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">buffer_index</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="n">pad_batch_to_sequences_of_same_size</span><span class="p">(</span>
                <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
                <span class="n">max_seq_len</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span>
                <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">batch_divisibility_req</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_divisibility_req</span><span class="p">,</span>
                <span class="n">view_requirements</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">view_requirements</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_tensor_dict</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_batches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">]</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="c1"># Batch (len=28, seq-lens=[4, 7, 4, 10, 3]):</span>
        <span class="c1"># 0123 0123456 0123 0123456789ABC</span>

        <span class="c1"># 1) split into n per-GPU sub batches (n=2).</span>
        <span class="c1"># [0123 0123456] [012] [3 0123456789 ABC]</span>
        <span class="c1"># (len=14, 14 seq-lens=[4, 7, 3] [1, 10, 3])</span>
        <span class="n">slices</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">timeslices</span><span class="p">(</span><span class="n">num_slices</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">))</span>

        <span class="c1"># 2) zero-padding (max-seq-len=10).</span>
        <span class="c1"># - [0123000000 0123456000 0120000000]</span>
        <span class="c1"># - [3000000000 0123456789 ABC0000000]</span>
        <span class="k">for</span> <span class="nb">slice</span> <span class="ow">in</span> <span class="n">slices</span><span class="p">:</span>
            <span class="n">pad_batch_to_sequences_of_same_size</span><span class="p">(</span>
                <span class="n">batch</span><span class="o">=</span><span class="nb">slice</span><span class="p">,</span>
                <span class="n">max_seq_len</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span>
                <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">batch_divisibility_req</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_divisibility_req</span><span class="p">,</span>
                <span class="n">view_requirements</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">view_requirements</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># 3) Load splits into the given buffer (consisting of n GPUs).</span>
        <span class="n">slices</span> <span class="o">=</span> <span class="p">[</span><span class="nb">slice</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="nb">slice</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">slices</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_batches</span><span class="p">[</span><span class="n">buffer_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">slices</span>

        <span class="c1"># Return loaded samples per-device.</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">slices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">get_num_samples_loaded_into_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">buffer_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;/cpu:0&quot;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">buffer_index</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_batches</span><span class="p">[</span><span class="n">buffer_index</span><span class="p">])</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">learn_on_loaded_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">offset</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">buffer_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_batches</span><span class="p">[</span><span class="n">buffer_index</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Must call Policy.load_batch_into_buffer() before &quot;</span>
                <span class="s2">&quot;Policy.learn_on_loaded_batch()!&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Get the correct slice of the already loaded batch to use,</span>
        <span class="c1"># based on offset and batch size.</span>
        <span class="n">device_batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;sgd_minibatch_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;train_batch_size&quot;</span><span class="p">]</span>
        <span class="p">)</span> <span class="o">//</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span>

        <span class="c1"># Set Model to train mode.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span><span class="p">:</span>
                <span class="n">t</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

        <span class="c1"># Shortcut for 1 CPU only: Batch should already be stored in</span>
        <span class="c1"># `self._loaded_batches`.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">buffer_index</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">device_batch_size</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_loaded_batches</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]):</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_batches</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_batches</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">offset</span> <span class="p">:</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">device_batch_size</span><span class="p">]</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">learn_on_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Copy weights of main model (tower-0) to all other towers.</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
            <span class="c1"># Just making sure tower-0 is really the same as self.model.</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>
            <span class="k">for</span> <span class="n">tower</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
                <span class="n">tower</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">device_batch_size</span> <span class="o">&gt;=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_batches</span><span class="p">[</span><span class="n">buffer_index</span><span class="p">]):</span>
            <span class="n">device_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_batches</span><span class="p">[</span><span class="n">buffer_index</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">device_batches</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">b</span><span class="p">[</span><span class="n">offset</span> <span class="p">:</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">device_batch_size</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_batches</span><span class="p">[</span><span class="n">buffer_index</span><span class="p">]</span>
            <span class="p">]</span>

        <span class="c1"># Callback handling.</span>
        <span class="n">batch_fetches</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">device_batches</span><span class="p">):</span>
            <span class="n">custom_metrics</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">on_learn_on_batch</span><span class="p">(</span>
                <span class="n">policy</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_batch</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span> <span class="n">result</span><span class="o">=</span><span class="n">custom_metrics</span>
            <span class="p">)</span>
            <span class="n">batch_fetches</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;tower_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;custom_metrics&quot;</span><span class="p">:</span> <span class="n">custom_metrics</span><span class="p">}</span>

        <span class="c1"># Do the (maybe parallelized) gradient calculation step.</span>
        <span class="n">tower_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multi_gpu_parallel_grad_calc</span><span class="p">(</span><span class="n">device_batches</span><span class="p">)</span>

        <span class="c1"># Mean-reduce gradients over GPU-towers (do this on CPU: self.device).</span>
        <span class="n">all_grads</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tower_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])):</span>
            <span class="k">if</span> <span class="n">tower_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">all_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tower_outputs</span><span class="p">]),</span>
                        <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">all_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
        <span class="c1"># Set main model&#39;s grads to mean-reduced values.</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
            <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">all_grads</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">_directStepOptimizerSingleton</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_grad_updates</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span><span class="p">,</span> <span class="n">device_batches</span><span class="p">)):</span>
            <span class="n">batch_fetches</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;tower_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="n">LEARNER_STATS_KEY</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">stats_fn</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span>
                    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="p">{}</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;_enable_rl_module_api&quot;</span><span class="p">]</span>
                    <span class="k">else</span> <span class="n">model</span><span class="o">.</span><span class="n">metrics</span><span class="p">(),</span>
                    <span class="n">NUM_GRAD_UPDATES_LIFETIME</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_grad_updates</span><span class="p">,</span>
                    <span class="c1"># -1, b/c we have to measure this diff before we do the update</span>
                    <span class="c1"># above.</span>
                    <span class="n">DIFF_NUM_GRAD_UPDATES_VS_SAMPLER_POLICY</span><span class="p">:</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">num_grad_updates</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">num_grad_updates</span> <span class="ow">or</span> <span class="mi">0</span><span class="p">)</span>
                    <span class="p">),</span>
                <span class="p">}</span>
            <span class="p">)</span>
        <span class="n">batch_fetches</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">extra_compute_grad_fetches</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">batch_fetches</span>

    <span class="nd">@with_lock</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">compute_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">postprocessed_batch</span><span class="p">:</span> <span class="n">SampleBatch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModelGradients</span><span class="p">:</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>

        <span class="c1"># If not done yet, see whether we have to zero-pad this batch.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">postprocessed_batch</span><span class="o">.</span><span class="n">zero_padded</span><span class="p">:</span>
            <span class="n">pad_batch_to_sequences_of_same_size</span><span class="p">(</span>
                <span class="n">batch</span><span class="o">=</span><span class="n">postprocessed_batch</span><span class="p">,</span>
                <span class="n">max_seq_len</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span>
                <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">batch_divisibility_req</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_divisibility_req</span><span class="p">,</span>
                <span class="n">view_requirements</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">view_requirements</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">postprocessed_batch</span><span class="o">.</span><span class="n">set_training</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_tensor_dict</span><span class="p">(</span><span class="n">postprocessed_batch</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># Do the (maybe parallelized) gradient calculation step.</span>
        <span class="n">tower_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multi_gpu_parallel_grad_calc</span><span class="p">([</span><span class="n">postprocessed_batch</span><span class="p">])</span>

        <span class="n">all_grads</span><span class="p">,</span> <span class="n">grad_info</span> <span class="o">=</span> <span class="n">tower_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">grad_info</span><span class="p">[</span><span class="s2">&quot;allreduce_latency&quot;</span><span class="p">]</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizers</span><span class="p">)</span>
        <span class="n">grad_info</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stats_fn</span><span class="p">(</span><span class="n">postprocessed_batch</span><span class="p">))</span>

        <span class="n">fetches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">extra_compute_grad_fetches</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">all_grads</span><span class="p">,</span> <span class="nb">dict</span><span class="p">(</span><span class="n">fetches</span><span class="p">,</span> <span class="o">**</span><span class="p">{</span><span class="n">LEARNER_STATS_KEY</span><span class="p">:</span> <span class="n">grad_info</span><span class="p">})</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">apply_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradients</span><span class="p">:</span> <span class="n">ModelGradients</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">gradients</span> <span class="o">==</span> <span class="n">_directStepOptimizerSingleton</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">opt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizers</span><span class="p">):</span>
                <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># TODO(sven): Not supported for multiple optimizers yet.</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="k">for</span> <span class="n">g</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
                <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">g</span><span class="p">):</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">g</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<div class="viewcode-block" id="TorchPolicyV2.get_tower_stats"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.get_tower_stats.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2.get_tower_stats">[docs]</a>    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">get_tower_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stats_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Returns list of per-tower stats, copied to this Policy&#39;s device.</span>

<span class="sd">        Args:</span>
<span class="sd">            stats_name: The name of the stats to average over (this str</span>
<span class="sd">                must exist as a key inside each tower&#39;s `tower_stats` dict).</span>

<span class="sd">        Returns:</span>
<span class="sd">            The list of stats tensor (structs) of all towers, copied to this</span>
<span class="sd">            Policy&#39;s device.</span>

<span class="sd">        Raises:</span>
<span class="sd">            AssertionError: If the `stats_name` cannot be found in any one</span>
<span class="sd">            of the tower&#39;s `tower_stats` dicts.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tower_stats</span><span class="p">:</span>
                <span class="n">tower_stats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tower_stats</span><span class="p">[</span><span class="n">model</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tower_stats</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tower_stats</span>

            <span class="k">if</span> <span class="n">stats_name</span> <span class="ow">in</span> <span class="n">tower_stats</span><span class="p">:</span>
                <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
                        <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">s</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">tower_stats</span><span class="p">[</span><span class="n">stats_name</span><span class="p">]</span>
                    <span class="p">)</span>
                <span class="p">)</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Stats `</span><span class="si">{</span><span class="n">stats_name</span><span class="si">}</span><span class="s2">` not found in any of the towers (you have &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span><span class="p">)</span><span class="si">}</span><span class="s2"> towers in total)! Make &quot;</span>
            <span class="s2">&quot;sure you call the loss function on at least one of the towers.&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">data</span></div>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModelWeights</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">ModelWeights</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">convert_to_torch_tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_enable_rl_module_api&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">is_recurrent</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_recurrent</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">num_state_tensors</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_initial_state</span><span class="p">())</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">get_initial_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">TensorType</span><span class="p">]:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_enable_rl_module_api&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="c1"># convert the tree of tensors to a tree to numpy arrays</span>
            <span class="k">return</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">convert_to_numpy</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_initial_state</span><span class="p">()</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_initial_state</span><span class="p">()]</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="nd">@OverrideToImplementCustomLogic_CallToSuperRecommended</span>
    <span class="k">def</span> <span class="nf">get_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PolicyState</span><span class="p">:</span>
        <span class="c1"># Legacy Policy state (w/o torch.nn.Module and w/o PolicySpec).</span>
        <span class="n">state</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>

        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_optimizer_variables&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># In the new Learner API stack, the optimizers live in the learner.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_enable_learner_api&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizers</span><span class="p">):</span>
                <span class="n">optim_state_dict</span> <span class="o">=</span> <span class="n">convert_to_numpy</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
                <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_optimizer_variables&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optim_state_dict</span><span class="p">)</span>
        <span class="c1"># Add exploration state.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_enable_rl_module_api&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span><span class="p">:</span>
            <span class="c1"># This is not compatible with RLModules, which have a method</span>
            <span class="c1"># `forward_exploration` to specify custom exploration behavior.</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_exploration_state&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">state</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="nd">@OverrideToImplementCustomLogic_CallToSuperRecommended</span>
    <span class="k">def</span> <span class="nf">set_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">PolicyState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Set optimizer vars first.</span>
        <span class="n">optimizer_vars</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_optimizer_variables&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">optimizer_vars</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer_vars</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizers</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">o</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizers</span><span class="p">,</span> <span class="n">optimizer_vars</span><span class="p">):</span>
                <span class="c1"># Torch optimizer param_groups include things like beta, etc. These</span>
                <span class="c1"># parameters should be left as scalar and not converted to tensors.</span>
                <span class="c1"># otherwise, torch.optim.step() will start to complain.</span>
                <span class="n">optim_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;param_groups&quot;</span><span class="p">:</span> <span class="n">s</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]}</span>
                <span class="n">optim_state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">convert_to_torch_tensor</span><span class="p">(</span>
                    <span class="n">s</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
                <span class="p">)</span>
                <span class="n">o</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">optim_state_dict</span><span class="p">)</span>
        <span class="c1"># Set exploration&#39;s state.</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;exploration&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;_exploration_state&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;_exploration_state&quot;</span><span class="p">])</span>

        <span class="c1"># Restore glbal timestep.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_timestep</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;global_timestep&quot;</span><span class="p">]</span>

        <span class="c1"># Then the Policy&#39;s (NN) weights and connectors.</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

<div class="viewcode-block" id="TorchPolicyV2.export_model"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.export_model.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2.export_model">[docs]</a>    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">export_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">export_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">onnx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Exports the Policy&#39;s Model to local directory for serving.</span>

<span class="sd">        Creates a TorchScript model and saves it.</span>

<span class="sd">        Args:</span>
<span class="sd">            export_dir: Local writable directory or filename.</span>
<span class="sd">            onnx: If given, will export model in ONNX format. The</span>
<span class="sd">                value of this parameter set the ONNX OpSet version to use.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">export_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">enable_rl_module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_enable_rl_module_api&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">enable_rl_module</span> <span class="ow">and</span> <span class="n">onnx</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;ONNX export not supported for RLModule API.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">onnx</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_tensor_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dummy_batch</span><span class="p">)</span>
            <span class="c1"># Provide dummy state inputs if not an RNN (torch cannot jit with</span>
            <span class="c1"># returned empty internal states list).</span>
            <span class="k">if</span> <span class="s2">&quot;state_in_0&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dummy_batch</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_dummy_batch</span><span class="p">[</span><span class="s2">&quot;state_in_0&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dummy_batch</span><span class="p">[</span>
                    <span class="n">SampleBatch</span><span class="o">.</span><span class="n">SEQ_LENS</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>
            <span class="n">seq_lens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dummy_batch</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">SEQ_LENS</span><span class="p">]</span>

            <span class="n">state_ins</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="s2">&quot;state_in_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dummy_batch</span><span class="p">:</span>
                <span class="n">state_ins</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dummy_batch</span><span class="p">[</span><span class="s2">&quot;state_in_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">dummy_inputs</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">k</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dummy_batch</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dummy_batch</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="s2">&quot;is_training&quot;</span>
            <span class="p">}</span>

            <span class="n">file_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">export_dir</span><span class="p">,</span> <span class="s2">&quot;model.onnx&quot;</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
                <span class="p">(</span><span class="n">dummy_inputs</span><span class="p">,</span> <span class="n">state_ins</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">),</span>
                <span class="n">file_name</span><span class="p">,</span>
                <span class="n">export_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">opset_version</span><span class="o">=</span><span class="n">onnx</span><span class="p">,</span>
                <span class="n">do_constant_folding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">input_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">dummy_inputs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
                <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;state_ins&quot;</span><span class="p">,</span> <span class="n">SampleBatch</span><span class="o">.</span><span class="n">SEQ_LENS</span><span class="p">],</span>
                <span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;state_outs&quot;</span><span class="p">],</span>
                <span class="n">dynamic_axes</span><span class="o">=</span><span class="p">{</span>
                    <span class="n">k</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">}</span>
                    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">dummy_inputs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
                    <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;state_ins&quot;</span><span class="p">,</span> <span class="n">SampleBatch</span><span class="o">.</span><span class="n">SEQ_LENS</span><span class="p">]</span>
                <span class="p">},</span>
            <span class="p">)</span>
        <span class="c1"># Save the torch.Model (architecture and weights, so it can be retrieved</span>
        <span class="c1"># w/o access to the original (custom) Model or Policy code).</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">export_dir</span><span class="p">,</span> <span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">filename</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">ERR_MSG_TORCH_POLICY_CANNOT_SAVE_MODEL</span><span class="p">)</span></div>

<div class="viewcode-block" id="TorchPolicyV2.import_model_from_h5"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.import_model_from_h5.html#ray.rllib.policy.torch_policy_v2.TorchPolicyV2.import_model_from_h5">[docs]</a>    <span class="nd">@override</span><span class="p">(</span><span class="n">Policy</span><span class="p">)</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">import_model_from_h5</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">import_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Imports weights into torch model.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">import_from_h5</span><span class="p">(</span><span class="n">import_file</span><span class="p">)</span></div>

    <span class="nd">@with_lock</span>
    <span class="k">def</span> <span class="nf">_compute_action_helper</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">state_batches</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">,</span> <span class="n">explore</span><span class="p">,</span> <span class="n">timestep</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Shared forward pass logic (w/ and w/o trajectory view API).</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple consisting of a) actions, b) state_out, c) extra_fetches.</span>
<span class="sd">            The input_dict is modified in-place to include a numpy copy of the computed</span>
<span class="sd">            actions under `SampleBatch.ACTIONS`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">explore</span> <span class="o">=</span> <span class="n">explore</span> <span class="k">if</span> <span class="n">explore</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;explore&quot;</span><span class="p">]</span>
        <span class="n">timestep</span> <span class="o">=</span> <span class="n">timestep</span> <span class="k">if</span> <span class="n">timestep</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_timestep</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_recurrent</span> <span class="o">=</span> <span class="n">state_batches</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">state_batches</span> <span class="o">!=</span> <span class="p">[]</span>

        <span class="c1"># Switch to eval mode.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="n">extra_fetches</span> <span class="o">=</span> <span class="n">dist_inputs</span> <span class="o">=</span> <span class="n">logp</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># New API stack: `self.model` is-a RLModule.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">RLModule</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">explore</span><span class="p">:</span>
                <span class="n">fwd_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward_exploration</span><span class="p">(</span><span class="n">input_dict</span><span class="p">)</span>

                <span class="c1"># ACTION_DIST_INPUTS field returned by `forward_exploration()` -&gt;</span>
                <span class="c1"># Create a distribution object.</span>
                <span class="n">action_dist</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">if</span> <span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTION_DIST_INPUTS</span> <span class="ow">in</span> <span class="n">fwd_out</span><span class="p">:</span>
                    <span class="n">dist_inputs</span> <span class="o">=</span> <span class="n">fwd_out</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTION_DIST_INPUTS</span><span class="p">]</span>
                    <span class="n">action_dist_class</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_exploration_action_dist_cls</span><span class="p">()</span>
                    <span class="n">action_dist</span> <span class="o">=</span> <span class="n">action_dist_class</span><span class="o">.</span><span class="n">from_logits</span><span class="p">(</span><span class="n">dist_inputs</span><span class="p">)</span>

                <span class="c1"># If `forward_exploration()` returned actions, use them here as-is.</span>
                <span class="k">if</span> <span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTIONS</span> <span class="ow">in</span> <span class="n">fwd_out</span><span class="p">:</span>
                    <span class="n">actions</span> <span class="o">=</span> <span class="n">fwd_out</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">]</span>
                <span class="c1"># Otherwise, sample actions from the distribution.</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">action_dist</span>
                    <span class="n">actions</span> <span class="o">=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

                <span class="c1"># Compute action-logp and action-prob from distribution and add to</span>
                <span class="c1"># `extra_fetches`, if possible.</span>
                <span class="k">if</span> <span class="n">action_dist</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">logp</span> <span class="o">=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">fwd_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward_inference</span><span class="p">(</span><span class="n">input_dict</span><span class="p">)</span>

                <span class="c1"># ACTION_DIST_INPUTS field returned by `forward_exploration()` -&gt;</span>
                <span class="c1"># Create a distribution object.</span>
                <span class="n">action_dist</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">if</span> <span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTION_DIST_INPUTS</span> <span class="ow">in</span> <span class="n">fwd_out</span><span class="p">:</span>
                    <span class="n">dist_inputs</span> <span class="o">=</span> <span class="n">fwd_out</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTION_DIST_INPUTS</span><span class="p">]</span>
                    <span class="n">action_dist_class</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_inference_action_dist_cls</span><span class="p">()</span>
                    <span class="n">action_dist</span> <span class="o">=</span> <span class="n">action_dist_class</span><span class="o">.</span><span class="n">from_logits</span><span class="p">(</span><span class="n">dist_inputs</span><span class="p">)</span>
                    <span class="n">action_dist</span> <span class="o">=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">to_deterministic</span><span class="p">()</span>

                <span class="c1"># If `forward_inference()` returned actions, use them here as-is.</span>
                <span class="k">if</span> <span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTIONS</span> <span class="ow">in</span> <span class="n">fwd_out</span><span class="p">:</span>
                    <span class="n">actions</span> <span class="o">=</span> <span class="n">fwd_out</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">]</span>
                <span class="c1"># Otherwise, sample actions from the distribution.</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">action_dist</span>
                    <span class="n">actions</span> <span class="o">=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

            <span class="c1"># Anything but actions and state_out is an extra fetch.</span>
            <span class="n">state_out</span> <span class="o">=</span> <span class="n">fwd_out</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">STATE_OUT</span><span class="p">,</span> <span class="p">{})</span>
            <span class="n">extra_fetches</span> <span class="o">=</span> <span class="n">fwd_out</span>

        <span class="k">elif</span> <span class="n">is_overridden</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_sampler_fn</span><span class="p">):</span>
            <span class="n">action_dist</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">actions</span><span class="p">,</span> <span class="n">logp</span><span class="p">,</span> <span class="n">dist_inputs</span><span class="p">,</span> <span class="n">state_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_sampler_fn</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
                <span class="n">obs_batch</span><span class="o">=</span><span class="n">input_dict</span><span class="p">,</span>
                <span class="n">state_batches</span><span class="o">=</span><span class="n">state_batches</span><span class="p">,</span>
                <span class="n">explore</span><span class="o">=</span><span class="n">explore</span><span class="p">,</span>
                <span class="n">timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Call the exploration before_compute_actions hook.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span><span class="o">.</span><span class="n">before_compute_actions</span><span class="p">(</span><span class="n">explore</span><span class="o">=</span><span class="n">explore</span><span class="p">,</span> <span class="n">timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_overridden</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_distribution_fn</span><span class="p">):</span>
                <span class="n">dist_inputs</span><span class="p">,</span> <span class="n">dist_class</span><span class="p">,</span> <span class="n">state_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_distribution_fn</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">obs_batch</span><span class="o">=</span><span class="n">input_dict</span><span class="p">,</span>
                    <span class="n">state_batches</span><span class="o">=</span><span class="n">state_batches</span><span class="p">,</span>
                    <span class="n">seq_lens</span><span class="o">=</span><span class="n">seq_lens</span><span class="p">,</span>
                    <span class="n">explore</span><span class="o">=</span><span class="n">explore</span><span class="p">,</span>
                    <span class="n">timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">,</span>
                    <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dist_class</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_class</span>
                <span class="n">dist_inputs</span><span class="p">,</span> <span class="n">state_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="n">state_batches</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">dist_class</span><span class="p">,</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">)</span>
                <span class="ow">or</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">dist_class</span><span class="p">,</span> <span class="n">TorchDistributionWrapper</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`dist_class` (</span><span class="si">{}</span><span class="s2">) not a TorchDistributionWrapper &quot;</span>
                    <span class="s2">&quot;subclass! Make sure your `action_distribution_fn` or &quot;</span>
                    <span class="s2">&quot;`make_model_and_action_dist` return a correct &quot;</span>
                    <span class="s2">&quot;distribution class.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dist_class</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="n">action_dist</span> <span class="o">=</span> <span class="n">dist_class</span><span class="p">(</span><span class="n">dist_inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>

            <span class="c1"># Get the exploration action from the forward results.</span>
            <span class="n">actions</span><span class="p">,</span> <span class="n">logp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span><span class="o">.</span><span class="n">get_exploration_action</span><span class="p">(</span>
                <span class="n">action_distribution</span><span class="o">=</span><span class="n">action_dist</span><span class="p">,</span> <span class="n">timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">,</span> <span class="n">explore</span><span class="o">=</span><span class="n">explore</span>
            <span class="p">)</span>

        <span class="c1"># Add default and custom fetches.</span>
        <span class="k">if</span> <span class="n">extra_fetches</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">extra_fetches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">extra_action_out</span><span class="p">(</span>
                <span class="n">input_dict</span><span class="p">,</span> <span class="n">state_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">action_dist</span>
            <span class="p">)</span>

        <span class="c1"># Action-dist inputs.</span>
        <span class="k">if</span> <span class="n">dist_inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">extra_fetches</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTION_DIST_INPUTS</span><span class="p">]</span> <span class="o">=</span> <span class="n">dist_inputs</span>

        <span class="c1"># Action-logp and action-prob.</span>
        <span class="k">if</span> <span class="n">logp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">extra_fetches</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTION_PROB</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logp</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
            <span class="n">extra_fetches</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTION_LOGP</span><span class="p">]</span> <span class="o">=</span> <span class="n">logp</span>

        <span class="c1"># Update our global timestep by the batch size.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_timestep</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_dict</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">CUR_OBS</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">convert_to_numpy</span><span class="p">((</span><span class="n">actions</span><span class="p">,</span> <span class="n">state_out</span><span class="p">,</span> <span class="n">extra_fetches</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_lazy_tensor_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">postprocessed_batch</span><span class="p">:</span> <span class="n">SampleBatch</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">postprocessed_batch</span><span class="p">,</span> <span class="n">SampleBatch</span><span class="p">):</span>
            <span class="n">postprocessed_batch</span> <span class="o">=</span> <span class="n">SampleBatch</span><span class="p">(</span><span class="n">postprocessed_batch</span><span class="p">)</span>
        <span class="n">postprocessed_batch</span><span class="o">.</span><span class="n">set_get_interceptor</span><span class="p">(</span>
            <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">convert_to_torch_tensor</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">postprocessed_batch</span>

    <span class="k">def</span> <span class="nf">_multi_gpu_parallel_grad_calc</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">sample_batches</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">SampleBatch</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TensorType</span><span class="p">],</span> <span class="n">GradInfoDict</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Performs a parallelized loss and gradient calculation over the batch.</span>

<span class="sd">        Splits up the given train batch into n shards (n=number of this</span>
<span class="sd">        Policy&#39;s devices) and passes each data shard (in parallel) through</span>
<span class="sd">        the loss function using the individual devices&#39; models</span>
<span class="sd">        (self.model_gpu_towers). Then returns each tower&#39;s outputs.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample_batches: A list of SampleBatch shards to</span>
<span class="sd">                calculate loss and gradients for.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list (one item per device) of 2-tuples, each with 1) gradient</span>
<span class="sd">            list and 2) grad info dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_batches</span><span class="p">)</span>
        <span class="n">lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">grad_enabled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">_worker</span><span class="p">(</span><span class="n">shard_idx</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">sample_batch</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">grad_enabled</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">NullContextManager</span><span class="p">()</span> <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span>  <span class="c1"># noqa: E501</span>
                    <span class="n">device</span>
                <span class="p">):</span>
                    <span class="n">loss_out</span> <span class="o">=</span> <span class="n">force_list</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_class</span><span class="p">,</span> <span class="n">sample_batch</span><span class="p">)</span>
                    <span class="p">)</span>

                    <span class="c1"># Call Model&#39;s custom-loss with Policy loss outputs and</span>
                    <span class="c1"># train_batch.</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;custom_loss&quot;</span><span class="p">):</span>
                        <span class="n">loss_out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">custom_loss</span><span class="p">(</span><span class="n">loss_out</span><span class="p">,</span> <span class="n">sample_batch</span><span class="p">)</span>

                    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_out</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizers</span><span class="p">)</span>

                    <span class="c1"># Loop through all optimizers.</span>
                    <span class="n">grad_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;allreduce_latency&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">}</span>

                    <span class="n">parameters</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
                    <span class="n">all_grads</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">))]</span>
                    <span class="k">for</span> <span class="n">opt_idx</span><span class="p">,</span> <span class="n">opt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizers</span><span class="p">):</span>
                        <span class="c1"># Erase gradients in all vars of the tower that this</span>
                        <span class="c1"># optimizer would affect.</span>
                        <span class="n">param_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_gpu_param_groups</span><span class="p">[</span><span class="n">opt_idx</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">param_idx</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
                            <span class="k">if</span> <span class="n">param_idx</span> <span class="ow">in</span> <span class="n">param_indices</span> <span class="ow">and</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                                <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
                        <span class="c1"># Recompute gradients of loss over all variables.</span>
                        <span class="n">loss_out</span><span class="p">[</span><span class="n">opt_idx</span><span class="p">]</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                        <span class="n">grad_info</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">extra_grad_process</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">loss_out</span><span class="p">[</span><span class="n">opt_idx</span><span class="p">])</span>
                        <span class="p">)</span>

                        <span class="n">grads</span> <span class="o">=</span> <span class="p">[]</span>
                        <span class="c1"># Note that return values are just references;</span>
                        <span class="c1"># Calling zero_grad would modify the values.</span>
                        <span class="k">for</span> <span class="n">param_idx</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
                            <span class="k">if</span> <span class="n">param_idx</span> <span class="ow">in</span> <span class="n">param_indices</span><span class="p">:</span>
                                <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                                    <span class="n">grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
                                <span class="n">all_grads</span><span class="p">[</span><span class="n">param_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span>

                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">distributed_world_size</span><span class="p">:</span>
                            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
                            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                                <span class="c1"># Sadly, allreduce_coalesced does not work with</span>
                                <span class="c1"># CUDA yet.</span>
                                <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">:</span>
                                    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span>
                                        <span class="n">g</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span>
                                    <span class="p">)</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce_coalesced</span><span class="p">(</span>
                                    <span class="n">grads</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span>
                                <span class="p">)</span>

                            <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                                <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                                    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                                        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distributed_world_size</span>

                            <span class="n">grad_info</span><span class="p">[</span><span class="s2">&quot;allreduce_latency&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

                <span class="k">with</span> <span class="n">lock</span><span class="p">:</span>
                    <span class="n">results</span><span class="p">[</span><span class="n">shard_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">all_grads</span><span class="p">,</span> <span class="n">grad_info</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="kn">import</span> <span class="nn">traceback</span>

                <span class="k">with</span> <span class="n">lock</span><span class="p">:</span>
                    <span class="n">results</span><span class="p">[</span><span class="n">shard_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="n">e</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                            <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> traceback&quot;</span>
                            <span class="o">+</span> <span class="n">traceback</span><span class="o">.</span><span class="n">format_exc</span><span class="p">()</span>
                            <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
                            <span class="o">+</span> <span class="s2">&quot;In tower </span><span class="si">{}</span><span class="s2"> on device </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">shard_idx</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
                        <span class="p">),</span>
                        <span class="n">e</span><span class="p">,</span>
                    <span class="p">)</span>

        <span class="c1"># Single device (GPU) or fake-GPU case (serialize for better</span>
        <span class="c1"># debugging).</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;_fake_gpus&quot;</span><span class="p">]:</span>
            <span class="k">for</span> <span class="n">shard_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sample_batch</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span><span class="p">,</span> <span class="n">sample_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">_worker</span><span class="p">(</span><span class="n">shard_idx</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">sample_batch</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
                <span class="c1"># Raise errors right away for better debugging.</span>
                <span class="n">last_result</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">last_result</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="ne">ValueError</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="n">last_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="kn">from</span> <span class="nn">last_result</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Multi device (GPU) case: Parallelize via threads.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">threads</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span>
                    <span class="n">target</span><span class="o">=</span><span class="n">_worker</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">shard_idx</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">sample_batch</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">shard_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sample_batch</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                    <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gpu_towers</span><span class="p">,</span> <span class="n">sample_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="p">]</span>

            <span class="k">for</span> <span class="n">thread</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
                <span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">thread</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
                <span class="n">thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>

        <span class="c1"># Gather all threads&#39; outputs and return.</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">shard_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample_batches</span><span class="p">)):</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="n">shard_idx</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="ne">Exception</span><span class="p">):</span>
                <span class="k">raise</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="kn">from</span> <span class="nn">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="n">shard_idx</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">outputs</span></div>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>