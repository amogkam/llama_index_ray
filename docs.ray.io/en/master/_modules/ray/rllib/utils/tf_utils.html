
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ray.rllib.utils.tf_utils &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../../../../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script src="../../../../_static/js/versionwarning.js"></script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../../../_static/js/docsearch.js"></script>
    <script src="../../../../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../../../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../../../_static/js/custom.js"></script>
    <script defer="defer" src="../../../../_static/js/top-navigation.js"></script>
    <script src="../../../../_static/js/tags.js"></script>
    <script src="../../../../_static/tabs.js"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../../_static/design-tabs.js"></script>
    <script async="async" src="../../../../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/_modules/ray/rllib/utils/tf_utils.html" />
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "_modules/ray/rllib/utils/tf_utils", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2F_modules/ray/rllib/utils/tf_utils.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for ray.rllib.utils.tf_utils</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>
<span class="kn">from</span> <span class="nn">gymnasium.spaces</span> <span class="kn">import</span> <span class="n">Discrete</span><span class="p">,</span> <span class="n">MultiDiscrete</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tree</span>  <span class="c1"># pip install dm_tree</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">from</span> <span class="nn">ray.rllib.utils.annotations</span> <span class="kn">import</span> <span class="n">PublicAPI</span><span class="p">,</span> <span class="n">DeveloperAPI</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.framework</span> <span class="kn">import</span> <span class="n">try_import_tf</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.spaces.space_utils</span> <span class="kn">import</span> <span class="n">get_base_struct_from_space</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">LocalOptimizer</span><span class="p">,</span>
    <span class="n">ModelGradients</span><span class="p">,</span>
    <span class="n">PartialAlgorithmConfigDict</span><span class="p">,</span>
    <span class="n">SpaceStruct</span><span class="p">,</span>
    <span class="n">TensorStructType</span><span class="p">,</span>
    <span class="n">TensorType</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>
    <span class="kn">from</span> <span class="nn">ray.rllib.core.learner.learner</span> <span class="kn">import</span> <span class="n">ParamDict</span>
    <span class="kn">from</span> <span class="nn">ray.rllib.policy.eager_tf_policy</span> <span class="kn">import</span> <span class="n">EagerTFPolicy</span>
    <span class="kn">from</span> <span class="nn">ray.rllib.policy.eager_tf_policy_v2</span> <span class="kn">import</span> <span class="n">EagerTFPolicyV2</span>
    <span class="kn">from</span> <span class="nn">ray.rllib.policy.tf_policy</span> <span class="kn">import</span> <span class="n">TFPolicy</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">tf1</span><span class="p">,</span> <span class="n">tf</span><span class="p">,</span> <span class="n">tfv</span> <span class="o">=</span> <span class="n">try_import_tf</span><span class="p">()</span>


<span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">clip_gradients</span><span class="p">(</span>
    <span class="n">gradients_dict</span><span class="p">:</span> <span class="s2">&quot;ParamDict&quot;</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">grad_clip</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">grad_clip_by</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Performs gradient clipping on a grad-dict based on a clip value and clip mode.</span>

<span class="sd">    Changes the provided gradient dict in place.</span>

<span class="sd">    Args:</span>
<span class="sd">        gradients_dict: The gradients dict, mapping str to gradient tensors.</span>
<span class="sd">        grad_clip: The value to clip with. The way gradients are clipped is defined</span>
<span class="sd">            by the `grad_clip_by` arg (see below).</span>
<span class="sd">        grad_clip_by: One of &#39;value&#39;, &#39;norm&#39;, or &#39;global_norm&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        If `grad_clip_by`=&quot;global_norm&quot; and `grad_clip` is not None, returns the global</span>
<span class="sd">        norm of all tensors, otherwise returns None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># No clipping, return.</span>
    <span class="k">if</span> <span class="n">grad_clip</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Clip by value (each gradient individually).</span>
    <span class="k">if</span> <span class="n">grad_clip_by</span> <span class="o">==</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">gradients_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">gradients_dict</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="o">-</span><span class="n">grad_clip</span><span class="p">,</span> <span class="n">grad_clip</span><span class="p">)</span>

    <span class="c1"># Clip by L2-norm (per gradient tensor).</span>
    <span class="k">elif</span> <span class="n">grad_clip_by</span> <span class="o">==</span> <span class="s2">&quot;norm&quot;</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">gradients_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">gradients_dict</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_norm</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">grad_clip</span><span class="p">)</span>

    <span class="c1"># Clip by global L2-norm (across all gradient tensors).</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">grad_clip_by</span> <span class="o">==</span> <span class="s2">&quot;global_norm&quot;</span>

        <span class="n">clipped_grads</span><span class="p">,</span> <span class="n">global_norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_global_norm</span><span class="p">(</span>
            <span class="nb">list</span><span class="p">(</span><span class="n">gradients_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="n">grad_clip</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">gradients_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">clipped_grads</span><span class="p">):</span>
            <span class="n">gradients_dict</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

        <span class="c1"># Return the computed global norm scalar.</span>
        <span class="k">return</span> <span class="n">global_norm</span>


<div class="viewcode-block" id="explained_variance"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.explained_variance.html#ray.rllib.utils.tf_utils.explained_variance">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">explained_variance</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span> <span class="n">pred</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Computes the explained variance for a pair of labels and predictions.</span>

<span class="sd">    The formula used is:</span>
<span class="sd">    max(-1.0, 1.0 - (std(y - pred)^2 / std(y)^2))</span>

<span class="sd">    Args:</span>
<span class="sd">        y: The labels.</span>
<span class="sd">        pred: The predictions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The explained variance given a pair of labels and predictions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">y_var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">moments</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">diff_var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">moments</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">pred</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">diff_var</span> <span class="o">/</span> <span class="n">y_var</span><span class="p">))</span></div>


<div class="viewcode-block" id="flatten_inputs_to_1d_tensor"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.flatten_inputs_to_1d_tensor.html#ray.rllib.utils.tf_utils.flatten_inputs_to_1d_tensor">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">flatten_inputs_to_1d_tensor</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">TensorStructType</span><span class="p">,</span>
    <span class="n">spaces_struct</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SpaceStruct</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">time_axis</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Flattens arbitrary input structs according to the given spaces struct.</span>

<span class="sd">    Returns a single 1D tensor resulting from the different input</span>
<span class="sd">    components&#39; values.</span>

<span class="sd">    Thereby:</span>
<span class="sd">    - Boxes (any shape) get flattened to (B, [T]?, -1). Note that image boxes</span>
<span class="sd">    are not treated differently from other types of Boxes and get</span>
<span class="sd">    flattened as well.</span>
<span class="sd">    - Discrete (int) values are one-hot&#39;d, e.g. a batch of [1, 0, 3] (B=3 with</span>
<span class="sd">    Discrete(4) space) results in [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1]].</span>
<span class="sd">    - MultiDiscrete values are multi-one-hot&#39;d, e.g. a batch of</span>
<span class="sd">    [[0, 2], [1, 4]] (B=2 with MultiDiscrete([2, 5]) space) results in</span>
<span class="sd">    [[1, 0,  0, 0, 1, 0, 0], [0, 1,  0, 0, 0, 0, 1]].</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs: The inputs to be flattened.</span>
<span class="sd">        spaces_struct: The structure of the spaces that behind the input</span>
<span class="sd">        time_axis: Whether all inputs have a time-axis (after the batch axis).</span>
<span class="sd">            If True, will keep not only the batch axis (0th), but the time axis</span>
<span class="sd">            (1st) as-is and flatten everything from the 2nd axis up.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A single 1D tensor resulting from concatenating all</span>
<span class="sd">        flattened/one-hot&#39;d input components. Depending on the time_axis flag,</span>
<span class="sd">        the shape is (B, n) or (B, T, n).</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # B=2</span>
<span class="sd">        &gt;&gt;&gt; from ray.rllib.utils.tf_utils import flatten_inputs_to_1d_tensor</span>
<span class="sd">        &gt;&gt;&gt; from gymnasium.spaces import Discrete, Box</span>
<span class="sd">        &gt;&gt;&gt; out = flatten_inputs_to_1d_tensor( # doctest: +SKIP</span>
<span class="sd">        ...     {&quot;a&quot;: [1, 0], &quot;b&quot;: [[[0.0], [0.1]], [1.0], [1.1]]},</span>
<span class="sd">        ...     spaces_struct=dict(a=Discrete(2), b=Box(shape=(2, 1)))</span>
<span class="sd">        ... ) # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; print(out) # doctest: +SKIP</span>
<span class="sd">        [[0.0, 1.0,  0.0, 0.1], [1.0, 0.0,  1.0, 1.1]]  # B=2 n=4</span>

<span class="sd">        &gt;&gt;&gt; # B=2; T=2</span>
<span class="sd">        &gt;&gt;&gt; out = flatten_inputs_to_1d_tensor( # doctest: +SKIP</span>
<span class="sd">        ...     ([[1, 0], [0, 1]],</span>
<span class="sd">        ...      [[[0.0, 0.1], [1.0, 1.1]], [[2.0, 2.1], [3.0, 3.1]]]),</span>
<span class="sd">        ...     spaces_struct=tuple([Discrete(2), Box(shape=(2, ))]),</span>
<span class="sd">        ...     time_axis=True</span>
<span class="sd">        ... ) # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; print(out) # doctest: +SKIP</span>
<span class="sd">        [[[0.0, 1.0, 0.0, 0.1], [1.0, 0.0, 1.0, 1.1]],\</span>
<span class="sd">        [[1.0, 0.0, 2.0, 2.1], [0.0, 1.0, 3.0, 3.1]]]  # B=2 T=2 n=4</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">flat_inputs</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">flat_spaces</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">tree</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">spaces_struct</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">spaces_struct</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_inputs</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">B</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">T</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">input_</span><span class="p">,</span> <span class="n">space</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_inputs</span><span class="p">,</span> <span class="n">flat_spaces</span><span class="p">):</span>
        <span class="n">input_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span>
        <span class="c1"># Store batch and (if applicable) time dimension.</span>
        <span class="k">if</span> <span class="n">B</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">B</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">time_axis</span><span class="p">:</span>
                <span class="n">T</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># One-hot encoding.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">Discrete</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">time_axis</span><span class="p">:</span>
                <span class="n">input_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="p">[</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">])</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">one_hot</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">space</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">MultiDiscrete</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">time_axis</span><span class="p">:</span>
                <span class="n">input_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="p">[</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">one_hot</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">space</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="c1"># Flatten.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">time_axis</span><span class="p">:</span>
                <span class="n">input_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="p">[</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

    <span class="n">merged</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Restore the time-dimension, if applicable.</span>
    <span class="k">if</span> <span class="n">time_axis</span><span class="p">:</span>
        <span class="n">merged</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">merged</span><span class="p">,</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">merged</span></div>


<div class="viewcode-block" id="get_gpu_devices"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.get_gpu_devices.html#ray.rllib.utils.tf_utils.get_gpu_devices">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">get_gpu_devices</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Returns a list of GPU device names, e.g. [&quot;/gpu:0&quot;, &quot;/gpu:1&quot;].</span>

<span class="sd">    Supports both tf1.x and tf2.x.</span>

<span class="sd">    Returns:</span>
<span class="sd">        List of GPU device names (str).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">tfv</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">tensorflow.python.client</span> <span class="kn">import</span> <span class="n">device_lib</span>

        <span class="n">devices</span> <span class="o">=</span> <span class="n">device_lib</span><span class="o">.</span><span class="n">list_local_devices</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">devices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="n">devices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">()</span>

    <span class="c1"># Expect &quot;GPU&quot;, but also stuff like: &quot;XLA_GPU&quot;.</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">d</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">devices</span> <span class="k">if</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">device_type</span><span class="p">]</span></div>


<div class="viewcode-block" id="get_placeholder"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.get_placeholder.html#ray.rllib.utils.tf_utils.get_placeholder">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">get_placeholder</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">space</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">gym</span><span class="o">.</span><span class="n">Space</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">time_axis</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">flatten</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;tf1.placeholder&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Returns a tf1.placeholder object given optional hints, such as a space.</span>

<span class="sd">    Note that the returned placeholder will always have a leading batch</span>
<span class="sd">    dimension (None).</span>

<span class="sd">    Args:</span>
<span class="sd">        space: An optional gym.Space to hint the shape and dtype of the</span>
<span class="sd">            placeholder.</span>
<span class="sd">        value: An optional value to hint the shape and dtype of the</span>
<span class="sd">            placeholder.</span>
<span class="sd">        name: An optional name for the placeholder.</span>
<span class="sd">        time_axis: Whether the placeholder should also receive a time</span>
<span class="sd">            dimension (None).</span>
<span class="sd">        flatten: Whether to flatten the given space into a plain Box space</span>
<span class="sd">            and then create the placeholder from the resulting space.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The tf1 placeholder.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">ray.rllib.models.catalog</span> <span class="kn">import</span> <span class="n">ModelCatalog</span>

    <span class="k">if</span> <span class="n">space</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Dict</span><span class="p">,</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Tuple</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">flatten</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">ModelCatalog</span><span class="o">.</span><span class="n">get_action_placeholder</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_structure_with_path</span><span class="p">(</span>
                    <span class="k">lambda</span> <span class="n">path</span><span class="p">,</span> <span class="n">component</span><span class="p">:</span> <span class="n">get_placeholder</span><span class="p">(</span>
                        <span class="n">space</span><span class="o">=</span><span class="n">component</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">path</span><span class="p">]),</span>
                    <span class="p">),</span>
                    <span class="n">get_base_struct_from_space</span><span class="p">(</span><span class="n">space</span><span class="p">),</span>
                <span class="p">)</span>
        <span class="k">return</span> <span class="n">tf1</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,)</span> <span class="o">+</span> <span class="p">((</span><span class="kc">None</span><span class="p">,)</span> <span class="k">if</span> <span class="n">time_axis</span> <span class="k">else</span> <span class="p">())</span> <span class="o">+</span> <span class="n">space</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span> <span class="k">if</span> <span class="n">space</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="k">else</span> <span class="n">space</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="k">return</span> <span class="n">tf1</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,)</span>
            <span class="o">+</span> <span class="p">((</span><span class="kc">None</span><span class="p">,)</span> <span class="k">if</span> <span class="n">time_axis</span> <span class="k">else</span> <span class="p">())</span>
            <span class="o">+</span> <span class="p">(</span><span class="n">shape</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="k">else</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">())),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span> <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="k">else</span> <span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="p">)</span></div>


<span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">get_tf_eager_cls_if_necessary</span><span class="p">(</span>
    <span class="n">orig_cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="s2">&quot;TFPolicy&quot;</span><span class="p">],</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">,</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Type</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="s2">&quot;TFPolicy&quot;</span><span class="p">,</span> <span class="s2">&quot;EagerTFPolicy&quot;</span><span class="p">,</span> <span class="s2">&quot;EagerTFPolicyV2&quot;</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Returns the corresponding tf-eager class for a given TFPolicy class.</span>

<span class="sd">    Args:</span>
<span class="sd">        orig_cls: The original TFPolicy class to get the corresponding tf-eager</span>
<span class="sd">            class for.</span>
<span class="sd">        config: The Algorithm config dict or AlgorithmConfig object.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The tf eager policy class corresponding to the given TFPolicy class.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">cls</span> <span class="o">=</span> <span class="n">orig_cls</span>
    <span class="n">framework</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;framework&quot;</span><span class="p">,</span> <span class="s2">&quot;tf&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">framework</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;tf2&quot;</span><span class="p">,</span> <span class="s2">&quot;tf&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">tf1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Could not import tensorflow!&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">framework</span> <span class="o">==</span> <span class="s2">&quot;tf2&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">tf1</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
            <span class="n">tf1</span><span class="o">.</span><span class="n">enable_eager_execution</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">tf1</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>

        <span class="kn">from</span> <span class="nn">ray.rllib.policy.tf_policy</span> <span class="kn">import</span> <span class="n">TFPolicy</span>
        <span class="kn">from</span> <span class="nn">ray.rllib.policy.eager_tf_policy</span> <span class="kn">import</span> <span class="n">EagerTFPolicy</span>
        <span class="kn">from</span> <span class="nn">ray.rllib.policy.eager_tf_policy_v2</span> <span class="kn">import</span> <span class="n">EagerTFPolicyV2</span>

        <span class="c1"># Create eager-class (if not already one).</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">orig_cls</span><span class="p">,</span> <span class="s2">&quot;as_eager&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">orig_cls</span><span class="p">,</span> <span class="n">EagerTFPolicy</span><span class="p">):</span>
            <span class="bp">cls</span> <span class="o">=</span> <span class="n">orig_cls</span><span class="o">.</span><span class="n">as_eager</span><span class="p">()</span>
        <span class="c1"># Could be some other type of policy or already</span>
        <span class="c1"># eager-ized.</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">orig_cls</span><span class="p">,</span> <span class="n">TFPolicy</span><span class="p">):</span>
            <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;This policy does not support eager execution: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">orig_cls</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Now that we know, policy is an eager one, add tracing, if necessary.</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eager_tracing&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span>
            <span class="bp">cls</span><span class="p">,</span> <span class="p">(</span><span class="n">EagerTFPolicy</span><span class="p">,</span> <span class="n">EagerTFPolicyV2</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="bp">cls</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">with_tracing</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">cls</span>


<div class="viewcode-block" id="huber_loss"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.huber_loss.html#ray.rllib.utils.tf_utils.huber_loss">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">huber_loss</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span> <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Computes the huber loss for a given term and delta parameter.</span>

<span class="sd">    Reference: https://en.wikipedia.org/wiki/Huber_loss</span>
<span class="sd">    Note that the factor of 0.5 is implicitly included in the calculation.</span>

<span class="sd">    Formula:</span>
<span class="sd">        L = 0.5 * x^2  for small abs x (delta threshold)</span>
<span class="sd">        L = delta * (abs(x) - 0.5*delta)  for larger abs x (delta threshold)</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The input term, e.g. a TD error.</span>
<span class="sd">        delta: The delta parmameter in the above formula.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The Huber loss resulting from `x` and `delta`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">delta</span><span class="p">,</span>  <span class="c1"># for small x -&gt; apply the Huber correction</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="n">delta</span> <span class="o">*</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">delta</span><span class="p">),</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="l2_loss"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.l2_loss.html#ray.rllib.utils.tf_utils.l2_loss">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">l2_loss</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Computes half the L2 norm over a tensor&#39;s values without the sqrt.</span>

<span class="sd">    output = 0.5 * sum(x ** 2)</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        0.5 times the L2 norm over the given tensor&#39;s values (w/o sqrt).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">))</span></div>


<div class="viewcode-block" id="make_tf_callable"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.make_tf_callable.html#ray.rllib.utils.tf_utils.make_tf_callable">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">make_tf_callable</span><span class="p">(</span>
    <span class="n">session_or_none</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;tf1.Session&quot;</span><span class="p">],</span> <span class="n">dynamic_shape</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Returns a function that can be executed in either graph or eager mode.</span>

<span class="sd">    The function must take only positional args.</span>

<span class="sd">    If eager is enabled, this will act as just a function. Otherwise, it</span>
<span class="sd">    will build a function that executes a session run with placeholders</span>
<span class="sd">    internally.</span>

<span class="sd">    Args:</span>
<span class="sd">        session_or_none: tf.Session if in graph mode, else None.</span>
<span class="sd">        dynamic_shape: True if the placeholders should have a dynamic</span>
<span class="sd">            batch dimension. Otherwise they will be fixed shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A function that can be called in either eager or static-graph mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
        <span class="k">assert</span> <span class="n">session_or_none</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">session_or_none</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">make_wrapper</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
        <span class="c1"># Static-graph mode: Create placeholders and make a session call each</span>
        <span class="c1"># time the wrapped function is called. Returns the output of this</span>
        <span class="c1"># session call.</span>
        <span class="k">if</span> <span class="n">session_or_none</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">args_placeholders</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">kwargs_placeholders</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="n">symbolic_out</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>

            <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                <span class="n">args_flat</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">list</span><span class="p">:</span>
                        <span class="n">args_flat</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">args_flat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
                <span class="n">args</span> <span class="o">=</span> <span class="n">args_flat</span>

                <span class="c1"># We have not built any placeholders yet: Do this once here,</span>
                <span class="c1"># then reuse the same placeholders each time we call this</span>
                <span class="c1"># function again.</span>
                <span class="k">if</span> <span class="n">symbolic_out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">with</span> <span class="n">session_or_none</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>

                        <span class="k">def</span> <span class="nf">_create_placeholders</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
                            <span class="k">if</span> <span class="n">dynamic_shape</span><span class="p">:</span>
                                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                                    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,)</span> <span class="o">+</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
                                <span class="k">else</span><span class="p">:</span>
                                    <span class="n">shape</span> <span class="o">=</span> <span class="p">()</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="n">shape</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span>
                            <span class="k">return</span> <span class="n">tf1</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span>
                                <span class="n">dtype</span><span class="o">=</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                                <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
                                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">path</span><span class="p">]),</span>
                            <span class="p">)</span>

                        <span class="n">placeholders</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_structure_with_path</span><span class="p">(</span>
                            <span class="n">_create_placeholders</span><span class="p">,</span> <span class="n">args</span>
                        <span class="p">)</span>
                        <span class="k">for</span> <span class="n">ph</span> <span class="ow">in</span> <span class="n">tree</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">placeholders</span><span class="p">):</span>
                            <span class="n">args_placeholders</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ph</span><span class="p">)</span>

                        <span class="n">placeholders</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_structure_with_path</span><span class="p">(</span>
                            <span class="n">_create_placeholders</span><span class="p">,</span> <span class="n">kwargs</span>
                        <span class="p">)</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">ph</span> <span class="ow">in</span> <span class="n">placeholders</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                            <span class="n">kwargs_placeholders</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">ph</span>

                        <span class="n">symbolic_out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args_placeholders</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_placeholders</span><span class="p">)</span>
                <span class="n">feed_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">args_placeholders</span><span class="p">,</span> <span class="n">tree</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">args</span><span class="p">)))</span>
                <span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
                    <span class="k">lambda</span> <span class="n">ph</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">feed_dict</span><span class="o">.</span><span class="fm">__setitem__</span><span class="p">(</span><span class="n">ph</span><span class="p">,</span> <span class="n">v</span><span class="p">),</span>
                    <span class="n">kwargs_placeholders</span><span class="p">,</span>
                    <span class="n">kwargs</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="n">session_or_none</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">symbolic_out</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">feed_dict</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">ret</span>

            <span class="k">return</span> <span class="n">call</span>
        <span class="c1"># Eager mode (call function as is).</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">fn</span>

    <span class="k">return</span> <span class="n">make_wrapper</span></div>


<span class="c1"># TODO (sven): Deprecate this function once we have moved completely to the Learner API.</span>
<span class="c1">#  Replaced with `clip_gradients()`.</span>
<div class="viewcode-block" id="minimize_and_clip"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.minimize_and_clip.html#ray.rllib.utils.tf_utils.minimize_and_clip">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">minimize_and_clip</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">LocalOptimizer</span><span class="p">,</span>
    <span class="n">objective</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span>
    <span class="n">var_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;tf.Variable&quot;</span><span class="p">],</span>
    <span class="n">clip_val</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10.0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModelGradients</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Computes, then clips gradients using objective, optimizer and var list.</span>

<span class="sd">    Ensures the norm of the gradients for each variable is clipped to</span>
<span class="sd">    `clip_val`.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer: Either a shim optimizer (tf eager) containing a</span>
<span class="sd">            tf.GradientTape under `self.tape` or a tf1 local optimizer</span>
<span class="sd">            object.</span>
<span class="sd">        objective: The loss tensor to calculate gradients on.</span>
<span class="sd">        var_list: The list of tf.Variables to compute gradients over.</span>
<span class="sd">        clip_val: The global norm clip value. Will clip around -clip_val and</span>
<span class="sd">            +clip_val.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The resulting model gradients (list or tuples of grads + vars)</span>
<span class="sd">        corresponding to the input `var_list`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Accidentally passing values &lt; 0.0 will break all gradients.</span>
    <span class="k">assert</span> <span class="n">clip_val</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">clip_val</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">clip_val</span>

    <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
        <span class="n">tape</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">tape</span>
        <span class="n">grads_and_vars</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">var_list</span><span class="p">)),</span> <span class="n">var_list</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">grads_and_vars</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">var_list</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">clip_by_norm</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">clip_val</span><span class="p">)</span> <span class="k">if</span> <span class="n">clip_val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">g</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="ow">in</span> <span class="n">grads_and_vars</span>
        <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">]</span></div>


<div class="viewcode-block" id="one_hot"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.one_hot.html#ray.rllib.utils.tf_utils.one_hot">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span> <span class="n">space</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Returns a one-hot tensor, given and int tensor and a space.</span>

<span class="sd">    Handles the MultiDiscrete case as well.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The input tensor.</span>
<span class="sd">        space: The space to use for generating the one-hot tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The resulting one-hot tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the given space is not a discrete one.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import gymnasium as gym</span>
<span class="sd">        &gt;&gt;&gt; import tensorflow as tf</span>
<span class="sd">        &gt;&gt;&gt; from ray.rllib.utils.tf_utils import one_hot</span>
<span class="sd">        &gt;&gt;&gt; x = tf.Variable([0, 3], dtype=tf.int32)  # batch-dim=2</span>
<span class="sd">        &gt;&gt;&gt; # Discrete space with 4 (one-hot) slots per batch item.</span>
<span class="sd">        &gt;&gt;&gt; s = gym.spaces.Discrete(4)</span>
<span class="sd">        &gt;&gt;&gt; one_hot(x, s) # doctest: +SKIP</span>
<span class="sd">        &lt;tf.Tensor &#39;one_hot:0&#39; shape=(2, 4) dtype=float32&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = tf.Variable([[0, 1, 2, 3]], dtype=tf.int32)  # batch-dim=1</span>
<span class="sd">        &gt;&gt;&gt; # MultiDiscrete space with 5 + 4 + 4 + 7 = 20 (one-hot) slots</span>
<span class="sd">        &gt;&gt;&gt; # per batch item.</span>
<span class="sd">        &gt;&gt;&gt; s = gym.spaces.MultiDiscrete([5, 4, 4, 7])</span>
<span class="sd">        &gt;&gt;&gt; one_hot(x, s) # doctest: +SKIP</span>
<span class="sd">        &lt;tf.Tensor &#39;concat:0&#39; shape=(1, 20) dtype=float32&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">Discrete</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">MultiDiscrete</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">space</span><span class="o">.</span><span class="n">nvec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">nvec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">space</span><span class="o">.</span><span class="n">nvec</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">nvec</span> <span class="o">=</span> <span class="n">space</span><span class="o">.</span><span class="n">nvec</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">nvec</span><span class="p">)],</span>
            <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unsupported space for `one_hot`: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">space</span><span class="p">))</span></div>


<div class="viewcode-block" id="reduce_mean_ignore_inf"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.reduce_mean_ignore_inf.html#ray.rllib.utils.tf_utils.reduce_mean_ignore_inf">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">reduce_mean_ignore_inf</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Same as tf.reduce_mean() but ignores -inf values.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The input tensor to reduce mean over.</span>
<span class="sd">        axis: The axis over which to reduce. None for all axes.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The mean reduced inputs, ignoring inf values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
    <span class="n">x_zeroed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">x_zeroed</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">axis</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="scope_vars"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.scope_vars.html#ray.rllib.utils.tf_utils.scope_vars">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">scope_vars</span><span class="p">(</span>
    <span class="n">scope</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;tf1.VariableScope&quot;</span><span class="p">],</span> <span class="n">trainable_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;tf.Variable&quot;</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Get variables inside a given scope.</span>

<span class="sd">    Args:</span>
<span class="sd">        scope: Scope in which the variables reside.</span>
<span class="sd">        trainable_only: Whether or not to return only the variables that were</span>
<span class="sd">            marked as trainable.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The list of variables in the given `scope`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tf1</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span>
        <span class="n">tf1</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span>
        <span class="k">if</span> <span class="n">trainable_only</span>
        <span class="k">else</span> <span class="n">tf1</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">VARIABLES</span><span class="p">,</span>
        <span class="n">scope</span><span class="o">=</span><span class="n">scope</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">scope</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
    <span class="p">)</span></div>


<span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">symlog</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="s2">&quot;tf.Tensor&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;tf.Tensor&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;The symlog function as described in [1]:</span>

<span class="sd">    [1] Mastering Diverse Domains through World Models - 2023</span>
<span class="sd">    D. Hafner, J. Pasukonis, J. Ba, T. Lillicrap</span>
<span class="sd">    https://arxiv.org/pdf/2301.04104v1.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>


<span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">inverse_symlog</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="s2">&quot;tf.Tensor&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;tf.Tensor&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Inverse of the `symlog` function as desribed in [1]:</span>

<span class="sd">    [1] Mastering Diverse Domains through World Models - 2023</span>
<span class="sd">    D. Hafner, J. Pasukonis, J. Ba, T. Lillicrap</span>
<span class="sd">    https://arxiv.org/pdf/2301.04104v1.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># To get to symlog inverse, we solve the symlog equation for x:</span>
    <span class="c1">#     y = sign(x) * log(|x| + 1)</span>
    <span class="c1"># &lt;=&gt; y / sign(x) = log(|x| + 1)</span>
    <span class="c1"># &lt;=&gt; y =  log( x + 1) V x &gt;= 0</span>
    <span class="c1">#    -y =  log(-x + 1) V x &lt;  0</span>
    <span class="c1"># &lt;=&gt; exp(y)  =  x + 1  V x &gt;= 0</span>
    <span class="c1">#     exp(-y) = -x + 1  V x &lt;  0</span>
    <span class="c1"># &lt;=&gt; exp(y)  - 1 =  x   V x &gt;= 0</span>
    <span class="c1">#     exp(-y) - 1 = -x   V x &lt;  0</span>
    <span class="c1"># &lt;=&gt;  exp(y)  - 1 = x   V x &gt;= 0 (if x &gt;= 0, then y must also be &gt;= 0)</span>
    <span class="c1">#     -exp(-y) - 1 = x   V x &lt;  0 (if x &lt; 0, then y must also be &lt; 0)</span>
    <span class="c1"># &lt;=&gt; sign(y) * (exp(|y|) - 1) = x</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>


<span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">two_hot</span><span class="p">(</span>
    <span class="n">value</span><span class="p">:</span> <span class="s2">&quot;tf.Tensor&quot;</span><span class="p">,</span>
    <span class="n">num_buckets</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">255</span><span class="p">,</span>
    <span class="n">lower_bound</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="o">-</span><span class="mf">20.0</span><span class="p">,</span>
    <span class="n">upper_bound</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">20.0</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a two-hot vector of dim=num_buckets with two entries that are non-zero.</span>

<span class="sd">    See [1] for more details:</span>
<span class="sd">    [1] Mastering Diverse Domains through World Models - 2023</span>
<span class="sd">    D. Hafner, J. Pasukonis, J. Ba, T. Lillicrap</span>
<span class="sd">    https://arxiv.org/pdf/2301.04104v1.pdf</span>

<span class="sd">    Entries in the vector represent equally sized buckets within some fixed range</span>
<span class="sd">    (`lower_bound` to `upper_bound`).</span>
<span class="sd">    Those entries not 0.0 at positions k and k+1 encode the actual `value` and sum</span>
<span class="sd">    up to 1.0. They are the weights multiplied by the buckets values at k and k+1 for</span>
<span class="sd">    retrieving `value`.</span>

<span class="sd">    Example:</span>
<span class="sd">        num_buckets=11</span>
<span class="sd">        lower_bound=-5</span>
<span class="sd">        upper_bound=5</span>
<span class="sd">        value=2.5</span>
<span class="sd">        -&gt; [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0]</span>
<span class="sd">        -&gt; [-5   -4   -3   -2   -1   0    1    2    3    4    5] (0.5*2 + 0.5*3=2.5)</span>

<span class="sd">    Example:</span>
<span class="sd">        num_buckets=5</span>
<span class="sd">        lower_bound=-1</span>
<span class="sd">        upper_bound=1</span>
<span class="sd">        value=0.1</span>
<span class="sd">        -&gt; [0.0, 0.0, 0.8, 0.2, 0.0]</span>
<span class="sd">        -&gt; [-1  -0.5   0   0.5   1] (0.2*0.5 + 0.8*0=0.1)</span>

<span class="sd">    Args:</span>
<span class="sd">        value: The input tensor of shape (B,) to be two-hot encoded.</span>
<span class="sd">        num_buckets: The number of buckets to two-hot encode into.</span>
<span class="sd">        lower_bound: The lower bound value used for the encoding. If input values are</span>
<span class="sd">            lower than this boundary, they will be encoded as `lower_bound`.</span>
<span class="sd">        upper_bound: The upper bound value used for the encoding. If input values are</span>
<span class="sd">            higher than this boundary, they will be encoded as `upper_bound`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The two-hot encoded tensor of shape (B, num_buckets).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># First make sure, values are clipped.</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">lower_bound</span><span class="p">,</span> <span class="n">upper_bound</span><span class="p">)</span>
    <span class="c1"># Tensor of batch indices: [0, B=batch size).</span>
    <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># Calculate the step deltas (how much space between each bucket&#39;s central value?).</span>
    <span class="n">bucket_delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">upper_bound</span> <span class="o">-</span> <span class="n">lower_bound</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_buckets</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Compute the float indices (might be non-int numbers: sitting between two buckets).</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">lower_bound</span> <span class="o">+</span> <span class="n">value</span><span class="p">)</span> <span class="o">/</span> <span class="n">bucket_delta</span>
    <span class="c1"># k</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
    <span class="c1"># k+1</span>
    <span class="n">kp1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
    <span class="c1"># In case k == kp1 (idx is exactly on the bucket boundary), move kp1 up by 1.0.</span>
    <span class="c1"># Otherwise, this would result in a NaN in the returned two-hot tensor.</span>
    <span class="n">kp1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">k</span> <span class="o">==</span> <span class="n">kp1</span><span class="p">,</span> <span class="n">kp1</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">kp1</span><span class="p">)</span>
    <span class="c1"># Iff `kp1` is one beyond our last index (because incoming value is larger than</span>
    <span class="c1"># `upper_bound`), move it to one before k (kp1&#39;s weight is going to be 0.0 anyways,</span>
    <span class="c1"># so it doesn&#39;t matter where it points to; we are just avoiding an index error</span>
    <span class="c1"># with this).</span>
    <span class="n">kp1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">kp1</span> <span class="o">==</span> <span class="n">num_buckets</span><span class="p">,</span> <span class="n">kp1</span> <span class="o">-</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">kp1</span><span class="p">)</span>
    <span class="c1"># The actual values found at k and k+1 inside the set of buckets.</span>
    <span class="n">values_k</span> <span class="o">=</span> <span class="n">lower_bound</span> <span class="o">+</span> <span class="n">k</span> <span class="o">*</span> <span class="n">bucket_delta</span>
    <span class="n">values_kp1</span> <span class="o">=</span> <span class="n">lower_bound</span> <span class="o">+</span> <span class="n">kp1</span> <span class="o">*</span> <span class="n">bucket_delta</span>
    <span class="c1"># Compute the two-hot weights (adding up to 1.0) to use at index k and k+1.</span>
    <span class="n">weights_k</span> <span class="o">=</span> <span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="n">values_kp1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">values_k</span> <span class="o">-</span> <span class="n">values_kp1</span><span class="p">)</span>
    <span class="n">weights_kp1</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">weights_k</span>
    <span class="c1"># Compile a tensor of full paths (indices from batch index to feature index) to</span>
    <span class="c1"># use for the scatter_nd op.</span>
    <span class="n">indices_k</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">batch_indices</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">indices_kp1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">batch_indices</span><span class="p">,</span> <span class="n">kp1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">indices_k</span><span class="p">,</span> <span class="n">indices_kp1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># The actual values (weights adding up to 1.0) to place at the computed indices.</span>
    <span class="n">updates</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">weights_k</span><span class="p">,</span> <span class="n">weights_kp1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># Call the actual scatter update op, returning a zero-filled tensor, only changed</span>
    <span class="c1"># at the given indices.</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">scatter_nd</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
        <span class="n">updates</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_buckets</span><span class="p">),</span>
    <span class="p">)</span>


<div class="viewcode-block" id="zero_logps_from_actions"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.zero_logps_from_actions.html#ray.rllib.utils.tf_utils.zero_logps_from_actions">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">zero_logps_from_actions</span><span class="p">(</span><span class="n">actions</span><span class="p">:</span> <span class="n">TensorStructType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Helper function useful for returning dummy logp&#39;s (0) for some actions.</span>

<span class="sd">    Args:</span>
<span class="sd">        actions: The input actions. This can be any struct</span>
<span class="sd">            of complex action components or a simple tensor of different</span>
<span class="sd">            dimensions, e.g. [B], [B, 2], or {&quot;a&quot;: [B, 4, 5], &quot;b&quot;: [B]}.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 1D tensor of 0.0 (dummy logp&#39;s) matching the batch</span>
<span class="sd">        dim of `actions` (shape=[B]).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Need to flatten `actions` in case we have a complex action space.</span>
    <span class="c1"># Take the 0th component to extract the batch dim.</span>
    <span class="n">action_component</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">actions</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">logp_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">action_component</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># Logp&#39;s should be single values (but with the same batch dim as</span>
    <span class="c1"># `deterministic_actions` or `stochastic_actions`). In case</span>
    <span class="c1"># actions are just [B], zeros_like works just fine here, but if</span>
    <span class="c1"># actions are [B, ...], we have to reduce logp back to just [B].</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">logp_</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">logp_</span> <span class="o">=</span> <span class="n">logp_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">logp_</span></div>


<div class="viewcode-block" id="warn_if_infinite_kl_divergence"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.warn_if_infinite_kl_divergence.html#ray.rllib.utils.tf_utils.warn_if_infinite_kl_divergence">[docs]</a><span class="nd">@DeveloperAPI</span>
<span class="k">def</span> <span class="nf">warn_if_infinite_kl_divergence</span><span class="p">(</span>
    <span class="n">policy</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="s2">&quot;TFPolicy&quot;</span><span class="p">],</span> <span class="n">mean_kl</span><span class="p">:</span> <span class="n">TensorType</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">print_warning</span><span class="p">():</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;KL divergence is non-finite, this will likely destabilize your model and&quot;</span>
            <span class="s2">&quot; the training process. Action(s) in a specific state have near-zero&quot;</span>
            <span class="s2">&quot; probability. This can happen naturally in deterministic environments&quot;</span>
            <span class="s2">&quot; where the optimal policy has zero mass for a specific action. To fix this&quot;</span>
            <span class="s2">&quot; issue, consider setting the coefficient for the KL loss term to zero or&quot;</span>
            <span class="s2">&quot; increasing policy entropy.&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">policy</span><span class="o">.</span><span class="n">loss_initialized</span><span class="p">():</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">is_inf</span><span class="p">(</span><span class="n">mean_kl</span><span class="p">),</span>
            <span class="n">false_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">true_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">print_warning</span><span class="p">(),</span>
        <span class="p">)</span></div>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>