
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ray.rllib.utils.torch_utils &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../../../../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script src="../../../../_static/js/versionwarning.js"></script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../../../_static/js/docsearch.js"></script>
    <script src="../../../../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../../../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../../../_static/js/custom.js"></script>
    <script defer="defer" src="../../../../_static/js/top-navigation.js"></script>
    <script src="../../../../_static/js/tags.js"></script>
    <script src="../../../../_static/tabs.js"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../../_static/design-tabs.js"></script>
    <script async="async" src="../../../../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/_modules/ray/rllib/utils/torch_utils.html" />
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "_modules/ray/rllib/utils/torch_utils", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2F_modules/ray/rllib/utils/torch_utils.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for ray.rllib.utils.torch_utils</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tree</span>  <span class="c1"># pip install dm_tree</span>
<span class="kn">from</span> <span class="nn">gymnasium.spaces</span> <span class="kn">import</span> <span class="n">Discrete</span><span class="p">,</span> <span class="n">MultiDiscrete</span>
<span class="kn">from</span> <span class="nn">packaging</span> <span class="kn">import</span> <span class="n">version</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray.rllib.models.repeated_values</span> <span class="kn">import</span> <span class="n">RepeatedValues</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.annotations</span> <span class="kn">import</span> <span class="n">Deprecated</span><span class="p">,</span> <span class="n">PublicAPI</span><span class="p">,</span> <span class="n">DeveloperAPI</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.framework</span> <span class="kn">import</span> <span class="n">try_import_torch</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">LocalOptimizer</span><span class="p">,</span>
    <span class="n">SpaceStruct</span><span class="p">,</span>
    <span class="n">TensorStructType</span><span class="p">,</span>
    <span class="n">TensorType</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">ray.rllib.core.learner.learner</span> <span class="kn">import</span> <span class="n">ParamDict</span>
    <span class="kn">from</span> <span class="nn">ray.rllib.policy.torch_policy</span> <span class="kn">import</span> <span class="n">TorchPolicy</span>
    <span class="kn">from</span> <span class="nn">ray.rllib.policy.torch_policy_v2</span> <span class="kn">import</span> <span class="n">TorchPolicyV2</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">torch</span><span class="p">,</span> <span class="n">nn</span> <span class="o">=</span> <span class="n">try_import_torch</span><span class="p">()</span>

<span class="c1"># Limit values suitable for use as close to a -inf logit. These are useful</span>
<span class="c1"># since -inf / inf cause NaNs during backprop.</span>
<span class="n">FLOAT_MIN</span> <span class="o">=</span> <span class="o">-</span><span class="mf">3.4e38</span>
<span class="n">FLOAT_MAX</span> <span class="o">=</span> <span class="mf">3.4e38</span>

<span class="k">if</span> <span class="n">torch</span><span class="p">:</span>
    <span class="n">TORCH_COMPILE_REQUIRED_VERSION</span> <span class="o">=</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s2">&quot;2.0.0&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">TORCH_COMPILE_REQUIRED_VERSION</span> <span class="o">=</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;torch is not installed. &quot;</span> <span class="s2">&quot;TORCH_COMPILE_REQUIRED_VERSION is &quot;</span> <span class="s2">&quot;not defined.&quot;</span>
    <span class="p">)</span>


<span class="c1"># TODO (sven): Deprecate this function once we have moved completely to the Learner API.</span>
<span class="c1">#  Replaced with `clip_gradients()`.</span>
<div class="viewcode-block" id="apply_grad_clipping"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.apply_grad_clipping.html#ray.rllib.utils.torch_utils.apply_grad_clipping">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">apply_grad_clipping</span><span class="p">(</span>
    <span class="n">policy</span><span class="p">:</span> <span class="s2">&quot;TorchPolicy&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">LocalOptimizer</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">TensorType</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Applies gradient clipping to already computed grads inside `optimizer`.</span>

<span class="sd">    Note: This function does NOT perform an analogous operation as</span>
<span class="sd">    tf.clip_by_global_norm. It merely clips by norm (per gradient tensor) and</span>
<span class="sd">    then computes the global norm across all given tensors (but without clipping</span>
<span class="sd">    by that global norm).</span>

<span class="sd">    Args:</span>
<span class="sd">        policy: The TorchPolicy, which calculated `loss`.</span>
<span class="sd">        optimizer: A local torch optimizer object.</span>
<span class="sd">        loss: The torch loss tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        An info dict containing the &quot;grad_norm&quot; key and the resulting clipped</span>
<span class="sd">        gradients.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">grad_gnorm</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">policy</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;grad_clip&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">clip_value</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;grad_clip&quot;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">clip_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="n">num_none_grads</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="c1"># Make sure we only pass params with grad != None into torch</span>
        <span class="c1"># clip_grad_norm_. Would fail otherwise.</span>
        <span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]))</span>
        <span class="k">if</span> <span class="n">params</span><span class="p">:</span>
            <span class="c1"># PyTorch clips gradients inplace and returns the norm before clipping</span>
            <span class="c1"># We therefore need to compute grad_gnorm further down (fixes #4965)</span>
            <span class="n">global_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">clip_value</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">global_norm</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">global_norm</span> <span class="o">=</span> <span class="n">global_norm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

            <span class="n">grad_gnorm</span> <span class="o">+=</span> <span class="nb">min</span><span class="p">(</span><span class="n">global_norm</span><span class="p">,</span> <span class="n">clip_value</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">num_none_grads</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Note (Kourosh): grads could indeed be zero. This method should still return</span>
    <span class="c1"># grad_gnorm in that case.</span>
    <span class="k">if</span> <span class="n">num_none_grads</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
        <span class="c1"># No grads available</span>
        <span class="k">return</span> <span class="p">{}</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;grad_gnorm&quot;</span><span class="p">:</span> <span class="n">grad_gnorm</span><span class="p">}</span></div>


<span class="nd">@Deprecated</span><span class="p">(</span><span class="n">old</span><span class="o">=</span><span class="s2">&quot;ray.rllib.utils.torch_utils.atanh&quot;</span><span class="p">,</span> <span class="n">new</span><span class="o">=</span><span class="s2">&quot;torch.math.atanh&quot;</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">atanh</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="k">pass</span>


<span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">clip_gradients</span><span class="p">(</span>
    <span class="n">gradients_dict</span><span class="p">:</span> <span class="s2">&quot;ParamDict&quot;</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">grad_clip</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">grad_clip_by</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;value&quot;</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Performs gradient clipping on a grad-dict based on a clip value and clip mode.</span>

<span class="sd">    Changes the provided gradient dict in place.</span>

<span class="sd">    Args:</span>
<span class="sd">        gradients_dict: The gradients dict, mapping str to gradient tensors.</span>
<span class="sd">        grad_clip: The value to clip with. The way gradients are clipped is defined</span>
<span class="sd">            by the `grad_clip_by` arg (see below).</span>
<span class="sd">        grad_clip_by: One of &#39;value&#39;, &#39;norm&#39;, or &#39;global_norm&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        If `grad_clip_by`=&quot;global_norm&quot; and `grad_clip` is not None, returns the global</span>
<span class="sd">        norm of all tensors, otherwise returns None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># No clipping, return.</span>
    <span class="k">if</span> <span class="n">grad_clip</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Clip by value (each gradient individually).</span>
    <span class="k">if</span> <span class="n">grad_clip_by</span> <span class="o">==</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">gradients_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">gradients_dict</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="kc">None</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="o">-</span><span class="n">grad_clip</span><span class="p">,</span> <span class="n">grad_clip</span><span class="p">)</span>
            <span class="p">)</span>

    <span class="c1"># Clip by L2-norm (per gradient tensor).</span>
    <span class="k">elif</span> <span class="n">grad_clip_by</span> <span class="o">==</span> <span class="s2">&quot;norm&quot;</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">gradients_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Compute the L2-norm of the gradient tensor.</span>
                <span class="n">norm</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
                <span class="c1"># Clip all the gradients.</span>
                <span class="k">if</span> <span class="n">norm</span> <span class="o">&gt;</span> <span class="n">grad_clip</span><span class="p">:</span>
                    <span class="n">v</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">grad_clip</span> <span class="o">/</span> <span class="n">norm</span><span class="p">)</span>

    <span class="c1"># Clip by global L2-norm (across all gradient tensors).</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">grad_clip_by</span> <span class="o">==</span> <span class="s2">&quot;global_norm&quot;</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;`grad_clip_by` (</span><span class="si">{</span><span class="n">grad_clip_by</span><span class="si">}</span><span class="s2">) must be one of [value|norm|global_norm]!&quot;</span>

        <span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">g</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">gradients_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">norm_type</span> <span class="o">=</span> <span class="mf">2.0</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span>

        <span class="n">total_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">norm_type</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">]),</span>
            <span class="n">norm_type</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">total_norm</span><span class="o">.</span><span class="n">isnan</span><span class="p">(),</span> <span class="n">total_norm</span><span class="o">.</span><span class="n">isinf</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The total norm of order </span><span class="si">{</span><span class="n">norm_type</span><span class="si">}</span><span class="s2"> for gradients from &quot;</span>
                <span class="s2">&quot;`parameters` is non-finite, so it cannot be clipped. &quot;</span>
            <span class="p">)</span>
        <span class="n">clip_coef</span> <span class="o">=</span> <span class="n">grad_clip</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_norm</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
        <span class="c1"># Note: multiplying by the clamped coef is redundant when the coef is clamped to</span>
        <span class="c1"># 1, but doing so avoids a `if clip_coef &lt; 1:` conditional which can require a</span>
        <span class="c1"># CPU &lt;=&gt; device synchronization when the gradients do not reside in CPU memory.</span>
        <span class="n">clip_coef_clamped</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">clip_coef</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">:</span>
            <span class="n">g</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">clip_coef_clamped</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">total_norm</span>


<div class="viewcode-block" id="concat_multi_gpu_td_errors"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.concat_multi_gpu_td_errors.html#ray.rllib.utils.torch_utils.concat_multi_gpu_td_errors">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">concat_multi_gpu_td_errors</span><span class="p">(</span>
    <span class="n">policy</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;TorchPolicy&quot;</span><span class="p">,</span> <span class="s2">&quot;TorchPolicyV2&quot;</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Concatenates multi-GPU (per-tower) TD error tensors given TorchPolicy.</span>

<span class="sd">    TD-errors are extracted from the TorchPolicy via its tower_stats property.</span>

<span class="sd">    Args:</span>
<span class="sd">        policy: The TorchPolicy to extract the TD-error values from.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A dict mapping strings &quot;td_error&quot; and &quot;mean_td_error&quot; to the</span>
<span class="sd">        corresponding concatenated and mean-reduced values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">td_error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">t</span><span class="o">.</span><span class="n">tower_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;td_error&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">policy</span><span class="o">.</span><span class="n">model_gpu_towers</span>
        <span class="p">],</span>
        <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">policy</span><span class="o">.</span><span class="n">td_error</span> <span class="o">=</span> <span class="n">td_error</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;td_error&quot;</span><span class="p">:</span> <span class="n">td_error</span><span class="p">,</span>
        <span class="s2">&quot;mean_td_error&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">td_error</span><span class="p">),</span>
    <span class="p">}</span></div>


<span class="nd">@Deprecated</span><span class="p">(</span><span class="n">new</span><span class="o">=</span><span class="s2">&quot;ray/rllib/utils/numpy.py::convert_to_numpy&quot;</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">convert_to_non_torch_type</span><span class="p">(</span><span class="n">stats</span><span class="p">:</span> <span class="n">TensorStructType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorStructType</span><span class="p">:</span>
    <span class="k">pass</span>


<div class="viewcode-block" id="convert_to_torch_tensor"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.convert_to_torch_tensor.html#ray.rllib.utils.torch_utils.convert_to_torch_tensor">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">convert_to_torch_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">TensorStructType</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts any struct to torch.Tensors.</span>

<span class="sd">    x: Any (possibly nested) struct, the values in which will be</span>
<span class="sd">        converted and returned as a new struct with all leaves converted</span>
<span class="sd">        to torch tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Any: A new struct with the same structure as `x`, but with all</span>
<span class="sd">            values converted to torch Tensor types. This does not convert possibly</span>
<span class="sd">            nested elements that are None because torch has no representation for that.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">mapping</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">item</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Torch has no representation for `None`, so we return None</span>
            <span class="k">return</span> <span class="n">item</span>

        <span class="c1"># Special handling of &quot;Repeated&quot; values.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">RepeatedValues</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">RepeatedValues</span><span class="p">(</span>
                <span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">mapping</span><span class="p">,</span> <span class="n">item</span><span class="o">.</span><span class="n">values</span><span class="p">),</span> <span class="n">item</span><span class="o">.</span><span class="n">lengths</span><span class="p">,</span> <span class="n">item</span><span class="o">.</span><span class="n">max_len</span>
            <span class="p">)</span>

        <span class="c1"># Already torch tensor -&gt; make sure it&#39;s on right device.</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">item</span>
        <span class="c1"># Numpy arrays.</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="c1"># Object type (e.g. info dicts in train batch): leave as-is.</span>
            <span class="c1"># str type (e.g. agent_id in train batch): leave as-is.</span>
            <span class="k">if</span> <span class="n">item</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="nb">object</span> <span class="ow">or</span> <span class="n">item</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">type</span> <span class="ow">is</span> <span class="n">np</span><span class="o">.</span><span class="n">str_</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">item</span>
            <span class="c1"># Non-writable numpy-arrays will cause PyTorch warning.</span>
            <span class="k">elif</span> <span class="n">item</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">writeable</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
                    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
            <span class="c1"># Already numpy: Wrap as torch tensor.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="c1"># Everything else: Convert to numpy, then wrap as torch tensor.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">item</span><span class="p">))</span>

        <span class="c1"># Floatify all float64 tensors.</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">():</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">tensor</span> <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">mapping</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></div>


<span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">copy_torch_tensors</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">TensorStructType</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a copy of `x` and makes deep copies torch.Tensors in x.</span>

<span class="sd">    Also moves the copied tensors to the specified device (if not None).</span>

<span class="sd">    Note if an object in x is not a torch.Tensor, it will be shallow-copied.</span>

<span class="sd">    Args:</span>
<span class="sd">        x : Any (possibly nested) struct possibly containing torch.Tensors.</span>
<span class="sd">        device : The device to move the tensors to.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Any: A new struct with the same structure as `x`, but with all</span>
<span class="sd">            torch.Tensors deep-copied and moved to the specified device.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">mapping</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
                <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="n">item</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">item</span>

    <span class="k">return</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">mapping</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="explained_variance"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.explained_variance.html#ray.rllib.utils.torch_utils.explained_variance">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">explained_variance</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span> <span class="n">pred</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Computes the explained variance for a pair of labels and predictions.</span>

<span class="sd">    The formula used is:</span>
<span class="sd">    max(-1.0, 1.0 - (std(y - pred)^2 / std(y)^2))</span>

<span class="sd">    Args:</span>
<span class="sd">        y: The labels.</span>
<span class="sd">        pred: The predictions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The explained variance given a pair of labels and predictions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">y_var</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="c1"># Model case in which y does not vary with explained variance of -1</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">diff_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">min_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">min_</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">diff_var</span> <span class="o">/</span> <span class="n">y_var</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="flatten_inputs_to_1d_tensor"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.flatten_inputs_to_1d_tensor.html#ray.rllib.utils.torch_utils.flatten_inputs_to_1d_tensor">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">flatten_inputs_to_1d_tensor</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">TensorStructType</span><span class="p">,</span>
    <span class="n">spaces_struct</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SpaceStruct</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">time_axis</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Flattens arbitrary input structs according to the given spaces struct.</span>

<span class="sd">    Returns a single 1D tensor resulting from the different input</span>
<span class="sd">    components&#39; values.</span>

<span class="sd">    Thereby:</span>
<span class="sd">    - Boxes (any shape) get flattened to (B, [T]?, -1). Note that image boxes</span>
<span class="sd">    are not treated differently from other types of Boxes and get</span>
<span class="sd">    flattened as well.</span>
<span class="sd">    - Discrete (int) values are one-hot&#39;d, e.g. a batch of [1, 0, 3] (B=3 with</span>
<span class="sd">    Discrete(4) space) results in [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1]].</span>
<span class="sd">    - MultiDiscrete values are multi-one-hot&#39;d, e.g. a batch of</span>
<span class="sd">    [[0, 2], [1, 4]] (B=2 with MultiDiscrete([2, 5]) space) results in</span>
<span class="sd">    [[1, 0,  0, 0, 1, 0, 0], [0, 1,  0, 0, 0, 0, 1]].</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs: The inputs to be flattened.</span>
<span class="sd">        spaces_struct: The structure of the spaces that behind the input</span>
<span class="sd">        time_axis: Whether all inputs have a time-axis (after the batch axis).</span>
<span class="sd">            If True, will keep not only the batch axis (0th), but the time axis</span>
<span class="sd">            (1st) as-is and flatten everything from the 2nd axis up.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A single 1D tensor resulting from concatenating all</span>
<span class="sd">        flattened/one-hot&#39;d input components. Depending on the time_axis flag,</span>
<span class="sd">        the shape is (B, n) or (B, T, n).</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # B=2</span>
<span class="sd">        &gt;&gt;&gt; from ray.rllib.utils.tf_utils import flatten_inputs_to_1d_tensor</span>
<span class="sd">        &gt;&gt;&gt; from gymnasium.spaces import Discrete, Box</span>
<span class="sd">        &gt;&gt;&gt; out = flatten_inputs_to_1d_tensor( # doctest: +SKIP</span>
<span class="sd">        ...     {&quot;a&quot;: [1, 0], &quot;b&quot;: [[[0.0], [0.1]], [1.0], [1.1]]},</span>
<span class="sd">        ...     spaces_struct=dict(a=Discrete(2), b=Box(shape=(2, 1))))</span>
<span class="sd">        ... ) # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; print(out) # doctest: +SKIP</span>
<span class="sd">        [[0.0, 1.0,  0.0, 0.1], [1.0, 0.0,  1.0, 1.1]]  # B=2 n=4</span>

<span class="sd">        &gt;&gt;&gt; # B=2; T=2</span>
<span class="sd">        &gt;&gt;&gt; out = flatten_inputs_to_1d_tensor( # doctest: +SKIP</span>
<span class="sd">        ...     ([[1, 0], [0, 1]],</span>
<span class="sd">        ...      [[[0.0, 0.1], [1.0, 1.1]], [[2.0, 2.1], [3.0, 3.1]]]),</span>
<span class="sd">        ...     spaces_struct=tuple([Discrete(2), Box(shape=(2, ))]),</span>
<span class="sd">        ...     time_axis=True</span>
<span class="sd">        ... ) # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; print(out) # doctest: +SKIP</span>
<span class="sd">        [[[0.0, 1.0, 0.0, 0.1], [1.0, 0.0, 1.0, 1.1]],\</span>
<span class="sd">        [[1.0, 0.0, 2.0, 2.1], [0.0, 1.0, 3.0, 3.1]]]  # B=2 T=2 n=4</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">flat_inputs</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">flat_spaces</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">tree</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">spaces_struct</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">spaces_struct</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_inputs</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">B</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">T</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">input_</span><span class="p">,</span> <span class="n">space</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_inputs</span><span class="p">,</span> <span class="n">flat_spaces</span><span class="p">):</span>
        <span class="c1"># Store batch and (if applicable) time dimension.</span>
        <span class="k">if</span> <span class="n">B</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">B</span> <span class="o">=</span> <span class="n">input_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">time_axis</span><span class="p">:</span>
                <span class="n">T</span> <span class="o">=</span> <span class="n">input_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># One-hot encoding.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">Discrete</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">time_axis</span><span class="p">:</span>
                <span class="n">input_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="p">[</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">])</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">one_hot</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">space</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
        <span class="c1"># Multi one-hot encoding.</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">MultiDiscrete</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">time_axis</span><span class="p">:</span>
                <span class="n">input_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="p">[</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">one_hot</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">space</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
        <span class="c1"># Box: Flatten.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">time_axis</span><span class="p">:</span>
                <span class="n">input_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="p">[</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>

    <span class="n">merged</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Restore the time-dimension, if applicable.</span>
    <span class="k">if</span> <span class="n">time_axis</span><span class="p">:</span>
        <span class="n">merged</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">merged</span><span class="p">,</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">merged</span></div>


<div class="viewcode-block" id="get_device"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.get_device.html#ray.rllib.utils.torch_utils.get_device">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">get_device</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a torch device edepending on a config and current worker index.&quot;&quot;&quot;</span>

    <span class="c1"># Figure out the number of GPUs to use on the local side (index=0) or on</span>
    <span class="c1"># the remote workers (index &gt; 0).</span>
    <span class="n">worker_idx</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;worker_index&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="ow">not</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;_fake_gpus&quot;</span><span class="p">]</span>
        <span class="ow">and</span> <span class="n">ray</span><span class="o">.</span><span class="n">_private</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">_mode</span><span class="p">()</span> <span class="o">==</span> <span class="n">ray</span><span class="o">.</span><span class="n">_private</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">LOCAL_MODE</span>
    <span class="p">):</span>
        <span class="n">num_gpus</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">worker_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">num_gpus</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_gpus&quot;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">num_gpus</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_gpus_per_worker&quot;</span><span class="p">]</span>
    <span class="c1"># All GPU IDs, if any.</span>
    <span class="n">gpu_ids</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()))</span>

    <span class="c1"># Place on one or more CPU(s) when either:</span>
    <span class="c1"># - Fake GPU mode.</span>
    <span class="c1"># - num_gpus=0 (either set by user or we are in local_mode=True).</span>
    <span class="c1"># - No GPUs available.</span>
    <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;_fake_gpus&quot;</span><span class="p">]</span> <span class="ow">or</span> <span class="n">num_gpus</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">gpu_ids</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="c1"># Place on one or more actual GPU(s), when:</span>
    <span class="c1"># - num_gpus &gt; 0 (set by user) AND</span>
    <span class="c1"># - local_mode=False AND</span>
    <span class="c1"># - actual GPUs available AND</span>
    <span class="c1"># - non-fake GPU mode.</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># We are a remote worker (WORKER_MODE=1):</span>
        <span class="c1"># GPUs should be assigned to us by ray.</span>
        <span class="k">if</span> <span class="n">ray</span><span class="o">.</span><span class="n">_private</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">_mode</span><span class="p">()</span> <span class="o">==</span> <span class="n">ray</span><span class="o">.</span><span class="n">_private</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">WORKER_MODE</span><span class="p">:</span>
            <span class="n">gpu_ids</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get_gpu_ids</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">gpu_ids</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">num_gpus</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;TorchPolicy was not able to find enough GPU IDs! Found &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">gpu_ids</span><span class="si">}</span><span class="s2">, but num_gpus=</span><span class="si">{</span><span class="n">num_gpus</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="global_norm"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.global_norm.html#ray.rllib.utils.torch_utils.global_norm">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">global_norm</span><span class="p">(</span><span class="n">tensors</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">TensorType</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Returns the global L2 norm over a list of tensors.</span>

<span class="sd">    output = sqrt(SUM(t ** 2 for t in tensors)),</span>
<span class="sd">        where SUM reduces over all tensors and over all elements in tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors: The list of tensors to calculate the global norm over.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The global L2 norm over the given tensor list.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># List of single tensors&#39; L2 norms: SQRT(SUM(xi^2)) over all xi in tensor.</span>
    <span class="n">single_l2s</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)),</span> <span class="mf">0.5</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">]</span>
    <span class="c1"># Compute global norm from all single tensors&#39; L2 norms.</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">l2</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span> <span class="k">for</span> <span class="n">l2</span> <span class="ow">in</span> <span class="n">single_l2s</span><span class="p">),</span> <span class="mf">0.5</span><span class="p">)</span></div>


<div class="viewcode-block" id="huber_loss"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.huber_loss.html#ray.rllib.utils.torch_utils.huber_loss">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">huber_loss</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span> <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Computes the huber loss for a given term and delta parameter.</span>

<span class="sd">    Reference: https://en.wikipedia.org/wiki/Huber_loss</span>
<span class="sd">    Note that the factor of 0.5 is implicitly included in the calculation.</span>

<span class="sd">    Formula:</span>
<span class="sd">        L = 0.5 * x^2  for small abs x (delta threshold)</span>
<span class="sd">        L = delta * (abs(x) - 0.5*delta)  for larger abs x (delta threshold)</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The input term, e.g. a TD error.</span>
<span class="sd">        delta: The delta parmameter in the above formula.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The Huber loss resulting from `x` and `delta`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">delta</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="n">delta</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">delta</span><span class="p">),</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="l2_loss"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.l2_loss.html#ray.rllib.utils.torch_utils.l2_loss">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">l2_loss</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Computes half the L2 norm over a tensor&#39;s values without the sqrt.</span>

<span class="sd">    output = 0.5 * sum(x ** 2)</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        0.5 times the L2 norm over the given tensor&#39;s values (w/o sqrt).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">))</span></div>


<div class="viewcode-block" id="minimize_and_clip"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.minimize_and_clip.html#ray.rllib.utils.torch_utils.minimize_and_clip">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">minimize_and_clip</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="s2">&quot;torch.optim.Optimizer&quot;</span><span class="p">,</span> <span class="n">clip_val</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10.0</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Clips grads found in `optimizer.param_groups` to given value in place.</span>

<span class="sd">    Ensures the norm of the gradients for each variable is clipped to</span>
<span class="sd">    `clip_val`.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer: The torch.optim.Optimizer to get the variables from.</span>
<span class="sd">        clip_val: The global norm clip value. Will clip around -clip_val and</span>
<span class="sd">            +clip_val.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Loop through optimizer&#39;s variables and norm per variable.</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">clip_val</span><span class="p">)</span></div>


<div class="viewcode-block" id="one_hot"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.one_hot.html#ray.rllib.utils.torch_utils.one_hot">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span> <span class="n">space</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Returns a one-hot tensor, given and int tensor and a space.</span>

<span class="sd">    Handles the MultiDiscrete case as well.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The input tensor.</span>
<span class="sd">        space: The space to use for generating the one-hot tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The resulting one-hot tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the given space is not a discrete one.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; import gymnasium as gym</span>
<span class="sd">        &gt;&gt;&gt; from ray.rllib.utils.torch_utils import one_hot</span>
<span class="sd">        &gt;&gt;&gt; x = torch.IntTensor([0, 3])  # batch-dim=2</span>
<span class="sd">        &gt;&gt;&gt; # Discrete space with 4 (one-hot) slots per batch item.</span>
<span class="sd">        &gt;&gt;&gt; s = gym.spaces.Discrete(4)</span>
<span class="sd">        &gt;&gt;&gt; one_hot(x, s) # doctest: +SKIP</span>
<span class="sd">        tensor([[1, 0, 0, 0], [0, 0, 0, 1]])</span>
<span class="sd">        &gt;&gt;&gt; x = torch.IntTensor([[0, 1, 2, 3]])  # batch-dim=1</span>
<span class="sd">        &gt;&gt;&gt; # MultiDiscrete space with 5 + 4 + 4 + 7 = 20 (one-hot) slots</span>
<span class="sd">        &gt;&gt;&gt; # per batch item.</span>
<span class="sd">        &gt;&gt;&gt; s = gym.spaces.MultiDiscrete([5, 4, 4, 7])</span>
<span class="sd">        &gt;&gt;&gt; one_hot(x, s) # doctest: +SKIP</span>
<span class="sd">        tensor([[1, 0, 0, 0, 0,</span>
<span class="sd">                 0, 1, 0, 0,</span>
<span class="sd">                 0, 0, 1, 0,</span>
<span class="sd">                 0, 0, 0, 1, 0, 0, 0]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">Discrete</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">space</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">MultiDiscrete</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">space</span><span class="o">.</span><span class="n">nvec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">nvec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">space</span><span class="o">.</span><span class="n">nvec</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">nvec</span> <span class="o">=</span> <span class="n">space</span><span class="o">.</span><span class="n">nvec</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">nvec</span><span class="p">)],</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unsupported space for `one_hot`: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">space</span><span class="p">))</span></div>


<div class="viewcode-block" id="reduce_mean_ignore_inf"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.reduce_mean_ignore_inf.html#ray.rllib.utils.torch_utils.reduce_mean_ignore_inf">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">reduce_mean_ignore_inf</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Same as torch.mean() but ignores -inf values.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The input tensor to reduce mean over.</span>
<span class="sd">        axis: The axis over which to reduce. None for all axes.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The mean reduced inputs, ignoring inf values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>
    <span class="n">x_zeroed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x_zeroed</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="sequence_mask"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.sequence_mask.html#ray.rllib.utils.torch_utils.sequence_mask">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">sequence_mask</span><span class="p">(</span>
    <span class="n">lengths</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span>
    <span class="n">maxlen</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">time_major</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Offers same behavior as tf.sequence_mask for torch.</span>

<span class="sd">    Thanks to Dimitris Papatheodorou</span>
<span class="sd">    (https://discuss.pytorch.org/t/pytorch-equivalent-for-tf-sequence-mask/</span>
<span class="sd">    39036).</span>

<span class="sd">    Args:</span>
<span class="sd">        lengths: The tensor of individual lengths to mask by.</span>
<span class="sd">        maxlen: The maximum length to use for the time axis. If None, use</span>
<span class="sd">            the max of `lengths`.</span>
<span class="sd">        dtype: The torch dtype to use for the resulting mask.</span>
<span class="sd">        time_major: Whether to return the mask as [B, T] (False; default) or</span>
<span class="sd">            as [T, B] (True).</span>

<span class="sd">    Returns:</span>
<span class="sd">         The sequence mask resulting from the given input and parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># If maxlen not given, use the longest lengths in the `lengths` tensor.</span>
    <span class="k">if</span> <span class="n">maxlen</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">maxlen</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">lengths</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>

    <span class="n">mask</span> <span class="o">=</span> <span class="o">~</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">lengths</span><span class="p">),</span> <span class="n">maxlen</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">lengths</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="o">&gt;</span> <span class="n">lengths</span>
    <span class="p">)</span>
    <span class="c1"># Time major transformation.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">time_major</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>

    <span class="c1"># By default, set the mask to be boolean.</span>
    <span class="n">mask</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span> <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mask</span></div>


<div class="viewcode-block" id="warn_if_infinite_kl_divergence"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.warn_if_infinite_kl_divergence.html#ray.rllib.utils.torch_utils.warn_if_infinite_kl_divergence">[docs]</a><span class="nd">@DeveloperAPI</span>
<span class="k">def</span> <span class="nf">warn_if_infinite_kl_divergence</span><span class="p">(</span>
    <span class="n">policy</span><span class="p">:</span> <span class="s2">&quot;TorchPolicy&quot;</span><span class="p">,</span>
    <span class="n">kl_divergence</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">policy</span><span class="o">.</span><span class="n">loss_initialized</span><span class="p">()</span> <span class="ow">and</span> <span class="n">kl_divergence</span><span class="o">.</span><span class="n">isinf</span><span class="p">():</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;KL divergence is non-finite, this will likely destabilize your model and&quot;</span>
            <span class="s2">&quot; the training process. Action(s) in a specific state have near-zero&quot;</span>
            <span class="s2">&quot; probability. This can happen naturally in deterministic environments&quot;</span>
            <span class="s2">&quot; where the optimal policy has zero mass for a specific action. To fix this&quot;</span>
            <span class="s2">&quot; issue, consider setting the coefficient for the KL loss term to zero or&quot;</span>
            <span class="s2">&quot; increasing policy entropy.&quot;</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="set_torch_seed"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.set_torch_seed.html#ray.rllib.utils.torch_utils.set_torch_seed">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">set_torch_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Sets the torch random seed to the given value.</span>

<span class="sd">    Args:</span>
<span class="sd">        seed: The seed to use or None for no seeding.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">torch</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="c1"># See https://github.com/pytorch/pytorch/issues/47672.</span>
        <span class="n">cuda_version</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span>
        <span class="k">if</span> <span class="n">cuda_version</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mf">10.2</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUBLAS_WORKSPACE_CONFIG&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;4096:8&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Not all Operations support this.</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">use_deterministic_algorithms</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># This is only for Convolution no problem.</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span></div>


<div class="viewcode-block" id="softmax_cross_entropy_with_logits"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.softmax_cross_entropy_with_logits.html#ray.rllib.utils.torch_utils.softmax_cross_entropy_with_logits">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">softmax_cross_entropy_with_logits</span><span class="p">(</span>
    <span class="n">logits</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">TensorType</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Same behavior as tf.nn.softmax_cross_entropy_with_logits.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: The input predictions.</span>
<span class="sd">        labels: The labels corresponding to `x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The resulting softmax cross-entropy given predictions and labels.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="n">labels</span> <span class="o">*</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span></div>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>