
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ray.rllib.algorithms.algorithm_config &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../../../../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script src="../../../../_static/js/versionwarning.js"></script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../../../_static/js/docsearch.js"></script>
    <script src="../../../../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../../../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../../../_static/js/custom.js"></script>
    <script defer="defer" src="../../../../_static/js/top-navigation.js"></script>
    <script src="../../../../_static/js/tags.js"></script>
    <script src="../../../../_static/tabs.js"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../../_static/design-tabs.js"></script>
    <script async="async" src="../../../../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/_modules/ray/rllib/algorithms/algorithm_config.html" />
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "_modules/ray/rllib/algorithms/algorithm_config", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2F_modules/ray/rllib/algorithms/algorithm_config.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for ray.rllib.algorithms.algorithm_config</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">TYPE_CHECKING</span><span class="p">,</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Container</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">Mapping</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Type</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">packaging</span> <span class="kn">import</span> <span class="n">version</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.callbacks</span> <span class="kn">import</span> <span class="n">DefaultCallbacks</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.learner.learner</span> <span class="kn">import</span> <span class="n">LearnerHyperparameters</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.learner.learner_group_config</span> <span class="kn">import</span> <span class="n">LearnerGroupConfig</span><span class="p">,</span> <span class="n">ModuleSpec</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.marl_module</span> <span class="kn">import</span> <span class="n">MultiAgentRLModuleSpec</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.rl_module</span> <span class="kn">import</span> <span class="n">ModuleID</span><span class="p">,</span> <span class="n">SingleAgentRLModuleSpec</span>
<span class="kn">from</span> <span class="nn">ray.rllib.env.env_context</span> <span class="kn">import</span> <span class="n">EnvContext</span>
<span class="kn">from</span> <span class="nn">ray.rllib.env.multi_agent_env</span> <span class="kn">import</span> <span class="n">MultiAgentEnv</span>
<span class="kn">from</span> <span class="nn">ray.rllib.env.wrappers.atari_wrappers</span> <span class="kn">import</span> <span class="n">is_atari</span>
<span class="kn">from</span> <span class="nn">ray.rllib.evaluation.collectors.sample_collector</span> <span class="kn">import</span> <span class="n">SampleCollector</span>
<span class="kn">from</span> <span class="nn">ray.rllib.evaluation.collectors.simple_list_collector</span> <span class="kn">import</span> <span class="n">SimpleListCollector</span>
<span class="kn">from</span> <span class="nn">ray.rllib.evaluation.episode</span> <span class="kn">import</span> <span class="n">Episode</span>
<span class="kn">from</span> <span class="nn">ray.rllib.models</span> <span class="kn">import</span> <span class="n">MODEL_DEFAULTS</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.policy</span> <span class="kn">import</span> <span class="n">Policy</span><span class="p">,</span> <span class="n">PolicySpec</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.sample_batch</span> <span class="kn">import</span> <span class="n">DEFAULT_POLICY_ID</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils</span> <span class="kn">import</span> <span class="n">deep_update</span><span class="p">,</span> <span class="n">merge_dicts</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.annotations</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ExperimentalAPI</span><span class="p">,</span>
    <span class="n">OverrideToImplementCustomLogic_CallToSuperRecommended</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.deprecation</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
    <span class="n">Deprecated</span><span class="p">,</span>
    <span class="n">deprecation_warning</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.framework</span> <span class="kn">import</span> <span class="n">try_import_tf</span><span class="p">,</span> <span class="n">try_import_torch</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.from_config</span> <span class="kn">import</span> <span class="n">NotProvided</span><span class="p">,</span> <span class="n">from_config</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.gym</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">convert_old_gym_space_to_gymnasium_space</span><span class="p">,</span>
    <span class="n">try_import_gymnasium_and_gym</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.policy</span> <span class="kn">import</span> <span class="n">validate_policy_id</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.schedules.scheduler</span> <span class="kn">import</span> <span class="n">Scheduler</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.serialization</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">NOT_SERIALIZABLE</span><span class="p">,</span>
    <span class="n">deserialize_type</span><span class="p">,</span>
    <span class="n">serialize_type</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.torch_utils</span> <span class="kn">import</span> <span class="n">TORCH_COMPILE_REQUIRED_VERSION</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AgentID</span><span class="p">,</span>
    <span class="n">AlgorithmConfigDict</span><span class="p">,</span>
    <span class="n">EnvConfigDict</span><span class="p">,</span>
    <span class="n">EnvType</span><span class="p">,</span>
    <span class="n">LearningRateOrSchedule</span><span class="p">,</span>
    <span class="n">MultiAgentPolicyConfigDict</span><span class="p">,</span>
    <span class="n">PartialAlgorithmConfigDict</span><span class="p">,</span>
    <span class="n">PolicyID</span><span class="p">,</span>
    <span class="n">ResultDict</span><span class="p">,</span>
    <span class="n">SampleBatchType</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.tune.logger</span> <span class="kn">import</span> <span class="n">Logger</span>
<span class="kn">from</span> <span class="nn">ray.tune.registry</span> <span class="kn">import</span> <span class="n">get_trainable_cls</span>
<span class="kn">from</span> <span class="nn">ray.tune.result</span> <span class="kn">import</span> <span class="n">TRIAL_INFO</span>
<span class="kn">from</span> <span class="nn">ray.tune.tune</span> <span class="kn">import</span> <span class="n">_Config</span>
<span class="kn">from</span> <span class="nn">ray.util</span> <span class="kn">import</span> <span class="n">log_once</span>

<span class="n">gym</span><span class="p">,</span> <span class="n">old_gym</span> <span class="o">=</span> <span class="n">try_import_gymnasium_and_gym</span><span class="p">()</span>
<span class="n">Space</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span>

<span class="sd">&quot;&quot;&quot;TODO(jungong, sven): in &quot;offline_data&quot; we can potentially unify all input types</span>
<span class="sd">under input and input_config keys. E.g.</span>
<span class="sd">input: sample</span>
<span class="sd">input_config {</span>
<span class="sd">env: CartPole-v1</span>
<span class="sd">}</span>
<span class="sd">or:</span>
<span class="sd">input: json_reader</span>
<span class="sd">input_config {</span>
<span class="sd">path: /tmp/</span>
<span class="sd">}</span>
<span class="sd">or:</span>
<span class="sd">input: dataset</span>
<span class="sd">input_config {</span>
<span class="sd">format: parquet</span>
<span class="sd">path: /tmp/</span>
<span class="sd">}</span>
<span class="sd">&quot;&quot;&quot;</span>


<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm</span> <span class="kn">import</span> <span class="n">Algorithm</span>
    <span class="kn">from</span> <span class="nn">ray.rllib.core.learner</span> <span class="kn">import</span> <span class="n">Learner</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_rl_module_spec</span><span class="p">(</span><span class="n">module_spec</span><span class="p">:</span> <span class="n">ModuleSpec</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module_spec</span><span class="p">,</span> <span class="p">(</span><span class="n">SingleAgentRLModuleSpec</span><span class="p">,</span> <span class="n">MultiAgentRLModuleSpec</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;rl_module_spec must be an instance of &quot;</span>
            <span class="s2">&quot;SingleAgentRLModuleSpec or MultiAgentRLModuleSpec.&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">module_spec</span><span class="p">)</span><span class="si">}</span><span class="s2"> instead.&quot;</span>
        <span class="p">)</span>


<div class="viewcode-block" id="AlgorithmConfig"><a class="viewcode-back" href="../../../../rllib/rllib-training.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig">[docs]</a><span class="k">class</span> <span class="nc">AlgorithmConfig</span><span class="p">(</span><span class="n">_Config</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A RLlib AlgorithmConfig builds an RLlib Algorithm from a given configuration.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; from ray.rllib.algorithms.algorithm_config import AlgorithmConfig</span>
<span class="sd">        &gt;&gt;&gt; from ray.rllib.algorithms.callbacks import MemoryTrackingCallbacks</span>
<span class="sd">        &gt;&gt;&gt; # Construct a generic config object, specifying values within different</span>
<span class="sd">        &gt;&gt;&gt; # sub-categories, e.g. &quot;training&quot;.</span>
<span class="sd">        &gt;&gt;&gt; config = AlgorithmConfig().training(gamma=0.9, lr=0.01)  # doctest: +SKIP</span>
<span class="sd">        ...     .environment(env=&quot;CartPole-v1&quot;)</span>
<span class="sd">        ...     .resources(num_gpus=0)</span>
<span class="sd">        ...     .rollouts(num_rollout_workers=4)</span>
<span class="sd">        ...     .callbacks(MemoryTrackingCallbacks)</span>
<span class="sd">        &gt;&gt;&gt; # A config object can be used to construct the respective Trainer.</span>
<span class="sd">        &gt;&gt;&gt; rllib_algo = config.build()  # doctest: +SKIP</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; from ray.rllib.algorithms.algorithm_config import AlgorithmConfig</span>
<span class="sd">        &gt;&gt;&gt; from ray import tune</span>
<span class="sd">        &gt;&gt;&gt; # In combination with a tune.grid_search:</span>
<span class="sd">        &gt;&gt;&gt; config = AlgorithmConfig()</span>
<span class="sd">        &gt;&gt;&gt; config.training(lr=tune.grid_search([0.01, 0.001])) # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; # Use `to_dict()` method to get the legacy plain python config dict</span>
<span class="sd">        &gt;&gt;&gt; # for usage with `tune.Tuner().fit()`.</span>
<span class="sd">        &gt;&gt;&gt; tune.Tuner(  # doctest: +SKIP</span>
<span class="sd">        ...     &quot;[registered trainer class]&quot;, param_space=config.to_dict()</span>
<span class="sd">        ...     ).fit()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">DEFAULT_POLICY_MAPPING_FN</span><span class="p">(</span><span class="n">aid</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">worker</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># The default policy mapping function to use if None provided.</span>
        <span class="c1"># Map any agent ID to &quot;default_policy&quot;.</span>
        <span class="k">return</span> <span class="n">DEFAULT_POLICY_ID</span>

<div class="viewcode-block" id="AlgorithmConfig.from_dict"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.from_dict.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.from_dict">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_dict</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">config_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Creates an AlgorithmConfig from a legacy python config dict.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from ray.rllib.algorithms.ppo.ppo import PPOConfig # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; ppo_config = PPOConfig.from_dict({...}) # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; ppo = ppo_config.build(env=&quot;Pendulum-v1&quot;) # doctest: +SKIP</span>

<span class="sd">        Args:</span>
<span class="sd">            config_dict: The legacy formatted python config dict for some algorithm.</span>

<span class="sd">        Returns:</span>
<span class="sd">             A new AlgorithmConfig object that matches the given python config dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Create a default config object of this class.</span>
        <span class="n">config_obj</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">()</span>
        <span class="c1"># Remove `_is_frozen` flag from config dict in case the AlgorithmConfig that</span>
        <span class="c1"># the dict was derived from was already frozen (we don&#39;t want to copy the</span>
        <span class="c1"># frozenness).</span>
        <span class="n">config_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_is_frozen&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">config_obj</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">config_dict</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">config_obj</span></div>

<div class="viewcode-block" id="AlgorithmConfig.overrides"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.overrides.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.overrides">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">overrides</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Generates and validates a set of config key/value pairs (passed via kwargs).</span>

<span class="sd">        Validation whether given config keys are valid is done immediately upon</span>
<span class="sd">        construction (by comparing against the properties of a default AlgorithmConfig</span>
<span class="sd">        object of this class).</span>
<span class="sd">        Allows combination with a full AlgorithmConfig object to yield a new</span>
<span class="sd">        AlgorithmConfig object.</span>

<span class="sd">        Used anywhere, we would like to enable the user to only define a few config</span>
<span class="sd">        settings that would change with respect to some main config, e.g. in multi-agent</span>
<span class="sd">        setups and evaluation configs.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from ray.rllib.algorithms.ppo import PPOConfig</span>
<span class="sd">            &gt;&gt;&gt; from ray.rllib.policy.policy import PolicySpec</span>
<span class="sd">            &gt;&gt;&gt; config = (</span>
<span class="sd">            ...     PPOConfig()</span>
<span class="sd">            ...     .multi_agent(</span>
<span class="sd">            ...         policies={</span>
<span class="sd">            ...             &quot;pol0&quot;: PolicySpec(config=PPOConfig.overrides(lambda_=0.95))</span>
<span class="sd">            ...         },</span>
<span class="sd">            ...     )</span>
<span class="sd">            ... )</span>

<span class="sd">            &gt;&gt;&gt; from ray.rllib.algorithms.algorithm_config import AlgorithmConfig</span>
<span class="sd">            &gt;&gt;&gt; from ray.rllib.algorithms.pg import PGConfig</span>
<span class="sd">            &gt;&gt;&gt; config = (</span>
<span class="sd">            ...     PGConfig()</span>
<span class="sd">            ...     .evaluation(</span>
<span class="sd">            ...         evaluation_num_workers=1,</span>
<span class="sd">            ...         evaluation_interval=1,</span>
<span class="sd">            ...         evaluation_config=AlgorithmConfig.overrides(explore=False),</span>
<span class="sd">            ...     )</span>
<span class="sd">            ... )</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dict mapping valid config property-names to values.</span>

<span class="sd">        Raises:</span>
<span class="sd">            KeyError: In case a non-existing property name (kwargs key) is being</span>
<span class="sd">            passed in. Valid property names are taken from a default AlgorithmConfig</span>
<span class="sd">            object of `cls`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">default_config</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">()</span>
        <span class="n">config_overrides</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">default_config</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Invalid property name </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> for config class </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">!&quot;</span>
                <span class="p">)</span>
            <span class="c1"># Allow things like &quot;lambda&quot; as well.</span>
            <span class="n">key</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_translate_special_keys</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">warn_deprecated</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">config_overrides</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

        <span class="k">return</span> <span class="n">config_overrides</span></div>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">algo_class</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># Define all settings and their default values.</span>

        <span class="c1"># Define the default RLlib Trainer class that this AlgorithmConfig will be</span>
        <span class="c1"># applied to.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">algo_class</span> <span class="o">=</span> <span class="n">algo_class</span>

        <span class="c1"># `self.python_environment()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">extra_python_environs_for_driver</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">extra_python_environs_for_worker</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># `self.resources()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_cpus_per_worker</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_gpus_per_worker</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fake_gpus</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_cpus_for_local_worker</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_learner_workers</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_gpus_per_learner_worker</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_cpus_per_learner_worker</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_gpu_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">custom_resources_per_worker</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">placement_strategy</span> <span class="o">=</span> <span class="s2">&quot;PACK&quot;</span>

        <span class="c1"># `self.framework()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span> <span class="o">=</span> <span class="s2">&quot;torch&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eager_tracing</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eager_max_retraces</span> <span class="o">=</span> <span class="mi">20</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tf_session_args</span> <span class="o">=</span> <span class="p">{</span>
            <span class="c1"># note: overridden by `local_tf_session_args`</span>
            <span class="s2">&quot;intra_op_parallelism_threads&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s2">&quot;inter_op_parallelism_threads&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s2">&quot;gpu_options&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;allow_growth&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="s2">&quot;log_device_placement&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;device_count&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
            <span class="c1"># Required by multi-GPU (num_gpus &gt; 1).</span>
            <span class="s2">&quot;allow_soft_placement&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_tf_session_args</span> <span class="o">=</span> <span class="p">{</span>
            <span class="c1"># Allow a higher level of parallelism by default, but not unlimited</span>
            <span class="c1"># since that can cause crashes with many concurrent drivers.</span>
            <span class="s2">&quot;intra_op_parallelism_threads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
            <span class="s2">&quot;inter_op_parallelism_threads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="c1"># Torch compile settings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_learner</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_learner_dynamo_backend</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;aot_eager&quot;</span> <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">platform</span> <span class="o">==</span> <span class="s2">&quot;darwin&quot;</span> <span class="k">else</span> <span class="s2">&quot;inductor&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_learner_dynamo_mode</span> <span class="o">=</span> <span class="s2">&quot;reduce-overhead&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_worker</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_worker_dynamo_backend</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;aot_eager&quot;</span> <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">platform</span> <span class="o">==</span> <span class="s2">&quot;darwin&quot;</span> <span class="k">else</span> <span class="s2">&quot;inductor&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_worker_dynamo_mode</span> <span class="o">=</span> <span class="s2">&quot;reduce-overhead&quot;</span>

        <span class="c1"># `self.environment()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env_config</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env_task_fn</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">render_env</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_rewards</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalize_actions</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_actions</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">disable_env_checking</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Whether this env is an atari env (for atari-specific preprocessing).</span>
        <span class="c1"># If not specified, we will try to auto-detect this.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_atari</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">auto_wrap_old_gym_envs</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_mask_key</span> <span class="o">=</span> <span class="s2">&quot;action_mask&quot;</span>

        <span class="c1"># `self.rollouts()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_cls</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_rollout_workers</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_envs_per_worker</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample_collector</span> <span class="o">=</span> <span class="n">SimpleListCollector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">create_env_on_local_worker</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample_async</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enable_connectors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_worker_filter_stats</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_worker_filter_stats</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rollout_fragment_length</span> <span class="o">=</span> <span class="mi">200</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_mode</span> <span class="o">=</span> <span class="s2">&quot;truncate_episodes&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">remote_worker_envs</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">remote_env_batch_wait_ms</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validate_workers_after_construction</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor_pref</span> <span class="o">=</span> <span class="s2">&quot;deepmind&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">observation_filter</span> <span class="o">=</span> <span class="s2">&quot;NoFilter&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compress_observations</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enable_tf1_exec_eagerly</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampler_perf_stats_ema_coef</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># `self.training()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_clip</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_clip_by</span> <span class="o">=</span> <span class="s2">&quot;global_norm&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span> <span class="o">=</span> <span class="mi">32</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">MODEL_DEFAULTS</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_requests_in_flight_per_sampler_worker</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learner_class</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_enable_learner_api</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># `self.callbacks()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks_class</span> <span class="o">=</span> <span class="n">DefaultCallbacks</span>

        <span class="c1"># `self.explore()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">explore</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># This is not compatible with RLModules, which have a method</span>
        <span class="c1"># `forward_exploration` to specify custom exploration behavior.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exploration_config</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># `self.multi_agent()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policies</span> <span class="o">=</span> <span class="p">{</span><span class="n">DEFAULT_POLICY_ID</span><span class="p">:</span> <span class="n">PolicySpec</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_config_overrides_per_module</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_map_capacity</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_mapping_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">DEFAULT_POLICY_MAPPING_FN</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policies_to_train</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_states_are_swappable</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">observation_fn</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">=</span> <span class="s2">&quot;env_steps&quot;</span>

        <span class="c1"># `self.offline_data()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_</span> <span class="o">=</span> <span class="s2">&quot;sampler&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_config</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actions_in_input_normalized</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">postprocess_inputs</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shuffle_buffer_size</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_config</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_compress_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;obs&quot;</span><span class="p">,</span> <span class="s2">&quot;new_obs&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_max_file_size</span> <span class="o">=</span> <span class="mi">64</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">offline_sampling</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># `self.evaluation()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_interval</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_duration</span> <span class="o">=</span> <span class="mi">10</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_duration_unit</span> <span class="o">=</span> <span class="s2">&quot;episodes&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_sample_timeout_s</span> <span class="o">=</span> <span class="mf">180.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_parallel_to_training</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">off_policy_estimation_methods</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ope_split_batch_by_episode</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_num_workers</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">custom_evaluation_function</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">always_attach_evaluation_results</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enable_async_evaluation</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># TODO: Set this flag still in the config or - much better - in the</span>
        <span class="c1">#  RolloutWorker as a property.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_evaluation</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sync_filters_on_rollout_workers_timeout_s</span> <span class="o">=</span> <span class="mf">60.0</span>

        <span class="c1"># `self.reporting()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_per_episode_custom_metrics</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics_episode_collection_timeout_s</span> <span class="o">=</span> <span class="mf">60.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics_num_episodes_for_smoothing</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_time_s_per_iteration</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_train_timesteps_per_iteration</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_sample_timesteps_per_iteration</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># `self.checkpointing()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">export_native_model_files</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_trainable_policies_only</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># `self.debugging()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger_creator</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger_config</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_level</span> <span class="o">=</span> <span class="s2">&quot;WARN&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_sys_usage</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fake_sampler</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># `self.fault_tolerance()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_worker_failures</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recreate_failed_workers</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># By default restart failed worker a thousand times.</span>
        <span class="c1"># This should be enough to handle normal transient failures.</span>
        <span class="c1"># This also prevents infinite number of restarts in case</span>
        <span class="c1"># the worker or env has a bug.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_num_worker_restarts</span> <span class="o">=</span> <span class="mi">1000</span>
        <span class="c1"># Small delay between worker restarts. In case rollout or</span>
        <span class="c1"># evaluation workers have remote dependencies, this delay can be</span>
        <span class="c1"># adjusted to make sure we don&#39;t flood them with re-connection</span>
        <span class="c1"># requests, and allow them enough time to recover.</span>
        <span class="c1"># This delay also gives Ray time to stream back error logging</span>
        <span class="c1"># and exceptions.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">delay_between_worker_restarts_s</span> <span class="o">=</span> <span class="mf">60.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">restart_failed_sub_environments</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_consecutive_worker_failures_tolerance</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker_health_probe_timeout_s</span> <span class="o">=</span> <span class="mi">60</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker_restore_timeout_s</span> <span class="o">=</span> <span class="mi">1800</span>

        <span class="c1"># `self.rl_module()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rl_module_spec</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_enable_rl_module_api</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Helper to keep track of the original exploration config when dis-/enabling</span>
        <span class="c1"># rl modules.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__prior_exploration_config</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># `self.experimental()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tf_policy_handles_more_than_one_loss</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_disable_preprocessor_api</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_disable_action_flattening</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_disable_execution_plan_api</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_disable_initialize_loss_from_dummy_batch</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Has this config object been frozen (cannot alter its attributes anymore).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_frozen</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># TODO: Remove, once all deprecation_warning calls upon using these keys</span>
        <span class="c1">#  have been removed.</span>
        <span class="c1"># === Deprecated keys ===</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">simple_optimizer</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">monitor</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_num_episodes</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics_smoothing_episodes</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timesteps_per_iteration</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_iter_time_s</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collect_metrics_timeout</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_time_s_per_reporting</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_train_timesteps_per_reporting</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_sample_timesteps_per_reporting</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_evaluation</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_map_cache</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker_cls</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">synchronize_filters</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>

        <span class="c1"># The following values have moved because of the new ReplayBuffer API</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer_size</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prioritized_replay</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_starts</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_batch_size</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="c1"># -1 = DEPRECATED_VALUE is a valid value for replay_sequence_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_sequence_length</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_mode</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prioritized_replay_alpha</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prioritized_replay_beta</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prioritized_replay_eps</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_time_s_per_reporting</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_train_timesteps_per_reporting</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_sample_timesteps_per_reporting</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span>

<div class="viewcode-block" id="AlgorithmConfig.to_dict"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.to_dict.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.to_dict">[docs]</a>    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AlgorithmConfigDict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Converts all settings into a legacy config dict for backward compatibility.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A complete AlgorithmConfigDict, usable in backward-compatible Tune/RLlib</span>
<span class="sd">            use cases, e.g. w/ `tune.Tuner().fit()`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="nb">vars</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>
        <span class="n">config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;algo_class&quot;</span><span class="p">)</span>
        <span class="n">config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_is_frozen&quot;</span><span class="p">)</span>

        <span class="c1"># Worst naming convention ever: NEVER EVER use reserved key-words...</span>
        <span class="k">if</span> <span class="s2">&quot;lambda_&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;lambda_&quot;</span><span class="p">)</span>
            <span class="n">config</span><span class="p">[</span><span class="s2">&quot;lambda&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;lambda_&quot;</span><span class="p">)</span>
            <span class="n">config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;lambda_&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;input_&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;input_&quot;</span><span class="p">)</span>
            <span class="n">config</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;input_&quot;</span><span class="p">)</span>
            <span class="n">config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;input_&quot;</span><span class="p">)</span>

        <span class="c1"># Convert `policies` (PolicySpecs?) into dict.</span>
        <span class="c1"># Convert policies dict such that each policy ID maps to a old-style.</span>
        <span class="c1"># 4-tuple: class, obs-, and action space, config.</span>
        <span class="k">if</span> <span class="s2">&quot;policies&quot;</span> <span class="ow">in</span> <span class="n">config</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;policies&quot;</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">policies_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">policy_id</span><span class="p">,</span> <span class="n">policy_spec</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;policies&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">policy_spec</span><span class="p">,</span> <span class="n">PolicySpec</span><span class="p">):</span>
                    <span class="n">policies_dict</span><span class="p">[</span><span class="n">policy_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">policy_spec</span><span class="o">.</span><span class="n">policy_class</span><span class="p">,</span>
                        <span class="n">policy_spec</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
                        <span class="n">policy_spec</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
                        <span class="n">policy_spec</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">policies_dict</span><span class="p">[</span><span class="n">policy_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">policy_spec</span>
            <span class="n">config</span><span class="p">[</span><span class="s2">&quot;policies&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">policies_dict</span>

        <span class="c1"># Switch out deprecated vs new config keys.</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;callbacks&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;callbacks_class&quot;</span><span class="p">,</span> <span class="n">DefaultCallbacks</span><span class="p">)</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;create_env_on_driver&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;create_env_on_local_worker&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;custom_eval_function&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;custom_evaluation_function&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;framework&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;framework_str&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_cpus_for_driver&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;num_cpus_for_local_worker&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;num_rollout_workers&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Simplify: Remove all deprecated keys that have as value `DEPRECATED_VALUE`.</span>
        <span class="c1"># These would be useless in the returned dict anyways.</span>
        <span class="k">for</span> <span class="n">dep_k</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="s2">&quot;monitor&quot;</span><span class="p">,</span>
            <span class="s2">&quot;evaluation_num_episodes&quot;</span><span class="p">,</span>
            <span class="s2">&quot;metrics_smoothing_episodes&quot;</span><span class="p">,</span>
            <span class="s2">&quot;timesteps_per_iteration&quot;</span><span class="p">,</span>
            <span class="s2">&quot;min_iter_time_s&quot;</span><span class="p">,</span>
            <span class="s2">&quot;collect_metrics_timeout&quot;</span><span class="p">,</span>
            <span class="s2">&quot;buffer_size&quot;</span><span class="p">,</span>
            <span class="s2">&quot;prioritized_replay&quot;</span><span class="p">,</span>
            <span class="s2">&quot;learning_starts&quot;</span><span class="p">,</span>
            <span class="s2">&quot;replay_batch_size&quot;</span><span class="p">,</span>
            <span class="s2">&quot;replay_mode&quot;</span><span class="p">,</span>
            <span class="s2">&quot;prioritized_replay_alpha&quot;</span><span class="p">,</span>
            <span class="s2">&quot;prioritized_replay_beta&quot;</span><span class="p">,</span>
            <span class="s2">&quot;prioritized_replay_eps&quot;</span><span class="p">,</span>
            <span class="s2">&quot;min_time_s_per_reporting&quot;</span><span class="p">,</span>
            <span class="s2">&quot;min_train_timesteps_per_reporting&quot;</span><span class="p">,</span>
            <span class="s2">&quot;min_sample_timesteps_per_reporting&quot;</span><span class="p">,</span>
            <span class="s2">&quot;input_evaluation&quot;</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">dep_k</span><span class="p">)</span> <span class="o">==</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
                <span class="n">config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">dep_k</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">config</span></div>

<div class="viewcode-block" id="AlgorithmConfig.update_from_dict"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.update_from_dict.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.update_from_dict">[docs]</a>    <span class="k">def</span> <span class="nf">update_from_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">config_dict</span><span class="p">:</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Modifies this AlgorithmConfig via the provided python config dict.</span>

<span class="sd">        Warns if `config_dict` contains deprecated keys.</span>
<span class="sd">        Silently sets even properties of `self` that do NOT exist. This way, this method</span>
<span class="sd">        may be used to configure custom Policies which do not have their own specific</span>
<span class="sd">        AlgorithmConfig classes, e.g.</span>
<span class="sd">        `ray.rllib.examples.policy.random_policy::RandomPolicy`.</span>

<span class="sd">        Args:</span>
<span class="sd">            config_dict: The old-style python config dict (PartialAlgorithmConfigDict)</span>
<span class="sd">                to use for overriding some properties defined in there.</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">eval_call</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># We deal with this special key before all others because it may influence</span>
        <span class="c1"># stuff like &quot;exploration_config&quot;.</span>
        <span class="c1"># Namely, we want to re-instantiate the exploration config this config had</span>
        <span class="c1"># inside `self.rl_module()` before potentially overwriting it in the following.</span>
        <span class="k">if</span> <span class="s2">&quot;_enable_rl_module_api&quot;</span> <span class="ow">in</span> <span class="n">config_dict</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rl_module</span><span class="p">(</span><span class="n">_enable_rl_module_api</span><span class="o">=</span><span class="n">config_dict</span><span class="p">[</span><span class="s2">&quot;_enable_rl_module_api&quot;</span><span class="p">])</span>

        <span class="c1"># Modify our properties one by one.</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">config_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_translate_special_keys</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">warn_deprecated</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="c1"># Ray Tune saves additional data under this magic keyword.</span>
            <span class="c1"># This should not get treated as AlgorithmConfig field.</span>
            <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="n">TRIAL_INFO</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;_enable_rl_module_api&quot;</span><span class="p">:</span>
                <span class="c1"># We&#39;ve dealt with this above.</span>
                <span class="k">continue</span>
            <span class="c1"># Set our multi-agent settings.</span>
            <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;multiagent&quot;</span><span class="p">:</span>
                <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">k</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span>
                        <span class="s2">&quot;policies&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;policy_map_capacity&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;policy_mapping_fn&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;policies_to_train&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;policy_states_are_swappable&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;observation_fn&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;count_steps_by&quot;</span><span class="p">,</span>
                    <span class="p">]</span>
                    <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">value</span>
                <span class="p">}</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="c1"># Some keys specify config sub-dicts and therefore should go through the</span>
            <span class="c1"># correct methods to properly `.update()` those from given config dict</span>
            <span class="c1"># (to not lose any sub-keys).</span>
            <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;callbacks_class&quot;</span> <span class="ow">and</span> <span class="n">value</span> <span class="o">!=</span> <span class="n">NOT_SERIALIZABLE</span><span class="p">:</span>
                <span class="c1"># For backward compatibility reasons, only resolve possible</span>
                <span class="c1"># classpath if value is a str type.</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                    <span class="n">value</span> <span class="o">=</span> <span class="n">deserialize_type</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">(</span><span class="n">callbacks_class</span><span class="o">=</span><span class="n">value</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;env_config&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env_config</span><span class="o">=</span><span class="n">value</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;evaluation_&quot;</span><span class="p">):</span>
                <span class="n">eval_call</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
            <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;exploration_config&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">config_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_enable_rl_module_api&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">exploration_config</span> <span class="o">=</span> <span class="n">value</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;type&quot;</span> <span class="ow">in</span> <span class="n">value</span><span class="p">:</span>
                    <span class="n">value</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">deserialize_type</span><span class="p">(</span><span class="n">value</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">exploration</span><span class="p">(</span><span class="n">exploration_config</span><span class="o">=</span><span class="n">value</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;model&quot;</span><span class="p">:</span>
                <span class="c1"># Resolve possible classpath.</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="n">value</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;custom_model&quot;</span><span class="p">):</span>
                    <span class="n">value</span><span class="p">[</span><span class="s2">&quot;custom_model&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">deserialize_type</span><span class="p">(</span><span class="n">value</span><span class="p">[</span><span class="s2">&quot;custom_model&quot;</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">})</span>
            <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">})</span>
            <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;type&quot;</span> <span class="ow">in</span> <span class="n">value</span><span class="p">:</span>
                    <span class="n">value</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">deserialize_type</span><span class="p">(</span><span class="n">value</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">})</span>
            <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;sample_collector&quot;</span><span class="p">:</span>
                <span class="c1"># Resolve possible classpath.</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">deserialize_type</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">sample_collector</span><span class="o">=</span><span class="n">value</span><span class="p">)</span>
            <span class="c1"># If config key matches a property, just set it, otherwise, warn and set.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span> <span class="ow">and</span> <span class="n">log_once</span><span class="p">(</span>
                    <span class="s2">&quot;unknown_property_in_algo_config&quot;</span>
                <span class="p">):</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Cannot create </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> from given &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;`config_dict`! Property </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> not supported.&quot;</span>
                    <span class="p">)</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation</span><span class="p">(</span><span class="o">**</span><span class="n">eval_call</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="c1"># TODO(sven): We might want to have a `deserialize` method as well. Right now,</span>
    <span class="c1">#  simply using the from_dict() API works in this same (deserializing) manner,</span>
    <span class="c1">#  whether the dict used is actually code-free (already serialized) or not</span>
    <span class="c1">#  (i.e. a classic RLlib config dict with e.g. &quot;callbacks&quot; key still pointing to</span>
    <span class="c1">#  a class).</span>
<div class="viewcode-block" id="AlgorithmConfig.serialize"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.serialize.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.serialize">[docs]</a>    <span class="k">def</span> <span class="nf">serialize</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Returns a mapping from str to JSON&#39;able values representing this config.</span>

<span class="sd">        The resulting values will not have any code in them.</span>
<span class="sd">        Classes (such as `callbacks_class`) will be converted to their full</span>
<span class="sd">        classpath, e.g. `ray.rllib.algorithms.callbacks.DefaultCallbacks`.</span>
<span class="sd">        Actual code such as lambda functions will be written as their source</span>
<span class="sd">        code (str) plus any closure information for properly restoring the</span>
<span class="sd">        code inside the AlgorithmConfig object made from the returned dict data.</span>
<span class="sd">        Dataclass objects get converted to dicts.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A mapping from str to JSON&#39;able values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_serialize_dict</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></div>

<div class="viewcode-block" id="AlgorithmConfig.copy"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.copy.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.copy">[docs]</a>    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">copy_frozen</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Creates a deep copy of this config and (un)freezes if necessary.</span>

<span class="sd">        Args:</span>
<span class="sd">            copy_frozen: Whether the created deep copy will be frozen or not. If None,</span>
<span class="sd">                keep the same frozen status that `self` currently has.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A deep copy of `self` that is (un)frozen.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cp</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">copy_frozen</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">cp</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">copy_frozen</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">cp</span><span class="o">.</span><span class="n">_is_frozen</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span> <span class="n">AlgorithmConfig</span><span class="p">):</span>
                <span class="n">cp</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">_is_frozen</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="n">cp</span></div>

<div class="viewcode-block" id="AlgorithmConfig.freeze"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.freeze.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.freeze">[docs]</a>    <span class="k">def</span> <span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Freezes this config object, such that no attributes can be set anymore.</span>

<span class="sd">        Algorithms should use this method to make sure that their config objects</span>
<span class="sd">        remain read-only after this.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_frozen</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_frozen</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Also freeze underlying eval config, if applicable.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span> <span class="n">AlgorithmConfig</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span></div>

        <span class="c1"># TODO: Flip out all set/dict/list values into frozen versions</span>
        <span class="c1">#  of themselves? This way, users won&#39;t even be able to alter those values</span>
        <span class="c1">#  directly anymore.</span>

    <span class="k">def</span> <span class="nf">_detect_atari_env</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns whether this configured env is an Atari env or not.</span>

<span class="sd">        Returns:</span>
<span class="sd">            True, if specified env is an Atari env, False otherwise.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Atari envs are usually specified via a string like &quot;PongNoFrameskip-v4&quot;</span>
        <span class="c1"># or &quot;ALE/Breakout-v5&quot;.</span>
        <span class="c1"># We do NOT attempt to auto-detect Atari env for other specified types like</span>
        <span class="c1"># a callable, to avoid running heavy logics in validate().</span>
        <span class="c1"># For these cases, users can explicitly set `environment(atari=True)`.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="p">)</span> <span class="o">==</span> <span class="nb">str</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;ALE/&quot;</span><span class="p">):</span>
                <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;GymV26Environment-v0&quot;</span><span class="p">,</span> <span class="n">env_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">gym</span><span class="o">.</span><span class="n">error</span><span class="o">.</span><span class="n">NameNotFound</span><span class="p">:</span>
            <span class="c1"># Not an Atari env if this is not a gym env.</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="k">return</span> <span class="n">is_atari</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

<div class="viewcode-block" id="AlgorithmConfig.validate"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic_CallToSuperRecommended</span>
    <span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Validates all values in this config.&quot;&quot;&quot;</span>

        <span class="c1"># Validate rollout settings.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rollout_fragment_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">rollout_fragment_length</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="p">)</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">rollout_fragment_length</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`rollout_fragment_length` must be int &gt;0 or &#39;auto&#39;!&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;truncate_episodes&quot;</span><span class="p">,</span> <span class="s2">&quot;complete_episodes&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`config.batch_mode` must be one of [truncate_episodes|&quot;</span>
                <span class="s2">&quot;complete_episodes]! Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_mode</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor_pref</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;rllib&quot;</span><span class="p">,</span> <span class="s2">&quot;deepmind&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`config.preprocessor_pref` must be either &#39;rllib&#39;, &#39;deepmind&#39; or None!&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_envs_per_worker</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`num_envs_per_worker` (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_envs_per_worker</span><span class="si">}</span><span class="s2">) must be &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;larger than 0!&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Check correct framework settings, and whether configured framework is</span>
        <span class="c1"># installed.</span>
        <span class="n">_tf1</span><span class="p">,</span> <span class="n">_tf</span><span class="p">,</span> <span class="n">_tfv</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
        <span class="n">_torch</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;tf&quot;</span><span class="p">,</span> <span class="s2">&quot;tf2&quot;</span><span class="p">}</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span> <span class="o">!=</span> <span class="s2">&quot;torch&quot;</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;tf&quot;</span><span class="p">,</span> <span class="s2">&quot;tf2&quot;</span><span class="p">}:</span>
            <span class="n">_tf1</span><span class="p">,</span> <span class="n">_tf</span><span class="p">,</span> <span class="n">_tfv</span> <span class="o">=</span> <span class="n">try_import_tf</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_torch</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">try_import_torch</span><span class="p">()</span>

        <span class="c1"># Check if torch framework supports torch.compile.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">_torch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span> <span class="o">==</span> <span class="s2">&quot;torch&quot;</span>
            <span class="ow">and</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">_torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">TORCH_COMPILE_REQUIRED_VERSION</span>
            <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_learner</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_worker</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;torch.compile is only supported from torch 2.0.0&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_check_if_correct_nn_framework_installed</span><span class="p">(</span><span class="n">_tf1</span><span class="p">,</span> <span class="n">_tf</span><span class="p">,</span> <span class="n">_torch</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_resolve_tf_settings</span><span class="p">(</span><span class="n">_tf1</span><span class="p">,</span> <span class="n">_tfv</span><span class="p">)</span>

        <span class="c1"># Check `policies_to_train` for invalid entries.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policies_to_train</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">set</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">pid</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">policies_to_train</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">pid</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">policies</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;`config.multi_agent(policies_to_train=..)` contains &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;policy ID (</span><span class="si">{</span><span class="n">pid</span><span class="si">}</span><span class="s2">) that was not defined in &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;`config.multi_agent(policies=..)`!&quot;</span>
                    <span class="p">)</span>

        <span class="c1"># If `evaluation_num_workers` &gt; 0, warn if `evaluation_interval` is</span>
        <span class="c1"># None.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_num_workers</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_interval</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You have specified </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_num_workers</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="s2">&quot;evaluation workers, but your `evaluation_interval` is None! &quot;</span>
                <span class="s2">&quot;Therefore, evaluation will not occur automatically with each&quot;</span>
                <span class="s2">&quot; call to `Algorithm.train()`. Instead, you will have to call &quot;</span>
                <span class="s2">&quot;`Algorithm.evaluate()` manually in order to trigger an &quot;</span>
                <span class="s2">&quot;evaluation run.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># If `evaluation_num_workers=0` and</span>
        <span class="c1"># `evaluation_parallel_to_training=True`, warn that you need</span>
        <span class="c1"># at least one remote eval worker for parallel training and</span>
        <span class="c1"># evaluation, and set `evaluation_parallel_to_training` to False.</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_num_workers</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_parallel_to_training</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`evaluation_parallel_to_training` can only be done if &quot;</span>
                <span class="s2">&quot;`evaluation_num_workers` &gt; 0! Try setting &quot;</span>
                <span class="s2">&quot;`config.evaluation_parallel_to_training` to False.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># If `evaluation_duration=auto`, error if</span>
        <span class="c1"># `evaluation_parallel_to_training=False`.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_duration</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_parallel_to_training</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`evaluation_duration=auto` not supported for &quot;</span>
                    <span class="s2">&quot;`evaluation_parallel_to_training=False`!&quot;</span>
                <span class="p">)</span>
        <span class="c1"># Make sure, it&#39;s an int otherwise.</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_duration</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_duration</span> <span class="o">&lt;=</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`evaluation_duration` (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_duration</span><span class="si">}</span><span class="s2">) must be an &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;int and &gt;0!&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Check model config.</span>
        <span class="c1"># If no preprocessing, propagate into model&#39;s config as well</span>
        <span class="c1"># (so model will know, whether inputs are preprocessed or not).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_disable_preprocessor_api</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="s2">&quot;_disable_preprocessor_api&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># If no action flattening, propagate into model&#39;s config as well</span>
        <span class="c1"># (so model will know, whether action inputs are already flattened or</span>
        <span class="c1"># not).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_disable_action_flattening</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="s2">&quot;_disable_action_flattening&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;custom_preprocessor&quot;</span><span class="p">):</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.training(model={&#39;custom_preprocessor&#39;: ...})&quot;</span><span class="p">,</span>
                <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Custom preprocessors are deprecated, &quot;</span>
                <span class="s2">&quot;since they sometimes conflict with the built-in &quot;</span>
                <span class="s2">&quot;preprocessors for handling complex observation spaces. &quot;</span>
                <span class="s2">&quot;Please use wrapper classes around your environment &quot;</span>
                <span class="s2">&quot;instead.&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># RLModule API only works with connectors and with Learner API.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_connectors</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_rl_module_api</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;RLModule API only works with connectors. &quot;</span>
                <span class="s2">&quot;Please enable connectors via &quot;</span>
                <span class="s2">&quot;`config.rollouts(enable_connectors=True)`.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Learner API requires RLModule API.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_learner_api</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_rl_module_api</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Learner API requires RLModule API and vice-versa! &quot;</span>
                <span class="s2">&quot;Enable RLModule API via &quot;</span>
                <span class="s2">&quot;`config.rl_module(_enable_rl_module_api=True)` and the Learner API &quot;</span>
                <span class="s2">&quot;via `config.training(_enable_learner_api=True)` (or set both to &quot;</span>
                <span class="s2">&quot;False).&quot;</span>
            <span class="p">)</span>
        <span class="c1"># TODO @Avnishn: This is a short-term work around due to</span>
        <span class="c1"># https://github.com/ray-project/ray/issues/35409</span>
        <span class="c1"># Remove this once we are able to specify placement group bundle index in RLlib</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_cpus_per_learner_worker</span> <span class="o">&gt;</span> <span class="mi">1</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_gpus_per_learner_worker</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot set both `num_cpus_per_learner_worker` and &quot;</span>
                <span class="s2">&quot; `num_gpus_per_learner_worker` &gt; 0! Users must set one&quot;</span>
                <span class="s2">&quot; or the other due to issues with placement group&quot;</span>
                <span class="s2">&quot; fragmentation. See &quot;</span>
                <span class="s2">&quot;https://github.com/ray-project/ray/issues/35409 for more details.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="nb">bool</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;RLLIB_ENABLE_RL_MODULE&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)):</span>
            <span class="c1"># Enable RLModule API and connectors if env variable is set</span>
            <span class="c1"># (to be used in unittesting)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rl_module</span><span class="p">(</span><span class="n">_enable_rl_module_api</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">_enable_learner_api</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">enable_connectors</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># LR-schedule checking.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_learner_api</span><span class="p">:</span>
            <span class="n">Scheduler</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span>
                <span class="n">fixed_value_or_schedule</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span>
                <span class="n">setting_name</span><span class="o">=</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span>
                <span class="n">description</span><span class="o">=</span><span class="s2">&quot;learning rate&quot;</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Validate grad clipping settings.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_clip_by</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">,</span> <span class="s2">&quot;norm&quot;</span><span class="p">,</span> <span class="s2">&quot;global_norm&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`grad_clip_by` (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_clip_by</span><span class="si">}</span><span class="s2">) must be one of: &#39;value&#39;, &quot;</span>
                <span class="s2">&quot;&#39;norm&#39;, or &#39;global_norm&#39;!&quot;</span>
            <span class="p">)</span>

        <span class="c1"># TODO: Deprecate self.simple_optimizer!</span>
        <span class="c1"># Multi-GPU settings.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">simple_optimizer</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="c1"># Multi-GPU setting: Must use MultiGPUTrainOneStep.</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_learner_api</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># TODO: AlphaStar uses &gt;1 GPUs differently (1 per policy actor), so this is</span>
            <span class="c1">#  ok for tf2 here.</span>
            <span class="c1">#  Remove this hacky check, once we have fully moved to the Learner API.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span> <span class="o">==</span> <span class="s2">&quot;tf2&quot;</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">!=</span> <span class="s2">&quot;AlphaStar&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`num_gpus` &gt; 1 not supported yet for &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;framework=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span><span class="si">}</span><span class="s2">!&quot;</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">simple_optimizer</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot use `simple_optimizer` if `num_gpus` &gt; 1! &quot;</span>
                    <span class="s2">&quot;Consider not setting `simple_optimizer` in your config.&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">simple_optimizer</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Auto-setting: Use simple-optimizer for tf-eager or multiagent,</span>
        <span class="c1"># otherwise: MultiGPUTrainOneStep (if supported by the algo&#39;s execution</span>
        <span class="c1"># plan).</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">simple_optimizer</span> <span class="o">==</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="c1"># tf-eager: Must use simple optimizer.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;tf&quot;</span><span class="p">,</span> <span class="s2">&quot;torch&quot;</span><span class="p">]:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">simple_optimizer</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="c1"># Multi-agent case: Try using MultiGPU optimizer (only</span>
            <span class="c1"># if all policies used are DynamicTFPolicies or TorchPolicies).</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_multi_agent</span><span class="p">():</span>
                <span class="kn">from</span> <span class="nn">ray.rllib.policy.dynamic_tf_policy</span> <span class="kn">import</span> <span class="n">DynamicTFPolicy</span>
                <span class="kn">from</span> <span class="nn">ray.rllib.policy.torch_policy</span> <span class="kn">import</span> <span class="n">TorchPolicy</span>

                <span class="n">default_policy_cls</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo_class</span><span class="p">:</span>
                    <span class="n">default_policy_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo_class</span><span class="o">.</span><span class="n">get_default_policy_class</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

                <span class="n">policies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policies</span>
                <span class="n">policy_specs</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">PolicySpec</span><span class="p">(</span><span class="o">*</span><span class="n">spec</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">spec</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="k">else</span> <span class="n">spec</span>
                        <span class="k">for</span> <span class="n">spec</span> <span class="ow">in</span> <span class="n">policies</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
                    <span class="p">]</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">policies</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
                    <span class="k">else</span> <span class="p">[</span><span class="n">PolicySpec</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">policies</span><span class="p">]</span>
                <span class="p">)</span>

                <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
                    <span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">policy_class</span> <span class="ow">or</span> <span class="n">default_policy_cls</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="ow">or</span> <span class="ow">not</span> <span class="nb">issubclass</span><span class="p">(</span>
                        <span class="n">spec</span><span class="o">.</span><span class="n">policy_class</span> <span class="ow">or</span> <span class="n">default_policy_cls</span><span class="p">,</span>
                        <span class="p">(</span><span class="n">DynamicTFPolicy</span><span class="p">,</span> <span class="n">TorchPolicy</span><span class="p">),</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">spec</span> <span class="ow">in</span> <span class="n">policy_specs</span>
                <span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">simple_optimizer</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">simple_optimizer</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">simple_optimizer</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># User manually set simple-optimizer to False -&gt; Error if tf-eager.</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">simple_optimizer</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span> <span class="o">==</span> <span class="s2">&quot;tf2&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`simple_optimizer=False` not supported for &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;config.framework(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span><span class="si">}</span><span class="s2">)!&quot;</span>
                <span class="p">)</span>

        <span class="c1"># Detect if specified env is an Atari env.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_atari</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_atari</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_detect_atari_env</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_</span> <span class="o">==</span> <span class="s2">&quot;sampler&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">off_policy_estimation_methods</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Off-policy estimation methods can only be used if the input is a &quot;</span>
                <span class="s2">&quot;dataset. We currently do not support applying off_policy_esitmation &quot;</span>
                <span class="s2">&quot;method on a sampler input.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_</span> <span class="o">==</span> <span class="s2">&quot;dataset&quot;</span><span class="p">:</span>
            <span class="c1"># if we need to read a ray dataset set the parallelism and</span>
            <span class="c1"># num_cpus_per_read_task from rollout worker settings</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_config</span><span class="p">[</span><span class="s2">&quot;num_cpus_per_read_task&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_cpus_per_worker</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_evaluation</span><span class="p">:</span>
                <span class="c1"># If using dataset for evaluation, the parallelism gets set to</span>
                <span class="c1"># evaluation_num_workers for backward compatibility and num_cpus gets</span>
                <span class="c1"># set to num_cpus_per_worker from rollout worker. User only needs to</span>
                <span class="c1"># set evaluation_num_workers.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">input_config</span><span class="p">[</span><span class="s2">&quot;parallelism&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_num_workers</span> <span class="ow">or</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># If using dataset for training, the parallelism and num_cpus gets set</span>
                <span class="c1"># based on rollout worker parameters. This is for backwards</span>
                <span class="c1"># compatibility for now. User only needs to set num_rollout_workers.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">input_config</span><span class="p">[</span><span class="s2">&quot;parallelism&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_rollout_workers</span> <span class="ow">or</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_rl_module_api</span><span class="p">:</span>
            <span class="n">default_rl_module_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_default_rl_module_spec</span><span class="p">()</span>
            <span class="n">_check_rl_module_spec</span><span class="p">(</span><span class="n">default_rl_module_spec</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rl_module_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Merge provided RL Module spec class with defaults</span>
                <span class="n">_check_rl_module_spec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rl_module_spec</span><span class="p">)</span>
                <span class="c1"># We can only merge if we have SingleAgentRLModuleSpecs.</span>
                <span class="c1"># TODO(Artur): Support merging for MultiAgentRLModuleSpecs.</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rl_module_spec</span><span class="p">,</span> <span class="n">SingleAgentRLModuleSpec</span><span class="p">):</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">default_rl_module_spec</span><span class="p">,</span> <span class="n">SingleAgentRLModuleSpec</span><span class="p">):</span>
                        <span class="n">default_rl_module_spec</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rl_module_spec</span><span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">rl_module_spec</span> <span class="o">=</span> <span class="n">default_rl_module_spec</span>
                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">default_rl_module_spec</span><span class="p">,</span> <span class="n">MultiAgentRLModuleSpec</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;Cannot merge MultiAgentRLModuleSpec with &quot;</span>
                            <span class="s2">&quot;SingleAgentRLModuleSpec!&quot;</span>
                        <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">rl_module_spec</span> <span class="o">=</span> <span class="n">default_rl_module_spec</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploration_config</span><span class="p">:</span>
                <span class="c1"># This is not compatible with RLModules, which have a method</span>
                <span class="c1"># `forward_exploration` to specify custom exploration behavior.</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;When RLModule API are enabled, exploration_config can not be &quot;</span>
                    <span class="s2">&quot;set. If you want to implement custom exploration behaviour, &quot;</span>
                    <span class="s2">&quot;please modify the `forward_exploration` method of the &quot;</span>
                    <span class="s2">&quot;RLModule at hand. On configs that have a default exploration &quot;</span>
                    <span class="s2">&quot;config, this must be done with &quot;</span>
                    <span class="s2">&quot;`config.exploration_config=</span><span class="si">{}</span><span class="s2">`.&quot;</span>
                <span class="p">)</span>

        <span class="c1"># make sure the resource requirements for learner_group is valid</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_learner_workers</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_gpus_per_worker</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;num_gpus_per_worker must be 0 (cpu) or 1 (gpu) when using local mode &quot;</span>
                <span class="s2">&quot;(i.e. num_learner_workers = 0)&quot;</span>
            <span class="p">)</span></div>

<div class="viewcode-block" id="AlgorithmConfig.build"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build">[docs]</a>    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">env</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">EnvType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logger_creator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="n">Logger</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_copy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Algorithm&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Builds an Algorithm from this AlgorithmConfig (or a copy thereof).</span>

<span class="sd">        Args:</span>
<span class="sd">            env: Name of the environment to use (e.g. a gym-registered str),</span>
<span class="sd">                a full class path (e.g.</span>
<span class="sd">                &quot;ray.rllib.examples.env.random_env.RandomEnv&quot;), or an Env</span>
<span class="sd">                class directly. Note that this arg can also be specified via</span>
<span class="sd">                the &quot;env&quot; key in `config`.</span>
<span class="sd">            logger_creator: Callable that creates a ray.tune.Logger</span>
<span class="sd">                object. If unspecified, a default logger is created.</span>
<span class="sd">            use_copy: Whether to deepcopy `self` and pass the copy to the Algorithm</span>
<span class="sd">                (instead of `self`) as config. This is useful in case you would like to</span>
<span class="sd">                recycle the same AlgorithmConfig over and over, e.g. in a test case, in</span>
<span class="sd">                which we loop over different DL-frameworks.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A ray.rllib.algorithms.algorithm.Algorithm object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">[</span><span class="s2">&quot;env&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">env</span>
        <span class="k">if</span> <span class="n">logger_creator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger_creator</span> <span class="o">=</span> <span class="n">logger_creator</span>

        <span class="n">algo_class</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo_class</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">algo_class</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">algo_class</span> <span class="o">=</span> <span class="n">get_trainable_cls</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">algo_class</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">algo_class</span><span class="p">(</span>
            <span class="n">config</span><span class="o">=</span><span class="bp">self</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">use_copy</span> <span class="k">else</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span>
            <span class="n">logger_creator</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logger_creator</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="AlgorithmConfig.python_environment"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.python_environment.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.python_environment">[docs]</a>    <span class="k">def</span> <span class="nf">python_environment</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">extra_python_environs_for_driver</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">extra_python_environs_for_worker</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the config&#39;s python environment settings.</span>

<span class="sd">        Args:</span>
<span class="sd">            extra_python_environs_for_driver: Any extra python env vars to set in the</span>
<span class="sd">                algorithm&#39;s process, e.g., {&quot;OMP_NUM_THREADS&quot;: &quot;16&quot;}.</span>
<span class="sd">            extra_python_environs_for_worker: The extra python environments need to set</span>
<span class="sd">                for worker processes.</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">extra_python_environs_for_driver</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extra_python_environs_for_driver</span> <span class="o">=</span> <span class="n">extra_python_environs_for_driver</span>
        <span class="k">if</span> <span class="n">extra_python_environs_for_worker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extra_python_environs_for_worker</span> <span class="o">=</span> <span class="n">extra_python_environs_for_worker</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="AlgorithmConfig.resources"><a class="viewcode-back" href="../../../../rllib/rllib-training.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.resources">[docs]</a>    <span class="k">def</span> <span class="nf">resources</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">num_gpus</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">_fake_gpus</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">num_cpus_per_worker</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">num_gpus_per_worker</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">num_cpus_for_local_worker</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">num_learner_workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">num_cpus_per_learner_worker</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">num_gpus_per_learner_worker</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">local_gpu_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">custom_resources_per_worker</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">placement_strategy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Specifies resources allocated for an Algorithm and its ray actors/workers.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_gpus: Number of GPUs to allocate to the algorithm process.</span>
<span class="sd">                Note that not all algorithms can take advantage of GPUs.</span>
<span class="sd">                Support for multi-GPU is currently only available for</span>
<span class="sd">                tf-[PPO/IMPALA/DQN/PG]. This can be fractional (e.g., 0.3 GPUs).</span>
<span class="sd">            _fake_gpus: Set to True for debugging (multi-)?GPU funcitonality on a</span>
<span class="sd">                CPU machine. GPU towers will be simulated by graphs located on</span>
<span class="sd">                CPUs in this case. Use `num_gpus` to test for different numbers of</span>
<span class="sd">                fake GPUs.</span>
<span class="sd">            num_cpus_per_worker: Number of CPUs to allocate per worker.</span>
<span class="sd">            num_gpus_per_worker: Number of GPUs to allocate per worker. This can be</span>
<span class="sd">                fractional. This is usually needed only if your env itself requires a</span>
<span class="sd">                GPU (i.e., it is a GPU-intensive video game), or model inference is</span>
<span class="sd">                unusually expensive.</span>
<span class="sd">            num_learner_workers: Number of workers used for training. A value of 0</span>
<span class="sd">                means training will take place on a local worker on head node CPUs or 1</span>
<span class="sd">                GPU (determined by `num_gpus_per_learner_worker`). For multi-gpu</span>
<span class="sd">                training, set number of workers greater than 1 and set</span>
<span class="sd">                `num_gpus_per_learner_worker` accordingly (e.g. 4 GPUs total, and model</span>
<span class="sd">                needs 2 GPUs: `num_learner_workers = 2` and</span>
<span class="sd">                `num_gpus_per_learner_worker = 2`)</span>
<span class="sd">            num_cpus_per_learner_worker: Number of CPUs allocated per trainer worker.</span>
<span class="sd">                Only necessary for custom processing pipeline inside each Learner</span>
<span class="sd">                requiring multiple CPU cores. Ignored if `num_learner_workers = 0`.</span>
<span class="sd">            num_gpus_per_learner_worker: Number of GPUs allocated per worker. If</span>
<span class="sd">                `num_learner_workers = 0`, any value greater than 0 will run the</span>
<span class="sd">                training on a single GPU on the head node, while a value of 0 will run</span>
<span class="sd">                the training on head node CPU cores. If num_gpus_per_learner_worker is</span>
<span class="sd">                set, then num_cpus_per_learner_worker cannot be set.</span>
<span class="sd">            local_gpu_idx: if num_gpus_per_worker &gt; 0, and num_workers&lt;2, then this gpu</span>
<span class="sd">                index will be used for training. This is an index into the available</span>
<span class="sd">                cuda devices. For example if os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;1&quot;</span>
<span class="sd">                then a local_gpu_idx of 0 will use the gpu with id 1 on the node.</span>
<span class="sd">            custom_resources_per_worker: Any custom Ray resources to allocate per</span>
<span class="sd">                worker.</span>
<span class="sd">            num_cpus_for_local_worker: Number of CPUs to allocate for the algorithm.</span>
<span class="sd">                Note: this only takes effect when running in Tune. Otherwise,</span>
<span class="sd">                the algorithm runs in the main program (driver).</span>
<span class="sd">            custom_resources_per_worker: Any custom Ray resources to allocate per</span>
<span class="sd">                worker.</span>
<span class="sd">            placement_strategy: The strategy for the placement group factory returned by</span>
<span class="sd">                `Algorithm.default_resource_request()`. A PlacementGroup defines, which</span>
<span class="sd">                devices (resources) should always be co-located on the same node.</span>
<span class="sd">                For example, an Algorithm with 2 rollout workers, running with</span>
<span class="sd">                num_gpus=1 will request a placement group with the bundles:</span>
<span class="sd">                [{&quot;gpu&quot;: 1, &quot;cpu&quot;: 1}, {&quot;cpu&quot;: 1}, {&quot;cpu&quot;: 1}], where the first bundle</span>
<span class="sd">                is for the driver and the other 2 bundles are for the two workers.</span>
<span class="sd">                These bundles can now be &quot;placed&quot; on the same or different</span>
<span class="sd">                nodes depending on the value of `placement_strategy`:</span>
<span class="sd">                &quot;PACK&quot;: Packs bundles into as few nodes as possible.</span>
<span class="sd">                &quot;SPREAD&quot;: Places bundles across distinct nodes as even as possible.</span>
<span class="sd">                &quot;STRICT_PACK&quot;: Packs bundles into one node. The group is not allowed</span>
<span class="sd">                to span multiple nodes.</span>
<span class="sd">                &quot;STRICT_SPREAD&quot;: Packs bundles across distinct nodes.</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">num_gpus</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">=</span> <span class="n">num_gpus</span>
        <span class="k">if</span> <span class="n">_fake_gpus</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fake_gpus</span> <span class="o">=</span> <span class="n">_fake_gpus</span>
        <span class="k">if</span> <span class="n">num_cpus_per_worker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_cpus_per_worker</span> <span class="o">=</span> <span class="n">num_cpus_per_worker</span>
        <span class="k">if</span> <span class="n">num_gpus_per_worker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_gpus_per_worker</span> <span class="o">=</span> <span class="n">num_gpus_per_worker</span>
        <span class="k">if</span> <span class="n">num_cpus_for_local_worker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_cpus_for_local_worker</span> <span class="o">=</span> <span class="n">num_cpus_for_local_worker</span>
        <span class="k">if</span> <span class="n">custom_resources_per_worker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">custom_resources_per_worker</span> <span class="o">=</span> <span class="n">custom_resources_per_worker</span>
        <span class="k">if</span> <span class="n">placement_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">placement_strategy</span> <span class="o">=</span> <span class="n">placement_strategy</span>

        <span class="k">if</span> <span class="n">num_learner_workers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_learner_workers</span> <span class="o">=</span> <span class="n">num_learner_workers</span>
        <span class="k">if</span> <span class="n">num_cpus_per_learner_worker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_cpus_per_learner_worker</span> <span class="o">=</span> <span class="n">num_cpus_per_learner_worker</span>
        <span class="k">if</span> <span class="n">num_gpus_per_learner_worker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_gpus_per_learner_worker</span> <span class="o">=</span> <span class="n">num_gpus_per_learner_worker</span>
        <span class="k">if</span> <span class="n">local_gpu_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">local_gpu_idx</span> <span class="o">=</span> <span class="n">local_gpu_idx</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="AlgorithmConfig.framework"><a class="viewcode-back" href="../../../../rllib/rllib-training.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.framework">[docs]</a>    <span class="k">def</span> <span class="nf">framework</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">framework</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">eager_tracing</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">eager_max_retraces</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">tf_session_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">local_tf_session_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">torch_compile_learner</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">torch_compile_learner_dynamo_mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">torch_compile_learner_dynamo_backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">torch_compile_worker</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">torch_compile_worker_dynamo_backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">torch_compile_worker_dynamo_mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the config&#39;s DL framework settings.</span>

<span class="sd">        Args:</span>
<span class="sd">            framework: torch: PyTorch; tf2: TensorFlow 2.x (eager execution or traced</span>
<span class="sd">                if eager_tracing=True); tf: TensorFlow (static-graph);</span>
<span class="sd">            eager_tracing: Enable tracing in eager mode. This greatly improves</span>
<span class="sd">                performance (speedup ~2x), but makes it slightly harder to debug</span>
<span class="sd">                since Python code won&#39;t be evaluated after the initial eager pass.</span>
<span class="sd">                Only possible if framework=tf2.</span>
<span class="sd">            eager_max_retraces: Maximum number of tf.function re-traces before a</span>
<span class="sd">                runtime error is raised. This is to prevent unnoticed retraces of</span>
<span class="sd">                methods inside the `..._eager_traced` Policy, which could slow down</span>
<span class="sd">                execution by a factor of 4, without the user noticing what the root</span>
<span class="sd">                cause for this slowdown could be.</span>
<span class="sd">                Only necessary for framework=tf2.</span>
<span class="sd">                Set to None to ignore the re-trace count and never throw an error.</span>
<span class="sd">            tf_session_args: Configures TF for single-process operation by default.</span>
<span class="sd">            local_tf_session_args: Override the following tf session args on the local</span>
<span class="sd">                worker</span>
<span class="sd">            torch_compile_learner: If True, forward_train methods on TorchRLModules</span>
<span class="sd">            on the learner are compiled. If not specified, the default is to compile</span>
<span class="sd">            forward train on the learner.</span>
<span class="sd">            torch_compile_learner_dynamo_backend: The torch dynamo backend to use on</span>
<span class="sd">                the learner.</span>
<span class="sd">            torch_compile_learner_dynamo_mode: The torch dynamo mode to use on the</span>
<span class="sd">                learner.</span>
<span class="sd">            torch_compile_worker: If True, forward exploration and inference methods on</span>
<span class="sd">                TorchRLModules on the workers are compiled. If not specified,</span>
<span class="sd">                the default is to not compile forward methods on the workers because</span>
<span class="sd">                retracing can be expensive.</span>
<span class="sd">            torch_compile_worker_dynamo_backend: The torch dynamo backend to use on</span>
<span class="sd">                the workers.</span>
<span class="sd">            torch_compile_worker_dynamo_mode: The torch dynamo mode to use on the</span>
<span class="sd">                workers.</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">framework</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">framework</span> <span class="o">==</span> <span class="s2">&quot;tfe&quot;</span><span class="p">:</span>
                <span class="n">deprecation_warning</span><span class="p">(</span>
                    <span class="n">old</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.framework(&#39;tfe&#39;)&quot;</span><span class="p">,</span>
                    <span class="n">new</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.framework(&#39;tf2&#39;)&quot;</span><span class="p">,</span>
                    <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span> <span class="o">=</span> <span class="n">framework</span>
        <span class="k">if</span> <span class="n">eager_tracing</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eager_tracing</span> <span class="o">=</span> <span class="n">eager_tracing</span>
        <span class="k">if</span> <span class="n">eager_max_retraces</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eager_max_retraces</span> <span class="o">=</span> <span class="n">eager_max_retraces</span>
        <span class="k">if</span> <span class="n">tf_session_args</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tf_session_args</span> <span class="o">=</span> <span class="n">tf_session_args</span>
        <span class="k">if</span> <span class="n">local_tf_session_args</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">local_tf_session_args</span> <span class="o">=</span> <span class="n">local_tf_session_args</span>

        <span class="k">if</span> <span class="n">torch_compile_learner</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_learner</span> <span class="o">=</span> <span class="n">torch_compile_learner</span>
        <span class="k">if</span> <span class="n">torch_compile_learner_dynamo_backend</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_learner_dynamo_backend</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch_compile_learner_dynamo_backend</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">torch_compile_learner_dynamo_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_learner_dynamo_mode</span> <span class="o">=</span> <span class="n">torch_compile_learner_dynamo_mode</span>
        <span class="k">if</span> <span class="n">torch_compile_worker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_worker</span> <span class="o">=</span> <span class="n">torch_compile_worker</span>
        <span class="k">if</span> <span class="n">torch_compile_worker_dynamo_backend</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_worker_dynamo_backend</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch_compile_worker_dynamo_backend</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">torch_compile_worker_dynamo_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_worker_dynamo_mode</span> <span class="o">=</span> <span class="n">torch_compile_worker_dynamo_mode</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="AlgorithmConfig.environment"><a class="viewcode-back" href="../../../../rllib/rllib-training.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.environment">[docs]</a>    <span class="k">def</span> <span class="nf">environment</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">env</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">EnvType</span><span class="p">]]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">env_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EnvConfigDict</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">observation_space</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">env_task_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Callable</span><span class="p">[[</span><span class="n">ResultDict</span><span class="p">,</span> <span class="n">EnvType</span><span class="p">,</span> <span class="n">EnvContext</span><span class="p">],</span> <span class="n">Any</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">render_env</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">clip_rewards</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">normalize_actions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">clip_actions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">disable_env_checking</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">is_atari</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">auto_wrap_old_gym_envs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">action_mask_key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the config&#39;s RL-environment settings.</span>

<span class="sd">        Args:</span>
<span class="sd">            env: The environment specifier. This can either be a tune-registered env,</span>
<span class="sd">                via `tune.register_env([name], lambda env_ctx: [env object])`,</span>
<span class="sd">                or a string specifier of an RLlib supported type. In the latter case,</span>
<span class="sd">                RLlib will try to interpret the specifier as either an Farama-Foundation</span>
<span class="sd">                gymnasium env, a PyBullet env, a ViZDoomGym env, or a fully qualified</span>
<span class="sd">                classpath to an Env class, e.g.</span>
<span class="sd">                &quot;ray.rllib.examples.env.random_env.RandomEnv&quot;.</span>
<span class="sd">            env_config: Arguments dict passed to the env creator as an EnvContext</span>
<span class="sd">                object (which is a dict plus the properties: num_rollout_workers,</span>
<span class="sd">                worker_index, vector_index, and remote).</span>
<span class="sd">            observation_space: The observation space for the Policies of this Algorithm.</span>
<span class="sd">            action_space: The action space for the Policies of this Algorithm.</span>
<span class="sd">            env_task_fn: A callable taking the last train results, the base env and the</span>
<span class="sd">                env context as args and returning a new task to set the env to.</span>
<span class="sd">                The env must be a `TaskSettableEnv` sub-class for this to work.</span>
<span class="sd">                See `examples/curriculum_learning.py` for an example.</span>
<span class="sd">            render_env: If True, try to render the environment on the local worker or on</span>
<span class="sd">                worker 1 (if num_rollout_workers &gt; 0). For vectorized envs, this usually</span>
<span class="sd">                means that only the first sub-environment will be rendered.</span>
<span class="sd">                In order for this to work, your env will have to implement the</span>
<span class="sd">                `render()` method which either:</span>
<span class="sd">                a) handles window generation and rendering itself (returning True) or</span>
<span class="sd">                b) returns a numpy uint8 image of shape [height x width x 3 (RGB)].</span>
<span class="sd">            clip_rewards: Whether to clip rewards during Policy&#39;s postprocessing.</span>
<span class="sd">                None (default): Clip for Atari only (r=sign(r)).</span>
<span class="sd">                True: r=sign(r): Fixed rewards -1.0, 1.0, or 0.0.</span>
<span class="sd">                False: Never clip.</span>
<span class="sd">                [float value]: Clip at -value and + value.</span>
<span class="sd">                Tuple[value1, value2]: Clip at value1 and value2.</span>
<span class="sd">            normalize_actions: If True, RLlib will learn entirely inside a normalized</span>
<span class="sd">                action space (0.0 centered with small stddev; only affecting Box</span>
<span class="sd">                components). We will unsquash actions (and clip, just in case) to the</span>
<span class="sd">                bounds of the env&#39;s action space before sending actions back to the env.</span>
<span class="sd">            clip_actions: If True, RLlib will clip actions according to the env&#39;s bounds</span>
<span class="sd">                before sending them back to the env.</span>
<span class="sd">                TODO: (sven) This option should be deprecated and always be False.</span>
<span class="sd">            disable_env_checking: If True, disable the environment pre-checking module.</span>
<span class="sd">            is_atari: This config can be used to explicitly specify whether the env is</span>
<span class="sd">                an Atari env or not. If not specified, RLlib will try to auto-detect</span>
<span class="sd">                this during config validation.</span>
<span class="sd">            auto_wrap_old_gym_envs: Whether to auto-wrap old gym environments (using</span>
<span class="sd">                the pre 0.24 gym APIs, e.g. reset() returning single obs and no info</span>
<span class="sd">                dict). If True, RLlib will automatically wrap the given gym env class</span>
<span class="sd">                with the gym-provided compatibility wrapper</span>
<span class="sd">                (gym.wrappers.EnvCompatibility). If False, RLlib will produce a</span>
<span class="sd">                descriptive error on which steps to perform to upgrade to gymnasium</span>
<span class="sd">                (or to switch this flag to True).</span>
<span class="sd">             action_mask_key: If observation is a dictionary, expect the value by</span>
<span class="sd">                the key `action_mask_key` to contain a valid actions mask (`numpy.int8`</span>
<span class="sd">                array of zeros and ones). Defaults to &quot;action_mask&quot;.</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span>
        <span class="k">if</span> <span class="n">env_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="n">deep_update</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_config</span><span class="p">,</span>
                <span class="n">env_config</span><span class="p">,</span>
                <span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">observation_space</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">observation_space</span>
        <span class="k">if</span> <span class="n">action_space</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">action_space</span>
        <span class="k">if</span> <span class="n">env_task_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_task_fn</span> <span class="o">=</span> <span class="n">env_task_fn</span>
        <span class="k">if</span> <span class="n">render_env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">render_env</span> <span class="o">=</span> <span class="n">render_env</span>
        <span class="k">if</span> <span class="n">clip_rewards</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">clip_rewards</span> <span class="o">=</span> <span class="n">clip_rewards</span>
        <span class="k">if</span> <span class="n">normalize_actions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">normalize_actions</span> <span class="o">=</span> <span class="n">normalize_actions</span>
        <span class="k">if</span> <span class="n">clip_actions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">clip_actions</span> <span class="o">=</span> <span class="n">clip_actions</span>
        <span class="k">if</span> <span class="n">disable_env_checking</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">disable_env_checking</span> <span class="o">=</span> <span class="n">disable_env_checking</span>
        <span class="k">if</span> <span class="n">is_atari</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_atari</span> <span class="o">=</span> <span class="n">is_atari</span>
        <span class="k">if</span> <span class="n">auto_wrap_old_gym_envs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">auto_wrap_old_gym_envs</span> <span class="o">=</span> <span class="n">auto_wrap_old_gym_envs</span>
        <span class="k">if</span> <span class="n">action_mask_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">action_mask_key</span> <span class="o">=</span> <span class="n">action_mask_key</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="AlgorithmConfig.rollouts"><a class="viewcode-back" href="../../../../rllib/rllib-training.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rollouts">[docs]</a>    <span class="k">def</span> <span class="nf">rollouts</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">env_runner_cls</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">type</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">num_rollout_workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">num_envs_per_worker</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">create_env_on_local_worker</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">sample_collector</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="n">SampleCollector</span><span class="p">]]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">sample_async</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">enable_connectors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">use_worker_filter_stats</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">update_worker_filter_stats</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">rollout_fragment_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">batch_mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">remote_worker_envs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">remote_env_batch_wait_ms</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">validate_workers_after_construction</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">preprocessor_pref</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">observation_filter</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">compress_observations</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">enable_tf1_exec_eagerly</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">sampler_perf_stats_ema_coef</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">ignore_worker_failures</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
        <span class="n">recreate_failed_workers</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
        <span class="n">restart_failed_sub_environments</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
        <span class="n">num_consecutive_worker_failures_tolerance</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
        <span class="n">worker_health_probe_timeout_s</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
        <span class="n">worker_restore_timeout_s</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
        <span class="n">synchronize_filter</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the rollout worker configuration.</span>

<span class="sd">        Args:</span>
<span class="sd">            env_runner_cls: The EnvRunner class to use for environment rollouts (data</span>
<span class="sd">                collection).</span>
<span class="sd">            num_rollout_workers: Number of rollout worker actors to create for</span>
<span class="sd">                parallel sampling. Setting this to 0 will force rollouts to be done in</span>
<span class="sd">                the local worker (driver process or the Algorithm&#39;s actor when using</span>
<span class="sd">                Tune).</span>
<span class="sd">            num_envs_per_worker: Number of environments to evaluate vector-wise per</span>
<span class="sd">                worker. This enables model inference batching, which can improve</span>
<span class="sd">                performance for inference bottlenecked workloads.</span>
<span class="sd">            sample_collector: The SampleCollector class to be used to collect and</span>
<span class="sd">                retrieve environment-, model-, and sampler data. Override the</span>
<span class="sd">                SampleCollector base class to implement your own</span>
<span class="sd">                collection/buffering/retrieval logic.</span>
<span class="sd">            create_env_on_local_worker: When `num_rollout_workers` &gt; 0, the driver</span>
<span class="sd">                (local_worker; worker-idx=0) does not need an environment. This is</span>
<span class="sd">                because it doesn&#39;t have to sample (done by remote_workers;</span>
<span class="sd">                worker_indices &gt; 0) nor evaluate (done by evaluation workers;</span>
<span class="sd">                see below).</span>
<span class="sd">            sample_async: Use a background thread for sampling (slightly off-policy,</span>
<span class="sd">                usually not advisable to turn on unless your env specifically requires</span>
<span class="sd">                it).</span>
<span class="sd">            enable_connectors: Use connector based environment runner, so that all</span>
<span class="sd">                preprocessing of obs and postprocessing of actions are done in agent</span>
<span class="sd">                and action connectors.</span>
<span class="sd">            use_worker_filter_stats: Whether to use the workers in the WorkerSet to</span>
<span class="sd">                update the central filters (held by the local worker). If False, stats</span>
<span class="sd">                from the workers will not be used and discarded.</span>
<span class="sd">            update_worker_filter_stats: Whether to push filter updates from the central</span>
<span class="sd">                filters (held by the local worker) to the remote workers&#39; filters.</span>
<span class="sd">                Setting this to True might be useful within the evaluation config in</span>
<span class="sd">                order to disable the usage of evaluation trajectories for synching</span>
<span class="sd">                the central filter (used for training).</span>
<span class="sd">            rollout_fragment_length: Divide episodes into fragments of this many steps</span>
<span class="sd">                each during rollouts. Trajectories of this size are collected from</span>
<span class="sd">                rollout workers and combined into a larger batch of `train_batch_size`</span>
<span class="sd">                for learning.</span>
<span class="sd">                For example, given rollout_fragment_length=100 and</span>
<span class="sd">                train_batch_size=1000:</span>
<span class="sd">                1. RLlib collects 10 fragments of 100 steps each from rollout workers.</span>
<span class="sd">                2. These fragments are concatenated and we perform an epoch of SGD.</span>
<span class="sd">                When using multiple envs per worker, the fragment size is multiplied by</span>
<span class="sd">                `num_envs_per_worker`. This is since we are collecting steps from</span>
<span class="sd">                multiple envs in parallel. For example, if num_envs_per_worker=5, then</span>
<span class="sd">                rollout workers will return experiences in chunks of 5*100 = 500 steps.</span>
<span class="sd">                The dataflow here can vary per algorithm. For example, PPO further</span>
<span class="sd">                divides the train batch into minibatches for multi-epoch SGD.</span>
<span class="sd">                Set to &quot;auto&quot; to have RLlib compute an exact `rollout_fragment_length`</span>
<span class="sd">                to match the given batch size.</span>
<span class="sd">            batch_mode: How to build individual batches with the EnvRunner(s). Batches</span>
<span class="sd">                coming from distributed EnvRunners are usually concat&#39;d to form the</span>
<span class="sd">                train batch. Note that &quot;steps&quot; below can mean different things (either</span>
<span class="sd">                env- or agent-steps) and depends on the `count_steps_by` setting,</span>
<span class="sd">                adjustable via `AlgorithmConfig.multi_agent(count_steps_by=..)`:</span>
<span class="sd">                1) &quot;truncate_episodes&quot;: Each call to `EnvRunner.sample()` will return a</span>
<span class="sd">                batch of at most `rollout_fragment_length * num_envs_per_worker` in</span>
<span class="sd">                size. The batch will be exactly `rollout_fragment_length * num_envs`</span>
<span class="sd">                in size if postprocessing does not change batch sizes. Episodes</span>
<span class="sd">                may be truncated in order to meet this size requirement.</span>
<span class="sd">                This mode guarantees evenly sized batches, but increases</span>
<span class="sd">                variance as the future return must now be estimated at truncation</span>
<span class="sd">                boundaries.</span>
<span class="sd">                2) &quot;complete_episodes&quot;: Each call to `EnvRunner.sample()` will return a</span>
<span class="sd">                batch of at least `rollout_fragment_length * num_envs_per_worker` in</span>
<span class="sd">                size. Episodes will not be truncated, but multiple episodes</span>
<span class="sd">                may be packed within one batch to meet the (minimum) batch size.</span>
<span class="sd">                Note that when `num_envs_per_worker &gt; 1`, episode steps will be buffered</span>
<span class="sd">                until the episode completes, and hence batches may contain</span>
<span class="sd">                significant amounts of off-policy data.</span>
<span class="sd">            remote_worker_envs: If using num_envs_per_worker &gt; 1, whether to create</span>
<span class="sd">                those new envs in remote processes instead of in the same worker.</span>
<span class="sd">                This adds overheads, but can make sense if your envs can take much</span>
<span class="sd">                time to step / reset (e.g., for StarCraft). Use this cautiously;</span>
<span class="sd">                overheads are significant.</span>
<span class="sd">            remote_env_batch_wait_ms: Timeout that remote workers are waiting when</span>
<span class="sd">                polling environments. 0 (continue when at least one env is ready) is</span>
<span class="sd">                a reasonable default, but optimal value could be obtained by measuring</span>
<span class="sd">                your environment step / reset and model inference perf.</span>
<span class="sd">            validate_workers_after_construction: Whether to validate that each created</span>
<span class="sd">                remote worker is healthy after its construction process.</span>
<span class="sd">            preprocessor_pref: Whether to use &quot;rllib&quot; or &quot;deepmind&quot; preprocessors by</span>
<span class="sd">                default. Set to None for using no preprocessor. In this case, the</span>
<span class="sd">                model will have to handle possibly complex observations from the</span>
<span class="sd">                environment.</span>
<span class="sd">            observation_filter: Element-wise observation filter, either &quot;NoFilter&quot;</span>
<span class="sd">                or &quot;MeanStdFilter&quot;.</span>
<span class="sd">            compress_observations: Whether to LZ4 compress individual observations</span>
<span class="sd">                in the SampleBatches collected during rollouts.</span>
<span class="sd">            enable_tf1_exec_eagerly: Explicitly tells the rollout worker to enable</span>
<span class="sd">                TF eager execution. This is useful for example when framework is</span>
<span class="sd">                &quot;torch&quot;, but a TF2 policy needs to be restored for evaluation or</span>
<span class="sd">                league-based purposes.</span>
<span class="sd">            sampler_perf_stats_ema_coef: If specified, perf stats are in EMAs. This</span>
<span class="sd">                is the coeff of how much new data points contribute to the averages.</span>
<span class="sd">                Default is None, which uses simple global average instead.</span>
<span class="sd">                The EMA update rule is: updated = (1 - ema_coef) * old + ema_coef * new</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">env_runner_cls</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_cls</span> <span class="o">=</span> <span class="n">env_runner_cls</span>
        <span class="k">if</span> <span class="n">num_rollout_workers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_rollout_workers</span> <span class="o">=</span> <span class="n">num_rollout_workers</span>
        <span class="k">if</span> <span class="n">num_envs_per_worker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_envs_per_worker</span> <span class="o">=</span> <span class="n">num_envs_per_worker</span>
        <span class="k">if</span> <span class="n">sample_collector</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sample_collector</span> <span class="o">=</span> <span class="n">sample_collector</span>
        <span class="k">if</span> <span class="n">create_env_on_local_worker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">create_env_on_local_worker</span> <span class="o">=</span> <span class="n">create_env_on_local_worker</span>
        <span class="k">if</span> <span class="n">sample_async</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sample_async</span> <span class="o">=</span> <span class="n">sample_async</span>
        <span class="k">if</span> <span class="n">enable_connectors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">enable_connectors</span> <span class="o">=</span> <span class="n">enable_connectors</span>
        <span class="k">if</span> <span class="n">use_worker_filter_stats</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_worker_filter_stats</span> <span class="o">=</span> <span class="n">use_worker_filter_stats</span>
        <span class="k">if</span> <span class="n">update_worker_filter_stats</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_worker_filter_stats</span> <span class="o">=</span> <span class="n">update_worker_filter_stats</span>
        <span class="k">if</span> <span class="n">rollout_fragment_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rollout_fragment_length</span> <span class="o">=</span> <span class="n">rollout_fragment_length</span>
        <span class="k">if</span> <span class="n">batch_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_mode</span> <span class="o">=</span> <span class="n">batch_mode</span>
        <span class="k">if</span> <span class="n">remote_worker_envs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">remote_worker_envs</span> <span class="o">=</span> <span class="n">remote_worker_envs</span>
        <span class="k">if</span> <span class="n">remote_env_batch_wait_ms</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">remote_env_batch_wait_ms</span> <span class="o">=</span> <span class="n">remote_env_batch_wait_ms</span>
        <span class="k">if</span> <span class="n">validate_workers_after_construction</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">validate_workers_after_construction</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">validate_workers_after_construction</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">preprocessor_pref</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor_pref</span> <span class="o">=</span> <span class="n">preprocessor_pref</span>
        <span class="k">if</span> <span class="n">observation_filter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">observation_filter</span> <span class="o">=</span> <span class="n">observation_filter</span>
        <span class="k">if</span> <span class="n">synchronize_filter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">synchronize_filters</span> <span class="o">=</span> <span class="n">synchronize_filter</span>
        <span class="k">if</span> <span class="n">compress_observations</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compress_observations</span> <span class="o">=</span> <span class="n">compress_observations</span>
        <span class="k">if</span> <span class="n">enable_tf1_exec_eagerly</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">enable_tf1_exec_eagerly</span> <span class="o">=</span> <span class="n">enable_tf1_exec_eagerly</span>
        <span class="k">if</span> <span class="n">sampler_perf_stats_ema_coef</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sampler_perf_stats_ema_coef</span> <span class="o">=</span> <span class="n">sampler_perf_stats_ema_coef</span>

        <span class="c1"># Deprecated settings.</span>
        <span class="k">if</span> <span class="n">synchronize_filter</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.rollouts(synchronize_filter=..)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.rollouts(update_worker_filter_stats=..)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_worker_filter_stats</span> <span class="o">=</span> <span class="n">synchronize_filter</span>
        <span class="k">if</span> <span class="n">ignore_worker_failures</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;ignore_worker_failures is deprecated, and will soon be a no-op&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ignore_worker_failures</span> <span class="o">=</span> <span class="n">ignore_worker_failures</span>
        <span class="k">if</span> <span class="n">recreate_failed_workers</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.rollouts(recreate_failed_workers=..)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.fault_tolerance(recreate_failed_workers=..)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">recreate_failed_workers</span> <span class="o">=</span> <span class="n">recreate_failed_workers</span>
        <span class="k">if</span> <span class="n">restart_failed_sub_environments</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.rollouts(restart_failed_sub_environments=..)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="p">(</span>
                    <span class="s2">&quot;AlgorithmConfig.fault_tolerance(&quot;</span>
                    <span class="s2">&quot;restart_failed_sub_environments=..)&quot;</span>
                <span class="p">),</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">restart_failed_sub_environments</span> <span class="o">=</span> <span class="n">restart_failed_sub_environments</span>
        <span class="k">if</span> <span class="n">num_consecutive_worker_failures_tolerance</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="p">(</span>
                    <span class="s2">&quot;AlgorithmConfig.rollouts(&quot;</span>
                    <span class="s2">&quot;num_consecutive_worker_failures_tolerance=..)&quot;</span>
                <span class="p">),</span>
                <span class="n">new</span><span class="o">=</span><span class="p">(</span>
                    <span class="s2">&quot;AlgorithmConfig.fault_tolerance(&quot;</span>
                    <span class="s2">&quot;num_consecutive_worker_failures_tolerance=..)&quot;</span>
                <span class="p">),</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_consecutive_worker_failures_tolerance</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">num_consecutive_worker_failures_tolerance</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">worker_health_probe_timeout_s</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.rollouts(worker_health_probe_timeout_s=..)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.fault_tolerance(worker_health_probe_timeout_s=..)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">worker_health_probe_timeout_s</span> <span class="o">=</span> <span class="n">worker_health_probe_timeout_s</span>
        <span class="k">if</span> <span class="n">worker_restore_timeout_s</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.rollouts(worker_restore_timeout_s=..)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.fault_tolerance(worker_restore_timeout_s=..)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">worker_restore_timeout_s</span> <span class="o">=</span> <span class="n">worker_restore_timeout_s</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="AlgorithmConfig.training"><a class="viewcode-back" href="../../../../rllib/rllib-training.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.training">[docs]</a>    <span class="k">def</span> <span class="nf">training</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">lr</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LearningRateOrSchedule</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">grad_clip</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">grad_clip_by</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">max_requests_in_flight_per_sampler_worker</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">_enable_learner_api</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">learner_class</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="s2">&quot;Learner&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the training related configuration.</span>

<span class="sd">        Args:</span>
<span class="sd">            gamma: Float specifying the discount factor of the Markov Decision process.</span>
<span class="sd">            lr: The learning rate (float) or learning rate schedule in the format of</span>
<span class="sd">                [[timestep, lr-value], [timestep, lr-value], ...]</span>
<span class="sd">                In case of a schedule, intermediary timesteps will be assigned to</span>
<span class="sd">                linearly interpolated learning rate values. A schedule config&#39;s first</span>
<span class="sd">                entry must start with timestep 0, i.e.: [[0, initial_value], [...]].</span>
<span class="sd">                Note: If you require a) more than one optimizer (per RLModule),</span>
<span class="sd">                b) optimizer types that are not Adam, c) a learning rate schedule that</span>
<span class="sd">                is not a linearly interpolated, piecewise schedule as described above,</span>
<span class="sd">                or d) specifying c&#39;tor arguments of the optimizer that are not the</span>
<span class="sd">                learning rate (e.g. Adam&#39;s epsilon), then you must override your</span>
<span class="sd">                Learner&#39;s `configure_optimizer_for_module()` method and handle</span>
<span class="sd">                lr-scheduling yourself.</span>
<span class="sd">            grad_clip: If None, no gradient clipping will be applied. Otherwise,</span>
<span class="sd">                depending on the setting of `grad_clip_by`, the (float) value of</span>
<span class="sd">                `grad_clip` will have the following effect:</span>
<span class="sd">                If `grad_clip_by=value`: Will clip all computed gradients individually</span>
<span class="sd">                inside the interval [-`grad_clip`, +`grad_clip`].</span>
<span class="sd">                If `grad_clip_by=norm`, will compute the L2-norm of each weight/bias</span>
<span class="sd">                gradient tensor individually and then clip all gradients such that these</span>
<span class="sd">                L2-norms do not exceed `grad_clip`. The L2-norm of a tensor is computed</span>
<span class="sd">                via: `sqrt(SUM(w0^2, w1^2, ..., wn^2))` where w[i] are the elements of</span>
<span class="sd">                the tensor (no matter what the shape of this tensor is).</span>
<span class="sd">                If `grad_clip_by=global_norm`, will compute the square of the L2-norm of</span>
<span class="sd">                each weight/bias gradient tensor individually, sum up all these squared</span>
<span class="sd">                L2-norms across all given gradient tensors (e.g. the entire module to</span>
<span class="sd">                be updated), square root that overall sum, and then clip all gradients</span>
<span class="sd">                such that this global L2-norm does not exceed the given value.</span>
<span class="sd">                The global L2-norm over a list of tensors (e.g. W and V) is computed</span>
<span class="sd">                via:</span>
<span class="sd">                `sqrt[SUM(w0^2, w1^2, ..., wn^2) + SUM(v0^2, v1^2, ..., vm^2)]`, where</span>
<span class="sd">                w[i] and v[j] are the elements of the tensors W and V (no matter what</span>
<span class="sd">                the shapes of these tensors are).</span>
<span class="sd">            grad_clip_by: See `grad_clip` for the effect of this setting on gradient</span>
<span class="sd">                clipping. Allowed values are `value`, `norm`, and `global_norm`.</span>
<span class="sd">            train_batch_size: Training batch size, if applicable.</span>
<span class="sd">            model: Arguments passed into the policy model. See models/catalog.py for a</span>
<span class="sd">                full list of the available model options.</span>
<span class="sd">                TODO: Provide ModelConfig objects instead of dicts.</span>
<span class="sd">            optimizer: Arguments to pass to the policy optimizer. This setting is not</span>
<span class="sd">                used when `_enable_learner_api=True`.</span>
<span class="sd">            max_requests_in_flight_per_sampler_worker: Max number of inflight requests</span>
<span class="sd">                to each sampling worker. See the FaultTolerantActorManager class for</span>
<span class="sd">                more details.</span>
<span class="sd">                Tuning these values is important when running experimens with</span>
<span class="sd">                large sample batches, where there is the risk that the object store may</span>
<span class="sd">                fill up, causing spilling of objects to disk. This can cause any</span>
<span class="sd">                asynchronous requests to become very slow, making your experiment run</span>
<span class="sd">                slow as well. You can inspect the object store during your experiment</span>
<span class="sd">                via a call to ray memory on your headnode, and by using the ray</span>
<span class="sd">                dashboard. If you&#39;re seeing that the object store is filling up,</span>
<span class="sd">                turn down the number of remote requests in flight, or enable compression</span>
<span class="sd">                in your experiment of timesteps.</span>
<span class="sd">            _enable_learner_api: Whether to enable the LearnerGroup and Learner</span>
<span class="sd">                for training. This API uses ray.train to run the training loop which</span>
<span class="sd">                allows for a more flexible distributed training.</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="k">if</span> <span class="n">lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="k">if</span> <span class="n">grad_clip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_clip</span> <span class="o">=</span> <span class="n">grad_clip</span>
        <span class="k">if</span> <span class="n">grad_clip_by</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_clip_by</span> <span class="o">=</span> <span class="n">grad_clip_by</span>
        <span class="k">if</span> <span class="n">train_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span> <span class="o">=</span> <span class="n">train_batch_size</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="c1"># Validate prev_a/r settings.</span>
            <span class="n">prev_a_r</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;lstm_use_prev_action_reward&quot;</span><span class="p">,</span> <span class="n">DEPRECATED_VALUE</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">prev_a_r</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
                <span class="n">deprecation_warning</span><span class="p">(</span>
                    <span class="s2">&quot;model.lstm_use_prev_action_reward&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;model.lstm_use_prev_action and model.lstm_use_prev_reward&quot;</span><span class="p">,</span>
                    <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">model</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_use_default_native_models&quot;</span><span class="p">,</span> <span class="n">DEPRECATED_VALUE</span><span class="p">)</span>
                <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span>
            <span class="p">):</span>
                <span class="n">deprecation_warning</span><span class="p">(</span>
                    <span class="n">old</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.training(_use_default_native_models=True)&quot;</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s2">&quot;_use_default_native_models is not supported &quot;</span>
                    <span class="s2">&quot;anymore. To get rid of this error, set `rl_module(&quot;</span>
                    <span class="s2">&quot;_enable_rl_module_api` to True. Native models will &quot;</span>
                    <span class="s2">&quot;be better supported by the upcoming RLModule API.&quot;</span><span class="p">,</span>
                    <span class="c1"># Error out if user tries to enable this</span>
                    <span class="n">error</span><span class="o">=</span><span class="n">model</span><span class="p">[</span><span class="s2">&quot;_use_default_native_models&quot;</span><span class="p">],</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">merge_dicts</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">max_requests_in_flight_per_sampler_worker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_requests_in_flight_per_sampler_worker</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">max_requests_in_flight_per_sampler_worker</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">_enable_learner_api</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_enable_learner_api</span> <span class="o">=</span> <span class="n">_enable_learner_api</span>
        <span class="k">if</span> <span class="n">learner_class</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_learner_class</span> <span class="o">=</span> <span class="n">learner_class</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="AlgorithmConfig.callbacks"><a class="viewcode-back" href="../../../../rllib/rllib-training.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.callbacks">[docs]</a>    <span class="k">def</span> <span class="nf">callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">callbacks_class</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the callbacks configuration.</span>

<span class="sd">        Args:</span>
<span class="sd">            callbacks_class: Callbacks class, whose methods will be run during</span>
<span class="sd">                various phases of training and environment sample collection.</span>
<span class="sd">                See the `DefaultCallbacks` class and</span>
<span class="sd">                `examples/custom_metrics_and_callbacks.py` for more usage information.</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">callbacks_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">callbacks_class</span> <span class="o">=</span> <span class="n">DefaultCallbacks</span>
        <span class="c1"># Check, whether given `callbacks` is a callable.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">callbacks_class</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`config.callbacks_class` must be a callable method that &quot;</span>
                <span class="s2">&quot;returns a subclass of DefaultCallbacks, got &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">callbacks_class</span><span class="si">}</span><span class="s2">!&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks_class</span> <span class="o">=</span> <span class="n">callbacks_class</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="AlgorithmConfig.exploration"><a class="viewcode-back" href="../../../../rllib/rllib-training.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.exploration">[docs]</a>    <span class="k">def</span> <span class="nf">exploration</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">explore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">exploration_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the config&#39;s exploration settings.</span>

<span class="sd">        Args:</span>
<span class="sd">            explore: Default exploration behavior, iff `explore=None` is passed into</span>
<span class="sd">                compute_action(s). Set to False for no exploration behavior (e.g.,</span>
<span class="sd">                for evaluation).</span>
<span class="sd">            exploration_config: A dict specifying the Exploration object&#39;s config.</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">explore</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">explore</span> <span class="o">=</span> <span class="n">explore</span>
        <span class="k">if</span> <span class="n">exploration_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="c1"># Override entire `exploration_config` if `type` key changes.</span>
            <span class="c1"># Update, if `type` key remains the same or is not specified.</span>
            <span class="n">new_exploration_config</span> <span class="o">=</span> <span class="n">deep_update</span><span class="p">(</span>
                <span class="p">{</span><span class="s2">&quot;exploration_config&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploration_config</span><span class="p">},</span>
                <span class="p">{</span><span class="s2">&quot;exploration_config&quot;</span><span class="p">:</span> <span class="n">exploration_config</span><span class="p">},</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="p">[</span><span class="s2">&quot;exploration_config&quot;</span><span class="p">],</span>
                <span class="p">[</span><span class="s2">&quot;exploration_config&quot;</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">exploration_config</span> <span class="o">=</span> <span class="n">new_exploration_config</span><span class="p">[</span><span class="s2">&quot;exploration_config&quot;</span><span class="p">]</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="AlgorithmConfig.evaluation"><a class="viewcode-back" href="../../../../rllib/rllib-training.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.evaluation">[docs]</a>    <span class="k">def</span> <span class="nf">evaluation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">evaluation_interval</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">evaluation_duration</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">evaluation_duration_unit</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">evaluation_sample_timeout_s</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">evaluation_parallel_to_training</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">evaluation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">,</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">off_policy_estimation_methods</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">ope_split_batch_by_episode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">evaluation_num_workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">custom_evaluation_function</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">always_attach_evaluation_results</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">enable_async_evaluation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="c1"># Deprecated args.</span>
        <span class="n">evaluation_num_episodes</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the config&#39;s evaluation settings.</span>

<span class="sd">        Args:</span>
<span class="sd">            evaluation_interval: Evaluate with every `evaluation_interval` training</span>
<span class="sd">                iterations. The evaluation stats will be reported under the &quot;evaluation&quot;</span>
<span class="sd">                metric key. Note that for Ape-X metrics are already only reported for</span>
<span class="sd">                the lowest epsilon workers (least random workers).</span>
<span class="sd">                Set to None (or 0) for no evaluation.</span>
<span class="sd">            evaluation_duration: Duration for which to run evaluation each</span>
<span class="sd">                `evaluation_interval`. The unit for the duration can be set via</span>
<span class="sd">                `evaluation_duration_unit` to either &quot;episodes&quot; (default) or</span>
<span class="sd">                &quot;timesteps&quot;. If using multiple evaluation workers</span>
<span class="sd">                (evaluation_num_workers &gt; 1), the load to run will be split amongst</span>
<span class="sd">                these.</span>
<span class="sd">                If the value is &quot;auto&quot;:</span>
<span class="sd">                - For `evaluation_parallel_to_training=True`: Will run as many</span>
<span class="sd">                episodes/timesteps that fit into the (parallel) training step.</span>
<span class="sd">                - For `evaluation_parallel_to_training=False`: Error.</span>
<span class="sd">            evaluation_duration_unit: The unit, with which to count the evaluation</span>
<span class="sd">                duration. Either &quot;episodes&quot; (default) or &quot;timesteps&quot;.</span>
<span class="sd">            evaluation_sample_timeout_s: The timeout (in seconds) for the ray.get call</span>
<span class="sd">                to the remote evaluation worker(s) `sample()` method. After this time,</span>
<span class="sd">                the user will receive a warning and instructions on how to fix the</span>
<span class="sd">                issue. This could be either to make sure the episode ends, increasing</span>
<span class="sd">                the timeout, or switching to `evaluation_duration_unit=timesteps`.</span>
<span class="sd">            evaluation_parallel_to_training: Whether to run evaluation in parallel to</span>
<span class="sd">                a Algorithm.train() call using threading. Default=False.</span>
<span class="sd">                E.g. evaluation_interval=2 -&gt; For every other training iteration,</span>
<span class="sd">                the Algorithm.train() and Algorithm.evaluate() calls run in parallel.</span>
<span class="sd">                Note: This is experimental. Possible pitfalls could be race conditions</span>
<span class="sd">                for weight synching at the beginning of the evaluation loop.</span>
<span class="sd">            evaluation_config: Typical usage is to pass extra args to evaluation env</span>
<span class="sd">                creator and to disable exploration by computing deterministic actions.</span>
<span class="sd">                IMPORTANT NOTE: Policy gradient algorithms are able to find the optimal</span>
<span class="sd">                policy, even if this is a stochastic one. Setting &quot;explore=False&quot; here</span>
<span class="sd">                will result in the evaluation workers not using this optimal policy!</span>
<span class="sd">            off_policy_estimation_methods: Specify how to evaluate the current policy,</span>
<span class="sd">                along with any optional config parameters. This only has an effect when</span>
<span class="sd">                reading offline experiences (&quot;input&quot; is not &quot;sampler&quot;).</span>
<span class="sd">                Available keys:</span>
<span class="sd">                {ope_method_name: {&quot;type&quot;: ope_type, ...}} where `ope_method_name`</span>
<span class="sd">                is a user-defined string to save the OPE results under, and</span>
<span class="sd">                `ope_type` can be any subclass of OffPolicyEstimator, e.g.</span>
<span class="sd">                ray.rllib.offline.estimators.is::ImportanceSampling</span>
<span class="sd">                or your own custom subclass, or the full class path to the subclass.</span>
<span class="sd">                You can also add additional config arguments to be passed to the</span>
<span class="sd">                OffPolicyEstimator in the dict, e.g.</span>
<span class="sd">                {&quot;qreg_dr&quot;: {&quot;type&quot;: DoublyRobust, &quot;q_model_type&quot;: &quot;qreg&quot;, &quot;k&quot;: 5}}</span>
<span class="sd">            ope_split_batch_by_episode: Whether to use SampleBatch.split_by_episode() to</span>
<span class="sd">                split the input batch to episodes before estimating the ope metrics. In</span>
<span class="sd">                case of bandits you should make this False to see improvements in ope</span>
<span class="sd">                evaluation speed. In case of bandits, it is ok to not split by episode,</span>
<span class="sd">                since each record is one timestep already. The default is True.</span>
<span class="sd">            evaluation_num_workers: Number of parallel workers to use for evaluation.</span>
<span class="sd">                Note that this is set to zero by default, which means evaluation will</span>
<span class="sd">                be run in the algorithm process (only if evaluation_interval is not</span>
<span class="sd">                None). If you increase this, it will increase the Ray resource usage of</span>
<span class="sd">                the algorithm since evaluation workers are created separately from</span>
<span class="sd">                rollout workers (used to sample data for training).</span>
<span class="sd">            custom_evaluation_function: Customize the evaluation method. This must be a</span>
<span class="sd">                function of signature (algo: Algorithm, eval_workers: WorkerSet) -&gt;</span>
<span class="sd">                metrics: dict. See the Algorithm.evaluate() method to see the default</span>
<span class="sd">                implementation. The Algorithm guarantees all eval workers have the</span>
<span class="sd">                latest policy state before this function is called.</span>
<span class="sd">            always_attach_evaluation_results: Make sure the latest available evaluation</span>
<span class="sd">                results are always attached to a step result dict. This may be useful</span>
<span class="sd">                if Tune or some other meta controller needs access to evaluation metrics</span>
<span class="sd">                all the time.</span>
<span class="sd">            enable_async_evaluation: If True, use an AsyncRequestsManager for</span>
<span class="sd">                the evaluation workers and use this manager to send `sample()` requests</span>
<span class="sd">                to the evaluation workers. This way, the Algorithm becomes more robust</span>
<span class="sd">                against long running episodes and/or failing (and restarting) workers.</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">evaluation_num_episodes</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.evaluation(evaluation_num_episodes=..)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.evaluation(evaluation_duration=.., &quot;</span>
                <span class="s2">&quot;evaluation_duration_unit=&#39;episodes&#39;)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">evaluation_duration</span> <span class="o">=</span> <span class="n">evaluation_num_episodes</span>

        <span class="k">if</span> <span class="n">evaluation_interval</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_interval</span> <span class="o">=</span> <span class="n">evaluation_interval</span>
        <span class="k">if</span> <span class="n">evaluation_duration</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_duration</span> <span class="o">=</span> <span class="n">evaluation_duration</span>
        <span class="k">if</span> <span class="n">evaluation_duration_unit</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_duration_unit</span> <span class="o">=</span> <span class="n">evaluation_duration_unit</span>
        <span class="k">if</span> <span class="n">evaluation_sample_timeout_s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_sample_timeout_s</span> <span class="o">=</span> <span class="n">evaluation_sample_timeout_s</span>
        <span class="k">if</span> <span class="n">evaluation_parallel_to_training</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_parallel_to_training</span> <span class="o">=</span> <span class="n">evaluation_parallel_to_training</span>
        <span class="k">if</span> <span class="n">evaluation_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="c1"># If user really wants to set this to None, we should allow this here,</span>
            <span class="c1"># instead of creating an empty dict.</span>
            <span class="k">if</span> <span class="n">evaluation_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="c1"># Update (don&#39;t replace) the existing overrides with the provided ones.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm</span> <span class="kn">import</span> <span class="n">Algorithm</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span> <span class="o">=</span> <span class="n">deep_update</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span> <span class="ow">or</span> <span class="p">{},</span>
                    <span class="n">evaluation_config</span><span class="p">,</span>
                    <span class="kc">True</span><span class="p">,</span>
                    <span class="n">Algorithm</span><span class="o">.</span><span class="n">_allow_unknown_subkeys</span><span class="p">,</span>
                    <span class="n">Algorithm</span><span class="o">.</span><span class="n">_override_all_subkeys_if_type_changes</span><span class="p">,</span>
                    <span class="n">Algorithm</span><span class="o">.</span><span class="n">_override_all_key_list</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="n">off_policy_estimation_methods</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">off_policy_estimation_methods</span> <span class="o">=</span> <span class="n">off_policy_estimation_methods</span>
        <span class="k">if</span> <span class="n">evaluation_num_workers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_num_workers</span> <span class="o">=</span> <span class="n">evaluation_num_workers</span>
        <span class="k">if</span> <span class="n">custom_evaluation_function</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">custom_evaluation_function</span> <span class="o">=</span> <span class="n">custom_evaluation_function</span>
        <span class="k">if</span> <span class="n">always_attach_evaluation_results</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">always_attach_evaluation_results</span> <span class="o">=</span> <span class="n">always_attach_evaluation_results</span>
        <span class="k">if</span> <span class="n">enable_async_evaluation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">enable_async_evaluation</span> <span class="o">=</span> <span class="n">enable_async_evaluation</span>
        <span class="k">if</span> <span class="n">ope_split_batch_by_episode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ope_split_batch_by_episode</span> <span class="o">=</span> <span class="n">ope_split_batch_by_episode</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="AlgorithmConfig.offline_data"><a class="viewcode-back" href="../../../../rllib/rllib-training.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.offline_data">[docs]</a>    <span class="k">def</span> <span class="nf">offline_data</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">input_</span><span class="o">=</span><span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">input_config</span><span class="o">=</span><span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">actions_in_input_normalized</span><span class="o">=</span><span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">input_evaluation</span><span class="o">=</span><span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">postprocess_inputs</span><span class="o">=</span><span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">shuffle_buffer_size</span><span class="o">=</span><span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">output</span><span class="o">=</span><span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">output_config</span><span class="o">=</span><span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">output_compress_columns</span><span class="o">=</span><span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">output_max_file_size</span><span class="o">=</span><span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">offline_sampling</span><span class="o">=</span><span class="n">NotProvided</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the config&#39;s offline data settings.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_: Specify how to generate experiences:</span>
<span class="sd">                - &quot;sampler&quot;: Generate experiences via online (env) simulation (default).</span>
<span class="sd">                - A local directory or file glob expression (e.g., &quot;/tmp/*.json&quot;).</span>
<span class="sd">                - A list of individual file paths/URIs (e.g., [&quot;/tmp/1.json&quot;,</span>
<span class="sd">                &quot;s3://bucket/2.json&quot;]).</span>
<span class="sd">                - A dict with string keys and sampling probabilities as values (e.g.,</span>
<span class="sd">                {&quot;sampler&quot;: 0.4, &quot;/tmp/*.json&quot;: 0.4, &quot;s3://bucket/expert.json&quot;: 0.2}).</span>
<span class="sd">                - A callable that takes an `IOContext` object as only arg and returns a</span>
<span class="sd">                ray.rllib.offline.InputReader.</span>
<span class="sd">                - A string key that indexes a callable with tune.registry.register_input</span>
<span class="sd">            input_config: Arguments that describe the settings for reading the input.</span>
<span class="sd">                If input is `sample`, this will be environment configuation, e.g.</span>
<span class="sd">                `env_name` and `env_config`, etc. See `EnvContext` for more info.</span>
<span class="sd">                If the input is `dataset`, this will be e.g. `format`, `path`.</span>
<span class="sd">            actions_in_input_normalized: True, if the actions in a given offline &quot;input&quot;</span>
<span class="sd">                are already normalized (between -1.0 and 1.0). This is usually the case</span>
<span class="sd">                when the offline file has been generated by another RLlib algorithm</span>
<span class="sd">                (e.g. PPO or SAC), while &quot;normalize_actions&quot; was set to True.</span>
<span class="sd">            postprocess_inputs: Whether to run postprocess_trajectory() on the</span>
<span class="sd">                trajectory fragments from offline inputs. Note that postprocessing will</span>
<span class="sd">                be done using the *current* policy, not the *behavior* policy, which</span>
<span class="sd">                is typically undesirable for on-policy algorithms.</span>
<span class="sd">            shuffle_buffer_size: If positive, input batches will be shuffled via a</span>
<span class="sd">                sliding window buffer of this number of batches. Use this if the input</span>
<span class="sd">                data is not in random enough order. Input is delayed until the shuffle</span>
<span class="sd">                buffer is filled.</span>
<span class="sd">            output: Specify where experiences should be saved:</span>
<span class="sd">                 - None: don&#39;t save any experiences</span>
<span class="sd">                 - &quot;logdir&quot; to save to the agent log dir</span>
<span class="sd">                 - a path/URI to save to a custom output directory (e.g., &quot;s3://bckt/&quot;)</span>
<span class="sd">                 - a function that returns a rllib.offline.OutputWriter</span>
<span class="sd">            output_config: Arguments accessible from the IOContext for configuring</span>
<span class="sd">                custom output.</span>
<span class="sd">            output_compress_columns: What sample batch columns to LZ4 compress in the</span>
<span class="sd">                output data.</span>
<span class="sd">            output_max_file_size: Max output file size (in bytes) before rolling over</span>
<span class="sd">                to a new file.</span>
<span class="sd">            offline_sampling: Whether sampling for the Algorithm happens via</span>
<span class="sd">                reading from offline data. If True, RolloutWorkers will NOT limit the</span>
<span class="sd">                number of collected batches within the same `sample()` call based on</span>
<span class="sd">                the number of sub-environments within the worker (no sub-environments</span>
<span class="sd">                present).</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">input_</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_</span> <span class="o">=</span> <span class="n">input_</span>
        <span class="k">if</span> <span class="n">input_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;input_config must be a dict, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">input_config</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="c1"># TODO (Kourosh) Once we use a complete sepration between rollout worker</span>
            <span class="c1"># and input dataset reader we can remove this.</span>
            <span class="c1"># For now Error out if user attempts to set these parameters.</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> should not be set in the input_config. RLlib will use </span><span class="si">{}</span><span class="s2"> instead.&quot;</span>
            <span class="k">if</span> <span class="n">input_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;num_cpus_per_read_task&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="s2">&quot;num_cpus_per_read_task&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;config.resources(num_cpus_per_worker=..)&quot;</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">input_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;parallelism&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_evaluation</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="s2">&quot;parallelism&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;config.evaluation(evaluation_num_workers=..)&quot;</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="s2">&quot;parallelism&quot;</span><span class="p">,</span> <span class="s2">&quot;config.rollouts(num_rollout_workers=..)&quot;</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_config</span> <span class="o">=</span> <span class="n">input_config</span>
        <span class="k">if</span> <span class="n">actions_in_input_normalized</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">actions_in_input_normalized</span> <span class="o">=</span> <span class="n">actions_in_input_normalized</span>
        <span class="k">if</span> <span class="n">input_evaluation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;offline_data(input_evaluation=</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input_evaluation</span><span class="p">),</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;evaluation(off_policy_estimation_methods=</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">input_evaluation</span>
                <span class="p">),</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Running OPE during training is not recommended.&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">postprocess_inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">postprocess_inputs</span> <span class="o">=</span> <span class="n">postprocess_inputs</span>
        <span class="k">if</span> <span class="n">shuffle_buffer_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shuffle_buffer_size</span> <span class="o">=</span> <span class="n">shuffle_buffer_size</span>
        <span class="k">if</span> <span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span>
        <span class="k">if</span> <span class="n">output_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_config</span> <span class="o">=</span> <span class="n">output_config</span>
        <span class="k">if</span> <span class="n">output_compress_columns</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_compress_columns</span> <span class="o">=</span> <span class="n">output_compress_columns</span>
        <span class="k">if</span> <span class="n">output_max_file_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_max_file_size</span> <span class="o">=</span> <span class="n">output_max_file_size</span>
        <span class="k">if</span> <span class="n">offline_sampling</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">offline_sampling</span> <span class="o">=</span> <span class="n">offline_sampling</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="AlgorithmConfig.multi_agent"><a class="viewcode-back" href="../../../../rllib/rllib-training.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.multi_agent">[docs]</a>    <span class="k">def</span> <span class="nf">multi_agent</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">policies</span><span class="o">=</span><span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">algorithm_config_overrides_per_module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Dict</span><span class="p">[</span><span class="n">ModuleID</span><span class="p">,</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">policy_map_capacity</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">policy_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Callable</span><span class="p">[[</span><span class="n">AgentID</span><span class="p">,</span> <span class="s2">&quot;Episode&quot;</span><span class="p">],</span> <span class="n">PolicyID</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">policies_to_train</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">Container</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">],</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="n">SampleBatchType</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">policy_states_are_swappable</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">observation_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">count_steps_by</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="c1"># Deprecated args:</span>
        <span class="n">replay_mode</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
        <span class="c1"># Now done via Ray object store, which has its own cloud-supported</span>
        <span class="c1"># spillover mechanism.</span>
        <span class="n">policy_map_cache</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the config&#39;s multi-agent settings.</span>

<span class="sd">        Validates the new multi-agent settings and translates everything into</span>
<span class="sd">        a unified multi-agent setup format. For example a `policies` list or set</span>
<span class="sd">        of IDs is properly converted into a dict mapping these IDs to PolicySpecs.</span>

<span class="sd">        Args:</span>
<span class="sd">            policies: Map of type MultiAgentPolicyConfigDict from policy ids to either</span>
<span class="sd">                4-tuples of (policy_cls, obs_space, act_space, config) or PolicySpecs.</span>
<span class="sd">                These tuples or PolicySpecs define the class of the policy, the</span>
<span class="sd">                observation- and action spaces of the policies, and any extra config.</span>
<span class="sd">            algorithm_config_overrides_per_module: Only used if both</span>
<span class="sd">                `_enable_learner_api` and `_enable_rl_module_api` are True.</span>
<span class="sd">                A mapping from ModuleIDs to</span>
<span class="sd">                per-module AlgorithmConfig override dicts, which apply certain settings,</span>
<span class="sd">                e.g. the learning rate, from the main AlgorithmConfig only to this</span>
<span class="sd">                particular module (within a MultiAgentRLModule).</span>
<span class="sd">                You can create override dicts by using the `AlgorithmConfig.overrides`</span>
<span class="sd">                utility. For example, to override your learning rate and (PPO) lambda</span>
<span class="sd">                setting just for a single RLModule with your MultiAgentRLModule, do:</span>
<span class="sd">                config.multi_agent(algorithm_config_overrides_per_module={</span>
<span class="sd">                &quot;module_1&quot;: PPOConfig.overrides(lr=0.0002, lambda_=0.75),</span>
<span class="sd">                })</span>
<span class="sd">            policy_map_capacity: Keep this many policies in the &quot;policy_map&quot; (before</span>
<span class="sd">                writing least-recently used ones to disk/S3).</span>
<span class="sd">            policy_mapping_fn: Function mapping agent ids to policy ids. The signature</span>
<span class="sd">                is: `(agent_id, episode, worker, **kwargs) -&gt; PolicyID`.</span>
<span class="sd">            policies_to_train: Determines those policies that should be updated.</span>
<span class="sd">                Options are:</span>
<span class="sd">                - None, for training all policies.</span>
<span class="sd">                - An iterable of PolicyIDs that should be trained.</span>
<span class="sd">                - A callable, taking a PolicyID and a SampleBatch or MultiAgentBatch</span>
<span class="sd">                and returning a bool (indicating whether the given policy is trainable</span>
<span class="sd">                or not, given the particular batch). This allows you to have a policy</span>
<span class="sd">                trained only on certain data (e.g. when playing against a certain</span>
<span class="sd">                opponent).</span>
<span class="sd">            policy_states_are_swappable: Whether all Policy objects in this map can be</span>
<span class="sd">                &quot;swapped out&quot; via a simple `state = A.get_state(); B.set_state(state)`,</span>
<span class="sd">                where `A` and `B` are policy instances in this map. You should set</span>
<span class="sd">                this to True for significantly speeding up the PolicyMap&#39;s cache lookup</span>
<span class="sd">                times, iff your policies all share the same neural network</span>
<span class="sd">                architecture and optimizer types. If True, the PolicyMap will not</span>
<span class="sd">                have to garbage collect old, least recently used policies, but instead</span>
<span class="sd">                keep them in memory and simply override their state with the state of</span>
<span class="sd">                the most recently accessed one.</span>
<span class="sd">                For example, in a league-based training setup, you might have 100s of</span>
<span class="sd">                the same policies in your map (playing against each other in various</span>
<span class="sd">                combinations), but all of them share the same state structure</span>
<span class="sd">                (are &quot;swappable&quot;).</span>
<span class="sd">            observation_fn: Optional function that can be used to enhance the local</span>
<span class="sd">                agent observations to include more state. See</span>
<span class="sd">                rllib/evaluation/observation_function.py for more info.</span>
<span class="sd">            count_steps_by: Which metric to use as the &quot;batch size&quot; when building a</span>
<span class="sd">                MultiAgentBatch. The two supported values are:</span>
<span class="sd">                &quot;env_steps&quot;: Count each time the env is &quot;stepped&quot; (no matter how many</span>
<span class="sd">                multi-agent actions are passed/how many multi-agent observations</span>
<span class="sd">                have been returned in the previous step).</span>
<span class="sd">                &quot;agent_steps&quot;: Count each individual agent step as one step.</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">policies</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="c1"># Make sure our Policy IDs are ok (this should work whether `policies`</span>
            <span class="c1"># is a dict or just any Sequence).</span>
            <span class="k">for</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">policies</span><span class="p">:</span>
                <span class="n">validate_policy_id</span><span class="p">(</span><span class="n">pid</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="c1"># Validate each policy spec in a given dict.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">policies</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">pid</span><span class="p">,</span> <span class="n">spec</span> <span class="ow">in</span> <span class="n">policies</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="c1"># If not a PolicySpec object, values must be lists/tuples of len 4.</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">spec</span><span class="p">,</span> <span class="n">PolicySpec</span><span class="p">):</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">spec</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">spec</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                                <span class="s2">&quot;Policy specs must be tuples/lists of &quot;</span>
                                <span class="s2">&quot;(cls or None, obs_space, action_space, config), &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;got </span><span class="si">{</span><span class="n">spec</span><span class="si">}</span><span class="s2"> for PolicyID=</span><span class="si">{</span><span class="n">pid</span><span class="si">}</span><span class="s2">&quot;</span>
                            <span class="p">)</span>
                    <span class="c1"># TODO: Switch from dict to AlgorithmConfigOverride, once available.</span>
                    <span class="c1"># Config not a dict.</span>
                    <span class="k">elif</span> <span class="p">(</span>
                        <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="p">(</span><span class="n">AlgorithmConfig</span><span class="p">,</span> <span class="nb">dict</span><span class="p">))</span>
                        <span class="ow">and</span> <span class="n">spec</span><span class="o">.</span><span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Multi-agent policy config for </span><span class="si">{</span><span class="n">pid</span><span class="si">}</span><span class="s2"> must be a dict or &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;AlgorithmConfig object, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">config</span><span class="p">)</span><span class="si">}</span><span class="s2">!&quot;</span>
                        <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">policies</span> <span class="o">=</span> <span class="n">policies</span>

        <span class="k">if</span> <span class="n">algorithm_config_overrides_per_module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_config_overrides_per_module</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">algorithm_config_overrides_per_module</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">policy_map_capacity</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">policy_map_capacity</span> <span class="o">=</span> <span class="n">policy_map_capacity</span>

        <span class="k">if</span> <span class="n">policy_mapping_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="c1"># Create `policy_mapping_fn` from a config dict.</span>
            <span class="c1"># Helpful is users would like to specify custom callable classes in</span>
            <span class="c1"># yaml files.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">policy_mapping_fn</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">policy_mapping_fn</span> <span class="o">=</span> <span class="n">from_config</span><span class="p">(</span><span class="n">policy_mapping_fn</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">policy_mapping_fn</span> <span class="o">=</span> <span class="n">policy_mapping_fn</span>

        <span class="k">if</span> <span class="n">observation_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">observation_fn</span> <span class="o">=</span> <span class="n">observation_fn</span>

        <span class="k">if</span> <span class="n">policy_map_cache</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.multi_agent(policy_map_cache=..)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">replay_mode</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.multi_agent(replay_mode=..)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.training(&quot;</span>
                <span class="s2">&quot;replay_buffer_config={&#39;replay_mode&#39;: ..})&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">count_steps_by</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">count_steps_by</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;env_steps&quot;</span><span class="p">,</span> <span class="s2">&quot;agent_steps&quot;</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;config.multi_agent(count_steps_by=..) must be one of &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;[env_steps|agent_steps], not </span><span class="si">{</span><span class="n">count_steps_by</span><span class="si">}</span><span class="s2">!&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">=</span> <span class="n">count_steps_by</span>

        <span class="k">if</span> <span class="n">policies_to_train</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">policies_to_train</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">set</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
                <span class="ow">or</span> <span class="n">callable</span><span class="p">(</span><span class="n">policies_to_train</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">policies_to_train</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="p">(</span>
                <span class="s2">&quot;ERROR: `policies_to_train` must be a [list|set|tuple] or a &quot;</span>
                <span class="s2">&quot;callable taking PolicyID and SampleBatch and returning &quot;</span>
                <span class="s2">&quot;True|False (trainable or not?) or None (for always training all &quot;</span>
                <span class="s2">&quot;policies).&quot;</span>
            <span class="p">)</span>
            <span class="c1"># Check `policies_to_train` for invalid entries.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">policies_to_train</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">set</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">policies_to_train</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;`config.multi_agent(policies_to_train=..)` is empty! &quot;</span>
                        <span class="s2">&quot;Make sure - if you would like to learn at least one policy - &quot;</span>
                        <span class="s2">&quot;to add its ID to that list.&quot;</span>
                    <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">policies_to_train</span> <span class="o">=</span> <span class="n">policies_to_train</span>

        <span class="k">if</span> <span class="n">policy_states_are_swappable</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">policy_states_are_swappable</span> <span class="o">=</span> <span class="n">policy_states_are_swappable</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="AlgorithmConfig.is_multi_agent"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_multi_agent.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_multi_agent">[docs]</a>    <span class="k">def</span> <span class="nf">is_multi_agent</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns whether this config specifies a multi-agent setup.</span>

<span class="sd">        Returns:</span>
<span class="sd">            True, if a) &gt;1 policies defined OR b) 1 policy defined, but its ID is NOT</span>
<span class="sd">            DEFAULT_POLICY_ID.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policies</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">DEFAULT_POLICY_ID</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">policies</span></div>

<div class="viewcode-block" id="AlgorithmConfig.reporting"><a class="viewcode-back" href="../../../../rllib/rllib-training.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.reporting">[docs]</a>    <span class="k">def</span> <span class="nf">reporting</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">keep_per_episode_custom_metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">metrics_episode_collection_timeout_s</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">metrics_num_episodes_for_smoothing</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">min_time_s_per_iteration</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">min_train_timesteps_per_iteration</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">min_sample_timesteps_per_iteration</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the config&#39;s reporting settings.</span>

<span class="sd">        Args:</span>
<span class="sd">            keep_per_episode_custom_metrics: Store raw custom metrics without</span>
<span class="sd">                calculating max, min, mean</span>
<span class="sd">            metrics_episode_collection_timeout_s: Wait for metric batches for at most</span>
<span class="sd">                this many seconds. Those that have not returned in time will be</span>
<span class="sd">                collected in the next train iteration.</span>
<span class="sd">            metrics_num_episodes_for_smoothing: Smooth rollout metrics over this many</span>
<span class="sd">                episodes, if possible.</span>
<span class="sd">                In case rollouts (sample collection) just started, there may be fewer</span>
<span class="sd">                than this many episodes in the buffer and we&#39;ll compute metrics</span>
<span class="sd">                over this smaller number of available episodes.</span>
<span class="sd">                In case there are more than this many episodes collected in a single</span>
<span class="sd">                training iteration, use all of these episodes for metrics computation,</span>
<span class="sd">                meaning don&#39;t ever cut any &quot;excess&quot; episodes.</span>
<span class="sd">            min_time_s_per_iteration: Minimum time to accumulate within a single</span>
<span class="sd">                `train()` call. This value does not affect learning,</span>
<span class="sd">                only the number of times `Algorithm.training_step()` is called by</span>
<span class="sd">                `Algorithm.train()`. If - after one such step attempt, the time taken</span>
<span class="sd">                has not reached `min_time_s_per_iteration`, will perform n more</span>
<span class="sd">                `training_step()` calls until the minimum time has been</span>
<span class="sd">                consumed. Set to 0 or None for no minimum time.</span>
<span class="sd">            min_train_timesteps_per_iteration: Minimum training timesteps to accumulate</span>
<span class="sd">                within a single `train()` call. This value does not affect learning,</span>
<span class="sd">                only the number of times `Algorithm.training_step()` is called by</span>
<span class="sd">                `Algorithm.train()`. If - after one such step attempt, the training</span>
<span class="sd">                timestep count has not been reached, will perform n more</span>
<span class="sd">                `training_step()` calls until the minimum timesteps have been</span>
<span class="sd">                executed. Set to 0 or None for no minimum timesteps.</span>
<span class="sd">            min_sample_timesteps_per_iteration: Minimum env sampling timesteps to</span>
<span class="sd">                accumulate within a single `train()` call. This value does not affect</span>
<span class="sd">                learning, only the number of times `Algorithm.training_step()` is</span>
<span class="sd">                called by `Algorithm.train()`. If - after one such step attempt, the env</span>
<span class="sd">                sampling timestep count has not been reached, will perform n more</span>
<span class="sd">                `training_step()` calls until the minimum timesteps have been</span>
<span class="sd">                executed. Set to 0 or None for no minimum timesteps.</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">keep_per_episode_custom_metrics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">keep_per_episode_custom_metrics</span> <span class="o">=</span> <span class="n">keep_per_episode_custom_metrics</span>
        <span class="k">if</span> <span class="n">metrics_episode_collection_timeout_s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics_episode_collection_timeout_s</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">metrics_episode_collection_timeout_s</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">metrics_num_episodes_for_smoothing</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics_num_episodes_for_smoothing</span> <span class="o">=</span> <span class="n">metrics_num_episodes_for_smoothing</span>
        <span class="k">if</span> <span class="n">min_time_s_per_iteration</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_time_s_per_iteration</span> <span class="o">=</span> <span class="n">min_time_s_per_iteration</span>
        <span class="k">if</span> <span class="n">min_train_timesteps_per_iteration</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_train_timesteps_per_iteration</span> <span class="o">=</span> <span class="n">min_train_timesteps_per_iteration</span>
        <span class="k">if</span> <span class="n">min_sample_timesteps_per_iteration</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_sample_timesteps_per_iteration</span> <span class="o">=</span> <span class="n">min_sample_timesteps_per_iteration</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="AlgorithmConfig.checkpointing"><a class="viewcode-back" href="../../../../rllib/rllib-training.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.checkpointing">[docs]</a>    <span class="k">def</span> <span class="nf">checkpointing</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">export_native_model_files</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">checkpoint_trainable_policies_only</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the config&#39;s checkpointing settings.</span>

<span class="sd">        Args:</span>
<span class="sd">            export_native_model_files: Whether an individual Policy-</span>
<span class="sd">                or the Algorithm&#39;s checkpoints also contain (tf or torch) native</span>
<span class="sd">                model files. These could be used to restore just the NN models</span>
<span class="sd">                from these files w/o requiring RLlib. These files are generated</span>
<span class="sd">                by calling the tf- or torch- built-in saving utility methods on</span>
<span class="sd">                the actual models.</span>
<span class="sd">            checkpoint_trainable_policies_only: Whether to only add Policies to the</span>
<span class="sd">                Algorithm checkpoint (in sub-directory &quot;policies/&quot;) that are trainable</span>
<span class="sd">                according to the `is_trainable_policy` callable of the local worker.</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">export_native_model_files</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">export_native_model_files</span> <span class="o">=</span> <span class="n">export_native_model_files</span>
        <span class="k">if</span> <span class="n">checkpoint_trainable_policies_only</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_trainable_policies_only</span> <span class="o">=</span> <span class="n">checkpoint_trainable_policies_only</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="AlgorithmConfig.debugging"><a class="viewcode-back" href="../../../../rllib/rllib-training.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.debugging">[docs]</a>    <span class="k">def</span> <span class="nf">debugging</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">logger_creator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="n">Logger</span><span class="p">]]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">logger_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">log_level</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">log_sys_usage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">fake_sampler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="c1"># deprecated</span>
        <span class="n">worker_cls</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the config&#39;s debugging settings.</span>

<span class="sd">        Args:</span>
<span class="sd">            logger_creator: Callable that creates a ray.tune.Logger</span>
<span class="sd">                object. If unspecified, a default logger is created.</span>
<span class="sd">            logger_config: Define logger-specific configuration to be used inside Logger</span>
<span class="sd">                Default value None allows overwriting with nested dicts.</span>
<span class="sd">            log_level: Set the ray.rllib.* log level for the agent process and its</span>
<span class="sd">                workers. Should be one of DEBUG, INFO, WARN, or ERROR. The DEBUG level</span>
<span class="sd">                will also periodically print out summaries of relevant internal dataflow</span>
<span class="sd">                (this is also printed out once at startup at the INFO level). When using</span>
<span class="sd">                the `rllib train` command, you can also use the `-v` and `-vv` flags as</span>
<span class="sd">                shorthand for INFO and DEBUG.</span>
<span class="sd">            log_sys_usage: Log system resource metrics to results. This requires</span>
<span class="sd">                `psutil` to be installed for sys stats, and `gputil` for GPU metrics.</span>
<span class="sd">            fake_sampler: Use fake (infinite speed) sampler. For testing only.</span>
<span class="sd">            seed: This argument, in conjunction with worker_index, sets the random</span>
<span class="sd">                seed of each worker, so that identically configured trials will have</span>
<span class="sd">                identical results. This makes experiments reproducible.</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">worker_cls</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.debugging(worker_cls=..)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.rollouts(env_runner_cls=...)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">logger_creator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger_creator</span> <span class="o">=</span> <span class="n">logger_creator</span>
        <span class="k">if</span> <span class="n">logger_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger_config</span> <span class="o">=</span> <span class="n">logger_config</span>
        <span class="k">if</span> <span class="n">log_level</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_level</span> <span class="o">=</span> <span class="n">log_level</span>
        <span class="k">if</span> <span class="n">log_sys_usage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_sys_usage</span> <span class="o">=</span> <span class="n">log_sys_usage</span>
        <span class="k">if</span> <span class="n">fake_sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fake_sampler</span> <span class="o">=</span> <span class="n">fake_sampler</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="AlgorithmConfig.fault_tolerance"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.fault_tolerance.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.fault_tolerance">[docs]</a>    <span class="k">def</span> <span class="nf">fault_tolerance</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">recreate_failed_workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">max_num_worker_restarts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">delay_between_worker_restarts_s</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">restart_failed_sub_environments</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">num_consecutive_worker_failures_tolerance</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">worker_health_probe_timeout_s</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">worker_restore_timeout_s</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the config&#39;s fault tolerance settings.</span>

<span class="sd">        Args:</span>
<span class="sd">            recreate_failed_workers: Whether - upon a worker failure - RLlib will try to</span>
<span class="sd">                recreate the lost worker as an identical copy of the failed one. The new</span>
<span class="sd">                worker will only differ from the failed one in its</span>
<span class="sd">                `self.recreated_worker=True` property value. It will have the same</span>
<span class="sd">                `worker_index` as the original one. If True, the</span>
<span class="sd">                `ignore_worker_failures` setting will be ignored.</span>
<span class="sd">            max_num_worker_restarts: The maximum number of times a worker is allowed to</span>
<span class="sd">                be restarted (if `recreate_failed_workers` is True).</span>
<span class="sd">            delay_between_worker_restarts_s: The delay (in seconds) between two</span>
<span class="sd">                consecutive worker restarts (if `recreate_failed_workers` is True).</span>
<span class="sd">            restart_failed_sub_environments: If True and any sub-environment (within</span>
<span class="sd">                a vectorized env) throws any error during env stepping, the</span>
<span class="sd">                Sampler will try to restart the faulty sub-environment. This is done</span>
<span class="sd">                without disturbing the other (still intact) sub-environment and without</span>
<span class="sd">                the RolloutWorker crashing.</span>
<span class="sd">            num_consecutive_worker_failures_tolerance: The number of consecutive times</span>
<span class="sd">                a rollout worker (or evaluation worker) failure is tolerated before</span>
<span class="sd">                finally crashing the Algorithm. Only useful if either</span>
<span class="sd">                `ignore_worker_failures` or `recreate_failed_workers` is True.</span>
<span class="sd">                Note that for `restart_failed_sub_environments` and sub-environment</span>
<span class="sd">                failures, the worker itself is NOT affected and won&#39;t throw any errors</span>
<span class="sd">                as the flawed sub-environment is silently restarted under the hood.</span>
<span class="sd">            worker_health_probe_timeout_s: Max amount of time we should spend waiting</span>
<span class="sd">                for health probe calls to finish. Health pings are very cheap, so the</span>
<span class="sd">                default is 1 minute.</span>
<span class="sd">            worker_restore_timeout_s: Max amount of time we should wait to restore</span>
<span class="sd">                states on recovered worker actors. Default is 30 mins.</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">recreate_failed_workers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">recreate_failed_workers</span> <span class="o">=</span> <span class="n">recreate_failed_workers</span>
        <span class="k">if</span> <span class="n">max_num_worker_restarts</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_num_worker_restarts</span> <span class="o">=</span> <span class="n">max_num_worker_restarts</span>
        <span class="k">if</span> <span class="n">delay_between_worker_restarts_s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">delay_between_worker_restarts_s</span> <span class="o">=</span> <span class="n">delay_between_worker_restarts_s</span>
        <span class="k">if</span> <span class="n">restart_failed_sub_environments</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">restart_failed_sub_environments</span> <span class="o">=</span> <span class="n">restart_failed_sub_environments</span>
        <span class="k">if</span> <span class="n">num_consecutive_worker_failures_tolerance</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_consecutive_worker_failures_tolerance</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">num_consecutive_worker_failures_tolerance</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">worker_health_probe_timeout_s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">worker_health_probe_timeout_s</span> <span class="o">=</span> <span class="n">worker_health_probe_timeout_s</span>
        <span class="k">if</span> <span class="n">worker_restore_timeout_s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">worker_restore_timeout_s</span> <span class="o">=</span> <span class="n">worker_restore_timeout_s</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="AlgorithmConfig.rl_module"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rl_module.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rl_module">[docs]</a>    <span class="nd">@ExperimentalAPI</span>
    <span class="k">def</span> <span class="nf">rl_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">rl_module_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ModuleSpec</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">_enable_rl_module_api</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the config&#39;s RLModule settings.</span>

<span class="sd">        Args:</span>
<span class="sd">            rl_module_spec: The RLModule spec to use for this config. It can be either</span>
<span class="sd">                a SingleAgentRLModuleSpec or a MultiAgentRLModuleSpec. If the</span>
<span class="sd">                observation_space, action_space, catalog_class, or the model config is</span>
<span class="sd">                not specified it will be inferred from the env and other parts of the</span>
<span class="sd">                algorithm config object.</span>
<span class="sd">            _enable_rl_module_api: Whether to enable the RLModule API for this config.</span>
<span class="sd">                By default if you call `config.rl_module(...)`, the</span>
<span class="sd">                RLModule API will NOT be enabled. If you want to enable it, you can call</span>
<span class="sd">                `config.rl_module(_enable_rl_module_api=True)`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">rl_module_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rl_module_spec</span> <span class="o">=</span> <span class="n">rl_module_spec</span>

        <span class="k">if</span> <span class="n">_enable_rl_module_api</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_enable_rl_module_api</span> <span class="o">=</span> <span class="n">_enable_rl_module_api</span>
            <span class="k">if</span> <span class="n">_enable_rl_module_api</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploration_config</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Setting `exploration_config=</span><span class="si">{}</span><span class="s2">` because you set &quot;</span>
                    <span class="s2">&quot;`_enable_rl_module_api=True`. When RLModule API are &quot;</span>
                    <span class="s2">&quot;enabled, exploration_config can not be &quot;</span>
                    <span class="s2">&quot;set. If you want to implement custom exploration behaviour, &quot;</span>
                    <span class="s2">&quot;please modify the `forward_exploration` method of the &quot;</span>
                    <span class="s2">&quot;RLModule at hand. On configs that have a default exploration &quot;</span>
                    <span class="s2">&quot;config, this must be done with &quot;</span>
                    <span class="s2">&quot;`config.exploration_config=</span><span class="si">{}</span><span class="s2">`.&quot;</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">__prior_exploration_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploration_config</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">exploration_config</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">elif</span> <span class="n">_enable_rl_module_api</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploration_config</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__prior_exploration_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;Setting `exploration_config=&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">__prior_exploration_config</span><span class="si">}</span><span class="s2">` because you set &quot;</span>
                        <span class="s2">&quot;`_enable_rl_module_api=False`. This exploration config was &quot;</span>
                        <span class="s2">&quot;restored from a prior exploration config that was overriden &quot;</span>
                        <span class="s2">&quot;when setting `_enable_rl_module_api=True`. This occurs &quot;</span>
                        <span class="s2">&quot;because when RLModule API are enabled, exploration_config &quot;</span>
                        <span class="s2">&quot;can not be set.&quot;</span>
                    <span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">exploration_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__prior_exploration_config</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">__prior_exploration_config</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;config._enable_rl_module_api was set to False, but no prior &quot;</span>
                        <span class="s2">&quot;exploration config was found to be restored.&quot;</span>
                    <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># throw a warning if the user has used this API but not enabled it.</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;You have called `config.rl_module(...)` but &quot;</span>
                <span class="s2">&quot;have not enabled the RLModule API. To enable it, call &quot;</span>
                <span class="s2">&quot;`config.rl_module(_enable_rl_module_api=True)`.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="AlgorithmConfig.experimental"><a class="viewcode-back" href="../../../../rllib/rllib-training.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.experimental">[docs]</a>    <span class="k">def</span> <span class="nf">experimental</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">_tf_policy_handles_more_than_one_loss</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">_disable_preprocessor_api</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">_disable_action_flattening</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">_disable_execution_plan_api</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
        <span class="n">_disable_initialize_loss_from_dummy_batch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">NotProvided</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the config&#39;s experimental settings.</span>

<span class="sd">        Args:</span>
<span class="sd">            _tf_policy_handles_more_than_one_loss: Experimental flag.</span>
<span class="sd">                If True, TFPolicy will handle more than one loss/optimizer.</span>
<span class="sd">                Set this to True, if you would like to return more than</span>
<span class="sd">                one loss term from your `loss_fn` and an equal number of optimizers</span>
<span class="sd">                from your `optimizer_fn`. In the future, the default for this will be</span>
<span class="sd">                True.</span>
<span class="sd">            _disable_preprocessor_api: Experimental flag.</span>
<span class="sd">                If True, no (observation) preprocessor will be created and</span>
<span class="sd">                observations will arrive in model as they are returned by the env.</span>
<span class="sd">                In the future, the default for this will be True.</span>
<span class="sd">            _disable_action_flattening: Experimental flag.</span>
<span class="sd">                If True, RLlib will no longer flatten the policy-computed actions into</span>
<span class="sd">                a single tensor (for storage in SampleCollectors/output files/etc..),</span>
<span class="sd">                but leave (possibly nested) actions as-is. Disabling flattening affects:</span>
<span class="sd">                - SampleCollectors: Have to store possibly nested action structs.</span>
<span class="sd">                - Models that have the previous action(s) as part of their input.</span>
<span class="sd">                - Algorithms reading from offline files (incl. action information).</span>
<span class="sd">            _disable_execution_plan_api: Experimental flag.</span>
<span class="sd">                If True, the execution plan API will not be used. Instead,</span>
<span class="sd">                a Algorithm&#39;s `training_iteration` method will be called as-is each</span>
<span class="sd">                training iteration.</span>

<span class="sd">        Returns:</span>
<span class="sd">            This updated AlgorithmConfig object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">_tf_policy_handles_more_than_one_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tf_policy_handles_more_than_one_loss</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">_tf_policy_handles_more_than_one_loss</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">_disable_preprocessor_api</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_disable_preprocessor_api</span> <span class="o">=</span> <span class="n">_disable_preprocessor_api</span>
        <span class="k">if</span> <span class="n">_disable_action_flattening</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_disable_action_flattening</span> <span class="o">=</span> <span class="n">_disable_action_flattening</span>
        <span class="k">if</span> <span class="n">_disable_execution_plan_api</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_disable_execution_plan_api</span> <span class="o">=</span> <span class="n">_disable_execution_plan_api</span>
        <span class="k">if</span> <span class="n">_disable_initialize_loss_from_dummy_batch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">NotProvided</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_disable_initialize_loss_from_dummy_batch</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">_disable_initialize_loss_from_dummy_batch</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">learner_class</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Type</span><span class="p">[</span><span class="s2">&quot;Learner&quot;</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Returns the Learner sub-class to use by this Algorithm.</span>

<span class="sd">        Either</span>
<span class="sd">        a) User sets a specific learner class via calling `.training(learner_class=...)`</span>
<span class="sd">        b) User leaves learner class unset (None) and the AlgorithmConfig itself</span>
<span class="sd">        figures out the actual learner class by calling its own</span>
<span class="sd">        `.get_default_learner_class()` method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learner_class</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_default_learner_class</span><span class="p">()</span>

    <span class="c1"># TODO: Make rollout_fragment_length as read-only property and replace the current</span>
    <span class="c1">#  self.rollout_fragment_length a private variable.</span>
<div class="viewcode-block" id="AlgorithmConfig.get_rollout_fragment_length"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_rollout_fragment_length.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_rollout_fragment_length">[docs]</a>    <span class="k">def</span> <span class="nf">get_rollout_fragment_length</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">worker_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Automatically infers a proper rollout_fragment_length setting if &quot;auto&quot;.</span>

<span class="sd">        Uses the simple formula:</span>
<span class="sd">        `rollout_fragment_length` = `train_batch_size` /</span>
<span class="sd">        (`num_envs_per_worker` * `num_rollout_workers`)</span>

<span class="sd">        If result is not a fraction AND `worker_index` is provided, will make</span>
<span class="sd">        those workers add another timestep, such that the overall batch size (across</span>
<span class="sd">        the workers) will add up to exactly the `train_batch_size`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The user-provided `rollout_fragment_length` or a computed one (if user</span>
<span class="sd">            value is &quot;auto&quot;).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rollout_fragment_length</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
            <span class="c1"># Example:</span>
            <span class="c1"># 2 workers, 2 envs per worker, 2000 train batch size:</span>
            <span class="c1"># -&gt; 2000 / 4 -&gt; 500</span>
            <span class="c1"># 4 workers, 3 envs per worker, 2500 train batch size:</span>
            <span class="c1"># -&gt; 2500 / 12 -&gt; 208.333 -&gt; diff=4 (208 * 12 = 2496)</span>
            <span class="c1"># -&gt; worker 1: 209, workers 2-4: 208</span>
            <span class="n">rollout_fragment_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span> <span class="o">/</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_envs_per_worker</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_rollout_workers</span> <span class="ow">or</span> <span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">rollout_fragment_length</span><span class="p">)</span> <span class="o">!=</span> <span class="n">rollout_fragment_length</span><span class="p">:</span>
                <span class="n">diff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span> <span class="o">-</span> <span class="nb">int</span><span class="p">(</span>
                    <span class="n">rollout_fragment_length</span>
                <span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_envs_per_worker</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_rollout_workers</span> <span class="ow">or</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">worker_index</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_envs_per_worker</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">diff</span><span class="p">:</span>
                    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">rollout_fragment_length</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">rollout_fragment_length</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rollout_fragment_length</span></div>

    <span class="c1"># TODO: Make evaluation_config as read-only property and replace the current</span>
    <span class="c1">#  self.evaluation_config a private variable.</span>
<div class="viewcode-block" id="AlgorithmConfig.get_evaluation_config_object"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_evaluation_config_object.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_evaluation_config_object">[docs]</a>    <span class="k">def</span> <span class="nf">get_evaluation_config_object</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Creates a full AlgorithmConfig object from `self.evaluation_config`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A fully valid AlgorithmConfig object that can be used for the evaluation</span>
<span class="sd">            WorkerSet. If `self` is already an evaluation config object, return None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_evaluation</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="n">evaluation_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span>
        <span class="c1"># Already an AlgorithmConfig -&gt; copy and use as-is.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">evaluation_config</span><span class="p">,</span> <span class="n">AlgorithmConfig</span><span class="p">):</span>
            <span class="n">eval_config_obj</span> <span class="o">=</span> <span class="n">evaluation_config</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">copy_frozen</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Create unfrozen copy of self to be used as the to-be-returned eval</span>
        <span class="c1"># AlgorithmConfig.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eval_config_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">copy_frozen</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="c1"># Update with evaluation override settings:</span>
            <span class="n">eval_config_obj</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">evaluation_config</span> <span class="ow">or</span> <span class="p">{})</span>

        <span class="c1"># Switch on the `in_evaluation` flag and remove `evaluation_config`</span>
        <span class="c1"># (set to None).</span>
        <span class="n">eval_config_obj</span><span class="o">.</span><span class="n">in_evaluation</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">eval_config_obj</span><span class="o">.</span><span class="n">evaluation_config</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Evaluation duration unit: episodes.</span>
        <span class="c1"># Switch on `complete_episode` rollouts. Also, make sure</span>
        <span class="c1"># rollout fragments are short so we never have more than one</span>
        <span class="c1"># episode in one rollout.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_duration_unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span><span class="p">:</span>
            <span class="n">eval_config_obj</span><span class="o">.</span><span class="n">batch_mode</span> <span class="o">=</span> <span class="s2">&quot;complete_episodes&quot;</span>
            <span class="n">eval_config_obj</span><span class="o">.</span><span class="n">rollout_fragment_length</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="c1"># Evaluation duration unit: timesteps.</span>
        <span class="c1"># - Set `batch_mode=truncate_episodes` so we don&#39;t perform rollouts</span>
        <span class="c1">#   strictly along episode borders.</span>
        <span class="c1"># Set `rollout_fragment_length` such that desired steps are divided</span>
        <span class="c1"># equally amongst workers or - in &quot;auto&quot; duration mode - set it</span>
        <span class="c1"># to a reasonably small number (10), such that a single `sample()`</span>
        <span class="c1"># call doesn&#39;t take too much time and we can stop evaluation as soon</span>
        <span class="c1"># as possible after the train step is completed.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eval_config_obj</span><span class="o">.</span><span class="n">batch_mode</span> <span class="o">=</span> <span class="s2">&quot;truncate_episodes&quot;</span>
            <span class="n">eval_config_obj</span><span class="o">.</span><span class="n">rollout_fragment_length</span> <span class="o">=</span> <span class="p">(</span>
                <span class="mi">10</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_duration</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span>
                <span class="k">else</span> <span class="nb">int</span><span class="p">(</span>
                    <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_duration</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_num_workers</span> <span class="ow">or</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">eval_config_obj</span></div>

<div class="viewcode-block" id="AlgorithmConfig.get_multi_agent_setup"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_agent_setup.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_agent_setup">[docs]</a>    <span class="k">def</span> <span class="nf">get_multi_agent_setup</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">policies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiAgentPolicyConfigDict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">env</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EnvType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">spaces</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Space</span><span class="p">,</span> <span class="n">Space</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">default_policy_class</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="n">Policy</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">MultiAgentPolicyConfigDict</span><span class="p">,</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="n">SampleBatchType</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compiles complete multi-agent config (dict) from the information in `self`.</span>

<span class="sd">        Infers the observation- and action spaces, the policy classes, and the policy&#39;s</span>
<span class="sd">        configs. The returned `MultiAgentPolicyConfigDict` is fully unified and strictly</span>
<span class="sd">        maps PolicyIDs to complete PolicySpec objects (with all their fields not-None).</span>

<span class="sd">        Examples:</span>
<span class="sd">        .. testcode::</span>

<span class="sd">            import gymnasium as gym</span>
<span class="sd">            from ray.rllib.algorithms.ppo import PPOConfig</span>
<span class="sd">            config = (</span>
<span class="sd">              PPOConfig()</span>
<span class="sd">              .environment(&quot;CartPole-v1&quot;)</span>
<span class="sd">              .framework(&quot;torch&quot;)</span>
<span class="sd">              .multi_agent(policies={&quot;pol1&quot;, &quot;pol2&quot;}, policies_to_train=[&quot;pol1&quot;])</span>
<span class="sd">            )</span>
<span class="sd">            policy_dict, is_policy_to_train = config.get_multi_agent_setup(</span>
<span class="sd">                env=gym.make(&quot;CartPole-v1&quot;))</span>
<span class="sd">            is_policy_to_train(&quot;pol1&quot;)</span>
<span class="sd">            is_policy_to_train(&quot;pol2&quot;)</span>

<span class="sd">        Args:</span>
<span class="sd">            policies: An optional multi-agent `policies` dict, mapping policy IDs</span>
<span class="sd">                to PolicySpec objects. If not provided, will use `self.policies`</span>
<span class="sd">                instead. Note that the `policy_class`, `observation_space`, and</span>
<span class="sd">                `action_space` properties in these PolicySpecs may be None and must</span>
<span class="sd">                therefore be inferred here.</span>
<span class="sd">            env: An optional env instance, from which to infer the different spaces for</span>
<span class="sd">                the different policies. If not provided, will try to infer from</span>
<span class="sd">                `spaces`. Otherwise from `self.observation_space` and</span>
<span class="sd">                `self.action_space`. If no information on spaces can be infered, will</span>
<span class="sd">                raise an error.</span>
<span class="sd">            spaces: Optional dict mapping policy IDs to tuples of 1) observation space</span>
<span class="sd">                and 2) action space that should be used for the respective policy.</span>
<span class="sd">                These spaces were usually provided by an already instantiated remote</span>
<span class="sd">                EnvRunner (usually a RolloutWorker). If not provided, will try to infer</span>
<span class="sd">                from `env`. Otherwise from `self.observation_space` and</span>
<span class="sd">                `self.action_space`. If no information on spaces can be inferred, will</span>
<span class="sd">                raise an error.</span>
<span class="sd">            default_policy_class: The Policy class to use should a PolicySpec have its</span>
<span class="sd">                policy_class property set to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple consisting of 1) a MultiAgentPolicyConfigDict and 2) a</span>
<span class="sd">            `is_policy_to_train(PolicyID, SampleBatchType) -&gt; bool` callable.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: In case, no spaces can be infered for the policy/ies.</span>
<span class="sd">            ValueError: In case, two agents in the env map to the same PolicyID</span>
<span class="sd">                (according to `self.policy_mapping_fn`), but have different action- or</span>
<span class="sd">                observation spaces according to the infered space information.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">policies</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">policies</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">policies</span><span class="p">)</span>

        <span class="c1"># Policies given as set/list/tuple (of PolicyIDs) -&gt; Setup each policy</span>
        <span class="c1"># automatically via empty PolicySpec (will make RLlib infer observation- and</span>
        <span class="c1"># action spaces as well as the Policy&#39;s class).</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">policies</span><span class="p">,</span> <span class="p">(</span><span class="nb">set</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="n">policies</span> <span class="o">=</span> <span class="p">{</span><span class="n">pid</span><span class="p">:</span> <span class="n">PolicySpec</span><span class="p">()</span> <span class="k">for</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">policies</span><span class="p">}</span>

        <span class="c1"># Try extracting spaces from env or from given spaces dict.</span>
        <span class="n">env_obs_space</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">env_act_space</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Env is a ray.remote: Get spaces via its (automatically added)</span>
        <span class="c1"># `_get_spaces()` method.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">ray</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">ActorHandle</span><span class="p">):</span>
            <span class="n">env_obs_space</span><span class="p">,</span> <span class="n">env_act_space</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">_get_spaces</span><span class="o">.</span><span class="n">remote</span><span class="p">())</span>
        <span class="c1"># Normal env (gym.Env or MultiAgentEnv): These should have the</span>
        <span class="c1"># `observation_space` and `action_space` properties.</span>
        <span class="k">elif</span> <span class="n">env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># `env` is a gymnasium.vector.Env.</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="s2">&quot;single_observation_space&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">env</span><span class="o">.</span><span class="n">single_observation_space</span><span class="p">,</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span>
            <span class="p">):</span>
                <span class="n">env_obs_space</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">single_observation_space</span>
            <span class="c1"># `env` is a gymnasium.Env.</span>
            <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="s2">&quot;observation_space&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span>
            <span class="p">):</span>
                <span class="n">env_obs_space</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span>

            <span class="c1"># `env` is a gymnasium.vector.Env.</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="s2">&quot;single_action_space&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">env</span><span class="o">.</span><span class="n">single_action_space</span><span class="p">,</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span>
            <span class="p">):</span>
                <span class="n">env_act_space</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">single_action_space</span>
            <span class="c1"># `env` is a gymnasium.Env.</span>
            <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="s2">&quot;action_space&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span>
            <span class="p">):</span>
                <span class="n">env_act_space</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span>

        <span class="c1"># Last resort: Try getting the env&#39;s spaces from the spaces</span>
        <span class="c1"># dict&#39;s special __env__ key.</span>
        <span class="k">if</span> <span class="n">spaces</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">env_obs_space</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">env_obs_space</span> <span class="o">=</span> <span class="n">spaces</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;__env__&quot;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">env_act_space</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">env_act_space</span> <span class="o">=</span> <span class="n">spaces</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;__env__&quot;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Check each defined policy ID and unify its spec.</span>
        <span class="k">for</span> <span class="n">pid</span><span class="p">,</span> <span class="n">policy_spec</span> <span class="ow">in</span> <span class="n">policies</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Convert to PolicySpec if plain list/tuple.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">policy_spec</span><span class="p">,</span> <span class="n">PolicySpec</span><span class="p">):</span>
                <span class="n">policies</span><span class="p">[</span><span class="n">pid</span><span class="p">]</span> <span class="o">=</span> <span class="n">policy_spec</span> <span class="o">=</span> <span class="n">PolicySpec</span><span class="p">(</span><span class="o">*</span><span class="n">policy_spec</span><span class="p">)</span>

            <span class="c1"># Infer policy classes for policies dict, if not provided (None).</span>
            <span class="k">if</span> <span class="n">policy_spec</span><span class="o">.</span><span class="n">policy_class</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">default_policy_class</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">policies</span><span class="p">[</span><span class="n">pid</span><span class="p">]</span><span class="o">.</span><span class="n">policy_class</span> <span class="o">=</span> <span class="n">default_policy_class</span>

            <span class="c1"># In case - somehow - an old gym Space made it to here, convert it</span>
            <span class="c1"># to the corresponding gymnasium space.</span>
            <span class="k">if</span> <span class="n">old_gym</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">policy_spec</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span> <span class="n">old_gym</span><span class="o">.</span><span class="n">Space</span><span class="p">):</span>
                <span class="n">policies</span><span class="p">[</span>
                    <span class="n">pid</span>
                <span class="p">]</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">convert_old_gym_space_to_gymnasium_space</span><span class="p">(</span>
                    <span class="n">policy_spec</span><span class="o">.</span><span class="n">observation_space</span>
                <span class="p">)</span>
            <span class="c1"># Infer observation space.</span>
            <span class="k">elif</span> <span class="n">policy_spec</span><span class="o">.</span><span class="n">observation_space</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">spaces</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">spaces</span><span class="p">:</span>
                    <span class="n">obs_space</span> <span class="o">=</span> <span class="n">spaces</span><span class="p">[</span><span class="n">pid</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">elif</span> <span class="n">env_obs_space</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># Multi-agent case AND different agents have different spaces:</span>
                    <span class="c1"># Need to reverse map spaces (for the different agents) to certain</span>
                    <span class="c1"># policy IDs.</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="nb">isinstance</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">MultiAgentEnv</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="s2">&quot;_obs_space_in_preferred_format&quot;</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="n">env</span><span class="o">.</span><span class="n">_obs_space_in_preferred_format</span>
                    <span class="p">):</span>
                        <span class="n">obs_space</span> <span class="o">=</span> <span class="kc">None</span>
                        <span class="n">mapping_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy_mapping_fn</span>
                        <span class="k">if</span> <span class="n">mapping_fn</span><span class="p">:</span>
                            <span class="k">for</span> <span class="n">aid</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">get_agent_ids</span><span class="p">():</span>
                                <span class="c1"># Match: Assign spaces for this agentID to the PolicyID.</span>
                                <span class="k">if</span> <span class="n">mapping_fn</span><span class="p">(</span><span class="n">aid</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">worker</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="n">pid</span><span class="p">:</span>
                                    <span class="c1"># Make sure, different agents that map to the same</span>
                                    <span class="c1"># policy don&#39;t have different spaces.</span>
                                    <span class="k">if</span> <span class="p">(</span>
                                        <span class="n">obs_space</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                                        <span class="ow">and</span> <span class="n">env_obs_space</span><span class="p">[</span><span class="n">aid</span><span class="p">]</span> <span class="o">!=</span> <span class="n">obs_space</span>
                                    <span class="p">):</span>
                                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                                            <span class="s2">&quot;Two agents in your environment map to the &quot;</span>
                                            <span class="s2">&quot;same policyID (as per your `policy_mapping&quot;</span>
                                            <span class="s2">&quot;_fn`), however, these agents also have &quot;</span>
                                            <span class="s2">&quot;different observation spaces!&quot;</span>
                                        <span class="p">)</span>
                                    <span class="n">obs_space</span> <span class="o">=</span> <span class="n">env_obs_space</span><span class="p">[</span><span class="n">aid</span><span class="p">]</span>
                    <span class="c1"># Otherwise, just use env&#39;s obs space as-is.</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">obs_space</span> <span class="o">=</span> <span class="n">env_obs_space</span>
                <span class="c1"># Space given directly in config.</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span><span class="p">:</span>
                    <span class="n">obs_space</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;`observation_space` not provided in PolicySpec for &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pid</span><span class="si">}</span><span class="s2"> and env does not have an observation space OR &quot;</span>
                        <span class="s2">&quot;no spaces received from other workers&#39; env(s) OR no &quot;</span>
                        <span class="s2">&quot;`observation_space` specified in config!&quot;</span>
                    <span class="p">)</span>

                <span class="n">policies</span><span class="p">[</span><span class="n">pid</span><span class="p">]</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">obs_space</span>

            <span class="c1"># In case - somehow - an old gym Space made it to here, convert it</span>
            <span class="c1"># to the corresponding gymnasium space.</span>
            <span class="k">if</span> <span class="n">old_gym</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">policy_spec</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span> <span class="n">old_gym</span><span class="o">.</span><span class="n">Space</span><span class="p">):</span>
                <span class="n">policies</span><span class="p">[</span><span class="n">pid</span><span class="p">]</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">convert_old_gym_space_to_gymnasium_space</span><span class="p">(</span>
                    <span class="n">policy_spec</span><span class="o">.</span><span class="n">action_space</span>
                <span class="p">)</span>
            <span class="c1"># Infer action space.</span>
            <span class="k">elif</span> <span class="n">policy_spec</span><span class="o">.</span><span class="n">action_space</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">spaces</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">spaces</span><span class="p">:</span>
                    <span class="n">act_space</span> <span class="o">=</span> <span class="n">spaces</span><span class="p">[</span><span class="n">pid</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
                <span class="k">elif</span> <span class="n">env_act_space</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># Multi-agent case AND different agents have different spaces:</span>
                    <span class="c1"># Need to reverse map spaces (for the different agents) to certain</span>
                    <span class="c1"># policy IDs.</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="nb">isinstance</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">MultiAgentEnv</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="s2">&quot;_action_space_in_preferred_format&quot;</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="n">env</span><span class="o">.</span><span class="n">_action_space_in_preferred_format</span>
                    <span class="p">):</span>
                        <span class="n">act_space</span> <span class="o">=</span> <span class="kc">None</span>
                        <span class="n">mapping_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy_mapping_fn</span>
                        <span class="k">if</span> <span class="n">mapping_fn</span><span class="p">:</span>
                            <span class="k">for</span> <span class="n">aid</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">get_agent_ids</span><span class="p">():</span>
                                <span class="c1"># Match: Assign spaces for this AgentID to the PolicyID.</span>
                                <span class="k">if</span> <span class="n">mapping_fn</span><span class="p">(</span><span class="n">aid</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">worker</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="n">pid</span><span class="p">:</span>
                                    <span class="c1"># Make sure, different agents that map to the same</span>
                                    <span class="c1"># policy don&#39;t have different spaces.</span>
                                    <span class="k">if</span> <span class="p">(</span>
                                        <span class="n">act_space</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                                        <span class="ow">and</span> <span class="n">env_act_space</span><span class="p">[</span><span class="n">aid</span><span class="p">]</span> <span class="o">!=</span> <span class="n">act_space</span>
                                    <span class="p">):</span>
                                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                                            <span class="s2">&quot;Two agents in your environment map to the &quot;</span>
                                            <span class="s2">&quot;same policyID (as per your `policy_mapping&quot;</span>
                                            <span class="s2">&quot;_fn`), however, these agents also have &quot;</span>
                                            <span class="s2">&quot;different action spaces!&quot;</span>
                                        <span class="p">)</span>
                                    <span class="n">act_space</span> <span class="o">=</span> <span class="n">env_act_space</span><span class="p">[</span><span class="n">aid</span><span class="p">]</span>
                    <span class="c1"># Otherwise, just use env&#39;s action space as-is.</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">act_space</span> <span class="o">=</span> <span class="n">env_act_space</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="p">:</span>
                    <span class="n">act_space</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;`action_space` not provided in PolicySpec for &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pid</span><span class="si">}</span><span class="s2"> and env does not have an action space OR &quot;</span>
                        <span class="s2">&quot;no spaces received from other workers&#39; env(s) OR no &quot;</span>
                        <span class="s2">&quot;`action_space` specified in config!&quot;</span>
                    <span class="p">)</span>
                <span class="n">policies</span><span class="p">[</span><span class="n">pid</span><span class="p">]</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">act_space</span>

            <span class="c1"># Create entire AlgorithmConfig object from the provided override.</span>
            <span class="c1"># If None, use {} as override.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">policies</span><span class="p">[</span><span class="n">pid</span><span class="p">]</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">AlgorithmConfig</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">policies</span><span class="p">[</span><span class="n">pid</span><span class="p">]</span><span class="o">.</span><span class="n">config</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">policies</span><span class="p">[</span><span class="n">pid</span><span class="p">]</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="nb">dict</span>
                <span class="p">)</span>
                <span class="n">policies</span><span class="p">[</span><span class="n">pid</span><span class="p">]</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">copy_frozen</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span>
                    <span class="n">policies</span><span class="p">[</span><span class="n">pid</span><span class="p">]</span><span class="o">.</span><span class="n">config</span> <span class="ow">or</span> <span class="p">{}</span>
                <span class="p">)</span>

        <span class="c1"># If container given, construct a simple default callable returning True</span>
        <span class="c1"># if the PolicyID is found in the list/set of IDs.</span>
        <span class="n">is_policy_to_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policies_to_train</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">policies_to_train</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policies_to_train</span><span class="p">):</span>
            <span class="n">pols</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policies_to_train</span><span class="p">)</span>

            <span class="k">def</span> <span class="nf">is_policy_to_train</span><span class="p">(</span><span class="n">pid</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">pols</span>

        <span class="k">return</span> <span class="n">policies</span><span class="p">,</span> <span class="n">is_policy_to_train</span></div>

    <span class="c1"># TODO: Move this to those algorithms that really need this, which is currently</span>
    <span class="c1">#  only A2C and PG.</span>
<div class="viewcode-block" id="AlgorithmConfig.validate_train_batch_size_vs_rollout_fragment_length"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate_train_batch_size_vs_rollout_fragment_length.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate_train_batch_size_vs_rollout_fragment_length">[docs]</a>    <span class="k">def</span> <span class="nf">validate_train_batch_size_vs_rollout_fragment_length</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Detects mismatches for `train_batch_size` vs `rollout_fragment_length`.</span>

<span class="sd">        Only applicable for algorithms, whose train_batch_size should be directly</span>
<span class="sd">        dependent on rollout_fragment_length (synchronous sampling, on-policy PG algos).</span>

<span class="sd">        If rollout_fragment_length != &quot;auto&quot;, makes sure that the product of</span>
<span class="sd">        `rollout_fragment_length` x `num_rollout_workers` x `num_envs_per_worker`</span>
<span class="sd">        roughly (10%) matches the provided `train_batch_size`. Otherwise, errors with</span>
<span class="sd">        asking the user to set rollout_fragment_length to `auto` or to a matching</span>
<span class="sd">        value.</span>

<span class="sd">        Also, only checks this if `train_batch_size` &gt; 0 (DDPPO sets this</span>
<span class="sd">        to -1 to auto-calculate the actual batch size later).</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If there is a mismatch between user provided</span>
<span class="sd">            `rollout_fragment_length` and `train_batch_size`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rollout_fragment_length</span> <span class="o">!=</span> <span class="s2">&quot;auto&quot;</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_evaluation</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="n">min_batch_size</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_rollout_workers</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_envs_per_worker</span>
                <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">rollout_fragment_length</span>
            <span class="p">)</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">min_batch_size</span>
            <span class="k">while</span> <span class="n">batch_size</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span><span class="p">:</span>
                <span class="n">batch_size</span> <span class="o">+=</span> <span class="n">min_batch_size</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">batch_size</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span> <span class="o">&gt;</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span>
                <span class="ow">or</span> <span class="n">batch_size</span> <span class="o">-</span> <span class="n">min_batch_size</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span>
                <span class="o">&gt;</span> <span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">suggested_rollout_fragment_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span> <span class="o">//</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">num_envs_per_worker</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_rollout_workers</span> <span class="ow">or</span> <span class="mi">1</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Your desired `train_batch_size` (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span><span class="si">}</span><span class="s2">) or a &quot;</span>
                    <span class="s2">&quot;value 10</span><span class="si">% o</span><span class="s2">ff of that cannot be achieved with your other &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;settings (num_rollout_workers=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_rollout_workers</span><span class="si">}</span><span class="s2">; &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;num_envs_per_worker=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_envs_per_worker</span><span class="si">}</span><span class="s2">; &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;rollout_fragment_length=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rollout_fragment_length</span><span class="si">}</span><span class="s2">)! &quot;</span>
                    <span class="s2">&quot;Try setting `rollout_fragment_length` to &#39;auto&#39; OR &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">suggested_rollout_fragment_length</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span></div>

<div class="viewcode-block" id="AlgorithmConfig.get_torch_compile_learner_config"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_torch_compile_learner_config.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_torch_compile_learner_config">[docs]</a>    <span class="k">def</span> <span class="nf">get_torch_compile_learner_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the TorchCompileConfig to use on learners.&quot;&quot;&quot;</span>

        <span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.torch.torch_compile_config</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">TorchCompileConfig</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">TorchCompileConfig</span><span class="p">(</span>
            <span class="n">compile_forward_train</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_learner</span><span class="p">,</span>
            <span class="n">torch_dynamo_backend</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_learner_dynamo_backend</span><span class="p">,</span>
            <span class="n">torch_dynamo_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_learner_dynamo_mode</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="AlgorithmConfig.get_torch_compile_worker_config"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_torch_compile_worker_config.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_torch_compile_worker_config">[docs]</a>    <span class="k">def</span> <span class="nf">get_torch_compile_worker_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the TorchCompileConfig to use on workers.&quot;&quot;&quot;</span>

        <span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.torch.torch_compile_config</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">TorchCompileConfig</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">TorchCompileConfig</span><span class="p">(</span>
            <span class="n">compile_forward_exploration</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_worker</span><span class="p">,</span>
            <span class="n">compile_forward_inference</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_worker</span><span class="p">,</span>
            <span class="n">torch_dynamo_backend</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_worker_dynamo_backend</span><span class="p">,</span>
            <span class="n">torch_dynamo_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_worker_dynamo_mode</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="AlgorithmConfig.get_default_rl_module_spec"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_rl_module_spec.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_rl_module_spec">[docs]</a>    <span class="k">def</span> <span class="nf">get_default_rl_module_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModuleSpec</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns the RLModule spec to use for this algorithm.</span>

<span class="sd">        Override this method in the sub-class to return the RLModule spec given</span>
<span class="sd">        the input framework.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The ModuleSpec (SingleAgentRLModuleSpec or MultiAgentRLModuleSpec) to use</span>
<span class="sd">            for this algorithm&#39;s RLModule.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="AlgorithmConfig.get_default_learner_class"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class">[docs]</a>    <span class="k">def</span> <span class="nf">get_default_learner_class</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="s2">&quot;Learner&quot;</span><span class="p">],</span> <span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Returns the Learner class to use for this algorithm.</span>

<span class="sd">        Override this method in the sub-class to return the Learner class type given</span>
<span class="sd">        the input framework.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The Learner class to use for this algorithm either as a class type or as</span>
<span class="sd">            a string (e.g. ray.rllib.core.learner.testing.torch.BCTrainer).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="AlgorithmConfig.get_marl_module_spec"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_marl_module_spec.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_marl_module_spec">[docs]</a>    <span class="k">def</span> <span class="nf">get_marl_module_spec</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">policy_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PolicySpec</span><span class="p">],</span>
        <span class="n">single_agent_rl_module_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SingleAgentRLModuleSpec</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiAgentRLModuleSpec</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns the MultiAgentRLModule spec based on the given policy spec dict.</span>

<span class="sd">        policy_dict could be a partial dict of the policies that we need to turn into</span>
<span class="sd">        an equivalent multi-agent RLModule spec.</span>

<span class="sd">        Args:</span>
<span class="sd">            policy_dict: The policy spec dict. Using this dict, we can determine the</span>
<span class="sd">                inferred values for observation_space, action_space, and config for</span>
<span class="sd">                each policy. If the module spec does not have these values specified,</span>
<span class="sd">                they will get auto-filled with these values obtrained from the policy</span>
<span class="sd">                spec dict. Here we are relying on the policy&#39;s logic for infering these</span>
<span class="sd">                values from other sources of information (e.g. environement)</span>
<span class="sd">            single_agent_rl_module_spec: The SingleAgentRLModuleSpec to use for</span>
<span class="sd">                constructing a MultiAgentRLModuleSpec. If None, the already</span>
<span class="sd">                configured spec (`self.rl_module_spec`) or the default ModuleSpec for</span>
<span class="sd">                this algorithm (`self.get_default_rl_module_spec()`) will be used.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO (Kourosh): When we replace policy entirely there will be no need for</span>
        <span class="c1">#  this function to map policy_dict to marl_module_specs anymore. The module</span>
        <span class="c1">#  spec will be directly given by the user or inferred from env and spaces.</span>

        <span class="c1"># TODO (Kourosh): Raise an error if the config is not frozen (validated)</span>
        <span class="c1"># If the module is single-agent convert it to multi-agent spec</span>

        <span class="c1"># The default ModuleSpec (might be multi-agent or single-agent).</span>
        <span class="n">default_rl_module_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_default_rl_module_spec</span><span class="p">()</span>
        <span class="c1"># The currently configured ModuleSpec (might be multi-agent or single-agent).</span>
        <span class="c1"># If None, use the default one.</span>
        <span class="n">current_rl_module_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rl_module_spec</span> <span class="ow">or</span> <span class="n">default_rl_module_spec</span>

        <span class="c1"># Algorithm is currently setup as a single-agent one.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">current_rl_module_spec</span><span class="p">,</span> <span class="n">SingleAgentRLModuleSpec</span><span class="p">):</span>
            <span class="c1"># Use either the provided `single_agent_rl_module_spec` (a</span>
            <span class="c1"># SingleAgentRLModuleSpec), the currently configured one of this</span>
            <span class="c1"># AlgorithmConfig object, or the default one.</span>
            <span class="n">single_agent_rl_module_spec</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">single_agent_rl_module_spec</span> <span class="ow">or</span> <span class="n">current_rl_module_spec</span>
            <span class="p">)</span>
            <span class="c1"># Now construct the proper MultiAgentRLModuleSpec.</span>
            <span class="n">marl_module_spec</span> <span class="o">=</span> <span class="n">MultiAgentRLModuleSpec</span><span class="p">(</span>
                <span class="n">module_specs</span><span class="o">=</span><span class="p">{</span>
                    <span class="n">k</span><span class="p">:</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">single_agent_rl_module_spec</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">policy_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                <span class="p">},</span>
            <span class="p">)</span>

        <span class="c1"># Algorithm is currently setup as a multi-agent one.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># The user currently has a MultiAgentSpec setup (either via</span>
            <span class="c1"># self.rl_module_spec or the default spec of this AlgorithmConfig).</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">current_rl_module_spec</span><span class="p">,</span> <span class="n">MultiAgentRLModuleSpec</span><span class="p">)</span>

            <span class="c1"># Default is single-agent but the user has provided a multi-agent spec</span>
            <span class="c1"># so the use-case is multi-agent.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">default_rl_module_spec</span><span class="p">,</span> <span class="n">SingleAgentRLModuleSpec</span><span class="p">):</span>
                <span class="c1"># The individual (single-agent) module specs are defined by the user</span>
                <span class="c1"># in the currently setup MultiAgentRLModuleSpec -&gt; Use that</span>
                <span class="c1"># SingleAgentRLModuleSpec.</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">current_rl_module_spec</span><span class="o">.</span><span class="n">module_specs</span><span class="p">,</span> <span class="n">SingleAgentRLModuleSpec</span>
                <span class="p">):</span>
                    <span class="n">single_agent_spec</span> <span class="o">=</span> <span class="n">single_agent_rl_module_spec</span> <span class="ow">or</span> <span class="p">(</span>
                        <span class="n">current_rl_module_spec</span><span class="o">.</span><span class="n">module_specs</span>
                    <span class="p">)</span>
                    <span class="n">module_specs</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="n">k</span><span class="p">:</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">single_agent_spec</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">policy_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                    <span class="p">}</span>

                <span class="c1"># The individual (single-agent) module specs have not been configured</span>
                <span class="c1"># via this AlgorithmConfig object -&gt; Use provided single-agent spec or</span>
                <span class="c1"># the the default spec (which is also a SingleAgentRLModuleSpec in this</span>
                <span class="c1"># case).</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">single_agent_spec</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">single_agent_rl_module_spec</span> <span class="ow">or</span> <span class="n">default_rl_module_spec</span>
                    <span class="p">)</span>
                    <span class="n">module_specs</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="n">k</span><span class="p">:</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span>
                            <span class="n">current_rl_module_spec</span><span class="o">.</span><span class="n">module_specs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                                <span class="n">k</span><span class="p">,</span> <span class="n">single_agent_spec</span>
                            <span class="p">)</span>
                        <span class="p">)</span>
                        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">policy_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                    <span class="p">}</span>

                <span class="c1"># Now construct the proper MultiAgentRLModuleSpec.</span>
                <span class="c1"># We need to infer the multi-agent class from `current_rl_module_spec`</span>
                <span class="c1"># and fill in the module_specs dict.</span>
                <span class="n">marl_module_spec</span> <span class="o">=</span> <span class="n">current_rl_module_spec</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
                    <span class="n">marl_module_class</span><span class="o">=</span><span class="n">current_rl_module_spec</span><span class="o">.</span><span class="n">marl_module_class</span><span class="p">,</span>
                    <span class="n">module_specs</span><span class="o">=</span><span class="n">module_specs</span><span class="p">,</span>
                    <span class="n">modules_to_load</span><span class="o">=</span><span class="n">current_rl_module_spec</span><span class="o">.</span><span class="n">modules_to_load</span><span class="p">,</span>
                    <span class="n">load_state_path</span><span class="o">=</span><span class="n">current_rl_module_spec</span><span class="o">.</span><span class="n">load_state_path</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="c1"># Default is multi-agent and user wants to override it -&gt; Don&#39;t use the</span>
            <span class="c1"># default.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Use has given an override SingleAgentRLModuleSpec -&gt; Use this to</span>
                <span class="c1"># construct the individual RLModules within the MultiAgentRLModuleSpec.</span>
                <span class="k">if</span> <span class="n">single_agent_rl_module_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">pass</span>
                <span class="c1"># User has NOT provided an override SingleAgentRLModuleSpec.</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># But the currently setup multi-agent spec has a SingleAgentRLModule</span>
                    <span class="c1"># spec defined -&gt; Use that to construct the individual RLModules</span>
                    <span class="c1"># within the MultiAgentRLModuleSpec.</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
                        <span class="n">current_rl_module_spec</span><span class="o">.</span><span class="n">module_specs</span><span class="p">,</span> <span class="n">SingleAgentRLModuleSpec</span>
                    <span class="p">):</span>
                        <span class="c1"># The individual module specs are not given, it is given as one</span>
                        <span class="c1"># SingleAgentRLModuleSpec to be re-used for all</span>
                        <span class="n">single_agent_rl_module_spec</span> <span class="o">=</span> <span class="p">(</span>
                            <span class="n">current_rl_module_spec</span><span class="o">.</span><span class="n">module_specs</span>
                        <span class="p">)</span>
                    <span class="c1"># The currently setup multi-agent spec has NO</span>
                    <span class="c1"># SingleAgentRLModuleSpec in it -&gt; Error (there is no way we can</span>
                    <span class="c1"># infer this information from anywhere at this point).</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;We have a MultiAgentRLModuleSpec &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">current_rl_module_spec</span><span class="si">}</span><span class="s2">), but no &quot;</span>
                            <span class="s2">&quot;`SingleAgentRLModuleSpec`s to compile the individual &quot;</span>
                            <span class="s2">&quot;RLModules&#39; specs! Use &quot;</span>
                            <span class="s2">&quot;`AlgorithmConfig.get_marl_module_spec(&quot;</span>
                            <span class="s2">&quot;policy_dict=.., single_agent_rl_module_spec=..)`.&quot;</span>
                        <span class="p">)</span>

                <span class="c1"># Now construct the proper MultiAgentRLModuleSpec.</span>
                <span class="n">marl_module_spec</span> <span class="o">=</span> <span class="n">current_rl_module_spec</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
                    <span class="n">marl_module_class</span><span class="o">=</span><span class="n">current_rl_module_spec</span><span class="o">.</span><span class="n">marl_module_class</span><span class="p">,</span>
                    <span class="n">module_specs</span><span class="o">=</span><span class="p">{</span>
                        <span class="n">k</span><span class="p">:</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">single_agent_rl_module_spec</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">policy_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                    <span class="p">},</span>
                    <span class="n">modules_to_load</span><span class="o">=</span><span class="n">current_rl_module_spec</span><span class="o">.</span><span class="n">modules_to_load</span><span class="p">,</span>
                    <span class="n">load_state_path</span><span class="o">=</span><span class="n">current_rl_module_spec</span><span class="o">.</span><span class="n">load_state_path</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="c1"># Make sure that policy_dict and marl_module_spec have similar keys</span>
        <span class="k">if</span> <span class="nb">set</span><span class="p">(</span><span class="n">policy_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">!=</span> <span class="nb">set</span><span class="p">(</span><span class="n">marl_module_spec</span><span class="o">.</span><span class="n">module_specs</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Policy dict and module spec have different keys! </span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;policy_dict keys: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">policy_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;module_spec keys: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">marl_module_spec</span><span class="o">.</span><span class="n">module_specs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Fill in the missing values from the specs that we already have. By combining</span>
        <span class="c1"># PolicySpecs and the default RLModuleSpec.</span>

        <span class="k">for</span> <span class="n">module_id</span> <span class="ow">in</span> <span class="n">policy_dict</span><span class="p">:</span>
            <span class="n">policy_spec</span> <span class="o">=</span> <span class="n">policy_dict</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span>
            <span class="n">module_spec</span> <span class="o">=</span> <span class="n">marl_module_spec</span><span class="o">.</span><span class="n">module_specs</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">module_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">default_rl_module_spec</span><span class="p">,</span> <span class="n">SingleAgentRLModuleSpec</span><span class="p">):</span>
                    <span class="n">module_spec</span><span class="o">.</span><span class="n">module_class</span> <span class="o">=</span> <span class="n">default_rl_module_spec</span><span class="o">.</span><span class="n">module_class</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">default_rl_module_spec</span><span class="o">.</span><span class="n">module_specs</span><span class="p">,</span> <span class="n">SingleAgentRLModuleSpec</span>
                <span class="p">):</span>
                    <span class="n">module_class</span> <span class="o">=</span> <span class="n">default_rl_module_spec</span><span class="o">.</span><span class="n">module_specs</span><span class="o">.</span><span class="n">module_class</span>
                    <span class="c1"># This should be already checked in validate() but we check it</span>
                    <span class="c1"># again here just in case</span>
                    <span class="k">if</span> <span class="n">module_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;The default rl_module spec cannot have an empty &quot;</span>
                            <span class="s2">&quot;module_class under its SingleAgentRLModuleSpec.&quot;</span>
                        <span class="p">)</span>
                    <span class="n">module_spec</span><span class="o">.</span><span class="n">module_class</span> <span class="o">=</span> <span class="n">module_class</span>
                <span class="k">elif</span> <span class="n">module_id</span> <span class="ow">in</span> <span class="n">default_rl_module_spec</span><span class="o">.</span><span class="n">module_specs</span><span class="p">:</span>
                    <span class="n">module_spec</span><span class="o">.</span><span class="n">module_class</span> <span class="o">=</span> <span class="n">default_rl_module_spec</span><span class="o">.</span><span class="n">module_specs</span><span class="p">[</span>
                        <span class="n">module_id</span>
                    <span class="p">]</span><span class="o">.</span><span class="n">module_class</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Module class for module </span><span class="si">{</span><span class="n">module_id</span><span class="si">}</span><span class="s2"> cannot be inferred. &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;It is neither provided in the rl_module_spec that &quot;</span>
                        <span class="s2">&quot;is passed in nor in the default module spec used in &quot;</span>
                        <span class="s2">&quot;the algorithm.&quot;</span>
                    <span class="p">)</span>
            <span class="k">if</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">catalog_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">default_rl_module_spec</span><span class="p">,</span> <span class="n">SingleAgentRLModuleSpec</span><span class="p">):</span>
                    <span class="n">module_spec</span><span class="o">.</span><span class="n">catalog_class</span> <span class="o">=</span> <span class="n">default_rl_module_spec</span><span class="o">.</span><span class="n">catalog_class</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">default_rl_module_spec</span><span class="o">.</span><span class="n">module_specs</span><span class="p">,</span> <span class="n">SingleAgentRLModuleSpec</span>
                <span class="p">):</span>
                    <span class="n">catalog_class</span> <span class="o">=</span> <span class="n">default_rl_module_spec</span><span class="o">.</span><span class="n">module_specs</span><span class="o">.</span><span class="n">catalog_class</span>
                    <span class="n">module_spec</span><span class="o">.</span><span class="n">catalog_class</span> <span class="o">=</span> <span class="n">catalog_class</span>
                <span class="k">elif</span> <span class="n">module_id</span> <span class="ow">in</span> <span class="n">default_rl_module_spec</span><span class="o">.</span><span class="n">module_specs</span><span class="p">:</span>
                    <span class="n">module_spec</span><span class="o">.</span><span class="n">catalog_class</span> <span class="o">=</span> <span class="n">default_rl_module_spec</span><span class="o">.</span><span class="n">module_specs</span><span class="p">[</span>
                        <span class="n">module_id</span>
                    <span class="p">]</span><span class="o">.</span><span class="n">catalog_class</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Catalog class for module </span><span class="si">{</span><span class="n">module_id</span><span class="si">}</span><span class="s2"> cannot be inferred. &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;It is neither provided in the rl_module_spec that &quot;</span>
                        <span class="s2">&quot;is passed in nor in the default module spec used in &quot;</span>
                        <span class="s2">&quot;the algorithm.&quot;</span>
                    <span class="p">)</span>

            <span class="k">if</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">observation_space</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">module_spec</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">policy_spec</span><span class="o">.</span><span class="n">observation_space</span>
            <span class="k">if</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">action_space</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">module_spec</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">policy_spec</span><span class="o">.</span><span class="n">action_space</span>
            <span class="k">if</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">model_config_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">module_spec</span><span class="o">.</span><span class="n">model_config_dict</span> <span class="o">=</span> <span class="n">policy_spec</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="p">{})</span>

        <span class="k">return</span> <span class="n">marl_module_spec</span></div>

    <span class="k">def</span> <span class="nf">get_learner_group_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_spec</span><span class="p">:</span> <span class="n">ModuleSpec</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LearnerGroupConfig</span><span class="p">:</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_frozen</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot call `get_learner_group_config()` on an unfrozen &quot;</span>
                <span class="s2">&quot;AlgorithmConfig! Please call `AlgorithmConfig.freeze()` first.&quot;</span>
            <span class="p">)</span>

        <span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">LearnerGroupConfig</span><span class="p">()</span>
            <span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="n">module_spec</span><span class="p">)</span>
            <span class="o">.</span><span class="n">learner</span><span class="p">(</span>
                <span class="n">learner_class</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_class</span><span class="p">,</span>
                <span class="n">learner_hyperparameters</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_learner_hyperparameters</span><span class="p">(),</span>
            <span class="p">)</span>
            <span class="o">.</span><span class="n">resources</span><span class="p">(</span>
                <span class="n">num_learner_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_learner_workers</span><span class="p">,</span>
                <span class="n">num_cpus_per_learner_worker</span><span class="o">=</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">num_cpus_per_learner_worker</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_gpus_per_learner_worker</span>
                    <span class="k">else</span> <span class="mi">0</span>
                <span class="p">),</span>
                <span class="n">num_gpus_per_learner_worker</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_gpus_per_learner_worker</span><span class="p">,</span>
                <span class="n">local_gpu_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">local_gpu_idx</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span> <span class="o">==</span> <span class="s2">&quot;torch&quot;</span><span class="p">:</span>
            <span class="n">config</span><span class="o">.</span><span class="n">framework</span><span class="p">(</span><span class="n">torch_compile_cfg</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_torch_compile_learner_config</span><span class="p">())</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span> <span class="o">==</span> <span class="s2">&quot;tf2&quot;</span><span class="p">:</span>
            <span class="n">config</span><span class="o">.</span><span class="n">framework</span><span class="p">(</span><span class="n">eager_tracing</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eager_tracing</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">config</span>

<div class="viewcode-block" id="AlgorithmConfig.get_learner_hyperparameters"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_learner_hyperparameters.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_learner_hyperparameters">[docs]</a>    <span class="k">def</span> <span class="nf">get_learner_hyperparameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LearnerHyperparameters</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns a new LearnerHyperparameters instance for the respective Learner.</span>

<span class="sd">        The LearnerHyperparameters is a dataclass containing only those config settings</span>
<span class="sd">        from AlgorithmConfig that are used by the algorithm&#39;s specific Learner</span>
<span class="sd">        sub-class. They allow distributing only those settings relevant for learning</span>
<span class="sd">        across a set of learner workers (instead of having to distribute the entire</span>
<span class="sd">        AlgorithmConfig object).</span>

<span class="sd">        Note that LearnerHyperparameters should always be derived directly from a</span>
<span class="sd">        AlgorithmConfig object&#39;s own settings and considered frozen/read-only.</span>

<span class="sd">        Returns:</span>
<span class="sd">             A LearnerHyperparameters instance for the respective Learner.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Compile the per-module learner hyperparameter instances (if applicable).</span>
        <span class="n">per_module_learner_hp_overrides</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_config_overrides_per_module</span><span class="p">:</span>
            <span class="k">for</span> <span class="p">(</span>
                <span class="n">module_id</span><span class="p">,</span>
                <span class="n">overrides</span><span class="p">,</span>
            <span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm_config_overrides_per_module</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># Copy this AlgorithmConfig object (unfreeze copy), update copy from</span>
                <span class="c1"># the provided override dict for this module_id, then</span>
                <span class="c1"># create a new LearnerHyperparameter object from this altered</span>
                <span class="c1"># AlgorithmConfig.</span>
                <span class="n">config_for_module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">copy_frozen</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span>
                    <span class="n">overrides</span>
                <span class="p">)</span>
                <span class="n">config_for_module</span><span class="o">.</span><span class="n">algorithm_config_overrides_per_module</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">per_module_learner_hp_overrides</span><span class="p">[</span>
                    <span class="n">module_id</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">config_for_module</span><span class="o">.</span><span class="n">get_learner_hyperparameters</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">LearnerHyperparameters</span><span class="p">(</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">grad_clip</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_clip</span><span class="p">,</span>
            <span class="n">grad_clip_by</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_clip_by</span><span class="p">,</span>
            <span class="n">_per_module_overrides</span><span class="o">=</span><span class="n">per_module_learner_hp_overrides</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gatekeeper in case we are in frozen state and need to error.&quot;&quot;&quot;</span>

        <span class="c1"># If we are frozen, do not allow to set any attributes anymore.</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_is_frozen&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_frozen</span><span class="p">:</span>
            <span class="c1"># TODO: Remove `simple_optimizer` entirely.</span>
            <span class="c1">#  Remove need to set `worker_index` in RolloutWorker&#39;s c&#39;tor.</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;simple_optimizer&quot;</span><span class="p">,</span> <span class="s2">&quot;worker_index&quot;</span><span class="p">,</span> <span class="s2">&quot;_is_frozen&quot;</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Cannot set attribute (</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">) of an already frozen &quot;</span>
                    <span class="s2">&quot;AlgorithmConfig!&quot;</span>
                <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Shim method to still support accessing properties by key lookup.</span>

<span class="sd">        This way, an AlgorithmConfig object can still be used as if a dict, e.g.</span>
<span class="sd">        by Ray Tune.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from ray.rllib.algorithms.algorithm_config import AlgorithmConfig</span>
<span class="sd">            &gt;&gt;&gt; config = AlgorithmConfig()</span>
<span class="sd">            &gt;&gt;&gt; print(config[&quot;lr&quot;])</span>
<span class="sd">            ... 0.001</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO: Uncomment this once all algorithms use AlgorithmConfigs under the</span>
        <span class="c1">#  hood (as well as Ray Tune).</span>
        <span class="c1"># if log_once(&quot;algo_config_getitem&quot;):</span>
        <span class="c1">#    logger.warning(</span>
        <span class="c1">#        &quot;AlgorithmConfig objects should NOT be used as dict! &quot;</span>
        <span class="c1">#        f&quot;Try accessing `{item}` directly as a property.&quot;</span>
        <span class="c1">#    )</span>
        <span class="c1"># In case user accesses &quot;old&quot; keys, e.g. &quot;num_workers&quot;, which need to</span>
        <span class="c1"># be translated to their correct property names.</span>
        <span class="n">item</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_translate_special_keys</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__setitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="c1"># TODO: Remove comments once all methods/functions only support</span>
        <span class="c1">#  AlgorithmConfigs and there is no more ambiguity anywhere in the code</span>
        <span class="c1">#  on whether an AlgorithmConfig is used or an old python config dict.</span>
        <span class="c1"># raise AttributeError(</span>
        <span class="c1">#    &quot;AlgorithmConfig objects should not have their values set like dicts&quot;</span>
        <span class="c1">#    f&quot;(`config[&#39;{key}&#39;] = {value}`), &quot;</span>
        <span class="c1">#    f&quot;but via setting their properties directly (config.{prop} = {value}).&quot;</span>
        <span class="c1"># )</span>
        <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;multiagent&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot set `multiagent` key in an AlgorithmConfig!</span><span class="se">\n</span><span class="s2">Try setting &quot;</span>
                <span class="s2">&quot;the multi-agent components of your AlgorithmConfig object via the &quot;</span>
                <span class="s2">&quot;`multi_agent()` method and its arguments.</span><span class="se">\n</span><span class="s2">E.g. `config.multi_agent(&quot;</span>
                <span class="s2">&quot;policies=.., policy_mapping_fn.., policies_to_train=..)`.&quot;</span>
            <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__contains__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Shim method to help pretend we are a dict.&quot;&quot;&quot;</span>
        <span class="n">prop</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_translate_special_keys</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">warn_deprecated</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prop</span><span class="p">)</span>

<div class="viewcode-block" id="AlgorithmConfig.get"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get">[docs]</a>    <span class="k">def</span> <span class="nf">get</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Shim method to help pretend we are a dict.&quot;&quot;&quot;</span>
        <span class="n">prop</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_translate_special_keys</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">warn_deprecated</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prop</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span></div>

<div class="viewcode-block" id="AlgorithmConfig.pop"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.pop.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.pop">[docs]</a>    <span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Shim method to help pretend we are a dict.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span></div>

<div class="viewcode-block" id="AlgorithmConfig.keys"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.keys.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.keys">[docs]</a>    <span class="k">def</span> <span class="nf">keys</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Shim method to help pretend we are a dict.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span></div>

<div class="viewcode-block" id="AlgorithmConfig.values"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.values.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.values">[docs]</a>    <span class="k">def</span> <span class="nf">values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Shim method to help pretend we are a dict.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">()</span></div>

<div class="viewcode-block" id="AlgorithmConfig.items"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.items.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.items">[docs]</a>    <span class="k">def</span> <span class="nf">items</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Shim method to help pretend we are a dict.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()</span></div>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_serialize_dict</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
        <span class="c1"># Serialize classes to classpaths:</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;callbacks&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">serialize_type</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;callbacks&quot;</span><span class="p">])</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;sample_collector&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">serialize_type</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;sample_collector&quot;</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;env&quot;</span><span class="p">],</span> <span class="nb">type</span><span class="p">):</span>
            <span class="n">config</span><span class="p">[</span><span class="s2">&quot;env&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">serialize_type</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;env&quot;</span><span class="p">])</span>
        <span class="k">if</span> <span class="s2">&quot;replay_buffer_config&quot;</span> <span class="ow">in</span> <span class="n">config</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;type&quot;</span><span class="p">),</span> <span class="nb">type</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">config</span><span class="p">[</span><span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">][</span><span class="s2">&quot;type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">serialize_type</span><span class="p">(</span>
                <span class="n">config</span><span class="p">[</span><span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">][</span><span class="s2">&quot;type&quot;</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;exploration_config&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;type&quot;</span><span class="p">),</span> <span class="nb">type</span><span class="p">):</span>
            <span class="n">config</span><span class="p">[</span><span class="s2">&quot;exploration_config&quot;</span><span class="p">][</span><span class="s2">&quot;type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">serialize_type</span><span class="p">(</span>
                <span class="n">config</span><span class="p">[</span><span class="s2">&quot;exploration_config&quot;</span><span class="p">][</span><span class="s2">&quot;type&quot;</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;custom_model&quot;</span><span class="p">),</span> <span class="nb">type</span><span class="p">):</span>
            <span class="n">config</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">][</span><span class="s2">&quot;custom_model&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">serialize_type</span><span class="p">(</span>
                <span class="n">config</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">][</span><span class="s2">&quot;custom_model&quot;</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="c1"># List&#39;ify `policies`, iff a set or tuple (these types are not JSON&#39;able).</span>
        <span class="n">ma_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;multiagent&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ma_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ma_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;policies&quot;</span><span class="p">),</span> <span class="p">(</span><span class="nb">set</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="n">ma_config</span><span class="p">[</span><span class="s2">&quot;policies&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ma_config</span><span class="p">[</span><span class="s2">&quot;policies&quot;</span><span class="p">])</span>
            <span class="c1"># Do NOT serialize functions/lambdas.</span>
            <span class="k">if</span> <span class="n">ma_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;policy_mapping_fn&quot;</span><span class="p">):</span>
                <span class="n">ma_config</span><span class="p">[</span><span class="s2">&quot;policy_mapping_fn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">NOT_SERIALIZABLE</span>
            <span class="k">if</span> <span class="n">ma_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;policies_to_train&quot;</span><span class="p">):</span>
                <span class="n">ma_config</span><span class="p">[</span><span class="s2">&quot;policies_to_train&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">NOT_SERIALIZABLE</span>
        <span class="c1"># However, if these &quot;multiagent&quot; settings have been provided directly</span>
        <span class="c1"># on the top-level (as they should), we override the settings under</span>
        <span class="c1"># &quot;multiagent&quot;. Note that the &quot;multiagent&quot; key should no longer be used anyways.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;policies&quot;</span><span class="p">),</span> <span class="p">(</span><span class="nb">set</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="n">config</span><span class="p">[</span><span class="s2">&quot;policies&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;policies&quot;</span><span class="p">])</span>
        <span class="c1"># Do NOT serialize functions/lambdas.</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;policy_mapping_fn&quot;</span><span class="p">):</span>
            <span class="n">config</span><span class="p">[</span><span class="s2">&quot;policy_mapping_fn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">NOT_SERIALIZABLE</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;policies_to_train&quot;</span><span class="p">):</span>
            <span class="n">config</span><span class="p">[</span><span class="s2">&quot;policies_to_train&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">NOT_SERIALIZABLE</span>

        <span class="k">return</span> <span class="n">config</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_translate_special_keys</span><span class="p">(</span><span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">warn_deprecated</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="c1"># Handle special key (str) -&gt; `AlgorithmConfig.[some_property]` cases.</span>
        <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;callbacks&quot;</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;callbacks_class&quot;</span>
        <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;create_env_on_driver&quot;</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;create_env_on_local_worker&quot;</span>
        <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;custom_eval_function&quot;</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;custom_evaluation_function&quot;</span>
        <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;framework_str&quot;</span>
        <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;input&quot;</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;input_&quot;</span>
        <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;lambda&quot;</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;lambda_&quot;</span>
        <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;num_cpus_for_driver&quot;</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;num_cpus_for_local_worker&quot;</span>
        <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;num_workers&quot;</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;num_rollout_workers&quot;</span>

        <span class="c1"># Deprecated keys.</span>
        <span class="k">if</span> <span class="n">warn_deprecated</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;collect_metrics_timeout&quot;</span><span class="p">:</span>
                <span class="n">deprecation_warning</span><span class="p">(</span>
                    <span class="n">old</span><span class="o">=</span><span class="s2">&quot;collect_metrics_timeout&quot;</span><span class="p">,</span>
                    <span class="n">new</span><span class="o">=</span><span class="s2">&quot;metrics_episode_collection_timeout_s&quot;</span><span class="p">,</span>
                    <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;metrics_smoothing_episodes&quot;</span><span class="p">:</span>
                <span class="n">deprecation_warning</span><span class="p">(</span>
                    <span class="n">old</span><span class="o">=</span><span class="s2">&quot;config.metrics_smoothing_episodes&quot;</span><span class="p">,</span>
                    <span class="n">new</span><span class="o">=</span><span class="s2">&quot;config.metrics_num_episodes_for_smoothing&quot;</span><span class="p">,</span>
                    <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;min_iter_time_s&quot;</span><span class="p">:</span>
                <span class="n">deprecation_warning</span><span class="p">(</span>
                    <span class="n">old</span><span class="o">=</span><span class="s2">&quot;config.min_iter_time_s&quot;</span><span class="p">,</span>
                    <span class="n">new</span><span class="o">=</span><span class="s2">&quot;config.min_time_s_per_iteration&quot;</span><span class="p">,</span>
                    <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;min_time_s_per_reporting&quot;</span><span class="p">:</span>
                <span class="n">deprecation_warning</span><span class="p">(</span>
                    <span class="n">old</span><span class="o">=</span><span class="s2">&quot;config.min_time_s_per_reporting&quot;</span><span class="p">,</span>
                    <span class="n">new</span><span class="o">=</span><span class="s2">&quot;config.min_time_s_per_iteration&quot;</span><span class="p">,</span>
                    <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;min_sample_timesteps_per_reporting&quot;</span><span class="p">:</span>
                <span class="n">deprecation_warning</span><span class="p">(</span>
                    <span class="n">old</span><span class="o">=</span><span class="s2">&quot;config.min_sample_timesteps_per_reporting&quot;</span><span class="p">,</span>
                    <span class="n">new</span><span class="o">=</span><span class="s2">&quot;config.min_sample_timesteps_per_iteration&quot;</span><span class="p">,</span>
                    <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;min_train_timesteps_per_reporting&quot;</span><span class="p">:</span>
                <span class="n">deprecation_warning</span><span class="p">(</span>
                    <span class="n">old</span><span class="o">=</span><span class="s2">&quot;config.min_train_timesteps_per_reporting&quot;</span><span class="p">,</span>
                    <span class="n">new</span><span class="o">=</span><span class="s2">&quot;config.min_train_timesteps_per_iteration&quot;</span><span class="p">,</span>
                    <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;timesteps_per_iteration&quot;</span><span class="p">:</span>
                <span class="n">deprecation_warning</span><span class="p">(</span>
                    <span class="n">old</span><span class="o">=</span><span class="s2">&quot;config.timesteps_per_iteration&quot;</span><span class="p">,</span>
                    <span class="n">new</span><span class="o">=</span><span class="s2">&quot;`config.min_sample_timesteps_per_iteration` OR &quot;</span>
                    <span class="s2">&quot;`config.min_train_timesteps_per_iteration`&quot;</span><span class="p">,</span>
                    <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;evaluation_num_episodes&quot;</span><span class="p">:</span>
                <span class="n">deprecation_warning</span><span class="p">(</span>
                    <span class="n">old</span><span class="o">=</span><span class="s2">&quot;config.evaluation_num_episodes&quot;</span><span class="p">,</span>
                    <span class="n">new</span><span class="o">=</span><span class="s2">&quot;`config.evaluation_duration` and &quot;</span>
                    <span class="s2">&quot;`config.evaluation_duration_unit=episodes`&quot;</span><span class="p">,</span>
                    <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">key</span>

    <span class="k">def</span> <span class="nf">_check_if_correct_nn_framework_installed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_tf1</span><span class="p">,</span> <span class="n">_tf</span><span class="p">,</span> <span class="n">_torch</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check if tf/torch experiment is running and tf/torch installed.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;tf&quot;</span><span class="p">,</span> <span class="s2">&quot;tf2&quot;</span><span class="p">}:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">_tf1</span> <span class="ow">or</span> <span class="n">_tf</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
                    <span class="p">(</span>
                        <span class="s2">&quot;TensorFlow was specified as the framework to use (via `config.&quot;</span>
                        <span class="s2">&quot;framework([tf|tf2])`)! However, no installation was &quot;</span>
                        <span class="s2">&quot;found. You can install TensorFlow via `pip install tensorflow`&quot;</span>
                    <span class="p">)</span>
                <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span> <span class="o">==</span> <span class="s2">&quot;torch&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">_torch</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
                    <span class="p">(</span>
                        <span class="s2">&quot;PyTorch was specified as the framework to use (via `config.&quot;</span>
                        <span class="s2">&quot;framework(&#39;torch&#39;)`)! However, no installation was found. You &quot;</span>
                        <span class="s2">&quot;can install PyTorch via `pip install torch`.&quot;</span>
                    <span class="p">)</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_resolve_tf_settings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_tf1</span><span class="p">,</span> <span class="n">_tfv</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check and resolve tf settings.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">_tf1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span> <span class="o">==</span> <span class="s2">&quot;tf2&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span> <span class="o">==</span> <span class="s2">&quot;tf2&quot;</span> <span class="ow">and</span> <span class="n">_tfv</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;You configured `framework`=tf2, but your installed &quot;</span>
                    <span class="s2">&quot;pip tf-version is &lt; 2.0! Make sure your TensorFlow &quot;</span>
                    <span class="s2">&quot;version is &gt;= 2.x.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">_tf1</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
                <span class="n">_tf1</span><span class="o">.</span><span class="n">enable_eager_execution</span><span class="p">()</span>
            <span class="c1"># Recommend setting tracing to True for speedups.</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Executing eagerly (framework=&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span><span class="si">}</span><span class="s2">&#39;),&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; with eager_tracing=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">eager_tracing</span><span class="si">}</span><span class="s2">. For &quot;</span>
                <span class="s2">&quot;production workloads, make sure to set eager_tracing=True&quot;</span>
                <span class="s2">&quot;  in order to match the speed of tf-static-graph &quot;</span>
                <span class="s2">&quot;(framework=&#39;tf&#39;). For debugging purposes, &quot;</span>
                <span class="s2">&quot;`eager_tracing=False` is the best choice.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Tf-static-graph (framework=tf): Recommend upgrading to tf2 and</span>
        <span class="c1"># enabling eager tracing for similar speed.</span>
        <span class="k">elif</span> <span class="n">_tf1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework_str</span> <span class="o">==</span> <span class="s2">&quot;tf&quot;</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Your framework setting is &#39;tf&#39;, meaning you are using &quot;</span>
                <span class="s2">&quot;static-graph mode. Set framework=&#39;tf2&#39; to enable eager &quot;</span>
                <span class="s2">&quot;execution with tf2.x. You may also then want to set &quot;</span>
                <span class="s2">&quot;eager_tracing=True in order to reach similar execution &quot;</span>
                <span class="s2">&quot;speed as with static-graph mode.&quot;</span>
            <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="nd">@Deprecated</span><span class="p">(</span><span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">multiagent</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Shim method to help pretend we are a dict with &#39;multiagent&#39; key.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;policies&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">policies</span><span class="p">,</span>
            <span class="s2">&quot;policy_mapping_fn&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy_mapping_fn</span><span class="p">,</span>
            <span class="s2">&quot;policies_to_train&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">policies_to_train</span><span class="p">,</span>
            <span class="s2">&quot;policy_map_capacity&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy_map_capacity</span><span class="p">,</span>
            <span class="s2">&quot;policy_map_cache&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy_map_cache</span><span class="p">,</span>
            <span class="s2">&quot;count_steps_by&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_steps_by</span><span class="p">,</span>
            <span class="s2">&quot;observation_fn&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">observation_fn</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="nd">@property</span>
    <span class="nd">@Deprecated</span><span class="p">(</span><span class="n">new</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.rollouts(num_rollout_workers=..)&quot;</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">num_workers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;For backward-compatibility purposes only.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_rollout_workers</span></div>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>