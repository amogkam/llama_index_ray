
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ray.rllib.algorithms.algorithm &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../../../../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script src="../../../../_static/js/versionwarning.js"></script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../../../_static/js/docsearch.js"></script>
    <script src="../../../../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../../../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../../../_static/js/custom.js"></script>
    <script defer="defer" src="../../../../_static/js/top-navigation.js"></script>
    <script src="../../../../_static/js/tags.js"></script>
    <script src="../../../../_static/tabs.js"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../../_static/design-tabs.js"></script>
    <script async="async" src="../../../../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/_modules/ray/rllib/algorithms/algorithm.html" />
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "_modules/ray/rllib/algorithms/algorithm", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2F_modules/ray/rllib/algorithms/algorithm.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for ray.rllib.algorithms.algorithm</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span> <span class="nn">concurrent</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">importlib</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">packaging</span> <span class="kn">import</span> <span class="n">version</span>
<span class="kn">import</span> <span class="nn">pkg_resources</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">tree</span>  <span class="c1"># pip install dm_tree</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Container</span><span class="p">,</span>
    <span class="n">DefaultDict</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Set</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Type</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray._private.usage.usage_lib</span> <span class="kn">import</span> <span class="n">TagKey</span><span class="p">,</span> <span class="n">record_extra_usage_tag</span>
<span class="kn">from</span> <span class="nn">ray.actor</span> <span class="kn">import</span> <span class="n">ActorHandle</span>
<span class="kn">from</span> <span class="nn">ray.air.checkpoint</span> <span class="kn">import</span> <span class="n">Checkpoint</span>
<span class="kn">import</span> <span class="nn">ray.cloudpickle</span> <span class="k">as</span> <span class="nn">pickle</span>

<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.registry</span> <span class="kn">import</span> <span class="n">ALGORITHMS_CLASS_TO_NAME</span> <span class="k">as</span> <span class="n">ALL_ALGORITHMS</span>
<span class="kn">from</span> <span class="nn">ray.rllib.connectors.agent.obs_preproc</span> <span class="kn">import</span> <span class="n">ObsPreprocessorConnector</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.rl_module</span> <span class="kn">import</span> <span class="n">SingleAgentRLModuleSpec</span>
<span class="kn">from</span> <span class="nn">ray.rllib.env.env_context</span> <span class="kn">import</span> <span class="n">EnvContext</span>
<span class="kn">from</span> <span class="nn">ray.rllib.env.utils</span> <span class="kn">import</span> <span class="n">_gym_env_creator</span>
<span class="kn">from</span> <span class="nn">ray.rllib.evaluation.episode</span> <span class="kn">import</span> <span class="n">Episode</span>
<span class="kn">from</span> <span class="nn">ray.rllib.evaluation.metrics</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">collect_episodes</span><span class="p">,</span>
    <span class="n">collect_metrics</span><span class="p">,</span>
    <span class="n">summarize_episodes</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.evaluation.rollout_worker</span> <span class="kn">import</span> <span class="n">RolloutWorker</span>
<span class="kn">from</span> <span class="nn">ray.rllib.evaluation.worker_set</span> <span class="kn">import</span> <span class="n">WorkerSet</span>
<span class="kn">from</span> <span class="nn">ray.rllib.execution.common</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">STEPS_TRAINED_THIS_ITER_COUNTER</span><span class="p">,</span>  <span class="c1"># TODO: Backward compatibility.</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.execution.rollout_ops</span> <span class="kn">import</span> <span class="n">synchronous_parallel_sample</span>
<span class="kn">from</span> <span class="nn">ray.rllib.execution.train_ops</span> <span class="kn">import</span> <span class="n">multi_gpu_train_one_step</span><span class="p">,</span> <span class="n">train_one_step</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline</span> <span class="kn">import</span> <span class="n">get_dataset_and_shards</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline.estimators</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">OffPolicyEstimator</span><span class="p">,</span>
    <span class="n">ImportanceSampling</span><span class="p">,</span>
    <span class="n">WeightedImportanceSampling</span><span class="p">,</span>
    <span class="n">DirectMethod</span><span class="p">,</span>
    <span class="n">DoublyRobust</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline.offline_evaluator</span> <span class="kn">import</span> <span class="n">OfflineEvaluator</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.policy</span> <span class="kn">import</span> <span class="n">Policy</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.sample_batch</span> <span class="kn">import</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span> <span class="n">SampleBatch</span><span class="p">,</span> <span class="n">concat_samples</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils</span> <span class="kn">import</span> <span class="n">deep_update</span><span class="p">,</span> <span class="n">FilterManager</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.annotations</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DeveloperAPI</span><span class="p">,</span>
    <span class="n">ExperimentalAPI</span><span class="p">,</span>
    <span class="n">OverrideToImplementCustomLogic</span><span class="p">,</span>
    <span class="n">OverrideToImplementCustomLogic_CallToSuperRecommended</span><span class="p">,</span>
    <span class="n">PublicAPI</span><span class="p">,</span>
    <span class="n">override</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.checkpoints</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">CHECKPOINT_VERSION</span><span class="p">,</span>
    <span class="n">CHECKPOINT_VERSION_LEARNER</span><span class="p">,</span>
    <span class="n">get_checkpoint_info</span><span class="p">,</span>
    <span class="n">try_import_msgpack</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.debug</span> <span class="kn">import</span> <span class="n">update_global_seed_if_necessary</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.deprecation</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
    <span class="n">Deprecated</span><span class="p">,</span>
    <span class="n">deprecation_warning</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.error</span> <span class="kn">import</span> <span class="n">ERR_MSG_INVALID_ENV_DESCRIPTOR</span><span class="p">,</span> <span class="n">EnvError</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.framework</span> <span class="kn">import</span> <span class="n">try_import_tf</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.from_config</span> <span class="kn">import</span> <span class="n">from_config</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.metrics</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">,</span>
    <span class="n">NUM_AGENT_STEPS_SAMPLED_THIS_ITER</span><span class="p">,</span>
    <span class="n">NUM_AGENT_STEPS_TRAINED</span><span class="p">,</span>
    <span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">,</span>
    <span class="n">NUM_ENV_STEPS_SAMPLED_THIS_ITER</span><span class="p">,</span>
    <span class="n">NUM_ENV_STEPS_TRAINED</span><span class="p">,</span>
    <span class="n">SYNCH_WORKER_WEIGHTS_TIMER</span><span class="p">,</span>
    <span class="n">TRAINING_ITERATION_TIMER</span><span class="p">,</span>
    <span class="n">SAMPLE_TIMER</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.metrics.learner_info</span> <span class="kn">import</span> <span class="n">LEARNER_INFO</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.policy</span> <span class="kn">import</span> <span class="n">validate_policy_id</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.replay_buffers</span> <span class="kn">import</span> <span class="n">MultiAgentReplayBuffer</span><span class="p">,</span> <span class="n">ReplayBuffer</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.serialization</span> <span class="kn">import</span> <span class="n">deserialize_type</span><span class="p">,</span> <span class="n">NOT_SERIALIZABLE</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.spaces</span> <span class="kn">import</span> <span class="n">space_utils</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AgentConnectorDataType</span><span class="p">,</span>
    <span class="n">AgentID</span><span class="p">,</span>
    <span class="n">AlgorithmConfigDict</span><span class="p">,</span>
    <span class="n">EnvCreator</span><span class="p">,</span>
    <span class="n">EnvInfoDict</span><span class="p">,</span>
    <span class="n">EnvType</span><span class="p">,</span>
    <span class="n">EpisodeID</span><span class="p">,</span>
    <span class="n">PartialAlgorithmConfigDict</span><span class="p">,</span>
    <span class="n">PolicyID</span><span class="p">,</span>
    <span class="n">PolicyState</span><span class="p">,</span>
    <span class="n">ResultDict</span><span class="p">,</span>
    <span class="n">SampleBatchType</span><span class="p">,</span>
    <span class="n">TensorStructType</span><span class="p">,</span>
    <span class="n">TensorType</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.tune.execution.placement_groups</span> <span class="kn">import</span> <span class="n">PlacementGroupFactory</span>
<span class="kn">from</span> <span class="nn">ray.tune.experiment.trial</span> <span class="kn">import</span> <span class="n">ExportFormat</span>
<span class="kn">from</span> <span class="nn">ray.tune.logger</span> <span class="kn">import</span> <span class="n">Logger</span><span class="p">,</span> <span class="n">UnifiedLogger</span>
<span class="kn">from</span> <span class="nn">ray.tune.registry</span> <span class="kn">import</span> <span class="n">ENV_CREATOR</span><span class="p">,</span> <span class="n">_global_registry</span>
<span class="kn">from</span> <span class="nn">ray.tune.resources</span> <span class="kn">import</span> <span class="n">Resources</span>
<span class="kn">from</span> <span class="nn">ray.tune.result</span> <span class="kn">import</span> <span class="n">DEFAULT_RESULTS_DIR</span>
<span class="kn">from</span> <span class="nn">ray.tune.trainable</span> <span class="kn">import</span> <span class="n">Trainable</span>
<span class="kn">from</span> <span class="nn">ray.util</span> <span class="kn">import</span> <span class="n">log_once</span>
<span class="kn">from</span> <span class="nn">ray.util.timer</span> <span class="kn">import</span> <span class="n">_Timer</span>
<span class="kn">from</span> <span class="nn">ray.tune.registry</span> <span class="kn">import</span> <span class="n">get_trainable_cls</span>

<span class="n">tf1</span><span class="p">,</span> <span class="n">tf</span><span class="p">,</span> <span class="n">tfv</span> <span class="o">=</span> <span class="n">try_import_tf</span><span class="p">()</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="nd">@Deprecated</span><span class="p">(</span>
    <span class="n">new</span><span class="o">=</span><span class="s2">&quot;config = AlgorithmConfig().update_from_dict({&#39;a&#39;: 1, &#39;b&#39;: 2}); ... ; &quot;</span>
    <span class="s2">&quot;print(config.lr) -&gt; 0.001; if config.a &gt; 0: [do something];&quot;</span><span class="p">,</span>
    <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">with_common_config</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">pass</span>


<div class="viewcode-block" id="Algorithm"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html#ray.rllib.algorithms.algorithm.Algorithm">[docs]</a><span class="nd">@PublicAPI</span>
<span class="k">class</span> <span class="nc">Algorithm</span><span class="p">(</span><span class="n">Trainable</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An RLlib algorithm responsible for optimizing one or more Policies.</span>

<span class="sd">    Algorithms contain a WorkerSet under `self.workers`. A WorkerSet is</span>
<span class="sd">    normally composed of a single local worker</span>
<span class="sd">    (self.workers.local_worker()), used to compute and apply learning updates,</span>
<span class="sd">    and optionally one or more remote workers used to generate environment</span>
<span class="sd">    samples in parallel.</span>
<span class="sd">    WorkerSet is fault tolerant and elastic. It tracks health states for all</span>
<span class="sd">    the managed remote worker actors. As a result, Algorithm should never</span>
<span class="sd">    access the underlying actor handles directly. Instead, always access them</span>
<span class="sd">    via all the foreach APIs with assigned IDs of the underlying workers.</span>

<span class="sd">    Each worker (remotes or local) contains a PolicyMap, which itself</span>
<span class="sd">    may contain either one policy for single-agent training or one or more</span>
<span class="sd">    policies for multi-agent training. Policies are synchronized</span>
<span class="sd">    automatically from time to time using ray.remote calls. The exact</span>
<span class="sd">    synchronization logic depends on the specific algorithm used,</span>
<span class="sd">    but this usually happens from local worker to all remote workers and</span>
<span class="sd">    after each training update.</span>

<span class="sd">    You can write your own Algorithm classes by sub-classing from `Algorithm`</span>
<span class="sd">    or any of its built-in sub-classes.</span>
<span class="sd">    This allows you to override the `training_step` method to implement</span>
<span class="sd">    your own algorithm logic. You can find the different built-in</span>
<span class="sd">    algorithms&#39; `training_step()` methods in their respective main .py files,</span>
<span class="sd">    e.g. rllib.algorithms.dqn.dqn.py or rllib.algorithms.impala.impala.py.</span>

<span class="sd">    The most important API methods a Algorithm exposes are `train()`,</span>
<span class="sd">    `evaluate()`, `save()` and `restore()`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Whether to allow unknown top-level config keys.</span>
    <span class="n">_allow_unknown_configs</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># List of top-level keys with value=dict, for which new sub-keys are</span>
    <span class="c1"># allowed to be added to the value dict.</span>
    <span class="n">_allow_unknown_subkeys</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;tf_session_args&quot;</span><span class="p">,</span>
        <span class="s2">&quot;local_tf_session_args&quot;</span><span class="p">,</span>
        <span class="s2">&quot;env_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="s2">&quot;optimizer&quot;</span><span class="p">,</span>
        <span class="s2">&quot;custom_resources_per_worker&quot;</span><span class="p">,</span>
        <span class="s2">&quot;evaluation_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;exploration_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;extra_python_environs_for_worker&quot;</span><span class="p">,</span>
        <span class="s2">&quot;input_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;output_config&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># List of top level keys with value=dict, for which we always override the</span>
    <span class="c1"># entire value (dict), iff the &quot;type&quot; key in that value dict changes.</span>
    <span class="n">_override_all_subkeys_if_type_changes</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;exploration_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># List of keys that are always fully overridden if present in any dict or sub-dict</span>
    <span class="n">_override_all_key_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;off_policy_estimation_methods&quot;</span><span class="p">,</span> <span class="s2">&quot;policies&quot;</span><span class="p">]</span>

    <span class="n">_progress_metrics</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;num_env_steps_sampled&quot;</span><span class="p">,</span>
        <span class="s2">&quot;num_env_steps_trained&quot;</span><span class="p">,</span>
        <span class="s2">&quot;episodes_total&quot;</span><span class="p">,</span>
        <span class="s2">&quot;sampler_results/episode_len_mean&quot;</span><span class="p">,</span>
        <span class="s2">&quot;sampler_results/episode_reward_mean&quot;</span><span class="p">,</span>
        <span class="s2">&quot;evaluation/sampler_results/episode_reward_mean&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<div class="viewcode-block" id="Algorithm.from_checkpoint"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint.html#ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">from_checkpoint</span><span class="p">(</span>
        <span class="n">checkpoint</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Checkpoint</span><span class="p">],</span>
        <span class="n">policy_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Container</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">AgentID</span><span class="p">,</span> <span class="n">EpisodeID</span><span class="p">],</span> <span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policies_to_train</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">Container</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">],</span>
                <span class="n">Callable</span><span class="p">[[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleBatchType</span><span class="p">]],</span> <span class="nb">bool</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Algorithm&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Creates a new algorithm instance from a given checkpoint.</span>

<span class="sd">        Note: This method must remain backward compatible from 2.0.0 on.</span>

<span class="sd">        Args:</span>
<span class="sd">            checkpoint: The path (str) to the checkpoint directory to use</span>
<span class="sd">                or an AIR Checkpoint instance to restore from.</span>
<span class="sd">            policy_ids: Optional list of PolicyIDs to recover. This allows users to</span>
<span class="sd">                restore an Algorithm with only a subset of the originally present</span>
<span class="sd">                Policies.</span>
<span class="sd">            policy_mapping_fn: An optional (updated) policy mapping function</span>
<span class="sd">                to use from here on.</span>
<span class="sd">            policies_to_train: An optional list of policy IDs to be trained</span>
<span class="sd">                or a callable taking PolicyID and SampleBatchType and</span>
<span class="sd">                returning a bool (trainable or not?).</span>
<span class="sd">                If None, will keep the existing setup in place. Policies,</span>
<span class="sd">                whose IDs are not in the list (or for which the callable</span>
<span class="sd">                returns False) will not be updated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The instantiated Algorithm.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">checkpoint_info</span> <span class="o">=</span> <span class="n">get_checkpoint_info</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

        <span class="c1"># Not possible for (v0.1) (algo class and config information missing</span>
        <span class="c1"># or very hard to retrieve).</span>
        <span class="k">if</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">version</span><span class="o">.</span><span class="n">Version</span><span class="p">(</span><span class="s2">&quot;0.1&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot restore a v0 checkpoint using `Algorithm.from_checkpoint()`!&quot;</span>
                <span class="s2">&quot;In this case, do the following:</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;1) Create a new Algorithm object using your original config.</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;2) Call the `restore()` method of this algo object passing it&quot;</span>
                <span class="s2">&quot; your checkpoint dir or AIR Checkpoint object.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">version</span><span class="o">.</span><span class="n">Version</span><span class="p">(</span><span class="s2">&quot;1.0&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`checkpoint_info[&#39;checkpoint_version&#39;]` in `Algorithm.from_checkpoint&quot;</span>
                <span class="s2">&quot;()` must be 1.0 or later! You are using a checkpoint with &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;version v</span><span class="si">{</span><span class="n">checkpoint_info</span><span class="p">[</span><span class="s1">&#39;checkpoint_version&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># This is a msgpack checkpoint.</span>
        <span class="k">if</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;format&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;msgpack&quot;</span><span class="p">:</span>
            <span class="c1"># User did not provide unserializable function with this call</span>
            <span class="c1"># (`policy_mapping_fn`). Note that if `policies_to_train` is None, it</span>
            <span class="c1"># defaults to training all policies (so it&#39;s ok to not provide this here).</span>
            <span class="k">if</span> <span class="n">policy_mapping_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Only DEFAULT_POLICY_ID present in this algorithm, provide default</span>
                <span class="c1"># implementations of these two functions.</span>
                <span class="k">if</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;policy_ids&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="p">{</span><span class="n">DEFAULT_POLICY_ID</span><span class="p">}:</span>
                    <span class="n">policy_mapping_fn</span> <span class="o">=</span> <span class="n">AlgorithmConfig</span><span class="o">.</span><span class="n">DEFAULT_POLICY_MAPPING_FN</span>
                <span class="c1"># Provide meaningful error message.</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;You are trying to restore a multi-agent algorithm from a &quot;</span>
                        <span class="s2">&quot;`msgpack` formatted checkpoint, which do NOT store the &quot;</span>
                        <span class="s2">&quot;`policy_mapping_fn` or `policies_to_train` &quot;</span>
                        <span class="s2">&quot;functions! Make sure that when using the &quot;</span>
                        <span class="s2">&quot;`Algorithm.from_checkpoint()` utility, you also pass the &quot;</span>
                        <span class="s2">&quot;args: `policy_mapping_fn` and `policies_to_train` with your &quot;</span>
                        <span class="s2">&quot;call. You might leave `policies_to_train=None` in case &quot;</span>
                        <span class="s2">&quot;you would like to train all policies anyways.&quot;</span>
                    <span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">Algorithm</span><span class="o">.</span><span class="n">_checkpoint_info_to_algorithm_state</span><span class="p">(</span>
            <span class="n">checkpoint_info</span><span class="o">=</span><span class="n">checkpoint_info</span><span class="p">,</span>
            <span class="n">policy_ids</span><span class="o">=</span><span class="n">policy_ids</span><span class="p">,</span>
            <span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">policy_mapping_fn</span><span class="p">,</span>
            <span class="n">policies_to_train</span><span class="o">=</span><span class="n">policies_to_train</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">Algorithm</span><span class="o">.</span><span class="n">from_state</span><span class="p">(</span><span class="n">state</span><span class="p">)</span></div>

<div class="viewcode-block" id="Algorithm.from_state"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.from_state.html#ray.rllib.algorithms.algorithm.Algorithm.from_state">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">from_state</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Algorithm&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Recovers an Algorithm from a state object.</span>

<span class="sd">        The `state` of an instantiated Algorithm can be retrieved by calling its</span>
<span class="sd">        `get_state` method. It contains all information necessary</span>
<span class="sd">        to create the Algorithm from scratch. No access to the original code (e.g.</span>
<span class="sd">        configs, knowledge of the Algorithm&#39;s class, etc..) is needed.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: The state to recover a new Algorithm instance from.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A new Algorithm instance.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">algorithm_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">Algorithm</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">algorithm_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;No `algorithm_class` key was found in given `state`! &quot;</span>
                <span class="s2">&quot;Cannot create new Algorithm.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># algo_class = get_trainable_cls(algo_class_name)</span>
        <span class="c1"># Create the new algo.</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;config&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No `config` found in given Algorithm state!&quot;</span><span class="p">)</span>
        <span class="n">new_algo</span> <span class="o">=</span> <span class="n">algorithm_class</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
        <span class="c1"># Set the new algo&#39;s state.</span>
        <span class="n">new_algo</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="c1"># Return the new algo.</span>
        <span class="k">return</span> <span class="n">new_algo</span></div>

<div class="viewcode-block" id="Algorithm.__init__"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.__init__.html#ray.rllib.algorithms.algorithm.Algorithm.__init__">[docs]</a>    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AlgorithmConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># deprecated arg</span>
        <span class="n">logger_creator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="n">Logger</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initializes an Algorithm instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            config: Algorithm-specific configuration object.</span>
<span class="sd">            logger_creator: Callable that creates a ray.tune.Logger</span>
<span class="sd">                object. If unspecified, a default logger is created.</span>
<span class="sd">            **kwargs: Arguments passed to the Trainable base class.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">config</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">()</span>

        <span class="c1"># Translate possible dict into an AlgorithmConfig object, as well as,</span>
        <span class="c1"># resolving generic config objects into specific ones (e.g. passing</span>
        <span class="c1"># an `AlgorithmConfig` super-class instance into a PPO constructor,</span>
        <span class="c1"># which normally would expect a PPOConfig object).</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">default_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">()</span>
            <span class="c1"># `self.get_default_config()` also returned a dict -&gt;</span>
            <span class="c1"># Last resort: Create core AlgorithmConfig from merged dicts.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">default_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">config</span> <span class="o">=</span> <span class="n">AlgorithmConfig</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                    <span class="n">config_dict</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">merge_trainer_configs</span><span class="p">(</span><span class="n">default_config</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="c1"># Default config is an AlgorithmConfig -&gt; update its properties</span>
            <span class="c1"># from the given config dict.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">config</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">default_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">()</span>
            <span class="c1"># Given AlgorithmConfig is not of the same type as the default config:</span>
            <span class="c1"># This could be the case e.g. if the user is building an algo from a</span>
            <span class="c1"># generic AlgorithmConfig() object.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">default_config</span><span class="p">)):</span>
                <span class="n">config</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>

        <span class="c1"># In case this algo is using a generic config (with no algo_class set), set it</span>
        <span class="c1"># here.</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">algo_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">config</span><span class="o">.</span><span class="n">algo_class</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;algo = Algorithm(env=&#39;</span><span class="si">{</span><span class="n">env</span><span class="si">}</span><span class="s2">&#39;, ...)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;algo = AlgorithmConfig().environment(&#39;</span><span class="si">{</span><span class="n">env</span><span class="si">}</span><span class="s2">&#39;).build()&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

        <span class="c1"># Validate and freeze our AlgorithmConfig object (no more changes possible).</span>
        <span class="n">config</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
        <span class="n">config</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="c1"># Convert `env` provided in config into a concrete env creator callable, which</span>
        <span class="c1"># takes an EnvContext (config dict) as arg and returning an RLlib supported Env</span>
        <span class="c1"># type (e.g. a gym.Env).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_env_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_creator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_env_id_and_creator</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="n">config</span>
        <span class="p">)</span>
        <span class="n">env_descr</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_env_id</span><span class="o">.</span><span class="vm">__name__</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_env_id</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_env_id</span>
        <span class="p">)</span>

        <span class="c1"># Placeholder for a local replay buffer instance.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Create a default logger creator if no logger_creator is specified</span>
        <span class="k">if</span> <span class="n">logger_creator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Default logdir prefix containing the agent&#39;s name and the</span>
            <span class="c1"># env id.</span>
            <span class="n">timestr</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">today</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">_%H-%M-%S&quot;</span><span class="p">)</span>
            <span class="n">env_descr_for_dir</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot;[/</span><span class="se">\\\\</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">env_descr</span><span class="p">))</span>
            <span class="n">logdir_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">env_descr_for_dir</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">timestr</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">DEFAULT_RESULTS_DIR</span><span class="p">):</span>
                <span class="c1"># Possible race condition if dir is created several times on</span>
                <span class="c1"># rollout workers</span>
                <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">DEFAULT_RESULTS_DIR</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">logdir</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="n">logdir_prefix</span><span class="p">,</span> <span class="nb">dir</span><span class="o">=</span><span class="n">DEFAULT_RESULTS_DIR</span><span class="p">)</span>

            <span class="c1"># Allow users to more precisely configure the created logger</span>
            <span class="c1"># via &quot;logger_config.type&quot;.</span>
            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">logger_config</span> <span class="ow">and</span> <span class="s2">&quot;type&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">logger_config</span><span class="p">:</span>

                <span class="k">def</span> <span class="nf">default_logger_creator</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
                    <span class="sd">&quot;&quot;&quot;Creates a custom logger with the default prefix.&quot;&quot;&quot;</span>
                    <span class="n">cfg</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;logger_config&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                    <span class="bp">cls</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;type&quot;</span><span class="p">)</span>
                    <span class="c1"># Provide default for logdir, in case the user does</span>
                    <span class="c1"># not specify this in the &quot;logger_config&quot; dict.</span>
                    <span class="n">logdir_</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;logdir&quot;</span><span class="p">,</span> <span class="n">logdir</span><span class="p">)</span>
                    <span class="k">return</span> <span class="n">from_config</span><span class="p">(</span><span class="bp">cls</span><span class="o">=</span><span class="bp">cls</span><span class="p">,</span> <span class="n">_args</span><span class="o">=</span><span class="p">[</span><span class="n">cfg</span><span class="p">],</span> <span class="n">logdir</span><span class="o">=</span><span class="n">logdir_</span><span class="p">)</span>

            <span class="c1"># If no `type` given, use tune&#39;s UnifiedLogger as last resort.</span>
            <span class="k">else</span><span class="p">:</span>

                <span class="k">def</span> <span class="nf">default_logger_creator</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
                    <span class="sd">&quot;&quot;&quot;Creates a Unified logger with the default prefix.&quot;&quot;&quot;</span>
                    <span class="k">return</span> <span class="n">UnifiedLogger</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">logdir</span><span class="p">,</span> <span class="n">loggers</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

            <span class="n">logger_creator</span> <span class="o">=</span> <span class="n">default_logger_creator</span>

        <span class="c1"># Metrics-related properties.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="n">_Timer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_episode_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_episodes_to_be_collected</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># The fully qualified AlgorithmConfig used for evaluation</span>
        <span class="c1"># (or None if evaluation not setup).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AlgorithmConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Evaluation WorkerSet and metrics last returned by `self.evaluate()`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">WorkerSet</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Initialize common evaluation_metrics to nan, before they become</span>
        <span class="c1"># available. We want to make sure the metrics are always present</span>
        <span class="c1"># (although their values may be nan), so that Tune does not complain</span>
        <span class="c1"># when we use these as stopping criteria.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_metrics</span> <span class="o">=</span> <span class="p">{</span>
            <span class="c1"># TODO: Don&#39;t dump sampler results into top-level.</span>
            <span class="s2">&quot;evaluation&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;episode_reward_max&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                <span class="s2">&quot;episode_reward_min&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                <span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                <span class="s2">&quot;sampler_results&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s2">&quot;episode_reward_max&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                    <span class="s2">&quot;episode_reward_min&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                    <span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                <span class="p">},</span>
            <span class="p">},</span>
        <span class="p">}</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
            <span class="n">logger_creator</span><span class="o">=</span><span class="n">logger_creator</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Check, whether `training_iteration` is still a tune.Trainable property</span>
        <span class="c1"># and has not been overridden by the user in the attempt to implement the</span>
        <span class="c1"># algos logic (this should be done now inside `training_step`).</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_iteration</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">AssertionError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="s2">&quot;Your Algorithm&#39;s `training_iteration` seems to be overridden by your &quot;</span>
                <span class="s2">&quot;custom training logic! To solve this problem, simply rename your &quot;</span>
                <span class="s2">&quot;`self.training_iteration()` method into `self.training_step`.&quot;</span>
            <span class="p">)</span></div>

    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">get_default_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AlgorithmConfig</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">AlgorithmConfig</span><span class="p">()</span>

    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span> <span class="nf">_remote_worker_ids_for_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Returns a list of remote worker IDs to fetch metrics from.</span>

<span class="sd">        Specific Algorithm implementations can override this method to</span>
<span class="sd">        use a subset of the workers for metrics collection.</span>

<span class="sd">        Returns:</span>
<span class="sd">            List of remote worker IDs to fetch metrics from.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">healthy_worker_ids</span><span class="p">()</span>

    <span class="nd">@OverrideToImplementCustomLogic_CallToSuperRecommended</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">AlgorithmConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

        <span class="c1"># Setup our config: Merge the user-supplied config dict (which could</span>
        <span class="c1"># be a partial config dict) with the class&#39; default.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">AlgorithmConfig</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">)</span>
            <span class="n">config_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config_obj</span><span class="p">,</span> <span class="n">AlgorithmConfig</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">)</span>
                <span class="n">config_obj</span> <span class="o">=</span> <span class="n">AlgorithmConfig</span><span class="p">()</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">config_obj</span><span class="p">)</span>
            <span class="n">config_obj</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
            <span class="n">config_obj</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_env_id</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config_obj</span>

        <span class="c1"># Set Algorithm&#39;s seed after we have - if necessary - enabled</span>
        <span class="c1"># tf eager-execution.</span>
        <span class="n">update_global_seed_if_necessary</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">framework_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_record_usage</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># Create the callbacks object.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">callbacks_class</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">log_level</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;WARN&quot;</span><span class="p">,</span> <span class="s2">&quot;ERROR&quot;</span><span class="p">]:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Current log_level is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">log_level</span><span class="si">}</span><span class="s2">. For more information, &quot;</span>
                <span class="s2">&quot;set &#39;log_level&#39;: &#39;INFO&#39; / &#39;DEBUG&#39; or use the -v and &quot;</span>
                <span class="s2">&quot;-vv flags.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">log_level</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;ray.rllib&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">log_level</span><span class="p">)</span>

        <span class="c1"># Create local replay buffer if necessary.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_local_replay_buffer_if_necessary</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span>
        <span class="p">)</span>

        <span class="c1"># Create a dict, mapping ActorHandles to sets of open remote</span>
        <span class="c1"># requests (object refs). This way, we keep track, of which actors</span>
        <span class="c1"># inside this Algorithm (e.g. a remote RolloutWorker) have</span>
        <span class="c1"># already been sent how many (e.g. `sample()`) requests.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">remote_requests_in_flight</span><span class="p">:</span> <span class="n">DefaultDict</span><span class="p">[</span>
            <span class="n">ActorHandle</span><span class="p">,</span> <span class="n">Set</span><span class="p">[</span><span class="n">ray</span><span class="o">.</span><span class="n">ObjectRef</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">set</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">WorkerSet</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_exec_impl</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Offline RL settings.</span>
        <span class="n">input_evaluation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;input_evaluation&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">input_evaluation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">input_evaluation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">ope_dict</span> <span class="o">=</span> <span class="p">{</span><span class="nb">str</span><span class="p">(</span><span class="n">ope</span><span class="p">):</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">ope</span><span class="p">}</span> <span class="k">for</span> <span class="n">ope</span> <span class="ow">in</span> <span class="n">input_evaluation</span><span class="p">}</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;config.input_evaluation=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input_evaluation</span><span class="p">),</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;config.evaluation(evaluation_config=config.overrides(&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;off_policy_estimation_methods=</span><span class="si">{</span><span class="n">ope_dict</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="s2">&quot;))&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Running OPE during training is not recommended.&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">off_policy_estimation_methods</span> <span class="o">=</span> <span class="n">ope_dict</span>

        <span class="c1"># Deprecated way of implementing Trainer sub-classes (or &quot;templates&quot;</span>
        <span class="c1"># via the `build_trainer` utility function).</span>
        <span class="c1"># Instead, sub-classes should override the Trainable&#39;s `setup()`</span>
        <span class="c1"># method and call super().setup() from within that override at some</span>
        <span class="c1"># point.</span>
        <span class="c1"># Old design: Override `Trainer._init`.</span>
        <span class="n">_init</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_creator</span><span class="p">)</span>
            <span class="n">_init</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># New design: Override `Trainable.setup()` (as indented by tune.Trainable)</span>
        <span class="c1"># and do or don&#39;t call `super().setup()` from within your override.</span>
        <span class="c1"># By default, `super().setup()` will create both worker sets:</span>
        <span class="c1"># &quot;rollout workers&quot; for collecting samples for training and - if</span>
        <span class="c1"># applicable - &quot;evaluation workers&quot; for evaluation runs in between or</span>
        <span class="c1"># parallel to training.</span>
        <span class="c1"># TODO: Deprecate `_init()` and remove this try/except block.</span>
        <span class="k">except</span> <span class="ne">NotImplementedError</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="c1"># Only if user did not override `_init()`:</span>
        <span class="k">if</span> <span class="n">_init</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="c1"># Create a set of env runner actors via a WorkerSet.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">workers</span> <span class="o">=</span> <span class="n">WorkerSet</span><span class="p">(</span>
                <span class="n">env_creator</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_creator</span><span class="p">,</span>
                <span class="n">validate_env</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">validate_env</span><span class="p">,</span>
                <span class="n">default_policy_class</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_default_policy_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">),</span>
                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
                <span class="n">num_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_rollout_workers</span><span class="p">,</span>
                <span class="n">local_worker</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">logdir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logdir</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># TODO (avnishn): Remove the execution plan API by q1 2023</span>
            <span class="c1"># Function defining one single training iteration&#39;s behavior.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_disable_execution_plan_api</span><span class="p">:</span>
                <span class="c1"># Ensure remote workers are initially in sync with the local worker.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">()</span>
            <span class="c1"># LocalIterator-creating &quot;execution plan&quot;.</span>
            <span class="c1"># Only call this once here to create `self.train_exec_impl`,</span>
            <span class="c1"># which is a ray.util.iter.LocalIterator that will be `next`&#39;d</span>
            <span class="c1"># on each training iteration.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_exec_impl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">execution_plan</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_kwargs_for_execution_plan</span><span class="p">()</span>
                <span class="p">)</span>

        <span class="c1"># Compile, validate, and freeze an evaluation config.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_evaluation_config_object</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="c1"># Evaluation WorkerSet setup.</span>
        <span class="c1"># User would like to setup a separate evaluation worker set.</span>
        <span class="c1"># Note: We skip workerset creation if we need to do offline evaluation</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_should_create_evaluation_rollout_workers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">):</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">env_creator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_env_id_and_creator</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span>
            <span class="p">)</span>

            <span class="c1"># Create a separate evaluation worker set for evaluation.</span>
            <span class="c1"># If evaluation_num_workers=0, use the evaluation set&#39;s local</span>
            <span class="c1"># worker for evaluation, otherwise, use its remote workers</span>
            <span class="c1"># (parallelized evaluation).</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="p">:</span> <span class="n">WorkerSet</span> <span class="o">=</span> <span class="n">WorkerSet</span><span class="p">(</span>
                <span class="n">env_creator</span><span class="o">=</span><span class="n">env_creator</span><span class="p">,</span>
                <span class="n">validate_env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">default_policy_class</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_default_policy_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">),</span>
                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span>
                <span class="n">num_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_num_workers</span><span class="p">,</span>
                <span class="n">logdir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logdir</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_async_evaluation</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_evaluation_weights_seq_number</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_dataset</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">off_policy_estimation_methods</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">ope_split_batch_by_episode</span>
        <span class="p">):</span>
            <span class="c1"># the num worker is set to 0 to avoid creating shards. The dataset will not</span>
            <span class="c1"># be repartioned to num_workers blocks.</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Creating evaluation dataset ...&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_dataset</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_dataset_and_shards</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Evaluation dataset created&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">OffPolicyEstimator</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">ope_types</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;is&quot;</span><span class="p">:</span> <span class="n">ImportanceSampling</span><span class="p">,</span>
            <span class="s2">&quot;wis&quot;</span><span class="p">:</span> <span class="n">WeightedImportanceSampling</span><span class="p">,</span>
            <span class="s2">&quot;dm&quot;</span><span class="p">:</span> <span class="n">DirectMethod</span><span class="p">,</span>
            <span class="s2">&quot;dr&quot;</span><span class="p">:</span> <span class="n">DoublyRobust</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">method_config</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">off_policy_estimation_methods</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">method_type</span> <span class="o">=</span> <span class="n">method_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;type&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">method_type</span> <span class="ow">in</span> <span class="n">ope_types</span><span class="p">:</span>
                <span class="n">deprecation_warning</span><span class="p">(</span>
                    <span class="n">old</span><span class="o">=</span><span class="n">method_type</span><span class="p">,</span>
                    <span class="n">new</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">ope_types</span><span class="p">[</span><span class="n">method_type</span><span class="p">]),</span>
                    <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">method_type</span> <span class="o">=</span> <span class="n">ope_types</span><span class="p">[</span><span class="n">method_type</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">method_type</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Trying to import from string: &quot;</span> <span class="o">+</span> <span class="n">method_type</span><span class="p">)</span>
                <span class="n">mod</span><span class="p">,</span> <span class="n">obj</span> <span class="o">=</span> <span class="n">method_type</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">mod</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">import_module</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>
                <span class="n">method_type</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">obj</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">method_type</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span>
                <span class="n">method_type</span><span class="p">,</span> <span class="n">OfflineEvaluator</span>
            <span class="p">):</span>
                <span class="c1"># TODO(kourosh) : Add an integration test for all these</span>
                <span class="c1"># offline evaluators.</span>
                <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">()</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">method_type</span><span class="p">,</span> <span class="n">OffPolicyEstimator</span><span class="p">):</span>
                    <span class="n">method_config</span><span class="p">[</span><span class="s2">&quot;gamma&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gamma</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">method_type</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="o">**</span><span class="n">method_config</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unknown off_policy_estimation type: </span><span class="si">{</span><span class="n">method_type</span><span class="si">}</span><span class="s2">! Must be &quot;</span>
                    <span class="s2">&quot;either a class path or a sub-class of ray.rllib.&quot;</span>
                    <span class="s2">&quot;offline.offline_evaluator::OfflineEvaluator&quot;</span>
                <span class="p">)</span>
            <span class="c1"># TODO (Rohan138): Refactor this and remove deprecated methods</span>
            <span class="c1"># Need to add back method_type in case Algorithm is restored from checkpoint</span>
            <span class="n">method_config</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">method_type</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_enable_learner_api</span><span class="p">:</span>
            <span class="c1"># TODO (Kourosh): This is an interim solution where policies and modules</span>
            <span class="c1">#  co-exist. In this world we have both policy_map and MARLModule that need</span>
            <span class="c1">#  to be consistent with one another. To make a consistent parity between</span>
            <span class="c1">#  the two we need to loop through the policy modules and create a simple</span>
            <span class="c1">#  MARLModule from the RLModule within each policy.</span>
            <span class="n">local_worker</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span>
            <span class="n">module_spec</span> <span class="o">=</span> <span class="n">local_worker</span><span class="o">.</span><span class="n">marl_module_spec</span>
            <span class="n">learner_group_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_learner_group_config</span><span class="p">(</span><span class="n">module_spec</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span> <span class="o">=</span> <span class="n">learner_group_config</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>

            <span class="c1"># check if there are modules to load from the module_spec</span>
            <span class="n">rl_module_ckpt_dirs</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">marl_module_ckpt_dir</span> <span class="o">=</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">load_state_path</span>
            <span class="n">modules_to_load</span> <span class="o">=</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">modules_to_load</span>
            <span class="k">for</span> <span class="n">module_id</span><span class="p">,</span> <span class="n">sub_module_spec</span> <span class="ow">in</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">module_specs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">sub_module_spec</span><span class="o">.</span><span class="n">load_state_path</span><span class="p">:</span>
                    <span class="n">rl_module_ckpt_dirs</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">sub_module_spec</span><span class="o">.</span><span class="n">load_state_path</span>
            <span class="k">if</span> <span class="n">marl_module_ckpt_dir</span> <span class="ow">or</span> <span class="n">rl_module_ckpt_dirs</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">load_module_state</span><span class="p">(</span>
                    <span class="n">marl_module_ckpt_dir</span><span class="o">=</span><span class="n">marl_module_ckpt_dir</span><span class="p">,</span>
                    <span class="n">modules_to_load</span><span class="o">=</span><span class="n">modules_to_load</span><span class="p">,</span>
                    <span class="n">rl_module_ckpt_dirs</span><span class="o">=</span><span class="n">rl_module_ckpt_dirs</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="c1"># sync the weights from the learner group to the rollout workers</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
            <span class="n">local_worker</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">()</span>

        <span class="c1"># Run `on_algorithm_init` callback after initialization is done.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">on_algorithm_init</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>

    <span class="c1"># TODO: Deprecated: In your sub-classes of Trainer, override `setup()`</span>
    <span class="c1">#  directly and call super().setup() from within it if you would like the</span>
    <span class="c1">#  default setup behavior plus some own setup logic.</span>
    <span class="c1">#  If you don&#39;t need the env/workers/config/etc.. setup for you by super,</span>
    <span class="c1">#  simply do not call super().setup() from your overridden method.</span>
    <span class="k">def</span> <span class="nf">_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">AlgorithmConfigDict</span><span class="p">,</span> <span class="n">env_creator</span><span class="p">:</span> <span class="n">EnvCreator</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

<div class="viewcode-block" id="Algorithm.get_default_policy_class"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_default_policy_class.html#ray.rllib.algorithms.algorithm.Algorithm.get_default_policy_class">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">get_default_policy_class</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">AlgorithmConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="n">Policy</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Returns a default Policy class to use, given a config.</span>

<span class="sd">        This class will be used by an Algorithm in case</span>
<span class="sd">        the policy class is not provided by the user in any single- or</span>
<span class="sd">        multi-agent PolicySpec.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="Algorithm.step"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.step.html#ray.rllib.algorithms.algorithm.Algorithm.step">[docs]</a>    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ResultDict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Implements the main `Trainer.train()` logic.</span>

<span class="sd">        Takes n attempts to perform a single training step. Thereby</span>
<span class="sd">        catches RayErrors resulting from worker failures. After n attempts,</span>
<span class="sd">        fails gracefully.</span>

<span class="sd">        Override this method in your Trainer sub-classes if you would like to</span>
<span class="sd">        handle worker failures yourself.</span>
<span class="sd">        Otherwise, override only `training_step()` to implement the core</span>
<span class="sd">        algorithm logic.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The results dict with stats/infos on sampling, training,</span>
<span class="sd">            and - if required - evaluation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Do we have to run `self.evaluate()` this iteration?</span>
        <span class="c1"># `self.iteration` gets incremented after this function returns,</span>
        <span class="c1"># meaning that e. g. the first time this function is called,</span>
        <span class="c1"># self.iteration will be 0.</span>
        <span class="n">evaluate_this_iter</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_interval</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_interval</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">)</span>

        <span class="c1"># Results dict for training (and if appolicable: evaluation).</span>
        <span class="n">results</span><span class="p">:</span> <span class="n">ResultDict</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Parallel eval + training: Kick off evaluation-loop and parallel train() call.</span>
        <span class="k">if</span> <span class="n">evaluate_this_iter</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_parallel_to_training</span><span class="p">:</span>
            <span class="p">(</span>
                <span class="n">results</span><span class="p">,</span>
                <span class="n">train_iter_ctx</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_training_iteration_and_evaluation_in_parallel</span><span class="p">()</span>
        <span class="c1"># - No evaluation necessary, just run the next training iteration.</span>
        <span class="c1"># - We have to evaluate in this training iteration, but no parallelism -&gt;</span>
        <span class="c1">#   evaluate after the training iteration is entirely done.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">results</span><span class="p">,</span> <span class="n">train_iter_ctx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_training_iteration</span><span class="p">()</span>

        <span class="c1"># Sequential: Train (already done above), then evaluate.</span>
        <span class="k">if</span> <span class="n">evaluate_this_iter</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_parallel_to_training</span><span class="p">:</span>
            <span class="n">results</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_run_one_evaluation</span><span class="p">(</span><span class="n">train_future</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>

        <span class="c1"># Attach latest available evaluation results to train results,</span>
        <span class="c1"># if necessary.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">evaluate_this_iter</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">always_attach_evaluation_results</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_metrics</span><span class="p">,</span> <span class="nb">dict</span>
            <span class="p">),</span> <span class="s2">&quot;Trainer.evaluate() needs to return a dict.&quot;</span>
            <span class="n">results</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_metrics</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;workers&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">,</span> <span class="n">WorkerSet</span><span class="p">):</span>
            <span class="c1"># Sync filters on workers.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sync_filters_if_needed</span><span class="p">(</span>
                <span class="n">central_worker</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">(),</span>
                <span class="n">workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">,</span>
                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># TODO (avnishn): Remove the execution plan API by q1 2023</span>
            <span class="c1"># Collect worker metrics and add combine them with `results`.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_disable_execution_plan_api</span><span class="p">:</span>
                <span class="n">episodes_this_iter</span> <span class="o">=</span> <span class="n">collect_episodes</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_remote_worker_ids_for_metrics</span><span class="p">(),</span>
                    <span class="n">timeout_seconds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">metrics_episode_collection_timeout_s</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compile_iteration_results</span><span class="p">(</span>
                    <span class="n">episodes_this_iter</span><span class="o">=</span><span class="n">episodes_this_iter</span><span class="p">,</span>
                    <span class="n">step_ctx</span><span class="o">=</span><span class="n">train_iter_ctx</span><span class="p">,</span>
                    <span class="n">iteration_results</span><span class="o">=</span><span class="n">results</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="c1"># Check `env_task_fn` for possible update of the env&#39;s task.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">env_task_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">env_task_fn</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`env_task_fn` must be None or a callable taking &quot;</span>
                    <span class="s2">&quot;[train_results, env, env_ctx] as args!&quot;</span>
                <span class="p">)</span>

            <span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">env_context</span><span class="p">,</span> <span class="n">task_fn</span><span class="p">):</span>
                <span class="n">new_task</span> <span class="o">=</span> <span class="n">task_fn</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">env_context</span><span class="p">)</span>
                <span class="n">cur_task</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">get_task</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">cur_task</span> <span class="o">!=</span> <span class="n">new_task</span><span class="p">:</span>
                    <span class="n">env</span><span class="o">.</span><span class="n">set_task</span><span class="p">(</span><span class="n">new_task</span><span class="p">)</span>

            <span class="n">fn</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">task_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">env_task_fn</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">foreach_env_with_context</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">results</span></div>

<div class="viewcode-block" id="Algorithm.evaluate"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.evaluate.html#ray.rllib.algorithms.algorithm.Algorithm.evaluate">[docs]</a>    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">duration_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Evaluates current policy under `evaluation_config` settings.</span>

<span class="sd">        Note that this default implementation does not do anything beyond</span>
<span class="sd">        merging evaluation_config with the normal trainer config.</span>

<span class="sd">        Args:</span>
<span class="sd">            duration_fn: An optional callable taking the already run</span>
<span class="sd">                num episodes as only arg and returning the number of</span>
<span class="sd">                episodes left to run. It&#39;s used to find out whether</span>
<span class="sd">                evaluation should continue.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Call the `_before_evaluate` hook.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_before_evaluate</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;evaluation&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_offline_evaluation</span><span class="p">()}</span>

        <span class="c1"># Sync weights to the evaluation WorkerSet.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_trainer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sync_filters_if_needed</span><span class="p">(</span>
                <span class="n">central_worker</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">(),</span>
                <span class="n">workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="p">,</span>
                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">on_evaluate_start</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Running custom eval function </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">metrics</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Custom eval function must return &quot;</span>
                    <span class="s2">&quot;dict of metrics, got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span><span class="o">.</span><span class="n">input_reader</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot evaluate w/o an evaluation worker set in &quot;</span>
                    <span class="s2">&quot;the Trainer or w/o an env on the local worker!</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="s2">&quot;Try one of the following:</span><span class="se">\n</span><span class="s2">1) Set &quot;</span>
                    <span class="s2">&quot;`evaluation_interval` &gt;= 0 to force creating a &quot;</span>
                    <span class="s2">&quot;separate evaluation worker set.</span><span class="se">\n</span><span class="s2">2) Set &quot;</span>
                    <span class="s2">&quot;`create_env_on_driver=True` to force the local &quot;</span>
                    <span class="s2">&quot;(non-eval) worker to have an environment to &quot;</span>
                    <span class="s2">&quot;evaluate on.&quot;</span>
                <span class="p">)</span>

            <span class="c1"># How many episodes/timesteps do we need to run?</span>
            <span class="c1"># In &quot;auto&quot; mode (only for parallel eval + training): Run as long</span>
            <span class="c1"># as training lasts.</span>
            <span class="n">unit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration_unit</span>
            <span class="n">eval_cfg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span>
            <span class="n">rollout</span> <span class="o">=</span> <span class="n">eval_cfg</span><span class="o">.</span><span class="n">rollout_fragment_length</span>
            <span class="n">num_envs</span> <span class="o">=</span> <span class="n">eval_cfg</span><span class="o">.</span><span class="n">num_envs_per_worker</span>
            <span class="n">auto</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span>
            <span class="n">duration</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">auto</span>
                <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_num_workers</span> <span class="ow">or</span> <span class="mi">1</span><span class="p">)</span>
                <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span> <span class="k">else</span> <span class="n">rollout</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">agent_steps_this_iter</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">env_steps_this_iter</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># Default done-function returns True, whenever num episodes</span>
            <span class="c1"># have been completed.</span>
            <span class="k">if</span> <span class="n">duration_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

                <span class="k">def</span> <span class="nf">duration_fn</span><span class="p">(</span><span class="n">num_units_done</span><span class="p">):</span>
                    <span class="k">return</span> <span class="n">duration</span> <span class="o">-</span> <span class="n">num_units_done</span>

            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluating current state of </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> for </span><span class="si">{</span><span class="n">duration</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">unit</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

            <span class="n">metrics</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">all_batches</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="c1"># No evaluation worker set -&gt;</span>
            <span class="c1"># Do evaluation using the local worker. Expect error due to the</span>
            <span class="c1"># local worker not having an env.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># If unit=episodes -&gt; Run n times `sample()` (each sample</span>
                <span class="c1"># produces exactly 1 episode).</span>
                <span class="c1"># If unit=ts -&gt; Run 1 `sample()` b/c the</span>
                <span class="c1"># `rollout_fragment_length` is exactly the desired ts.</span>
                <span class="n">iters</span> <span class="o">=</span> <span class="n">duration</span> <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span> <span class="k">else</span> <span class="mi">1</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
                    <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
                    <span class="n">agent_steps_this_iter</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span>
                    <span class="n">env_steps_this_iter</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">:</span>
                        <span class="n">all_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
                <span class="n">metrics</span> <span class="o">=</span> <span class="n">collect_metrics</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">,</span>
                    <span class="n">keep_custom_metrics</span><span class="o">=</span><span class="n">eval_cfg</span><span class="o">.</span><span class="n">keep_per_episode_custom_metrics</span><span class="p">,</span>
                    <span class="n">timeout_seconds</span><span class="o">=</span><span class="n">eval_cfg</span><span class="o">.</span><span class="n">metrics_episode_collection_timeout_s</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="c1"># Evaluation worker set only has local worker.</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">num_remote_workers</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># If unit=episodes -&gt; Run n times `sample()` (each sample</span>
                <span class="c1"># produces exactly 1 episode).</span>
                <span class="c1"># If unit=ts -&gt; Run 1 `sample()` b/c the</span>
                <span class="c1"># `rollout_fragment_length` is exactly the desired ts.</span>
                <span class="n">iters</span> <span class="o">=</span> <span class="n">duration</span> <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span> <span class="k">else</span> <span class="mi">1</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
                    <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
                    <span class="n">agent_steps_this_iter</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span>
                    <span class="n">env_steps_this_iter</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">:</span>
                        <span class="n">all_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="c1"># Evaluation worker set has n remote workers.</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># How many episodes have we run (across all eval workers)?</span>
                <span class="n">num_units_done</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">_round</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="c1"># In case all of the remote evaluation workers die during a round</span>
                <span class="c1"># of evaluation, we need to stop.</span>
                <span class="k">while</span> <span class="kc">True</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">units_left_to_do</span> <span class="o">=</span> <span class="n">duration_fn</span><span class="p">(</span><span class="n">num_units_done</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">units_left_to_do</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="k">break</span>

                    <span class="n">_round</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">unit_per_remote_worker</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="mi">1</span> <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span> <span class="k">else</span> <span class="n">rollout</span> <span class="o">*</span> <span class="n">num_envs</span>
                    <span class="p">)</span>
                    <span class="c1"># Select proper number of evaluation workers for this round.</span>
                    <span class="n">selected_eval_worker_ids</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="n">worker_id</span>
                        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">worker_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">healthy_worker_ids</span><span class="p">()</span>
                        <span class="p">)</span>
                        <span class="k">if</span> <span class="n">i</span> <span class="o">*</span> <span class="n">unit_per_remote_worker</span> <span class="o">&lt;</span> <span class="n">units_left_to_do</span>
                    <span class="p">]</span>
                    <span class="n">batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span>
                        <span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">w</span><span class="o">.</span><span class="n">sample</span><span class="p">(),</span>
                        <span class="n">local_worker</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">remote_worker_ids</span><span class="o">=</span><span class="n">selected_eval_worker_ids</span><span class="p">,</span>
                        <span class="n">timeout_seconds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_sample_timeout_s</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">selected_eval_worker_ids</span><span class="p">):</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                            <span class="s2">&quot;Calling `sample()` on your remote evaluation worker(s) &quot;</span>
                            <span class="s2">&quot;resulted in a timeout (after the configured &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_sample_timeout_s</span><span class="si">}</span><span class="s2"> seconds)! &quot;</span>
                            <span class="s2">&quot;Try to set `evaluation_sample_timeout_s` in your config&quot;</span>
                            <span class="s2">&quot; to a larger value.&quot;</span>
                            <span class="o">+</span> <span class="p">(</span>
                                <span class="s2">&quot; If your episodes don&#39;t terminate easily, you may &quot;</span>
                                <span class="s2">&quot;also want to set `evaluation_duration_unit` to &quot;</span>
                                <span class="s2">&quot;&#39;timesteps&#39; (instead of &#39;episodes&#39;).&quot;</span>
                                <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span>
                                <span class="k">else</span> <span class="s2">&quot;&quot;</span>
                            <span class="p">)</span>
                        <span class="p">)</span>
                        <span class="k">break</span>

                    <span class="n">_agent_steps</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">)</span>
                    <span class="n">_env_steps</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">)</span>
                    <span class="c1"># 1 episode per returned batch.</span>
                    <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span><span class="p">:</span>
                        <span class="n">num_units_done</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
                        <span class="c1"># Make sure all batches are exactly one episode.</span>
                        <span class="k">for</span> <span class="n">ma_batch</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
                            <span class="n">ma_batch</span> <span class="o">=</span> <span class="n">ma_batch</span><span class="o">.</span><span class="n">as_multi_agent</span><span class="p">()</span>
                            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">ma_batch</span><span class="o">.</span><span class="n">policy_batches</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                                <span class="k">assert</span> <span class="n">batch</span><span class="o">.</span><span class="n">is_terminated_or_truncated</span><span class="p">()</span>
                    <span class="c1"># n timesteps per returned batch.</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">num_units_done</span> <span class="o">+=</span> <span class="p">(</span>
                            <span class="n">_agent_steps</span>
                            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;agent_steps&quot;</span>
                            <span class="k">else</span> <span class="n">_env_steps</span>
                        <span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">:</span>
                        <span class="c1"># TODO: (kourosh) This approach will cause an OOM issue when</span>
                        <span class="c1"># the dataset gets huge (should be ok for now).</span>
                        <span class="n">all_batches</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>

                    <span class="n">agent_steps_this_iter</span> <span class="o">+=</span> <span class="n">_agent_steps</span>
                    <span class="n">env_steps_this_iter</span> <span class="o">+=</span> <span class="n">_env_steps</span>

                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Ran round </span><span class="si">{</span><span class="n">_round</span><span class="si">}</span><span class="s2"> of non-parallel evaluation &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">num_units_done</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">duration</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">auto</span> <span class="k">else</span> <span class="s1">&#39;?&#39;</span><span class="si">}</span><span class="s2"> &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">unit</span><span class="si">}</span><span class="s2"> done)&quot;</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Can&#39;t find a good way to run this evaluation.</span>
                <span class="c1"># Wait for next iteration.</span>
                <span class="k">pass</span>

            <span class="k">if</span> <span class="n">metrics</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">metrics</span> <span class="o">=</span> <span class="n">collect_metrics</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="p">,</span>
                    <span class="n">keep_custom_metrics</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">keep_per_episode_custom_metrics</span><span class="p">,</span>
                    <span class="n">timeout_seconds</span><span class="o">=</span><span class="n">eval_cfg</span><span class="o">.</span><span class="n">metrics_episode_collection_timeout_s</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="c1"># TODO: Don&#39;t dump sampler results into top-level.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span><span class="p">:</span>
                <span class="n">metrics</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">({</span><span class="s2">&quot;sampler_results&quot;</span><span class="p">:</span> <span class="n">metrics</span><span class="p">},</span> <span class="o">**</span><span class="n">metrics</span><span class="p">)</span>

            <span class="n">metrics</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED_THIS_ITER</span><span class="p">]</span> <span class="o">=</span> <span class="n">agent_steps_this_iter</span>
            <span class="n">metrics</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED_THIS_ITER</span><span class="p">]</span> <span class="o">=</span> <span class="n">env_steps_this_iter</span>
            <span class="c1"># TODO: Remove this key at some point. Here for backward compatibility.</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;timesteps_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">env_steps_this_iter</span>

            <span class="c1"># Compute off-policy estimates</span>
            <span class="n">estimates</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
            <span class="c1"># for each batch run the estimator&#39;s fwd pass</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">all_batches</span><span class="p">:</span>
                    <span class="n">estimate_result</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">estimate</span><span class="p">(</span>
                        <span class="n">batch</span><span class="p">,</span>
                        <span class="n">split_batch_by_episode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">ope_split_batch_by_episode</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">estimates</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">estimate_result</span><span class="p">)</span>

            <span class="c1"># collate estimates from all batches</span>
            <span class="k">if</span> <span class="n">estimates</span><span class="p">:</span>
                <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;off_policy_estimator&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">estimate_list</span> <span class="ow">in</span> <span class="n">estimates</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">avg_estimate</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
                        <span class="k">lambda</span> <span class="o">*</span><span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="o">*</span><span class="n">estimate_list</span>
                    <span class="p">)</span>
                    <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;off_policy_estimator&quot;</span><span class="p">][</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">avg_estimate</span>

        <span class="c1"># Evaluation does not run for every step.</span>
        <span class="c1"># Save evaluation metrics on trainer, so it can be attached to</span>
        <span class="c1"># subsequent step results as latest evaluation result.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;evaluation&quot;</span><span class="p">:</span> <span class="n">metrics</span><span class="p">}</span>

        <span class="c1"># Trigger `on_evaluate_end` callback.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">on_evaluate_end</span><span class="p">(</span>
            <span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">evaluation_metrics</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_metrics</span>
        <span class="p">)</span>

        <span class="c1"># Also return the results here for convenience.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_metrics</span></div>

    <span class="nd">@ExperimentalAPI</span>
    <span class="k">def</span> <span class="nf">_evaluate_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">duration_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Evaluates current policy under `evaluation_config` settings.</span>

<span class="sd">        Uses the AsyncParallelRequests manager to send frequent `sample.remote()`</span>
<span class="sd">        requests to the evaluation RolloutWorkers and collect the results of these</span>
<span class="sd">        calls. Handles worker failures (or slowdowns) gracefully due to the asynch&#39;ness</span>
<span class="sd">        and the fact that other eval RolloutWorkers can thus cover the workload.</span>

<span class="sd">        Important Note: This will replace the current `self.evaluate()` method as the</span>
<span class="sd">        default in the future.</span>

<span class="sd">        Args:</span>
<span class="sd">            duration_fn: An optional callable taking the already run</span>
<span class="sd">                num episodes as only arg and returning the number of</span>
<span class="sd">                episodes left to run. It&#39;s used to find out whether</span>
<span class="sd">                evaluation should continue.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># How many episodes/timesteps do we need to run?</span>
        <span class="c1"># In &quot;auto&quot; mode (only for parallel eval + training): Run as long</span>
        <span class="c1"># as training lasts.</span>
        <span class="n">unit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration_unit</span>
        <span class="n">eval_cfg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span>
        <span class="n">rollout</span> <span class="o">=</span> <span class="n">eval_cfg</span><span class="o">.</span><span class="n">rollout_fragment_length</span>
        <span class="n">num_envs</span> <span class="o">=</span> <span class="n">eval_cfg</span><span class="o">.</span><span class="n">num_envs_per_worker</span>
        <span class="n">auto</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span>
        <span class="n">duration</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">auto</span>
            <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_num_workers</span> <span class="ow">or</span> <span class="mi">1</span><span class="p">)</span>
            <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span> <span class="k">else</span> <span class="n">rollout</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Call the `_before_evaluate` hook.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_before_evaluate</span><span class="p">()</span>

        <span class="c1"># TODO(Jun): Implement solution via connectors.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_filters_if_needed</span><span class="p">(</span>
            <span class="n">central_worker</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">(),</span>
            <span class="n">workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="p">,</span>
            <span class="n">config</span><span class="o">=</span><span class="n">eval_cfg</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`config.custom_evaluation_function` not supported in combination &quot;</span>
                <span class="s2">&quot;with `enable_async_evaluation=True` config setting!&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span><span class="o">.</span><span class="n">input_reader</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_num_workers</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Evaluation w/o eval workers (calling Algorithm.evaluate() w/o &quot;</span>
                <span class="s2">&quot;evaluation specifically set up) OR evaluation without input reader &quot;</span>
                <span class="s2">&quot;OR evaluation with only a local evaluation worker &quot;</span>
                <span class="s2">&quot;(`evaluation_num_workers=0`) not supported in combination &quot;</span>
                <span class="s2">&quot;with `enable_async_evaluation=True` config setting!&quot;</span>
            <span class="p">)</span>

        <span class="n">agent_steps_this_iter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">env_steps_this_iter</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluating current state of </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> for </span><span class="si">{</span><span class="n">duration</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">unit</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="n">all_batches</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Default done-function returns True, whenever num episodes</span>
        <span class="c1"># have been completed.</span>
        <span class="k">if</span> <span class="n">duration_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

            <span class="k">def</span> <span class="nf">duration_fn</span><span class="p">(</span><span class="n">num_units_done</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">duration</span> <span class="o">-</span> <span class="n">num_units_done</span>

        <span class="c1"># Put weights only once into object store and use same object</span>
        <span class="c1"># ref to synch to all workers.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_evaluation_weights_seq_number</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">weights_ref</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span><span class="o">.</span><span class="n">get_weights</span><span class="p">())</span>
        <span class="n">weights_seq_no</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluation_weights_seq_number</span>

        <span class="k">def</span> <span class="nf">remote_fn</span><span class="p">(</span><span class="n">worker</span><span class="p">):</span>
            <span class="c1"># Pass in seq-no so that eval workers may ignore this call if no update has</span>
            <span class="c1"># happened since the last call to `remote_fn` (sample).</span>
            <span class="n">worker</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span>
                <span class="n">weights</span><span class="o">=</span><span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">weights_ref</span><span class="p">),</span> <span class="n">weights_seq_no</span><span class="o">=</span><span class="n">weights_seq_no</span>
            <span class="p">)</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">worker</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="n">worker</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">batch</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">weights_seq_no</span>

        <span class="n">rollout_metrics</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># How many episodes have we run (across all eval workers)?</span>
        <span class="n">num_units_done</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">_round</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">units_left_to_do</span> <span class="o">=</span> <span class="n">duration_fn</span><span class="p">(</span><span class="n">num_units_done</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">units_left_to_do</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">_round</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c1"># Get ready evaluation results and metrics asynchronously.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">foreach_worker_async</span><span class="p">(</span>
                <span class="n">func</span><span class="o">=</span><span class="n">remote_fn</span><span class="p">,</span>
                <span class="n">healthy_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">fetch_ready_async_reqs</span><span class="p">()</span>

            <span class="n">batches</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">eval_results</span><span class="p">:</span>
                <span class="n">batch</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">seq_no</span> <span class="o">=</span> <span class="n">result</span>
                <span class="c1"># Ignore results, if the weights seq-number does not match (is</span>
                <span class="c1"># from a previous evaluation step) OR if we have already reached</span>
                <span class="c1"># the configured duration (e.g. number of episodes to evaluate</span>
                <span class="c1"># for).</span>
                <span class="k">if</span> <span class="n">seq_no</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluation_weights_seq_number</span> <span class="ow">and</span> <span class="p">(</span>
                    <span class="n">i</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span> <span class="k">else</span> <span class="n">rollout</span> <span class="o">*</span> <span class="n">num_envs</span><span class="p">)</span>
                    <span class="o">&lt;</span> <span class="n">units_left_to_do</span>
                <span class="p">):</span>
                    <span class="n">batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
                    <span class="n">rollout_metrics</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="n">_agent_steps</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">)</span>
            <span class="n">_env_steps</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">)</span>

            <span class="c1"># 1 episode per returned batch.</span>
            <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span><span class="p">:</span>
                <span class="n">num_units_done</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
                <span class="c1"># Make sure all batches are exactly one episode.</span>
                <span class="k">for</span> <span class="n">ma_batch</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
                    <span class="n">ma_batch</span> <span class="o">=</span> <span class="n">ma_batch</span><span class="o">.</span><span class="n">as_multi_agent</span><span class="p">()</span>
                    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">ma_batch</span><span class="o">.</span><span class="n">policy_batches</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                        <span class="k">assert</span> <span class="n">batch</span><span class="o">.</span><span class="n">is_terminated_or_truncated</span><span class="p">()</span>
            <span class="c1"># n timesteps per returned batch.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">num_units_done</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="n">_agent_steps</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;agent_steps&quot;</span>
                    <span class="k">else</span> <span class="n">_env_steps</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">:</span>
                <span class="n">all_batches</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>

            <span class="n">agent_steps_this_iter</span> <span class="o">+=</span> <span class="n">_agent_steps</span>
            <span class="n">env_steps_this_iter</span> <span class="o">+=</span> <span class="n">_env_steps</span>

            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Ran round </span><span class="si">{</span><span class="n">_round</span><span class="si">}</span><span class="s2"> of parallel evaluation &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">num_units_done</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">duration</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">auto</span> <span class="k">else</span> <span class="s1">&#39;?&#39;</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">unit</span><span class="si">}</span><span class="s2"> done)&quot;</span>
            <span class="p">)</span>

        <span class="n">sampler_results</span> <span class="o">=</span> <span class="n">summarize_episodes</span><span class="p">(</span>
            <span class="n">rollout_metrics</span><span class="p">,</span>
            <span class="n">keep_custom_metrics</span><span class="o">=</span><span class="n">eval_cfg</span><span class="p">[</span><span class="s2">&quot;keep_per_episode_custom_metrics&quot;</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="c1"># TODO: Don&#39;t dump sampler results into top-level.</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">({</span><span class="s2">&quot;sampler_results&quot;</span><span class="p">:</span> <span class="n">sampler_results</span><span class="p">},</span> <span class="o">**</span><span class="n">sampler_results</span><span class="p">)</span>

        <span class="n">metrics</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED_THIS_ITER</span><span class="p">]</span> <span class="o">=</span> <span class="n">agent_steps_this_iter</span>
        <span class="n">metrics</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED_THIS_ITER</span><span class="p">]</span> <span class="o">=</span> <span class="n">env_steps_this_iter</span>
        <span class="c1"># TODO: Remove this key at some point. Here for backward compatibility.</span>
        <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;timesteps_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">env_steps_this_iter</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">:</span>
            <span class="c1"># Compute off-policy estimates</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;off_policy_estimator&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">total_batch</span> <span class="o">=</span> <span class="n">concat_samples</span><span class="p">(</span><span class="n">all_batches</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">estimates</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">estimate</span><span class="p">(</span><span class="n">total_batch</span><span class="p">)</span>
                <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;off_policy_estimator&quot;</span><span class="p">][</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimates</span>

        <span class="c1"># Evaluation does not run for every step.</span>
        <span class="c1"># Save evaluation metrics on trainer, so it can be attached to</span>
        <span class="c1"># subsequent step results as latest evaluation result.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;evaluation&quot;</span><span class="p">:</span> <span class="n">metrics</span><span class="p">}</span>

        <span class="c1"># Trigger `on_evaluate_end` callback.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">on_evaluate_end</span><span class="p">(</span>
            <span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">evaluation_metrics</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_metrics</span>
        <span class="p">)</span>

        <span class="c1"># Return evaluation results.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_metrics</span>

<div class="viewcode-block" id="Algorithm.restore_workers"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.restore_workers.html#ray.rllib.algorithms.algorithm.Algorithm.restore_workers">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">restore_workers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">workers</span><span class="p">:</span> <span class="n">WorkerSet</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Try to restore failed workers if necessary.</span>

<span class="sd">        Algorithms that use custom RolloutWorkers may override this method to</span>
<span class="sd">        disable default, and create custom restoration logics.</span>

<span class="sd">        Args:</span>
<span class="sd">            workers: The WorkerSet to restore. This may be Rollout or Evaluation</span>
<span class="sd">                workers.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If `workers` is None, or</span>
        <span class="c1"># 1. `workers` (WorkerSet) does not have a local worker, and</span>
        <span class="c1"># 2. `self.workers` (WorkerSet used for training) does not have a local worker</span>
        <span class="c1"># -&gt; we don&#39;t have a local worker to get state from, so we can&#39;t recover</span>
        <span class="c1"># remote worker in this case.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">workers</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="k">return</span>

        <span class="c1"># This is really cheap, since probe_unhealthy_workers() is a no-op</span>
        <span class="c1"># if there are no unhealthy workers.</span>
        <span class="n">restored</span> <span class="o">=</span> <span class="n">workers</span><span class="o">.</span><span class="n">probe_unhealthy_workers</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">restored</span><span class="p">:</span>
            <span class="n">from_worker</span> <span class="o">=</span> <span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span>
            <span class="c1"># Get the state of the correct (reference) worker. E.g. The local worker</span>
            <span class="c1"># of the main WorkerSet.</span>
            <span class="n">state_ref</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">from_worker</span><span class="o">.</span><span class="n">get_state</span><span class="p">())</span>

            <span class="c1"># By default, entire local worker state is synced after restoration</span>
            <span class="c1"># to bring these workers up to date.</span>
            <span class="n">workers</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span>
                <span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">w</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">state_ref</span><span class="p">)),</span>
                <span class="n">remote_worker_ids</span><span class="o">=</span><span class="n">restored</span><span class="p">,</span>
                <span class="c1"># Don&#39;t update the local_worker, b/c it&#39;s the one we are synching from.</span>
                <span class="n">local_worker</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">timeout_seconds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">worker_restore_timeout_s</span><span class="p">,</span>
                <span class="c1"># Bring back actor after successful state syncing.</span>
                <span class="n">mark_healthy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span></div>

<div class="viewcode-block" id="Algorithm.training_step"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.training_step.html#ray.rllib.algorithms.algorithm.Algorithm.training_step">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ResultDict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Default single iteration logic of an algorithm.</span>

<span class="sd">        - Collect on-policy samples (SampleBatches) in parallel using the</span>
<span class="sd">          Trainer&#39;s RolloutWorkers (@ray.remote).</span>
<span class="sd">        - Concatenate collected SampleBatches into one train batch.</span>
<span class="sd">        - Note that we may have more than one policy in the multi-agent case:</span>
<span class="sd">          Call the different policies&#39; `learn_on_batch` (simple optimizer) OR</span>
<span class="sd">          `load_batch_into_buffer` + `learn_on_loaded_batch` (multi-GPU</span>
<span class="sd">          optimizer) methods to calculate loss and update the model(s).</span>
<span class="sd">        - Return all collected metrics for the iteration.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The results dict from executing the training iteration.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Collect SampleBatches from sample workers until we have a full batch.</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">SAMPLE_TIMER</span><span class="p">]:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;agent_steps&quot;</span><span class="p">:</span>
                <span class="n">train_batch</span> <span class="o">=</span> <span class="n">synchronous_parallel_sample</span><span class="p">(</span>
                    <span class="n">worker_set</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">,</span>
                    <span class="n">max_agent_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">train_batch_size</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">train_batch</span> <span class="o">=</span> <span class="n">synchronous_parallel_sample</span><span class="p">(</span>
                    <span class="n">worker_set</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">,</span> <span class="n">max_env_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">train_batch_size</span>
                <span class="p">)</span>
        <span class="n">train_batch</span> <span class="o">=</span> <span class="n">train_batch</span><span class="o">.</span><span class="n">as_multi_agent</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">]</span> <span class="o">+=</span> <span class="n">train_batch</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">]</span> <span class="o">+=</span> <span class="n">train_batch</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span>

        <span class="c1"># Only train if train_batch is not empty.</span>
        <span class="c1"># In an extreme situation, all rollout workers die during the</span>
        <span class="c1"># synchronous_parallel_sample() call above.</span>
        <span class="c1"># In which case, we should skip training, wait a little bit, then probe again.</span>
        <span class="n">train_results</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">train_batch</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Use simple optimizer (only for multi-agent or tf-eager; all other</span>
            <span class="c1"># cases should use the multi-GPU optimizer, even if only using 1 GPU).</span>
            <span class="c1"># TODO: (sven) rename MultiGPUOptimizer into something more</span>
            <span class="c1">#  meaningful.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_enable_learner_api</span><span class="p">:</span>
                <span class="n">is_module_trainable</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span><span class="o">.</span><span class="n">is_policy_to_train</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">set_is_module_trainable</span><span class="p">(</span><span class="n">is_module_trainable</span><span class="p">)</span>
                <span class="n">train_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;simple_optimizer&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">train_results</span> <span class="o">=</span> <span class="n">train_one_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_batch</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">train_results</span> <span class="o">=</span> <span class="n">multi_gpu_train_one_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_batch</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Wait 1 sec before probing again via weight syncing.</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Update weights and global_vars - after learning on the local worker - on all</span>
        <span class="c1"># remote workers (only those policies that were actually trained).</span>
        <span class="n">global_vars</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;timestep&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">],</span>
        <span class="p">}</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">SYNCH_WORKER_WEIGHTS_TIMER</span><span class="p">]:</span>
            <span class="c1"># TODO (Avnish): Implement this on learner_group.get_weights().</span>
            <span class="c1"># TODO (Kourosh): figure out how we are going to sync MARLModule</span>
            <span class="c1"># weights to MARLModule weights under the policy_map objects?</span>
            <span class="n">from_worker_or_trainer</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_enable_learner_api</span><span class="p">:</span>
                <span class="n">from_worker_or_trainer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_trainer</span><span class="o">=</span><span class="n">from_worker_or_trainer</span><span class="p">,</span>
                <span class="n">policies</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">train_results</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                <span class="n">global_vars</span><span class="o">=</span><span class="n">global_vars</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">train_results</span></div>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">execution_plan</span><span class="p">(</span><span class="n">workers</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;It is not longer recommended to use Trainer&#39;s `execution_plan` method/API.&quot;</span>
            <span class="s2">&quot; Set `_disable_execution_plan_api=True` in your config and override the &quot;</span>
            <span class="s2">&quot;`Trainer.training_step()` method with your algo&#39;s custom &quot;</span>
            <span class="s2">&quot;execution logic.&quot;</span>
        <span class="p">)</span>

<div class="viewcode-block" id="Algorithm.compute_single_action"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.compute_single_action.html#ray.rllib.algorithms.algorithm.Algorithm.compute_single_action">[docs]</a>    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="nf">compute_single_action</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">observation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prev_action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prev_reward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EnvInfoDict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleBatch</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
        <span class="n">full_fetch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">explore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timestep</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">episode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Episode</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">unsquash_action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">clip_action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Deprecated args.</span>
        <span class="n">unsquash_actions</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
        <span class="n">clip_actions</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
        <span class="c1"># Kwargs placeholder for future compatibility.</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span>
        <span class="n">TensorStructType</span><span class="p">,</span>
        <span class="n">Tuple</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TensorType</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]],</span>
    <span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Computes an action for the specified policy on the local worker.</span>

<span class="sd">        Note that you can also access the policy object through</span>
<span class="sd">        self.get_policy(policy_id) and call compute_single_action() on it</span>
<span class="sd">        directly.</span>

<span class="sd">        Args:</span>
<span class="sd">            observation: Single (unbatched) observation from the</span>
<span class="sd">                environment.</span>
<span class="sd">            state: List of all RNN hidden (single, unbatched) state tensors.</span>
<span class="sd">            prev_action: Single (unbatched) previous action value.</span>
<span class="sd">            prev_reward: Single (unbatched) previous reward value.</span>
<span class="sd">            info: Env info dict, if any.</span>
<span class="sd">            input_dict: An optional SampleBatch that holds all the values</span>
<span class="sd">                for: obs, state, prev_action, and prev_reward, plus maybe</span>
<span class="sd">                custom defined views of the current env trajectory. Note</span>
<span class="sd">                that only one of `obs` or `input_dict` must be non-None.</span>
<span class="sd">            policy_id: Policy to query (only applies to multi-agent).</span>
<span class="sd">                Default: &quot;default_policy&quot;.</span>
<span class="sd">            full_fetch: Whether to return extra action fetch results.</span>
<span class="sd">                This is always set to True if `state` is specified.</span>
<span class="sd">            explore: Whether to apply exploration to the action.</span>
<span class="sd">                Default: None -&gt; use self.config.explore.</span>
<span class="sd">            timestep: The current (sampling) time step.</span>
<span class="sd">            episode: This provides access to all of the internal episodes&#39;</span>
<span class="sd">                state, which may be useful for model-based or multi-agent</span>
<span class="sd">                algorithms.</span>
<span class="sd">            unsquash_action: Should actions be unsquashed according to the</span>
<span class="sd">                env&#39;s/Policy&#39;s action space? If None, use the value of</span>
<span class="sd">                self.config.normalize_actions.</span>
<span class="sd">            clip_action: Should actions be clipped according to the</span>
<span class="sd">                env&#39;s/Policy&#39;s action space? If None, use the value of</span>
<span class="sd">                self.config.clip_actions.</span>

<span class="sd">        Keyword Args:</span>
<span class="sd">            kwargs: forward compatibility placeholder</span>

<span class="sd">        Returns:</span>
<span class="sd">            The computed action if full_fetch=False, or a tuple of a) the</span>
<span class="sd">            full output of policy.compute_actions() if full_fetch=True</span>
<span class="sd">            or we have an RNN-based Policy.</span>

<span class="sd">        Raises:</span>
<span class="sd">            KeyError: If the `policy_id` cannot be found in this Trainer&#39;s</span>
<span class="sd">                local worker.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">clip_actions</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;Trainer.compute_single_action(`clip_actions`=...)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;Trainer.compute_single_action(`clip_action`=...)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">clip_action</span> <span class="o">=</span> <span class="n">clip_actions</span>
        <span class="k">if</span> <span class="n">unsquash_actions</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;Trainer.compute_single_action(`unsquash_actions`=...)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;Trainer.compute_single_action(`unsquash_action`=...)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">unsquash_action</span> <span class="o">=</span> <span class="n">unsquash_actions</span>

        <span class="c1"># `unsquash_action` is None: Use value of config[&#39;normalize_actions&#39;].</span>
        <span class="k">if</span> <span class="n">unsquash_action</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">unsquash_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">normalize_actions</span>
        <span class="c1"># `clip_action` is None: Use value of config[&#39;clip_actions&#39;].</span>
        <span class="k">elif</span> <span class="n">clip_action</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">clip_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">clip_actions</span>

        <span class="c1"># User provided an input-dict: Assert that `obs`, `prev_a|r`, `state`</span>
        <span class="c1"># are all None.</span>
        <span class="n">err_msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;Provide either `input_dict` OR [`observation`, ...] as &quot;</span>
            <span class="s2">&quot;args to Trainer.compute_single_action!&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">input_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">observation</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">prev_action</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">prev_reward</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">state</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="n">err_msg</span>
            <span class="n">observation</span> <span class="o">=</span> <span class="n">input_dict</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">OBS</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">observation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="n">err_msg</span>

        <span class="c1"># Get the policy to compute the action for (in the multi-agent case,</span>
        <span class="c1"># Trainer may hold &gt;1 policies).</span>
        <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">policy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;PolicyID &#39;</span><span class="si">{</span><span class="n">policy_id</span><span class="si">}</span><span class="s2">&#39; not found in PolicyMap of the &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Trainer&#39;s local worker!&quot;</span>
            <span class="p">)</span>
        <span class="n">local_worker</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;enable_connectors&quot;</span><span class="p">):</span>
            <span class="c1"># Check the preprocessor and preprocess, if necessary.</span>
            <span class="n">pp</span> <span class="o">=</span> <span class="n">local_worker</span><span class="o">.</span><span class="n">preprocessors</span><span class="p">[</span><span class="n">policy_id</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">pp</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">pp</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">!=</span> <span class="s2">&quot;NoPreprocessor&quot;</span><span class="p">:</span>
                <span class="n">observation</span> <span class="o">=</span> <span class="n">pp</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
            <span class="n">observation</span> <span class="o">=</span> <span class="n">local_worker</span><span class="o">.</span><span class="n">filters</span><span class="p">[</span><span class="n">policy_id</span><span class="p">](</span><span class="n">observation</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Just preprocess observations, similar to how it used to be done before.</span>
            <span class="n">pp</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">agent_connectors</span><span class="p">[</span><span class="n">ObsPreprocessorConnector</span><span class="p">]</span>

            <span class="c1"># convert the observation to array if possible</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="nb">dict</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">observation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Observation type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span><span class="si">}</span><span class="s2"> cannot be converted to &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;np.ndarray.&quot;</span>
                    <span class="p">)</span>
            <span class="k">if</span> <span class="n">pp</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pp</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Only one preprocessor should be in the pipeline&quot;</span>
                <span class="n">pp</span> <span class="o">=</span> <span class="n">pp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="n">pp</span><span class="o">.</span><span class="n">is_identity</span><span class="p">():</span>
                    <span class="c1"># Note(Kourosh): This call will leave the policy&#39;s connector</span>
                    <span class="c1"># in eval mode. would that be a problem?</span>
                    <span class="n">pp</span><span class="o">.</span><span class="n">in_eval</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">observation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">_input_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">OBS</span><span class="p">:</span> <span class="n">observation</span><span class="p">}</span>
                    <span class="k">elif</span> <span class="n">input_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">_input_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">OBS</span><span class="p">:</span> <span class="n">input_dict</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">OBS</span><span class="p">]}</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;Either observation or input_dict must be provided.&quot;</span>
                        <span class="p">)</span>

                    <span class="c1"># TODO (Kourosh): Create a new util method for algorithm that</span>
                    <span class="c1"># computes actions based on raw inputs from env and can keep track</span>
                    <span class="c1"># of its own internal state.</span>
                    <span class="n">acd</span> <span class="o">=</span> <span class="n">AgentConnectorDataType</span><span class="p">(</span><span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="n">_input_dict</span><span class="p">)</span>
                    <span class="c1"># make sure the state is reset since we are only applying the</span>
                    <span class="c1"># preprocessor</span>
                    <span class="n">pp</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">env_id</span><span class="o">=</span><span class="s2">&quot;0&quot;</span><span class="p">)</span>
                    <span class="n">ac_o</span> <span class="o">=</span> <span class="n">pp</span><span class="p">([</span><span class="n">acd</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">observation</span> <span class="o">=</span> <span class="n">ac_o</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">OBS</span><span class="p">]</span>

        <span class="c1"># Input-dict.</span>
        <span class="k">if</span> <span class="n">input_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">OBS</span><span class="p">]</span> <span class="o">=</span> <span class="n">observation</span>
            <span class="n">action</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">extra</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">compute_single_action</span><span class="p">(</span>
                <span class="n">input_dict</span><span class="o">=</span><span class="n">input_dict</span><span class="p">,</span>
                <span class="n">explore</span><span class="o">=</span><span class="n">explore</span><span class="p">,</span>
                <span class="n">timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">,</span>
                <span class="n">episode</span><span class="o">=</span><span class="n">episode</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># Individual args.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">extra</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">compute_single_action</span><span class="p">(</span>
                <span class="n">obs</span><span class="o">=</span><span class="n">observation</span><span class="p">,</span>
                <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span>
                <span class="n">prev_action</span><span class="o">=</span><span class="n">prev_action</span><span class="p">,</span>
                <span class="n">prev_reward</span><span class="o">=</span><span class="n">prev_reward</span><span class="p">,</span>
                <span class="n">info</span><span class="o">=</span><span class="n">info</span><span class="p">,</span>
                <span class="n">explore</span><span class="o">=</span><span class="n">explore</span><span class="p">,</span>
                <span class="n">timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">,</span>
                <span class="n">episode</span><span class="o">=</span><span class="n">episode</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># If we work in normalized action space (normalize_actions=True),</span>
        <span class="c1"># we re-translate here into the env&#39;s action space.</span>
        <span class="k">if</span> <span class="n">unsquash_action</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">space_utils</span><span class="o">.</span><span class="n">unsquash_action</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">action_space_struct</span><span class="p">)</span>
        <span class="c1"># Clip, according to env&#39;s action space.</span>
        <span class="k">elif</span> <span class="n">clip_action</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">space_utils</span><span class="o">.</span><span class="n">clip_action</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">action_space_struct</span><span class="p">)</span>

        <span class="c1"># Return 3-Tuple: Action, states, and extra-action fetches.</span>
        <span class="k">if</span> <span class="n">state</span> <span class="ow">or</span> <span class="n">full_fetch</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">action</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">extra</span>
        <span class="c1"># Ensure backward compatibility.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">action</span></div>

<div class="viewcode-block" id="Algorithm.compute_actions"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.compute_actions.html#ray.rllib.algorithms.algorithm.Algorithm.compute_actions">[docs]</a>    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="nf">compute_actions</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">observations</span><span class="p">:</span> <span class="n">TensorStructType</span><span class="p">,</span>
        <span class="n">state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prev_action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prev_reward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EnvInfoDict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
        <span class="n">full_fetch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">explore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timestep</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">episodes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Episode</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">unsquash_actions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">clip_actions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Deprecated.</span>
        <span class="n">normalize_actions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes an action for the specified policy on the local Worker.</span>

<span class="sd">        Note that you can also access the policy object through</span>
<span class="sd">        self.get_policy(policy_id) and call compute_actions() on it directly.</span>

<span class="sd">        Args:</span>
<span class="sd">            observation: Observation from the environment.</span>
<span class="sd">            state: RNN hidden state, if any. If state is not None,</span>
<span class="sd">                then all of compute_single_action(...) is returned</span>
<span class="sd">                (computed action, rnn state(s), logits dictionary).</span>
<span class="sd">                Otherwise compute_single_action(...)[0] is returned</span>
<span class="sd">                (computed action).</span>
<span class="sd">            prev_action: Previous action value, if any.</span>
<span class="sd">            prev_reward: Previous reward, if any.</span>
<span class="sd">            info: Env info dict, if any.</span>
<span class="sd">            policy_id: Policy to query (only applies to multi-agent).</span>
<span class="sd">            full_fetch: Whether to return extra action fetch results.</span>
<span class="sd">                This is always set to True if RNN state is specified.</span>
<span class="sd">            explore: Whether to pick an exploitation or exploration</span>
<span class="sd">                action (default: None -&gt; use self.config.explore).</span>
<span class="sd">            timestep: The current (sampling) time step.</span>
<span class="sd">            episodes: This provides access to all of the internal episodes&#39;</span>
<span class="sd">                state, which may be useful for model-based or multi-agent</span>
<span class="sd">                algorithms.</span>
<span class="sd">            unsquash_actions: Should actions be unsquashed according</span>
<span class="sd">                to the env&#39;s/Policy&#39;s action space? If None, use</span>
<span class="sd">                self.config.normalize_actions.</span>
<span class="sd">            clip_actions: Should actions be clipped according to the</span>
<span class="sd">                env&#39;s/Policy&#39;s action space? If None, use</span>
<span class="sd">                self.config.clip_actions.</span>

<span class="sd">        Keyword Args:</span>
<span class="sd">            kwargs: forward compatibility placeholder</span>

<span class="sd">        Returns:</span>
<span class="sd">            The computed action if full_fetch=False, or a tuple consisting of</span>
<span class="sd">            the full output of policy.compute_actions_from_input_dict() if</span>
<span class="sd">            full_fetch=True or we have an RNN-based Policy.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">normalize_actions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;Trainer.compute_actions(`normalize_actions`=...)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;Trainer.compute_actions(`unsquash_actions`=...)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">unsquash_actions</span> <span class="o">=</span> <span class="n">normalize_actions</span>

        <span class="c1"># `unsquash_actions` is None: Use value of config[&#39;normalize_actions&#39;].</span>
        <span class="k">if</span> <span class="n">unsquash_actions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">unsquash_actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">normalize_actions</span>
        <span class="c1"># `clip_actions` is None: Use value of config[&#39;clip_actions&#39;].</span>
        <span class="k">elif</span> <span class="n">clip_actions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">clip_actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">clip_actions</span>

        <span class="c1"># Preprocess obs and states.</span>
        <span class="n">state_defined</span> <span class="o">=</span> <span class="n">state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span>
        <span class="n">filtered_obs</span><span class="p">,</span> <span class="n">filtered_state</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">ob</span> <span class="ow">in</span> <span class="n">observations</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">worker</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span>
            <span class="n">preprocessed</span> <span class="o">=</span> <span class="n">worker</span><span class="o">.</span><span class="n">preprocessors</span><span class="p">[</span><span class="n">policy_id</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">ob</span><span class="p">)</span>
            <span class="n">filtered</span> <span class="o">=</span> <span class="n">worker</span><span class="o">.</span><span class="n">filters</span><span class="p">[</span><span class="n">policy_id</span><span class="p">](</span><span class="n">preprocessed</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">filtered_obs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">filtered</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">elif</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
                <span class="n">filtered_state</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="n">agent_id</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">filtered_state</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">get_initial_state</span><span class="p">())</span>

        <span class="c1"># Batch obs and states</span>
        <span class="n">obs_batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">filtered_obs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">filtered_state</span><span class="p">))</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">state</span><span class="p">]</span>

        <span class="n">input_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">OBS</span><span class="p">:</span> <span class="n">obs_batch</span><span class="p">}</span>

        <span class="c1"># prev_action and prev_reward can be None, np.ndarray, or tensor-like structure.</span>
        <span class="c1"># Explicitly check for None here to avoid the error message &quot;The truth value of</span>
        <span class="c1"># an array with more than one element is ambiguous.&quot;, when np arrays are passed</span>
        <span class="c1"># as arguments.</span>
        <span class="k">if</span> <span class="n">prev_action</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">PREV_ACTIONS</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev_action</span>
        <span class="k">if</span> <span class="n">prev_reward</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">PREV_REWARDS</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev_reward</span>
        <span class="k">if</span> <span class="n">info</span><span class="p">:</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">INFOS</span><span class="p">]</span> <span class="o">=</span> <span class="n">info</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;state_in_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>

        <span class="c1"># Batch compute actions</span>
        <span class="n">actions</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">compute_actions_from_input_dict</span><span class="p">(</span>
            <span class="n">input_dict</span><span class="o">=</span><span class="n">input_dict</span><span class="p">,</span>
            <span class="n">explore</span><span class="o">=</span><span class="n">explore</span><span class="p">,</span>
            <span class="n">timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">,</span>
            <span class="n">episodes</span><span class="o">=</span><span class="n">episodes</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Unbatch actions for the environment into a multi-agent dict.</span>
        <span class="n">single_actions</span> <span class="o">=</span> <span class="n">space_utils</span><span class="o">.</span><span class="n">unbatch</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">single_actions</span><span class="p">):</span>
            <span class="c1"># If we work in normalized action space (normalize_actions=True),</span>
            <span class="c1"># we re-translate here into the env&#39;s action space.</span>
            <span class="k">if</span> <span class="n">unsquash_actions</span><span class="p">:</span>
                <span class="n">a</span> <span class="o">=</span> <span class="n">space_utils</span><span class="o">.</span><span class="n">unsquash_action</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">action_space_struct</span><span class="p">)</span>
            <span class="c1"># Clip, according to env&#39;s action space.</span>
            <span class="k">elif</span> <span class="n">clip_actions</span><span class="p">:</span>
                <span class="n">a</span> <span class="o">=</span> <span class="n">space_utils</span><span class="o">.</span><span class="n">clip_action</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">action_space_struct</span><span class="p">)</span>
            <span class="n">actions</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span>

        <span class="c1"># Unbatch states into a multi-agent dict.</span>
        <span class="n">unbatched_states</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">observations</span><span class="p">):</span>
            <span class="n">unbatched_states</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">states</span><span class="p">]</span>

        <span class="c1"># Return only actions or full tuple</span>
        <span class="k">if</span> <span class="n">state_defined</span> <span class="ow">or</span> <span class="n">full_fetch</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">actions</span><span class="p">,</span> <span class="n">unbatched_states</span><span class="p">,</span> <span class="n">infos</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">actions</span></div>

<div class="viewcode-block" id="Algorithm.get_policy"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_policy.html#ray.rllib.algorithms.algorithm.Algorithm.get_policy">[docs]</a>    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="nf">get_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Policy</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return policy for the specified id, or None.</span>

<span class="sd">        Args:</span>
<span class="sd">            policy_id: ID of the policy to return.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="Algorithm.get_weights"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_weights.html#ray.rllib.algorithms.algorithm.Algorithm.get_weights">[docs]</a>    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="nf">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return a dictionary of policy ids to weights.</span>

<span class="sd">        Args:</span>
<span class="sd">            policies: Optional list of policies to return weights for,</span>
<span class="sd">                or None for all policies.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span><span class="o">.</span><span class="n">get_weights</span><span class="p">(</span><span class="n">policies</span><span class="p">)</span></div>

<div class="viewcode-block" id="Algorithm.set_weights"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.set_weights.html#ray.rllib.algorithms.algorithm.Algorithm.set_weights">[docs]</a>    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;Set policy weights by policy id.</span>

<span class="sd">        Args:</span>
<span class="sd">            weights: Map of policy ids to weights to set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span></div>

<div class="viewcode-block" id="Algorithm.add_policy"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.add_policy.html#ray.rllib.algorithms.algorithm.Algorithm.add_policy">[docs]</a>    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="nf">add_policy</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span><span class="p">,</span>
        <span class="n">policy_cls</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="n">Policy</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">observation_space</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">AlgorithmConfig</span><span class="p">,</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PolicyState</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">AgentID</span><span class="p">,</span> <span class="n">EpisodeID</span><span class="p">],</span> <span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policies_to_train</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">Container</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">],</span>
                <span class="n">Callable</span><span class="p">[[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleBatchType</span><span class="p">]],</span> <span class="nb">bool</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">evaluation_workers</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">module_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SingleAgentRLModuleSpec</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Deprecated.</span>
        <span class="n">workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">RolloutWorker</span><span class="p">,</span> <span class="n">ActorHandle</span><span class="p">]]]</span> <span class="o">=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Adds a new policy to this Algorithm.</span>

<span class="sd">        Args:</span>
<span class="sd">            policy_id: ID of the policy to add.</span>
<span class="sd">                IMPORTANT: Must not contain characters that</span>
<span class="sd">                are also not allowed in Unix/Win filesystems, such as: `&lt;&gt;:&quot;/|?*`,</span>
<span class="sd">                or a dot, space or backslash at the end of the ID.</span>
<span class="sd">            policy_cls: The Policy class to use for constructing the new Policy.</span>
<span class="sd">                Note: Only one of `policy_cls` or `policy` must be provided.</span>
<span class="sd">            policy: The Policy instance to add to this algorithm. If not None, the</span>
<span class="sd">                given Policy object will be directly inserted into the Algorithm&#39;s</span>
<span class="sd">                local worker and clones of that Policy will be created on all remote</span>
<span class="sd">                workers as well as all evaluation workers.</span>
<span class="sd">                Note: Only one of `policy_cls` or `policy` must be provided.</span>
<span class="sd">            observation_space: The observation space of the policy to add.</span>
<span class="sd">                If None, try to infer this space from the environment.</span>
<span class="sd">            action_space: The action space of the policy to add.</span>
<span class="sd">                If None, try to infer this space from the environment.</span>
<span class="sd">            config: The config object or overrides for the policy to add.</span>
<span class="sd">            policy_state: Optional state dict to apply to the new</span>
<span class="sd">                policy instance, right after its construction.</span>
<span class="sd">            policy_mapping_fn: An optional (updated) policy mapping function</span>
<span class="sd">                to use from here on. Note that already ongoing episodes will</span>
<span class="sd">                not change their mapping but will use the old mapping till</span>
<span class="sd">                the end of the episode.</span>
<span class="sd">            policies_to_train: An optional list of policy IDs to be trained</span>
<span class="sd">                or a callable taking PolicyID and SampleBatchType and</span>
<span class="sd">                returning a bool (trainable or not?).</span>
<span class="sd">                If None, will keep the existing setup in place. Policies,</span>
<span class="sd">                whose IDs are not in the list (or for which the callable</span>
<span class="sd">                returns False) will not be updated.</span>
<span class="sd">            evaluation_workers: Whether to add the new policy also</span>
<span class="sd">                to the evaluation WorkerSet.</span>
<span class="sd">            module_spec: In the new RLModule API we need to pass in the module_spec for</span>
<span class="sd">                the new module that is supposed to be added. Knowing the policy spec is</span>
<span class="sd">                not sufficient.</span>
<span class="sd">            workers: A list of RolloutWorker/ActorHandles (remote</span>
<span class="sd">                RolloutWorkers) to add this policy to. If defined, will only</span>
<span class="sd">                add the given policy to these workers.</span>


<span class="sd">        Returns:</span>
<span class="sd">            The newly added policy (the copy that got added to the local</span>
<span class="sd">            worker). If `workers` was provided, None is returned.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">validate_policy_id</span><span class="p">(</span><span class="n">policy_id</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">workers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;Algorithm.add_policy(.., workers=..)&quot;</span><span class="p">,</span>
                <span class="n">help</span><span class="o">=</span><span class="p">(</span>
                    <span class="s2">&quot;The `workers` argument to `Algorithm.add_policy()` is deprecated! &quot;</span>
                    <span class="s2">&quot;Please do not use it anymore.&quot;</span>
                <span class="p">),</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">add_policy</span><span class="p">(</span>
            <span class="n">policy_id</span><span class="p">,</span>
            <span class="n">policy_cls</span><span class="p">,</span>
            <span class="n">policy</span><span class="p">,</span>
            <span class="n">observation_space</span><span class="o">=</span><span class="n">observation_space</span><span class="p">,</span>
            <span class="n">action_space</span><span class="o">=</span><span class="n">action_space</span><span class="p">,</span>
            <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
            <span class="n">policy_state</span><span class="o">=</span><span class="n">policy_state</span><span class="p">,</span>
            <span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">policy_mapping_fn</span><span class="p">,</span>
            <span class="n">policies_to_train</span><span class="o">=</span><span class="n">policies_to_train</span><span class="p">,</span>
            <span class="n">module_spec</span><span class="o">=</span><span class="n">module_spec</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># If learner API is enabled, we need to also add the underlying module</span>
        <span class="c1"># to the learner group.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_enable_learner_api</span><span class="p">:</span>
            <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span>
            <span class="n">module</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">model</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="n">module_id</span><span class="o">=</span><span class="n">policy_id</span><span class="p">,</span>
                <span class="n">module_spec</span><span class="o">=</span><span class="n">SingleAgentRLModuleSpec</span><span class="o">.</span><span class="n">from_module</span><span class="p">(</span><span class="n">module</span><span class="p">),</span>
            <span class="p">)</span>

            <span class="n">weights</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">set_weights</span><span class="p">({</span><span class="n">policy_id</span><span class="p">:</span> <span class="n">weights</span><span class="p">})</span>

        <span class="c1"># Add to evaluation workers, if necessary.</span>
        <span class="k">if</span> <span class="n">evaluation_workers</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">add_policy</span><span class="p">(</span>
                <span class="n">policy_id</span><span class="p">,</span>
                <span class="n">policy_cls</span><span class="p">,</span>
                <span class="n">policy</span><span class="p">,</span>
                <span class="n">observation_space</span><span class="o">=</span><span class="n">observation_space</span><span class="p">,</span>
                <span class="n">action_space</span><span class="o">=</span><span class="n">action_space</span><span class="p">,</span>
                <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
                <span class="n">policy_state</span><span class="o">=</span><span class="n">policy_state</span><span class="p">,</span>
                <span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">policy_mapping_fn</span><span class="p">,</span>
                <span class="n">policies_to_train</span><span class="o">=</span><span class="n">policies_to_train</span><span class="p">,</span>
                <span class="n">module_spec</span><span class="o">=</span><span class="n">module_spec</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Return newly added policy (from the local rollout worker).</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="Algorithm.remove_policy"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.remove_policy.html#ray.rllib.algorithms.algorithm.Algorithm.remove_policy">[docs]</a>    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="nf">remove_policy</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">policy_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">AgentID</span><span class="p">],</span> <span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policies_to_train</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">Container</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">],</span>
                <span class="n">Callable</span><span class="p">[[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleBatchType</span><span class="p">]],</span> <span class="nb">bool</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">evaluation_workers</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Removes a new policy from this Algorithm.</span>

<span class="sd">        Args:</span>
<span class="sd">            policy_id: ID of the policy to be removed.</span>
<span class="sd">            policy_mapping_fn: An optional (updated) policy mapping function</span>
<span class="sd">                to use from here on. Note that already ongoing episodes will</span>
<span class="sd">                not change their mapping but will use the old mapping till</span>
<span class="sd">                the end of the episode.</span>
<span class="sd">            policies_to_train: An optional list of policy IDs to be trained</span>
<span class="sd">                or a callable taking PolicyID and SampleBatchType and</span>
<span class="sd">                returning a bool (trainable or not?).</span>
<span class="sd">                If None, will keep the existing setup in place. Policies,</span>
<span class="sd">                whose IDs are not in the list (or for which the callable</span>
<span class="sd">                returns False) will not be updated.</span>
<span class="sd">            evaluation_workers: Whether to also remove the policy from the</span>
<span class="sd">                evaluation WorkerSet.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">worker</span><span class="p">):</span>
            <span class="n">worker</span><span class="o">.</span><span class="n">remove_policy</span><span class="p">(</span>
                <span class="n">policy_id</span><span class="o">=</span><span class="n">policy_id</span><span class="p">,</span>
                <span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">policy_mapping_fn</span><span class="p">,</span>
                <span class="n">policies_to_train</span><span class="o">=</span><span class="n">policies_to_train</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">local_worker</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">healthy_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">evaluation_workers</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span>
                <span class="n">fn</span><span class="p">,</span>
                <span class="n">local_worker</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">healthy_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span></div>

<div class="viewcode-block" id="Algorithm.export_policy_model"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.export_policy_model.html#ray.rllib.algorithms.algorithm.Algorithm.export_policy_model">[docs]</a>    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">export_policy_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">export_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
        <span class="n">onnx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Exports policy model with given policy_id to a local directory.</span>

<span class="sd">        Args:</span>
<span class="sd">            export_dir: Writable local directory.</span>
<span class="sd">            policy_id: Optional policy id to export.</span>
<span class="sd">            onnx: If given, will export model in ONNX format. The</span>
<span class="sd">                value of this parameter set the ONNX OpSet version to use.</span>
<span class="sd">                If None, the output format will be DL framework specific.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; from ray.rllib.algorithms.ppo import PPO</span>
<span class="sd">            &gt;&gt;&gt; # Use an Algorithm from RLlib or define your own.</span>
<span class="sd">            &gt;&gt;&gt; algo = PPO(...) # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; for _ in range(10): # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt;     algo.train() # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; algo.export_policy_model(&quot;/tmp/dir&quot;) # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; algo.export_policy_model(&quot;/tmp/dir/onnx&quot;, onnx=1) # doctest: +SKIP</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span><span class="o">.</span><span class="n">export_model</span><span class="p">(</span><span class="n">export_dir</span><span class="p">,</span> <span class="n">onnx</span><span class="p">)</span></div>

<div class="viewcode-block" id="Algorithm.export_policy_checkpoint"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.export_policy_checkpoint.html#ray.rllib.algorithms.algorithm.Algorithm.export_policy_checkpoint">[docs]</a>    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">export_policy_checkpoint</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">export_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">filename_prefix</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>  <span class="c1"># deprecated arg, do not use anymore</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Exports Policy checkpoint to a local directory and returns an AIR Checkpoint.</span>

<span class="sd">        Args:</span>
<span class="sd">            export_dir: Writable local directory to store the AIR Checkpoint</span>
<span class="sd">                information into.</span>
<span class="sd">            policy_id: Optional policy ID to export. If not provided, will export</span>
<span class="sd">                &quot;default_policy&quot;. If `policy_id` does not exist in this Algorithm,</span>
<span class="sd">                will raise a KeyError.</span>

<span class="sd">        Raises:</span>
<span class="sd">            KeyError if `policy_id` cannot be found in this Algorithm.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; from ray.rllib.algorithms.ppo import PPO</span>
<span class="sd">            &gt;&gt;&gt; # Use an Algorithm from RLlib or define your own.</span>
<span class="sd">            &gt;&gt;&gt; algo = PPO(...) # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; for _ in range(10): # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt;     algo.train() # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; algo.export_policy_checkpoint(&quot;/tmp/export_dir&quot;) # doctest: +SKIP</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># `filename_prefix` should not longer be used as new Policy checkpoints</span>
        <span class="c1"># contain more than one file with a fixed filename structure.</span>
        <span class="k">if</span> <span class="n">filename_prefix</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;Algorithm.export_policy_checkpoint(filename_prefix=...)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">policy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Policy with ID </span><span class="si">{</span><span class="n">policy_id</span><span class="si">}</span><span class="s2"> not found in Algorithm!&quot;</span><span class="p">)</span>
        <span class="n">policy</span><span class="o">.</span><span class="n">export_checkpoint</span><span class="p">(</span><span class="n">export_dir</span><span class="p">)</span></div>

<div class="viewcode-block" id="Algorithm.import_policy_model_from_h5"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.import_policy_model_from_h5.html#ray.rllib.algorithms.algorithm.Algorithm.import_policy_model_from_h5">[docs]</a>    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">import_policy_model_from_h5</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">import_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Imports a policy&#39;s model with given policy_id from a local h5 file.</span>

<span class="sd">        Args:</span>
<span class="sd">            import_file: The h5 file to import from.</span>
<span class="sd">            policy_id: Optional policy id to import into.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; from ray.rllib.algorithms.ppo import PPO</span>
<span class="sd">            &gt;&gt;&gt; algo = PPO(...) # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; algo.import_policy_model_from_h5(&quot;/tmp/weights.h5&quot;) # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; for _ in range(10): # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt;     algo.train() # doctest: +SKIP</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span><span class="o">.</span><span class="n">import_model_from_h5</span><span class="p">(</span><span class="n">import_file</span><span class="p">)</span>
        <span class="c1"># Sync new weights to remote workers.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_weights_to_workers</span><span class="p">(</span><span class="n">worker_set</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">)</span></div>

<div class="viewcode-block" id="Algorithm.save_checkpoint"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.save_checkpoint.html#ray.rllib.algorithms.algorithm.Algorithm.save_checkpoint">[docs]</a>    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Exports AIR Checkpoint to a local directory and returns its directory path.</span>

<span class="sd">        The structure of an Algorithm checkpoint dir will be as follows::</span>

<span class="sd">            policies/</span>
<span class="sd">                pol_1/</span>
<span class="sd">                    policy_state.pkl</span>
<span class="sd">                pol_2/</span>
<span class="sd">                    policy_state.pkl</span>
<span class="sd">            learner/</span>
<span class="sd">                learner_state.json</span>
<span class="sd">                module_state/</span>
<span class="sd">                    module_1/</span>
<span class="sd">                        ...</span>
<span class="sd">                optimizer_state/</span>
<span class="sd">                    optimizers_module_1/</span>
<span class="sd">                        ...</span>
<span class="sd">            rllib_checkpoint.json</span>
<span class="sd">            algorithm_state.pkl</span>

<span class="sd">        Note: `rllib_checkpoint.json` contains a &quot;version&quot; key (e.g. with value 0.1)</span>
<span class="sd">        helping RLlib to remain backward compatible wrt. restoring from checkpoints from</span>
<span class="sd">        Ray 2.0 onwards.</span>

<span class="sd">        Args:</span>
<span class="sd">            checkpoint_dir: The directory where the checkpoint files will be stored.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The path to the created AIR Checkpoint directory.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__getstate__</span><span class="p">()</span>

        <span class="c1"># Extract policy states from worker state (Policies get their own</span>
        <span class="c1"># checkpoint sub-dirs).</span>
        <span class="n">policy_states</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="s2">&quot;worker&quot;</span> <span class="ow">in</span> <span class="n">state</span> <span class="ow">and</span> <span class="s2">&quot;policy_states&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">]:</span>
            <span class="n">policy_states</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;policy_states&quot;</span><span class="p">,</span> <span class="p">{})</span>

        <span class="c1"># Add RLlib checkpoint version.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_enable_learner_api</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">CHECKPOINT_VERSION_LEARNER</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">CHECKPOINT_VERSION</span>

        <span class="c1"># Write state (w/o policies) to disk.</span>
        <span class="n">state_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s2">&quot;algorithm_state.pkl&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">state_file</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

        <span class="c1"># Write rllib_checkpoint.json.</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s2">&quot;rllib_checkpoint.json&quot;</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;Algorithm&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;checkpoint_version&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]),</span>
                    <span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;cloudpickle&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;state_file&quot;</span><span class="p">:</span> <span class="n">state_file</span><span class="p">,</span>
                    <span class="s2">&quot;policy_ids&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="n">policy_states</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                    <span class="s2">&quot;ray_version&quot;</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span>
                    <span class="s2">&quot;ray_commit&quot;</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">__commit__</span><span class="p">,</span>
                <span class="p">},</span>
                <span class="n">f</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Write individual policies to disk, each in their own sub-directory.</span>
        <span class="k">for</span> <span class="n">pid</span><span class="p">,</span> <span class="n">policy_state</span> <span class="ow">in</span> <span class="n">policy_states</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># From here on, disallow policyIDs that would not work as directory names.</span>
            <span class="n">validate_policy_id</span><span class="p">(</span><span class="n">pid</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">policy_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s2">&quot;policies&quot;</span><span class="p">,</span> <span class="n">pid</span><span class="p">)</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">policy_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">pid</span><span class="p">)</span>
            <span class="n">policy</span><span class="o">.</span><span class="n">export_checkpoint</span><span class="p">(</span><span class="n">policy_dir</span><span class="p">,</span> <span class="n">policy_state</span><span class="o">=</span><span class="n">policy_state</span><span class="p">)</span>

        <span class="c1"># if we are using the learner API, save the learner group state</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_enable_learner_api</span><span class="p">:</span>
            <span class="n">learner_state_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s2">&quot;learner&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">save_state</span><span class="p">(</span><span class="n">learner_state_dir</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">checkpoint_dir</span></div>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Checkpoint is provided as a directory name.</span>
        <span class="c1"># Restore from the checkpoint file or dir.</span>

        <span class="n">checkpoint_info</span> <span class="o">=</span> <span class="n">get_checkpoint_info</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
        <span class="n">checkpoint_data</span> <span class="o">=</span> <span class="n">Algorithm</span><span class="o">.</span><span class="n">_checkpoint_info_to_algorithm_state</span><span class="p">(</span><span class="n">checkpoint_info</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">checkpoint_data</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_enable_learner_api</span><span class="p">:</span>
            <span class="n">learner_state_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="s2">&quot;learner&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">load_state</span><span class="p">(</span><span class="n">learner_state_dir</span><span class="p">)</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">log_result</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="p">:</span> <span class="n">ResultDict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Log after the callback is invoked, so that the user has a chance</span>
        <span class="c1"># to mutate the result.</span>
        <span class="c1"># TODO: Remove `trainer` arg at some point to fully deprecate the old signature.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">on_train_result</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="o">=</span><span class="n">result</span><span class="p">)</span>
        <span class="c1"># Then log according to Trainable&#39;s logging logic.</span>
        <span class="n">Trainable</span><span class="o">.</span><span class="n">log_result</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">cleanup</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Stop all workers.</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;workers&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;evaluation_workers&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>

    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@classmethod</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">default_resource_request</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AlgorithmConfig</span><span class="p">,</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Resources</span><span class="p">,</span> <span class="n">PlacementGroupFactory</span><span class="p">]:</span>

        <span class="c1"># Default logic for RLlib Algorithms:</span>
        <span class="c1"># Create one bundle per individual worker (local or remote).</span>
        <span class="c1"># Use `num_cpus_for_local_worker` and `num_gpus` for the local worker and</span>
        <span class="c1"># `num_cpus_per_worker` and `num_gpus_per_worker` for the remote</span>
        <span class="c1"># workers to determine their CPU/GPU resource needs.</span>

        <span class="c1"># Convenience config handles.</span>
        <span class="n">cf</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">()</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">cf</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
        <span class="n">cf</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="c1"># get evaluation config</span>
        <span class="n">eval_cf</span> <span class="o">=</span> <span class="n">cf</span><span class="o">.</span><span class="n">get_evaluation_config_object</span><span class="p">()</span>
        <span class="n">eval_cf</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
        <span class="n">eval_cf</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="c1"># resources for the driver of this trainable</span>
        <span class="k">if</span> <span class="n">cf</span><span class="o">.</span><span class="n">_enable_learner_api</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_learner_workers</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># in this case local_worker only does sampling and training is done on</span>
                <span class="c1"># local learner worker</span>
                <span class="n">driver</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="c1"># sampling and training is not done concurrently when local is</span>
                    <span class="c1"># used, so pick the max.</span>
                    <span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span>
                        <span class="n">cf</span><span class="o">.</span><span class="n">num_cpus_per_learner_worker</span><span class="p">,</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_cpus_for_local_worker</span>
                    <span class="p">),</span>
                    <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_gpus_per_learner_worker</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># in this case local_worker only does sampling and training is done on</span>
                <span class="c1"># remote learner workers</span>
                <span class="n">driver</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_cpus_for_local_worker</span><span class="p">,</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Without Learner API, the local_worker can do both sampling and training.</span>
            <span class="c1"># So, we need to allocate the same resources for the driver as for the</span>
            <span class="c1"># local_worker.</span>
            <span class="n">driver</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_cpus_for_local_worker</span><span class="p">,</span>
                <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">cf</span><span class="o">.</span><span class="n">_fake_gpus</span> <span class="k">else</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_gpus</span><span class="p">,</span>
            <span class="p">}</span>

        <span class="c1"># resources for remote rollout env samplers</span>
        <span class="n">rollout_bundles</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_cpus_per_worker</span><span class="p">,</span>
                <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_gpus_per_worker</span><span class="p">,</span>
                <span class="o">**</span><span class="n">cf</span><span class="o">.</span><span class="n">custom_resources_per_worker</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cf</span><span class="o">.</span><span class="n">num_rollout_workers</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># resources for remote evaluation env samplers or datasets (if any)</span>
        <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_should_create_evaluation_rollout_workers</span><span class="p">(</span><span class="n">eval_cf</span><span class="p">):</span>
            <span class="c1"># Evaluation workers.</span>
            <span class="c1"># Note: The local eval worker is located on the driver CPU.</span>
            <span class="n">evaluation_bundles</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="n">eval_cf</span><span class="o">.</span><span class="n">num_cpus_per_worker</span><span class="p">,</span>
                    <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="n">eval_cf</span><span class="o">.</span><span class="n">num_gpus_per_worker</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">eval_cf</span><span class="o">.</span><span class="n">custom_resources_per_worker</span><span class="p">,</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">eval_cf</span><span class="o">.</span><span class="n">evaluation_num_workers</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># resources for offline dataset readers during evaluation</span>
            <span class="c1"># Note (Kourosh): we should not claim extra workers for</span>
            <span class="c1"># training on the offline dataset, since rollout workers have already</span>
            <span class="c1"># claimed it.</span>
            <span class="c1"># Another Note (Kourosh): dataset reader will not use placement groups so</span>
            <span class="c1"># whatever we specify here won&#39;t matter because dataset won&#39;t even use it.</span>
            <span class="c1"># Disclaimer: using ray dataset in tune may cause deadlock when multiple</span>
            <span class="c1"># tune trials get scheduled on the same node and do not leave any spare</span>
            <span class="c1"># resources for dataset operations. The workaround is to limit the</span>
            <span class="c1"># max_concurrent trials so that some spare cpus are left for dataset</span>
            <span class="c1"># operations. This behavior should get fixed by the dataset team. more info</span>
            <span class="c1"># found here:</span>
            <span class="c1"># https://docs.ray.io/en/master/data/dataset-internals.html#datasets-tune</span>
            <span class="n">evaluation_bundles</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># resources for remote learner workers</span>
        <span class="n">learner_bundles</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">cf</span><span class="o">.</span><span class="n">_enable_learner_api</span> <span class="ow">and</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_learner_workers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># can&#39;t specify cpus for learner workers at the same</span>
            <span class="c1"># time as gpus</span>
            <span class="k">if</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_gpus_per_learner_worker</span><span class="p">:</span>
                <span class="n">learner_bundles</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_gpus_per_learner_worker</span><span class="p">,</span>
                    <span class="p">}</span>
                    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cf</span><span class="o">.</span><span class="n">num_learner_workers</span><span class="p">)</span>
                <span class="p">]</span>
            <span class="k">elif</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_cpus_per_learner_worker</span><span class="p">:</span>
                <span class="n">learner_bundles</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_cpus_per_learner_worker</span><span class="p">,</span>
                    <span class="p">}</span>
                    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cf</span><span class="o">.</span><span class="n">num_learner_workers</span><span class="p">)</span>
                <span class="p">]</span>

        <span class="n">bundles</span> <span class="o">=</span> <span class="p">[</span><span class="n">driver</span><span class="p">]</span> <span class="o">+</span> <span class="n">rollout_bundles</span> <span class="o">+</span> <span class="n">evaluation_bundles</span> <span class="o">+</span> <span class="n">learner_bundles</span>

        <span class="c1"># Return PlacementGroupFactory containing all needed resources</span>
        <span class="c1"># (already properly defined as device bundles).</span>
        <span class="k">return</span> <span class="n">PlacementGroupFactory</span><span class="p">(</span>
            <span class="n">bundles</span><span class="o">=</span><span class="n">bundles</span><span class="p">,</span>
            <span class="n">strategy</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;placement_strategy&quot;</span><span class="p">,</span> <span class="s2">&quot;PACK&quot;</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">_before_evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Pre-evaluation callback.&quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_get_env_id_and_creator</span><span class="p">(</span>
        <span class="n">env_specifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">EnvType</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">config</span><span class="p">:</span> <span class="n">AlgorithmConfig</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">EnvCreator</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Returns env_id and creator callable given original env id from config.</span>

<span class="sd">        Args:</span>
<span class="sd">            env_specifier: An env class, an already tune registered env ID, a known</span>
<span class="sd">                gym env name, or None (if no env is used).</span>
<span class="sd">            config: The AlgorithmConfig object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple consisting of a) env ID string and b) env creator callable.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Environment is specified via a string.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="c1"># An already registered env.</span>
            <span class="k">if</span> <span class="n">_global_registry</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="n">ENV_CREATOR</span><span class="p">,</span> <span class="n">env_specifier</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">env_specifier</span><span class="p">,</span> <span class="n">_global_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">ENV_CREATOR</span><span class="p">,</span> <span class="n">env_specifier</span><span class="p">)</span>

            <span class="c1"># A class path specifier.</span>
            <span class="k">elif</span> <span class="s2">&quot;.&quot;</span> <span class="ow">in</span> <span class="n">env_specifier</span><span class="p">:</span>

                <span class="k">def</span> <span class="nf">env_creator_from_classpath</span><span class="p">(</span><span class="n">env_context</span><span class="p">):</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">env_obj</span> <span class="o">=</span> <span class="n">from_config</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">,</span> <span class="n">env_context</span><span class="p">)</span>
                    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="n">EnvError</span><span class="p">(</span>
                            <span class="n">ERR_MSG_INVALID_ENV_DESCRIPTOR</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">)</span>
                        <span class="p">)</span>
                    <span class="k">return</span> <span class="n">env_obj</span>

                <span class="k">return</span> <span class="n">env_specifier</span><span class="p">,</span> <span class="n">env_creator_from_classpath</span>
            <span class="c1"># Try gym/PyBullet/Vizdoom.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">env_specifier</span><span class="p">,</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                    <span class="n">_gym_env_creator</span><span class="p">,</span> <span class="n">env_descriptor</span><span class="o">=</span><span class="n">env_specifier</span>
                <span class="p">)</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">,</span> <span class="nb">type</span><span class="p">):</span>
            <span class="n">env_id</span> <span class="o">=</span> <span class="n">env_specifier</span>  <span class="c1"># .__name__</span>

            <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;remote_worker_envs&quot;</span><span class="p">]:</span>
                <span class="c1"># Check gym version (0.22 or higher?).</span>
                <span class="c1"># If &gt; 0.21, can&#39;t perform auto-wrapping of the given class as this</span>
                <span class="c1"># would lead to a pickle error.</span>
                <span class="n">gym_version</span> <span class="o">=</span> <span class="n">pkg_resources</span><span class="o">.</span><span class="n">get_distribution</span><span class="p">(</span><span class="s2">&quot;gym&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">version</span>
                <span class="k">if</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">gym_version</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s2">&quot;0.22&quot;</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Cannot specify a gym.Env class via `config.env` while setting &quot;</span>
                        <span class="s2">&quot;`config.remote_worker_env=True` AND your gym version is &gt;= &quot;</span>
                        <span class="s2">&quot;0.22! Try installing an older version of gym or set `config.&quot;</span>
                        <span class="s2">&quot;remote_worker_env=False`.&quot;</span>
                    <span class="p">)</span>

                <span class="nd">@ray</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">num_cpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">class</span> <span class="nc">_wrapper</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">):</span>
                    <span class="c1"># Add convenience `_get_spaces` and `_is_multi_agent`</span>
                    <span class="c1"># methods:</span>
                    <span class="k">def</span> <span class="nf">_get_spaces</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span>

                    <span class="k">def</span> <span class="nf">_is_multi_agent</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                        <span class="kn">from</span> <span class="nn">ray.rllib.env.multi_agent_env</span> <span class="kn">import</span> <span class="n">MultiAgentEnv</span>

                        <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">MultiAgentEnv</span><span class="p">)</span>

                <span class="k">return</span> <span class="n">env_id</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">_wrapper</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
            <span class="c1"># gym.Env-subclass: Also go through our RLlib gym-creator.</span>
            <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">,</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">env_id</span><span class="p">,</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                    <span class="n">_gym_env_creator</span><span class="p">,</span>
                    <span class="n">env_descriptor</span><span class="o">=</span><span class="n">env_specifier</span><span class="p">,</span>
                    <span class="n">auto_wrap_old_gym_envs</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;auto_wrap_old_gym_envs&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="c1"># All other env classes: Call c&#39;tor directly.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">env_id</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">env_specifier</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>

        <span class="c1"># No env -&gt; Env creator always returns None.</span>
        <span class="k">elif</span> <span class="n">env_specifier</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">env_config</span><span class="p">:</span> <span class="kc">None</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> is an invalid env specifier. &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">)</span>
                <span class="o">+</span> <span class="s2">&quot;You can specify a custom env as either a class &quot;</span>
                <span class="s1">&#39;(e.g., YourEnvCls) or a registered env id (e.g., &quot;your_env&quot;).&#39;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sync_filters_if_needed</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">central_worker</span><span class="p">:</span> <span class="n">RolloutWorker</span><span class="p">,</span>
        <span class="n">workers</span><span class="p">:</span> <span class="n">WorkerSet</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">AlgorithmConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Synchronizes the filter stats from `workers` to `central_worker`.</span>

<span class="sd">        .. and broadcasts the central_worker&#39;s filter stats back to all `workers`</span>
<span class="sd">        (if configured).</span>

<span class="sd">        Args:</span>
<span class="sd">            central_worker: The worker to sync/aggregate all `workers`&#39; filter stats to</span>
<span class="sd">                and from which to (possibly) broadcast the updated filter stats back to</span>
<span class="sd">                `workers`.</span>
<span class="sd">            workers: The WorkerSet, whose workers&#39; filter stats should be used for</span>
<span class="sd">                aggregation on `central_worker` and which (possibly) get updated</span>
<span class="sd">                from `central_worker` after the sync.</span>
<span class="sd">            config: The algorithm config instance. This is used to determine, whether</span>
<span class="sd">                syncing from `workers` should happen at all and whether broadcasting</span>
<span class="sd">                back to `workers` (after possible syncing) should happen.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">central_worker</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">observation_filter</span> <span class="o">!=</span> <span class="s2">&quot;NoFilter&quot;</span><span class="p">:</span>
            <span class="n">FilterManager</span><span class="o">.</span><span class="n">synchronize</span><span class="p">(</span>
                <span class="n">central_worker</span><span class="o">.</span><span class="n">filters</span><span class="p">,</span>
                <span class="n">workers</span><span class="p">,</span>
                <span class="n">update_remote</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">update_worker_filter_stats</span><span class="p">,</span>
                <span class="n">timeout_seconds</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">sync_filters_on_rollout_workers_timeout_s</span><span class="p">,</span>
                <span class="n">use_remote_data_for_update</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_worker_filter_stats</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">_sync_weights_to_workers</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">worker_set</span><span class="p">:</span> <span class="n">WorkerSet</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sync &quot;main&quot; weights to given WorkerSet or list of workers.&quot;&quot;&quot;</span>
        <span class="c1"># Broadcast the new policy weights to all remote workers in worker_set.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Synchronizing weights to workers.&quot;</span><span class="p">)</span>
        <span class="n">worker_set</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">()</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">resource_help</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AlgorithmConfig</span><span class="p">,</span> <span class="n">AlgorithmConfigDict</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">You can adjust the resource requests of RLlib Algorithms by calling &quot;</span>
            <span class="s2">&quot;`AlgorithmConfig.resources(&quot;</span>
            <span class="s2">&quot;num_gpus=.., num_cpus_per_worker=.., num_gpus_per_worker=.., ..)` or &quot;</span>
            <span class="s2">&quot;`AgorithmConfig.rollouts(num_rollout_workers=..)`. See &quot;</span>
            <span class="s2">&quot;the `ray.rllib.algorithms.algorithm_config.AlgorithmConfig` classes &quot;</span>
            <span class="s2">&quot;(each Algorithm has its own subclass of this class) for more info.</span><span class="se">\n\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;The config of this Algorithm is: </span><span class="si">{</span><span class="n">config</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_auto_filled_metrics</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">now</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">datetime</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">time_this_iter</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timestamp</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">debug_metrics_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="c1"># Override this method to make sure, the `config` key of the returned results</span>
        <span class="c1"># contains the proper Tune config dict (instead of an AlgorithmConfig object).</span>
        <span class="n">auto_filled</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_auto_filled_metrics</span><span class="p">(</span>
            <span class="n">now</span><span class="p">,</span> <span class="n">time_this_iter</span><span class="p">,</span> <span class="n">timestamp</span><span class="p">,</span> <span class="n">debug_metrics_only</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;config&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">auto_filled</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;`config` key not found in auto-filled results dict!&quot;</span><span class="p">)</span>

        <span class="c1"># If `config` key is no dict (but AlgorithmConfig object) -&gt;</span>
        <span class="c1"># make sure, it&#39;s a dict to not break Tune APIs.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">auto_filled</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">auto_filled</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">],</span> <span class="n">AlgorithmConfig</span><span class="p">)</span>
            <span class="n">auto_filled</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">auto_filled</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">auto_filled</span>

<div class="viewcode-block" id="Algorithm.merge_trainer_configs"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.merge_trainer_configs.html#ray.rllib.algorithms.algorithm.Algorithm.merge_trainer_configs">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">merge_trainer_configs</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">config1</span><span class="p">:</span> <span class="n">AlgorithmConfigDict</span><span class="p">,</span>
        <span class="n">config2</span><span class="p">:</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">,</span>
        <span class="n">_allow_unknown_configs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AlgorithmConfigDict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Merges a complete Algorithm config dict with a partial override dict.</span>

<span class="sd">        Respects nested structures within the config dicts. The values in the</span>
<span class="sd">        partial override dict take priority.</span>

<span class="sd">        Args:</span>
<span class="sd">            config1: The complete Algorithm&#39;s dict to be merged (overridden)</span>
<span class="sd">                with `config2`.</span>
<span class="sd">            config2: The partial override config dict to merge on top of</span>
<span class="sd">                `config1`.</span>
<span class="sd">            _allow_unknown_configs: If True, keys in `config2` that don&#39;t exist</span>
<span class="sd">                in `config1` are allowed and will be added to the final config.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The merged full algorithm config dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">config1</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">config1</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;callbacks&quot;</span> <span class="ow">in</span> <span class="n">config2</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">config2</span><span class="p">[</span><span class="s2">&quot;callbacks&quot;</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">dict</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="s2">&quot;callbacks dict interface&quot;</span><span class="p">,</span>
                <span class="s2">&quot;a class extending rllib.algorithms.callbacks.DefaultCallbacks; &quot;</span>
                <span class="s2">&quot;see `rllib/examples/custom_metrics_and_callbacks.py` for an example.&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">_allow_unknown_configs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_allow_unknown_configs</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_allow_unknown_configs</span>
        <span class="k">return</span> <span class="n">deep_update</span><span class="p">(</span>
            <span class="n">config1</span><span class="p">,</span>
            <span class="n">config2</span><span class="p">,</span>
            <span class="n">_allow_unknown_configs</span><span class="p">,</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_allow_unknown_subkeys</span><span class="p">,</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_override_all_subkeys_if_type_changes</span><span class="p">,</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_override_all_key_list</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="Algorithm.validate_env"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.validate_env.html#ray.rllib.algorithms.algorithm.Algorithm.validate_env">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="nd">@ExperimentalAPI</span>
    <span class="k">def</span> <span class="nf">validate_env</span><span class="p">(</span><span class="n">env</span><span class="p">:</span> <span class="n">EnvType</span><span class="p">,</span> <span class="n">env_context</span><span class="p">:</span> <span class="n">EnvContext</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Env validator function for this Algorithm class.</span>

<span class="sd">        Override this in child classes to define custom validation</span>
<span class="sd">        behavior.</span>

<span class="sd">        Args:</span>
<span class="sd">            env: The (sub-)environment to validate. This is normally a</span>
<span class="sd">                single sub-environment (e.g. a gym.Env) within a vectorized</span>
<span class="sd">                setup.</span>
<span class="sd">            env_context: The EnvContext to configure the environment.</span>

<span class="sd">        Raises:</span>
<span class="sd">            Exception in case something is wrong with the given environment.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">_export_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">export_formats</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">export_dir</span><span class="p">:</span> <span class="nb">str</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
        <span class="n">ExportFormat</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span><span class="n">export_formats</span><span class="p">)</span>
        <span class="n">exported</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">CHECKPOINT</span> <span class="ow">in</span> <span class="n">export_formats</span><span class="p">:</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">export_dir</span><span class="p">,</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">CHECKPOINT</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">export_policy_checkpoint</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
            <span class="n">exported</span><span class="p">[</span><span class="n">ExportFormat</span><span class="o">.</span><span class="n">CHECKPOINT</span><span class="p">]</span> <span class="o">=</span> <span class="n">path</span>
        <span class="k">if</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">MODEL</span> <span class="ow">in</span> <span class="n">export_formats</span><span class="p">:</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">export_dir</span><span class="p">,</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">MODEL</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">export_policy_model</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
            <span class="n">exported</span><span class="p">[</span><span class="n">ExportFormat</span><span class="o">.</span><span class="n">MODEL</span><span class="p">]</span> <span class="o">=</span> <span class="n">path</span>
        <span class="k">if</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">ONNX</span> <span class="ow">in</span> <span class="n">export_formats</span><span class="p">:</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">export_dir</span><span class="p">,</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">ONNX</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">export_policy_model</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">onnx</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ONNX_OPSET&quot;</span><span class="p">,</span> <span class="s2">&quot;11&quot;</span><span class="p">)))</span>
            <span class="n">exported</span><span class="p">[</span><span class="n">ExportFormat</span><span class="o">.</span><span class="n">ONNX</span><span class="p">]</span> <span class="o">=</span> <span class="n">path</span>
        <span class="k">return</span> <span class="n">exported</span>

<div class="viewcode-block" id="Algorithm.import_model"><a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.import_model.html#ray.rllib.algorithms.algorithm.Algorithm.import_model">[docs]</a>    <span class="k">def</span> <span class="nf">import_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">import_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Imports a model from import_file.</span>

<span class="sd">        Note: Currently, only h5 files are supported.</span>

<span class="sd">        Args:</span>
<span class="sd">            import_file: The file to import the model from.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dict that maps ExportFormats to successfully exported models.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check for existence.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">import_file</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span>
                <span class="s2">&quot;`import_file` &#39;</span><span class="si">{}</span><span class="s2">&#39; does not exist! Can&#39;t import Model.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">import_file</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="c1"># Get the format of the given file.</span>
        <span class="n">import_format</span> <span class="o">=</span> <span class="s2">&quot;h5&quot;</span>  <span class="c1"># TODO(sven): Support checkpoint loading.</span>

        <span class="n">ExportFormat</span><span class="o">.</span><span class="n">validate</span><span class="p">([</span><span class="n">import_format</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">import_format</span> <span class="o">!=</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">H5</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">import_policy_model_from_h5</span><span class="p">(</span><span class="n">import_file</span><span class="p">)</span></div>

    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns current state of Algorithm, sufficient to restore it from scratch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The current state dict of this Algorithm, which can be used to sufficiently</span>
<span class="sd">            restore the algorithm from scratch without any other information.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Add config to state so complete Algorithm can be reproduced w/o it.</span>
        <span class="n">state</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;algorithm_class&quot;</span><span class="p">:</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span>
            <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;workers&quot;</span><span class="p">):</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>

        <span class="c1"># TODO: Experimental functionality: Store contents of replay buffer</span>
        <span class="c1">#  to checkpoint, only if user has configured this.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;store_buffer_in_checkpoints&quot;</span>
        <span class="p">):</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;local_replay_buffer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_exec_impl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;train_exec_impl&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_exec_impl</span><span class="o">.</span><span class="n">shared_metrics</span><span class="o">.</span><span class="n">get</span><span class="p">()</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;counters&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span>

        <span class="k">return</span> <span class="n">state</span>

    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the algorithm to the provided state.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: The state dict to restore this Algorithm instance to. `state` may</span>
<span class="sd">                have been returned by a call to an Algorithm&#39;s `__getstate__()` method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO (sven): Validate that our config and the config in state are compatible.</span>
        <span class="c1">#  For example, the model architectures may differ.</span>
        <span class="c1">#  Also, what should the behavior be if e.g. some training parameter</span>
        <span class="c1">#  (e.g. lr) changed?</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;workers&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;worker&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">])</span>
            <span class="n">remote_state</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">w</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">remote_state</span><span class="p">)),</span>
                <span class="n">local_worker</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">healthy_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="p">:</span>
                <span class="c1"># If evaluation workers are used, also restore the policies</span>
                <span class="c1"># there in case they are used for evaluation purpose.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span>
                    <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">w</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">remote_state</span><span class="p">)),</span>
                    <span class="n">healthy_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="c1"># If necessary, restore replay data as well.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># TODO: Experimental functionality: Restore contents of replay</span>
            <span class="c1">#  buffer from checkpoint, only if user has configured this.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;store_buffer_in_checkpoints&quot;</span><span class="p">):</span>
                <span class="k">if</span> <span class="s2">&quot;local_replay_buffer&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;local_replay_buffer&quot;</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;`store_buffer_in_checkpoints` is True, but no replay &quot;</span>
                        <span class="s2">&quot;data found in state!&quot;</span>
                    <span class="p">)</span>
            <span class="k">elif</span> <span class="s2">&quot;local_replay_buffer&quot;</span> <span class="ow">in</span> <span class="n">state</span> <span class="ow">and</span> <span class="n">log_once</span><span class="p">(</span>
                <span class="s2">&quot;no_store_buffer_in_checkpoints_but_data_found&quot;</span>
            <span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;`store_buffer_in_checkpoints` is False, but some replay &quot;</span>
                    <span class="s2">&quot;data found in state!&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_exec_impl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_exec_impl</span><span class="o">.</span><span class="n">shared_metrics</span><span class="o">.</span><span class="n">get</span><span class="p">()</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;train_exec_impl&quot;</span><span class="p">])</span>
        <span class="k">elif</span> <span class="s2">&quot;counters&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;counters&quot;</span><span class="p">]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_checkpoint_info_to_algorithm_state</span><span class="p">(</span>
        <span class="n">checkpoint_info</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
        <span class="n">policy_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Container</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">AgentID</span><span class="p">,</span> <span class="n">EpisodeID</span><span class="p">],</span> <span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policies_to_train</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">Container</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">],</span>
                <span class="n">Callable</span><span class="p">[[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleBatchType</span><span class="p">]],</span> <span class="nb">bool</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Converts a checkpoint info or object to a proper Algorithm state dict.</span>

<span class="sd">        The returned state dict can be used inside self.__setstate__().</span>

<span class="sd">        Args:</span>
<span class="sd">            checkpoint_info: A checkpoint info dict as returned by</span>
<span class="sd">                `ray.rllib.utils.checkpoints.get_checkpoint_info(</span>
<span class="sd">                [checkpoint dir or AIR Checkpoint])`.</span>
<span class="sd">            policy_ids: Optional list/set of PolicyIDs. If not None, only those policies</span>
<span class="sd">                listed here will be included in the returned state. Note that</span>
<span class="sd">                state items such as filters, the `is_policy_to_train` function, as</span>
<span class="sd">                well as the multi-agent `policy_ids` dict will be adjusted as well,</span>
<span class="sd">                based on this arg.</span>
<span class="sd">            policy_mapping_fn: An optional (updated) policy mapping function</span>
<span class="sd">                to include in the returned state.</span>
<span class="sd">            policies_to_train: An optional list of policy IDs to be trained</span>
<span class="sd">                or a callable taking PolicyID and SampleBatchType and</span>
<span class="sd">                returning a bool (trainable or not?) to include in the returned state.</span>

<span class="sd">        Returns:</span>
<span class="sd">             The state dict usable within the `self.__setstate__()` method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;Algorithm&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`checkpoint` arg passed to &quot;</span>
                <span class="s2">&quot;`Algorithm._checkpoint_info_to_algorithm_state()` must be an &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Algorithm checkpoint (but is </span><span class="si">{</span><span class="n">checkpoint_info</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">)!&quot;</span>
            <span class="p">)</span>

        <span class="n">msgpack</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">checkpoint_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;format&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;msgpack&quot;</span><span class="p">:</span>
            <span class="n">msgpack</span> <span class="o">=</span> <span class="n">try_import_msgpack</span><span class="p">(</span><span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;state_file&quot;</span><span class="p">],</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">msgpack</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">msgpack</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

        <span class="c1"># New checkpoint format: Policies are in separate sub-dirs.</span>
        <span class="c1"># Note: Algorithms like ES/ARS don&#39;t have a WorkerSet, so we just return</span>
        <span class="c1"># the plain state here.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">version</span><span class="o">.</span><span class="n">Version</span><span class="p">(</span><span class="s2">&quot;0.1&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;worker&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="n">worker_state</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">]</span>

            <span class="c1"># Retrieve the set of all required policy IDs.</span>
            <span class="n">policy_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
                <span class="n">policy_ids</span> <span class="k">if</span> <span class="n">policy_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;policy_ids&quot;</span><span class="p">]</span>
            <span class="p">)</span>

            <span class="c1"># Remove those policies entirely from filters that are not in</span>
            <span class="c1"># `policy_ids`.</span>
            <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;filters&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">pid</span><span class="p">:</span> <span class="nb">filter</span>
                <span class="k">for</span> <span class="n">pid</span><span class="p">,</span> <span class="nb">filter</span> <span class="ow">in</span> <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;filters&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">policy_ids</span>
            <span class="p">}</span>

            <span class="c1"># Get Algorithm class.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
                <span class="c1"># Try deserializing from a full classpath.</span>
                <span class="c1"># Or as a last resort: Tune registered algorithm name.</span>
                <span class="n">state</span><span class="p">[</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">deserialize_type</span><span class="p">(</span>
                    <span class="n">state</span><span class="p">[</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">]</span>
                <span class="p">)</span> <span class="ow">or</span> <span class="n">get_trainable_cls</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">])</span>
            <span class="c1"># Compile actual config object.</span>
            <span class="n">default_config</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">default_config</span><span class="p">,</span> <span class="n">AlgorithmConfig</span><span class="p">):</span>
                <span class="n">new_config</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_config</span> <span class="o">=</span> <span class="n">Algorithm</span><span class="o">.</span><span class="n">merge_trainer_configs</span><span class="p">(</span>
                    <span class="n">default_config</span><span class="p">,</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span>
                <span class="p">)</span>

            <span class="c1"># Remove policies from multiagent dict that are not in `policy_ids`.</span>
            <span class="n">new_policies</span> <span class="o">=</span> <span class="n">new_config</span><span class="o">.</span><span class="n">policies</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_policies</span><span class="p">,</span> <span class="p">(</span><span class="nb">set</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="n">new_policies</span> <span class="o">=</span> <span class="p">{</span><span class="n">pid</span> <span class="k">for</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">new_policies</span> <span class="k">if</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">policy_ids</span><span class="p">}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_policies</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">pid</span><span class="p">:</span> <span class="n">spec</span> <span class="k">for</span> <span class="n">pid</span><span class="p">,</span> <span class="n">spec</span> <span class="ow">in</span> <span class="n">new_policies</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">policy_ids</span>
                <span class="p">}</span>
            <span class="n">new_config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span>
                <span class="n">policies</span><span class="o">=</span><span class="n">new_policies</span><span class="p">,</span>
                <span class="n">policies_to_train</span><span class="o">=</span><span class="n">policies_to_train</span><span class="p">,</span>
                <span class="o">**</span><span class="p">(</span>
                    <span class="p">{</span><span class="s2">&quot;policy_mapping_fn&quot;</span><span class="p">:</span> <span class="n">policy_mapping_fn</span><span class="p">}</span>
                    <span class="k">if</span> <span class="n">policy_mapping_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="p">{}</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_config</span>

            <span class="c1"># Prepare local `worker` state to add policies&#39; states into it,</span>
            <span class="c1"># read from separate policy checkpoint files.</span>
            <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;policy_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">policy_ids</span><span class="p">:</span>
                <span class="n">policy_state_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;checkpoint_dir&quot;</span><span class="p">],</span>
                    <span class="s2">&quot;policies&quot;</span><span class="p">,</span>
                    <span class="n">pid</span><span class="p">,</span>
                    <span class="s2">&quot;policy_state.&quot;</span>
                    <span class="o">+</span> <span class="p">(</span><span class="s2">&quot;msgpck&quot;</span> <span class="k">if</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;format&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;msgpack&quot;</span> <span class="k">else</span> <span class="s2">&quot;pkl&quot;</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">policy_state_file</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Given checkpoint does not seem to be valid! No policy &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;state file found for PID=</span><span class="si">{</span><span class="n">pid</span><span class="si">}</span><span class="s2">. &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;The file not found is: </span><span class="si">{</span><span class="n">policy_state_file</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>

                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">policy_state_file</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">msgpack</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;policy_states&quot;</span><span class="p">][</span><span class="n">pid</span><span class="p">]</span> <span class="o">=</span> <span class="n">msgpack</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;policy_states&quot;</span><span class="p">][</span><span class="n">pid</span><span class="p">]</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

            <span class="c1"># These two functions are never serialized in a msgpack checkpoint (which</span>
            <span class="c1"># does not store code, unlike a cloudpickle checkpoint). Hence the user has</span>
            <span class="c1"># to provide them with the `Algorithm.from_checkpoint()` call.</span>
            <span class="k">if</span> <span class="n">policy_mapping_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;policy_mapping_fn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">policy_mapping_fn</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">policies_to_train</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="c1"># `policies_to_train` might be left None in case all policies should be</span>
                <span class="c1"># trained.</span>
                <span class="ow">or</span> <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;is_policy_to_train&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">NOT_SERIALIZABLE</span>
            <span class="p">):</span>
                <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;is_policy_to_train&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">policies_to_train</span>

        <span class="k">return</span> <span class="n">state</span>

    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">_create_local_replay_buffer_if_necessary</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">PartialAlgorithmConfigDict</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiAgentReplayBuffer</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Create a MultiAgentReplayBuffer instance if necessary.</span>

<span class="sd">        Args:</span>
<span class="sd">            config: Algorithm-specific configuration data.</span>

<span class="sd">        Returns:</span>
<span class="sd">            MultiAgentReplayBuffer instance based on algorithm config.</span>
<span class="sd">            None, if local replay buffer is not needed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;no_local_replay_buffer&quot;</span>
        <span class="p">):</span>
            <span class="k">return</span>

        <span class="k">return</span> <span class="n">from_config</span><span class="p">(</span><span class="n">ReplayBuffer</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">])</span>

    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">_kwargs_for_execution_plan</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;local_replay_buffer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span>
        <span class="k">return</span> <span class="n">kwargs</span>

    <span class="k">def</span> <span class="nf">_run_one_training_iteration</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ResultDict</span><span class="p">,</span> <span class="s2">&quot;TrainIterCtx&quot;</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Runs one training iteration (self.iteration will be +1 after this).</span>

<span class="sd">        Calls `self.training_step()` repeatedly until the minimum time (sec),</span>
<span class="sd">        sample- or training steps have been reached.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The results dict from the training iteration.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># In case we are training (in a thread) parallel to evaluation,</span>
        <span class="c1"># we may have to re-enable eager mode here (gets disabled in the</span>
        <span class="c1"># thread).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;framework&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;tf2&quot;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
            <span class="n">tf1</span><span class="o">.</span><span class="n">enable_eager_execution</span><span class="p">()</span>

        <span class="n">results</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Create a step context ...</span>
        <span class="k">with</span> <span class="n">TrainIterCtx</span><span class="p">(</span><span class="n">algo</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span> <span class="k">as</span> <span class="n">train_iter_ctx</span><span class="p">:</span>
            <span class="c1"># .. so we can query it whether we should stop the iteration loop (e.g.</span>
            <span class="c1"># when we have reached `min_time_s_per_iteration`).</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="n">train_iter_ctx</span><span class="o">.</span><span class="n">should_stop</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
                <span class="c1"># Try to train one step.</span>
                <span class="c1"># TODO (avnishn): Remove the execution plan API by q1 2023</span>
                <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">TRAINING_ITERATION_TIMER</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_disable_execution_plan_api</span><span class="p">:</span>
                        <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_step</span><span class="p">()</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">results</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_exec_impl</span><span class="p">)</span>

        <span class="c1"># With training step done. Try to bring failed workers back.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">restore_workers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">results</span><span class="p">,</span> <span class="n">train_iter_ctx</span>

    <span class="k">def</span> <span class="nf">_run_one_evaluation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">train_future</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">concurrent</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">ThreadPoolExecutor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ResultDict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Runs evaluation step via `self.evaluate()` and handling worker failures.</span>

<span class="sd">        Args:</span>
<span class="sd">            train_future: In case, we are training and avaluating in parallel,</span>
<span class="sd">                this arg carries the currently running ThreadPoolExecutor</span>
<span class="sd">                object that runs the training iteration</span>

<span class="sd">        Returns:</span>
<span class="sd">            The results dict from the evaluation call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">eval_func_to_use</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_evaluate_async</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_async_evaluation</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">train_future</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_parallel_to_training</span>
            <span class="p">)</span>
            <span class="n">unit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration_unit</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="n">eval_func_to_use</span><span class="p">(</span>
                <span class="n">duration_fn</span><span class="o">=</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_automatic_evaluation_duration_fn</span><span class="p">,</span>
                    <span class="n">unit</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_num_workers</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span>
                    <span class="n">train_future</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="c1"># Run `self.evaluate()` only once per training iteration.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="n">eval_func_to_use</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># After evaluation, do a round of health check to see if any of</span>
            <span class="c1"># the failed workers are back.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">restore_workers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="p">)</span>

            <span class="c1"># Add number of healthy evaluation workers after this iteration.</span>
            <span class="n">eval_results</span><span class="p">[</span><span class="s2">&quot;evaluation&quot;</span><span class="p">][</span>
                <span class="s2">&quot;num_healthy_workers&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span>
            <span class="n">eval_results</span><span class="p">[</span><span class="s2">&quot;evaluation&quot;</span><span class="p">][</span>
                <span class="s2">&quot;num_in_flight_async_reqs&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">num_in_flight_async_reqs</span><span class="p">()</span>
            <span class="n">eval_results</span><span class="p">[</span><span class="s2">&quot;evaluation&quot;</span><span class="p">][</span>
                <span class="s2">&quot;num_remote_worker_restarts&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_workers</span><span class="o">.</span><span class="n">num_remote_worker_restarts</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">eval_results</span>

    <span class="k">def</span> <span class="nf">_run_one_training_iteration_and_evaluation_in_parallel</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ResultDict</span><span class="p">,</span> <span class="s2">&quot;TrainIterCtx&quot;</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Runs one training iteration and one evaluation step in parallel.</span>

<span class="sd">        First starts the training iteration (via `self._run_one_training_iteration()`)</span>
<span class="sd">        within a ThreadPoolExecutor, then runs the evaluation step in parallel.</span>
<span class="sd">        In auto-duration mode (config.evaluation_duration=auto), makes sure the</span>
<span class="sd">        evaluation step takes roughly the same time as the training iteration.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The accumulated training and evaluation results.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">concurrent</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">ThreadPoolExecutor</span><span class="p">()</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
            <span class="n">train_future</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_training_iteration</span><span class="p">())</span>
            <span class="c1"># Pass the train_future into `self._run_one_evaluation()` to allow it</span>
            <span class="c1"># to run exactly as long as the training iteration takes in case</span>
            <span class="c1"># evaluation_duration=auto.</span>
            <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_evaluation</span><span class="p">(</span><span class="n">train_future</span><span class="p">)</span>
            <span class="c1"># Collect the training results from the future.</span>
            <span class="n">train_results</span><span class="p">,</span> <span class="n">train_iter_ctx</span> <span class="o">=</span> <span class="n">train_future</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>
            <span class="n">results</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">train_results</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">results</span><span class="p">,</span> <span class="n">train_iter_ctx</span>

    <span class="k">def</span> <span class="nf">_run_offline_evaluation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Runs offline evaluation via `OfflineEvaluator.estimate_on_dataset()` API.</span>

<span class="sd">        This method will be used when `evaluation_dataset` is provided.</span>
<span class="sd">        Note: This will only work if the policy is a single agent policy.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The results dict from the offline evaluation call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span><span class="o">.</span><span class="n">policy_map</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>

        <span class="n">parallelism</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">evaluation_num_workers</span> <span class="ow">or</span> <span class="mi">1</span>
        <span class="n">offline_eval_results</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;off_policy_estimator&quot;</span><span class="p">:</span> <span class="p">{}}</span>
        <span class="k">for</span> <span class="n">evaluator_name</span><span class="p">,</span> <span class="n">offline_evaluator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">offline_eval_results</span><span class="p">[</span><span class="s2">&quot;off_policy_estimator&quot;</span><span class="p">][</span>
                <span class="n">evaluator_name</span>
            <span class="p">]</span> <span class="o">=</span> <span class="n">offline_evaluator</span><span class="o">.</span><span class="n">estimate_on_dataset</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_dataset</span><span class="p">,</span>
                <span class="n">n_parallelism</span><span class="o">=</span><span class="n">parallelism</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">offline_eval_results</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_should_create_evaluation_rollout_workers</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">eval_config</span><span class="p">:</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Determines whether we need to create evaluation workers.</span>

<span class="sd">        Returns False if we need to run offline evaluation</span>
<span class="sd">        (with ope.estimate_on_dastaset API) or when local worker is to be used for</span>
<span class="sd">        evaluation. Note: We only use estimate_on_dataset API with bandits for now.</span>
<span class="sd">        That is when ope_split_batch_by_episode is False.</span>
<span class="sd">        TODO: In future we will do the same for episodic RL OPE.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">run_offline_evaluation</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">eval_config</span><span class="o">.</span><span class="n">off_policy_estimation_methods</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="n">eval_config</span><span class="o">.</span><span class="n">ope_split_batch_by_episode</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="ow">not</span> <span class="n">run_offline_evaluation</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="n">eval_config</span><span class="o">.</span><span class="n">evaluation_num_workers</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">eval_config</span><span class="o">.</span><span class="n">evaluation_interval</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_automatic_evaluation_duration_fn</span><span class="p">(</span>
        <span class="n">unit</span><span class="p">,</span> <span class="n">num_eval_workers</span><span class="p">,</span> <span class="n">eval_cfg</span><span class="p">,</span> <span class="n">train_future</span><span class="p">,</span> <span class="n">num_units_done</span>
    <span class="p">):</span>
        <span class="c1"># Training is done and we already ran at least one</span>
        <span class="c1"># evaluation -&gt; Nothing left to run.</span>
        <span class="k">if</span> <span class="n">num_units_done</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">train_future</span><span class="o">.</span><span class="n">done</span><span class="p">():</span>
            <span class="k">return</span> <span class="mi">0</span>
        <span class="c1"># Count by episodes. -&gt; Run n more</span>
        <span class="c1"># (n=num eval workers).</span>
        <span class="k">elif</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">num_eval_workers</span>
        <span class="c1"># Count by timesteps. -&gt; Run n*m*p more</span>
        <span class="c1"># (n=num eval workers; m=rollout fragment length;</span>
        <span class="c1"># p=num-envs-per-worker).</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">num_eval_workers</span>
                <span class="o">*</span> <span class="n">eval_cfg</span><span class="p">[</span><span class="s2">&quot;rollout_fragment_length&quot;</span><span class="p">]</span>
                <span class="o">*</span> <span class="n">eval_cfg</span><span class="p">[</span><span class="s2">&quot;num_envs_per_worker&quot;</span><span class="p">]</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_compile_iteration_results</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">episodes_this_iter</span><span class="p">,</span> <span class="n">step_ctx</span><span class="p">,</span> <span class="n">iteration_results</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
        <span class="c1"># Return dict.</span>
        <span class="n">results</span><span class="p">:</span> <span class="n">ResultDict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">iteration_results</span> <span class="o">=</span> <span class="n">iteration_results</span> <span class="ow">or</span> <span class="p">{}</span>

        <span class="c1"># Evaluation results.</span>
        <span class="k">if</span> <span class="s2">&quot;evaluation&quot;</span> <span class="ow">in</span> <span class="n">iteration_results</span><span class="p">:</span>
            <span class="n">results</span><span class="p">[</span><span class="s2">&quot;evaluation&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iteration_results</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;evaluation&quot;</span><span class="p">)</span>

        <span class="c1"># Custom metrics and episode media.</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;custom_metrics&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iteration_results</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;custom_metrics&quot;</span><span class="p">,</span> <span class="p">{})</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;episode_media&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iteration_results</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;episode_media&quot;</span><span class="p">,</span> <span class="p">{})</span>

        <span class="c1"># Learner info.</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;info&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">LEARNER_INFO</span><span class="p">:</span> <span class="n">iteration_results</span><span class="p">}</span>

        <span class="c1"># Calculate how many (if any) of older, historical episodes we have to add to</span>
        <span class="c1"># `episodes_this_iter` in order to reach the required smoothing window.</span>
        <span class="n">episodes_for_metrics</span> <span class="o">=</span> <span class="n">episodes_this_iter</span><span class="p">[:]</span>
        <span class="n">missing</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">metrics_num_episodes_for_smoothing</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span>
            <span class="n">episodes_this_iter</span>
        <span class="p">)</span>
        <span class="c1"># We have to add some older episodes to reach the smoothing window size.</span>
        <span class="k">if</span> <span class="n">missing</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">episodes_for_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_episode_history</span><span class="p">[</span><span class="o">-</span><span class="n">missing</span><span class="p">:]</span> <span class="o">+</span> <span class="n">episodes_this_iter</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">episodes_for_metrics</span><span class="p">)</span>
                <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">metrics_num_episodes_for_smoothing</span>
            <span class="p">)</span>
        <span class="c1"># Note that when there are more than `metrics_num_episodes_for_smoothing`</span>
        <span class="c1"># episodes in `episodes_for_metrics`, leave them as-is. In this case, we&#39;ll</span>
        <span class="c1"># compute the stats over that larger number.</span>

        <span class="c1"># Add new episodes to our history and make sure it doesn&#39;t grow larger than</span>
        <span class="c1"># needed.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_episode_history</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">episodes_this_iter</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_episode_history</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_episode_history</span><span class="p">[</span>
            <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">metrics_num_episodes_for_smoothing</span> <span class="p">:</span>
        <span class="p">]</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;sampler_results&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">summarize_episodes</span><span class="p">(</span>
            <span class="n">episodes_for_metrics</span><span class="p">,</span>
            <span class="n">episodes_this_iter</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">keep_per_episode_custom_metrics</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># TODO: Don&#39;t dump sampler results into top-level.</span>
        <span class="n">results</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s2">&quot;sampler_results&quot;</span><span class="p">])</span>

        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;num_healthy_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;num_in_flight_async_reqs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">num_in_flight_async_reqs</span><span class="p">()</span>
        <span class="n">results</span><span class="p">[</span>
            <span class="s2">&quot;num_remote_worker_restarts&quot;</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">num_remote_worker_restarts</span><span class="p">()</span>

        <span class="c1"># Train-steps- and env/agent-steps this iteration.</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">,</span>
            <span class="n">NUM_AGENT_STEPS_TRAINED</span><span class="p">,</span>
            <span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">,</span>
            <span class="n">NUM_ENV_STEPS_TRAINED</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="n">results</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">c</span><span class="p">]</span>
        <span class="n">time_taken_sec</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">get_time_taken_sec</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;agent_steps&quot;</span><span class="p">:</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span> <span class="o">+</span> <span class="s2">&quot;_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">sampled</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_TRAINED</span> <span class="o">+</span> <span class="s2">&quot;_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">trained</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span> <span class="o">+</span> <span class="s2">&quot;_throughput_per_sec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">step_ctx</span><span class="o">.</span><span class="n">sampled</span> <span class="o">/</span> <span class="n">time_taken_sec</span>
            <span class="p">)</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_TRAINED</span> <span class="o">+</span> <span class="s2">&quot;_throughput_per_sec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">step_ctx</span><span class="o">.</span><span class="n">trained</span> <span class="o">/</span> <span class="n">time_taken_sec</span>
            <span class="p">)</span>
            <span class="c1"># TODO: For CQL and other algos, count by trained steps.</span>
            <span class="n">results</span><span class="p">[</span><span class="s2">&quot;timesteps_total&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span> <span class="o">+</span> <span class="s2">&quot;_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">sampled</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_TRAINED</span> <span class="o">+</span> <span class="s2">&quot;_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">trained</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span> <span class="o">+</span> <span class="s2">&quot;_throughput_per_sec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">step_ctx</span><span class="o">.</span><span class="n">sampled</span> <span class="o">/</span> <span class="n">time_taken_sec</span>
            <span class="p">)</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_TRAINED</span> <span class="o">+</span> <span class="s2">&quot;_throughput_per_sec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">step_ctx</span><span class="o">.</span><span class="n">trained</span> <span class="o">/</span> <span class="n">time_taken_sec</span>
            <span class="p">)</span>
            <span class="c1"># TODO: For CQL and other algos, count by trained steps.</span>
            <span class="n">results</span><span class="p">[</span><span class="s2">&quot;timesteps_total&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">]</span>

        <span class="c1"># TODO: Backward compatibility.</span>
        <span class="n">results</span><span class="p">[</span><span class="n">STEPS_TRAINED_THIS_ITER_COUNTER</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">trained</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;agent_timesteps_total&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">]</span>

        <span class="c1"># Process timer results.</span>
        <span class="n">timers</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">timer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">timers</span><span class="p">[</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_time_ms&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">timer</span><span class="o">.</span><span class="n">mean</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">timer</span><span class="o">.</span><span class="n">has_units_processed</span><span class="p">():</span>
                <span class="n">timers</span><span class="p">[</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_throughput&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">timer</span><span class="o">.</span><span class="n">mean_throughput</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;timers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">timers</span>

        <span class="c1"># Process counter results.</span>
        <span class="n">counters</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">counter</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">counters</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">counter</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;counters&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">counters</span>
        <span class="c1"># TODO: Backward compatibility.</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;info&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">counters</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">results</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">def</span> <span class="nf">_record_usage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Record the framework and algorithm used.</span>

<span class="sd">        Args:</span>
<span class="sd">            config: Algorithm config dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">record_extra_usage_tag</span><span class="p">(</span><span class="n">TagKey</span><span class="o">.</span><span class="n">RLLIB_FRAMEWORK</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;framework&quot;</span><span class="p">])</span>
        <span class="n">record_extra_usage_tag</span><span class="p">(</span><span class="n">TagKey</span><span class="o">.</span><span class="n">RLLIB_NUM_WORKERS</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_workers&quot;</span><span class="p">]))</span>
        <span class="n">alg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="c1"># We do not want to collect user defined algorithm names.</span>
        <span class="k">if</span> <span class="n">alg</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ALL_ALGORITHMS</span><span class="p">:</span>
            <span class="n">alg</span> <span class="o">=</span> <span class="s2">&quot;USER_DEFINED&quot;</span>
        <span class="n">record_extra_usage_tag</span><span class="p">(</span><span class="n">TagKey</span><span class="o">.</span><span class="n">RLLIB_ALGORITHM</span><span class="p">,</span> <span class="n">alg</span><span class="p">)</span>

    <span class="nd">@Deprecated</span><span class="p">(</span><span class="n">new</span><span class="o">=</span><span class="s2">&quot;Algorithm.compute_single_action()&quot;</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">compute_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_single_action</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@Deprecated</span><span class="p">(</span><span class="n">new</span><span class="o">=</span><span class="s2">&quot;construct WorkerSet(...) instance directly&quot;</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">_make_workers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="nd">@Deprecated</span><span class="p">(</span><span class="n">new</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.validate()&quot;</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">validate_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="nd">@staticmethod</span>
    <span class="nd">@Deprecated</span><span class="p">(</span><span class="n">new</span><span class="o">=</span><span class="s2">&quot;AlgorithmConfig.validate()&quot;</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">_validate_config</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">trainer_or_none</span><span class="p">):</span>
        <span class="k">pass</span></div>


<span class="c1"># TODO: Create a dict that throw a deprecation warning once we have fully moved</span>
<span class="c1">#  to AlgorithmConfig() objects (some algos still missing).</span>
<span class="n">COMMON_CONFIG</span><span class="p">:</span> <span class="n">AlgorithmConfigDict</span> <span class="o">=</span> <span class="n">AlgorithmConfig</span><span class="p">(</span><span class="n">Algorithm</span><span class="p">)</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">TrainIterCtx</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">algo</span><span class="p">:</span> <span class="n">Algorithm</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">algo</span> <span class="o">=</span> <span class="n">algo</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_start</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_stop</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Before first call to `step()`, `results` is expected to be None -&gt;</span>
        <span class="c1"># Start with self.failures=-1 -&gt; set to 0 before the very first call</span>
        <span class="c1"># to `self.step()`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">failures</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">time_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampled</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_sampled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_trained</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_TRAINED</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_sampled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_trained</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_TRAINED</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">failure_tolerance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="p">[</span>
            <span class="s2">&quot;num_consecutive_worker_failures_tolerance&quot;</span>
        <span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_stop</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_time_taken_sec</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns the time we spent in the context in seconds.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_stop</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_start</span>

    <span class="k">def</span> <span class="nf">should_stop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">results</span><span class="p">):</span>

        <span class="c1"># Before first call to `step()`.</span>
        <span class="k">if</span> <span class="n">results</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Fail after n retries.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">failures</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">failures</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">failure_tolerance</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;More than `num_consecutive_worker_failures_tolerance=&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">failure_tolerance</span><span class="si">}</span><span class="s2">` consecutive worker failures! &quot;</span>
                    <span class="s2">&quot;Exiting.&quot;</span>
                <span class="p">)</span>
            <span class="c1"># Continue to very first `step()` call or retry `step()` after</span>
            <span class="c1"># a (tolerable) failure.</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># Stopping criteria.</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_disable_execution_plan_api</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;agent_steps&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sampled</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">]</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_sampled</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_TRAINED</span><span class="p">]</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_trained</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sampled</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">]</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_sampled</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_TRAINED</span><span class="p">]</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_trained</span>
                <span class="p">)</span>

            <span class="n">min_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;min_time_s_per_iteration&quot;</span><span class="p">]</span>
            <span class="n">min_sample_ts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;min_sample_timesteps_per_iteration&quot;</span><span class="p">]</span>
            <span class="n">min_train_ts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;min_train_timesteps_per_iteration&quot;</span><span class="p">]</span>
            <span class="c1"># Repeat if not enough time has passed or if not enough</span>
            <span class="c1"># env|train timesteps have been processed (or these min</span>
            <span class="c1"># values are not provided by the user).</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="p">(</span><span class="ow">not</span> <span class="n">min_t</span> <span class="ow">or</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_start</span> <span class="o">&gt;=</span> <span class="n">min_t</span><span class="p">)</span>
                <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">min_sample_ts</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampled</span> <span class="o">&gt;=</span> <span class="n">min_sample_ts</span><span class="p">)</span>
                <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">min_train_ts</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">&gt;=</span> <span class="n">min_train_ts</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="k">return</span> <span class="kc">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">False</span>
        <span class="c1"># No errors (we got results != None) -&gt; Return True</span>
        <span class="c1"># (meaning: yes, should stop -&gt; no further step attempts).</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>