
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ray.data.iterator &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/js/versionwarning.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../../_static/js/docsearch.js"></script>
    <script src="../../../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../../_static/js/custom.js"></script>
    <script defer="defer" src="../../../_static/js/top-navigation.js"></script>
    <script src="../../../_static/js/tags.js"></script>
    <script src="../../../_static/tabs.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script async="async" src="../../../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/_modules/ray/data/iterator.html" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "_modules/ray/data/iterator", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2F_modules/ray/data/iterator.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for ray.data.iterator</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">abc</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">TYPE_CHECKING</span><span class="p">,</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">Iterator</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">ray.data._internal.block_batching</span> <span class="kn">import</span> <span class="n">batch_block_refs</span>
<span class="kn">from</span> <span class="nn">ray.data._internal.block_batching.iter_batches</span> <span class="kn">import</span> <span class="n">iter_batches</span>
<span class="kn">from</span> <span class="nn">ray.data._internal.stats</span> <span class="kn">import</span> <span class="n">DatasetStats</span>
<span class="kn">from</span> <span class="nn">ray.data.block</span> <span class="kn">import</span> <span class="n">Block</span><span class="p">,</span> <span class="n">BlockAccessor</span><span class="p">,</span> <span class="n">BlockMetadata</span><span class="p">,</span> <span class="n">DataBatch</span>
<span class="kn">from</span> <span class="nn">ray.data.context</span> <span class="kn">import</span> <span class="n">DataContext</span>
<span class="kn">from</span> <span class="nn">ray.types</span> <span class="kn">import</span> <span class="n">ObjectRef</span>
<span class="kn">from</span> <span class="nn">ray.util.annotations</span> <span class="kn">import</span> <span class="n">PublicAPI</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
    <span class="kn">import</span> <span class="nn">torch</span>

    <span class="kn">from</span> <span class="nn">ray.data._internal.torch_iterable_dataset</span> <span class="kn">import</span> <span class="n">TorchTensorBatchType</span>
    <span class="kn">from</span> <span class="nn">ray.data.dataset</span> <span class="kn">import</span> <span class="n">Schema</span><span class="p">,</span> <span class="n">TensorFlowTensorBatchType</span>


<div class="viewcode-block" id="DataIterator"><a class="viewcode-back" href="../../../data/api/data_iterator.html#ray.data.DataIterator">[docs]</a><span class="nd">@PublicAPI</span><span class="p">(</span><span class="n">stability</span><span class="o">=</span><span class="s2">&quot;beta&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">DataIterator</span><span class="p">(</span><span class="n">abc</span><span class="o">.</span><span class="n">ABC</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An iterator for reading records from a :class:`~Dataset` or</span>
<span class="sd">    :class:`~DatasetPipeline`.</span>

<span class="sd">    For Datasets, each iteration call represents a complete read of all items in the</span>
<span class="sd">    Dataset. For DatasetPipelines, each iteration call represents one pass (epoch)</span>
<span class="sd">    over the base Dataset. Note that for DatasetPipelines, each pass iterates over</span>
<span class="sd">    the original Dataset, instead of a window (if ``.window()`` was used).</span>

<span class="sd">    If using Ray AIR, each trainer actor should get its own iterator by calling</span>
<span class="sd">    :meth:`session.get_dataset_shard(&quot;train&quot;)</span>
<span class="sd">    &lt;ray.air.session.get_dataset_shard&gt;`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import ray</span>
<span class="sd">        &gt;&gt;&gt; ds = ray.data.range(5)</span>
<span class="sd">        &gt;&gt;&gt; ds</span>
<span class="sd">        Dataset(num_blocks=5, num_rows=5, schema={id: int64})</span>
<span class="sd">        &gt;&gt;&gt; ds.iterator()</span>
<span class="sd">        DataIterator(Dataset(num_blocks=5, num_rows=5, schema={id: int64}))</span>

<span class="sd">    .. tip::</span>
<span class="sd">        For debugging purposes, use</span>
<span class="sd">        :meth:`~ray.air.util.check_ingest.make_local_dataset_iterator` to create a</span>
<span class="sd">        local `DataIterator` from a :class:`~ray.data.Dataset`, a</span>
<span class="sd">        :class:`~ray.data.Preprocessor`, and a :class:`~ray.air.DatasetConfig`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">_to_block_iterator</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span>
        <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">ObjectRef</span><span class="p">[</span><span class="n">Block</span><span class="p">],</span> <span class="n">BlockMetadata</span><span class="p">]],</span>
        <span class="n">Optional</span><span class="p">[</span><span class="n">DatasetStats</span><span class="p">],</span>
        <span class="nb">bool</span><span class="p">,</span>
    <span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Returns the iterator to use for `iter_batches`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple. The first item of the tuple is an iterator over pairs of Block</span>
<span class="sd">            object references and their corresponding metadata. The second item of the</span>
<span class="sd">            tuple is a DatasetStats object used for recording stats during iteration.</span>
<span class="sd">            The third item is a boolean indicating if the blocks can be safely cleared</span>
<span class="sd">            after use.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

<div class="viewcode-block" id="DataIterator.iter_batches"><a class="viewcode-back" href="../../../data/api/doc/ray.data.DataIterator.iter_batches.html#ray.data.DataIterator.iter_batches">[docs]</a>    <span class="k">def</span> <span class="nf">iter_batches</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prefetch_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">batch_format</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">local_shuffle_buffer_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">local_shuffle_seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">_collate_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">DataBatch</span><span class="p">],</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Deprecated.</span>
        <span class="n">prefetch_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">DataBatch</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Return a local batched iterator over the dataset.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import ray</span>
<span class="sd">            &gt;&gt;&gt; for batch in ray.data.range(</span>
<span class="sd">            ...     1000000</span>
<span class="sd">            ... ).iterator().iter_batches(): # doctest: +SKIP</span>
<span class="sd">            ...     print(batch) # doctest: +SKIP</span>

<span class="sd">        Time complexity: O(1)</span>

<span class="sd">        Args:</span>
<span class="sd">            prefetch_batches: The number of batches to fetch ahead of the current batch</span>
<span class="sd">                to fetch. If set to greater than 0, a separate threadpool will be used</span>
<span class="sd">                to fetch the objects to the local node, format the batches, and apply</span>
<span class="sd">                the collate_fn. Defaults to 1. You can revert back to the old</span>
<span class="sd">                prefetching behavior that uses `prefetch_blocks` by setting</span>
<span class="sd">                `use_legacy_iter_batches` to True in the DataContext.</span>
<span class="sd">            batch_size: The number of rows in each batch, or None to use entire blocks</span>
<span class="sd">                as batches (blocks may contain different number of rows).</span>
<span class="sd">                The final batch may include fewer than ``batch_size`` rows if</span>
<span class="sd">                ``drop_last`` is ``False``. Defaults to 256.</span>
<span class="sd">            batch_format: Specify ``&quot;default&quot;`` to use the default block format</span>
<span class="sd">                (NumPy), ``&quot;pandas&quot;`` to select ``pandas.DataFrame``, &quot;pyarrow&quot; to</span>
<span class="sd">                select ``pyarrow.Table``, or ``&quot;numpy&quot;`` to select</span>
<span class="sd">                ``Dict[str, numpy.ndarray]``, or None to return the underlying block</span>
<span class="sd">                exactly as is with no additional formatting.</span>
<span class="sd">            drop_last: Whether to drop the last batch if it&#39;s incomplete.</span>
<span class="sd">            local_shuffle_buffer_size: If non-None, the data will be randomly shuffled</span>
<span class="sd">                using a local in-memory shuffle buffer, and this value will serve as the</span>
<span class="sd">                minimum number of rows that must be in the local in-memory shuffle</span>
<span class="sd">                buffer in order to yield a batch. When there are no more rows to add to</span>
<span class="sd">                the buffer, the remaining rows in the buffer will be drained.</span>
<span class="sd">            local_shuffle_seed: The seed to use for the local random shuffle.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An iterator over record batches.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">context</span> <span class="o">=</span> <span class="n">DataContext</span><span class="o">.</span><span class="n">get_current</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">use_streaming_executor</span><span class="p">:</span>
            <span class="c1"># Always use legacy iter_batches for bulk executor.</span>
            <span class="n">use_legacy</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">use_legacy</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">use_legacy_iter_batches</span>

        <span class="k">if</span> <span class="n">prefetch_blocks</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">use_legacy</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">DeprecationWarning</span><span class="p">(</span>
                <span class="s2">&quot;`prefetch_blocks` arg is deprecated in Ray 2.4. Use &quot;</span>
                <span class="s2">&quot;the `prefetch_batches` arg instead to specify the amount of &quot;</span>
                <span class="s2">&quot;prefetching in terms of batches instead of blocks. If you &quot;</span>
                <span class="s2">&quot;would like to use the legacy `iter_batches` codepath, &quot;</span>
                <span class="s2">&quot;you can enable it by setting `use_legacy_iter_batches` &quot;</span>
                <span class="s2">&quot;to True in the DataContext.&quot;</span>
            <span class="p">)</span>

        <span class="n">time_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>

        <span class="n">block_iterator</span><span class="p">,</span> <span class="n">stats</span><span class="p">,</span> <span class="n">blocks_owned_by_consumer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to_block_iterator</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">use_legacy</span><span class="p">:</span>
            <span class="c1"># Legacy iter_batches does not use metadata.</span>
            <span class="k">def</span> <span class="nf">drop_metadata</span><span class="p">(</span><span class="n">block_iterator</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">block_ref</span><span class="p">,</span> <span class="n">metadata</span> <span class="ow">in</span> <span class="n">block_iterator</span><span class="p">:</span>
                    <span class="k">yield</span> <span class="n">block_ref</span>

            <span class="k">yield from</span> <span class="n">batch_block_refs</span><span class="p">(</span>
                <span class="n">drop_metadata</span><span class="p">(</span><span class="n">block_iterator</span><span class="p">),</span>
                <span class="n">stats</span><span class="o">=</span><span class="n">stats</span><span class="p">,</span>
                <span class="n">prefetch_blocks</span><span class="o">=</span><span class="n">prefetch_blocks</span><span class="p">,</span>
                <span class="n">clear_block_after_read</span><span class="o">=</span><span class="n">blocks_owned_by_consumer</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">batch_format</span><span class="o">=</span><span class="n">batch_format</span><span class="p">,</span>
                <span class="n">drop_last</span><span class="o">=</span><span class="n">drop_last</span><span class="p">,</span>
                <span class="n">collate_fn</span><span class="o">=</span><span class="n">_collate_fn</span><span class="p">,</span>
                <span class="n">shuffle_buffer_min_size</span><span class="o">=</span><span class="n">local_shuffle_buffer_size</span><span class="p">,</span>
                <span class="n">shuffle_seed</span><span class="o">=</span><span class="n">local_shuffle_seed</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">yield from</span> <span class="n">iter_batches</span><span class="p">(</span>
                <span class="n">block_iterator</span><span class="p">,</span>
                <span class="n">stats</span><span class="o">=</span><span class="n">stats</span><span class="p">,</span>
                <span class="n">clear_block_after_read</span><span class="o">=</span><span class="n">blocks_owned_by_consumer</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">batch_format</span><span class="o">=</span><span class="n">batch_format</span><span class="p">,</span>
                <span class="n">drop_last</span><span class="o">=</span><span class="n">drop_last</span><span class="p">,</span>
                <span class="n">collate_fn</span><span class="o">=</span><span class="n">_collate_fn</span><span class="p">,</span>
                <span class="n">shuffle_buffer_min_size</span><span class="o">=</span><span class="n">local_shuffle_buffer_size</span><span class="p">,</span>
                <span class="n">shuffle_seed</span><span class="o">=</span><span class="n">local_shuffle_seed</span><span class="p">,</span>
                <span class="n">prefetch_batches</span><span class="o">=</span><span class="n">prefetch_batches</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">stats</span><span class="p">:</span>
            <span class="n">stats</span><span class="o">.</span><span class="n">iter_total_s</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">time_start</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">iter_rows</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">prefetch_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Return a local row iterator over the dataset.</span>

<span class="sd">        If the dataset is a tabular dataset (Arrow/Pandas blocks), dicts</span>
<span class="sd">        are yielded for each row by the iterator. If the dataset is not tabular,</span>
<span class="sd">        the raw row is yielded.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import ray</span>
<span class="sd">            &gt;&gt;&gt; dataset = ray.data.range(10)</span>
<span class="sd">            &gt;&gt;&gt; next(iter(dataset.iterator().iter_rows()))</span>
<span class="sd">            {&#39;id&#39;: 0}</span>

<span class="sd">        Time complexity: O(1)</span>

<span class="sd">        Args:</span>
<span class="sd">            prefetch_blocks: The number of blocks to prefetch ahead of the</span>
<span class="sd">                current block during the scan.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An iterator over rows of the dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">iter_batch_args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;batch_format&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>

        <span class="n">context</span> <span class="o">=</span> <span class="n">DataContext</span><span class="o">.</span><span class="n">get_current</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">use_legacy_iter_batches</span><span class="p">:</span>
            <span class="n">iter_batch_args</span><span class="p">[</span><span class="s2">&quot;prefetch_blocks&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">prefetch_blocks</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Since batch_size is None, 1 block is exactly 1 batch.</span>
            <span class="n">iter_batch_args</span><span class="p">[</span><span class="s2">&quot;prefetch_batches&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">prefetch_blocks</span>

        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span><span class="o">**</span><span class="n">iter_batch_args</span><span class="p">):</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">BlockAccessor</span><span class="o">.</span><span class="n">for_block</span><span class="p">(</span><span class="n">BlockAccessor</span><span class="o">.</span><span class="n">batch_to_block</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">iter_rows</span><span class="p">(</span><span class="n">public_row_format</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">row</span>

<div class="viewcode-block" id="DataIterator.stats"><a class="viewcode-back" href="../../../data/api/doc/ray.data.DataIterator.stats.html#ray.data.DataIterator.stats">[docs]</a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">stats</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns a string containing execution timing information.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">schema</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Schema&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return the schema of the dataset iterated over.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

<div class="viewcode-block" id="DataIterator.iter_torch_batches"><a class="viewcode-back" href="../../../data/api/doc/ray.data.DataIterator.iter_torch_batches.html#ray.data.DataIterator.iter_torch_batches">[docs]</a>    <span class="k">def</span> <span class="nf">iter_torch_batches</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prefetch_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">dtypes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.dtype&quot;</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;torch.dtype&quot;</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Callable</span><span class="p">[[</span><span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]],</span> <span class="n">Any</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">local_shuffle_buffer_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">local_shuffle_seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Deprecated.</span>
        <span class="n">prefetch_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s2">&quot;TorchTensorBatchType&quot;</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Return a local batched iterator of Torch Tensors over the dataset.</span>

<span class="sd">        This iterator will yield single-tensor batches if the underlying dataset</span>
<span class="sd">        consists of a single column; otherwise, it will yield a dictionary of</span>
<span class="sd">        column-tensors. If looking for more flexibility in the tensor conversion (e.g.</span>
<span class="sd">        casting dtypes) or the batch format, try using `.iter_batches` directly.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import ray</span>
<span class="sd">            &gt;&gt;&gt; for row in ray.data.range(</span>
<span class="sd">            ...     1000000</span>
<span class="sd">            ... ).iterator().iter_rows(): # doctest: +SKIP</span>
<span class="sd">            ...     print(row) # doctest: +SKIP</span>

<span class="sd">        Time complexity: O(1)</span>

<span class="sd">        Args:</span>
<span class="sd">            prefetch_batches: The number of batches to fetch ahead of the current batch</span>
<span class="sd">                to fetch. If set to greater than 0, a separate threadpool will be used</span>
<span class="sd">                to fetch the objects to the local node, format the batches, and apply</span>
<span class="sd">                the collate_fn. Defaults to 1. You can revert back to the old</span>
<span class="sd">                prefetching behavior that uses `prefetch_blocks` by setting</span>
<span class="sd">                `use_legacy_iter_batches` to True in the DataContext.</span>
<span class="sd">            batch_size: The number of rows in each batch, or None to use entire blocks</span>
<span class="sd">                as batches (blocks may contain different number of rows).</span>
<span class="sd">                The final batch may include fewer than ``batch_size`` rows if</span>
<span class="sd">                ``drop_last`` is ``False``. Defaults to 256.</span>
<span class="sd">            dtypes: The Torch dtype(s) for the created tensor(s); if None, the dtype</span>
<span class="sd">                will be inferred from the tensor data.</span>
<span class="sd">            device: The device on which the tensor should be placed; if None, the Torch</span>
<span class="sd">                tensor will be constructed on the CPU.</span>
<span class="sd">            collate_fn: A function to convert a Numpy batch to a PyTorch tensor batch.</span>
<span class="sd">                Potential use cases include collating along a dimension other than the</span>
<span class="sd">                first, padding sequences of various lengths, or generally handling</span>
<span class="sd">                batches of different length tensors. If not provided, the default</span>
<span class="sd">                collate function is used which simply converts the batch of numpy</span>
<span class="sd">                arrays to a batch of PyTorch tensors. This API is still experimental</span>
<span class="sd">                and is subject to change.</span>
<span class="sd">            drop_last: Whether to drop the last batch if it&#39;s incomplete.</span>
<span class="sd">            local_shuffle_buffer_size: If non-None, the data will be randomly shuffled</span>
<span class="sd">                using a local in-memory shuffle buffer, and this value will serve as the</span>
<span class="sd">                minimum number of rows that must be in the local in-memory shuffle</span>
<span class="sd">                buffer in order to yield a batch. When there are no more rows to add to</span>
<span class="sd">                the buffer, the remaining rows in the buffer will be drained. This</span>
<span class="sd">                buffer size must be greater than or equal to ``batch_size``, and</span>
<span class="sd">                therefore ``batch_size`` must also be specified when using local</span>
<span class="sd">                shuffling.</span>
<span class="sd">            local_shuffle_seed: The seed to use for the local random shuffle.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An iterator over Torch Tensor batches.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="kn">from</span> <span class="nn">ray.air._internal.torch_utils</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">convert_ndarray_batch_to_torch_tensor_batch</span><span class="p">,</span>
            <span class="n">get_device</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">collate_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">dtypes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;collate_fn cannot be used with dtypes and device. It is expected that&quot;</span>
                <span class="s2">&quot;the provided `collate_fn` will move the output Torch tensors to the&quot;</span>
                <span class="s2">&quot;appropriate dtype and device.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">collate_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

            <span class="c1"># Automatically move torch tensors to the appropriate device.</span>
            <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">default_device</span> <span class="o">=</span> <span class="n">get_device</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">default_device</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
                    <span class="n">device</span> <span class="o">=</span> <span class="n">default_device</span>

            <span class="k">def</span> <span class="nf">collate_fn</span><span class="p">(</span><span class="n">batch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]):</span>
                <span class="k">return</span> <span class="n">convert_ndarray_batch_to_torch_tensor_batch</span><span class="p">(</span>
                    <span class="n">batch</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">=</span><span class="n">dtypes</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
                <span class="p">)</span>

        <span class="k">yield from</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span>
            <span class="n">prefetch_batches</span><span class="o">=</span><span class="n">prefetch_batches</span><span class="p">,</span>
            <span class="n">prefetch_blocks</span><span class="o">=</span><span class="n">prefetch_blocks</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">batch_format</span><span class="o">=</span><span class="s2">&quot;numpy&quot;</span><span class="p">,</span>
            <span class="n">drop_last</span><span class="o">=</span><span class="n">drop_last</span><span class="p">,</span>
            <span class="n">local_shuffle_buffer_size</span><span class="o">=</span><span class="n">local_shuffle_buffer_size</span><span class="p">,</span>
            <span class="n">local_shuffle_seed</span><span class="o">=</span><span class="n">local_shuffle_seed</span><span class="p">,</span>
            <span class="n">_collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">iter_tf_batches</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prefetch_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">dtypes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="s2">&quot;tf.dtypes.DType&quot;</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;tf.dtypes.DType&quot;</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">local_shuffle_buffer_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">local_shuffle_seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Deprecated.</span>
        <span class="n">prefetch_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s2">&quot;TensorFlowTensorBatchType&quot;</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Return a local batched iterator of TensorFlow Tensors over the dataset.</span>

<span class="sd">        This iterator will yield single-tensor batches of the underlying dataset</span>
<span class="sd">        consists of a single column; otherwise, it will yield a dictionary of</span>
<span class="sd">        column-tensors.</span>

<span class="sd">        .. tip::</span>
<span class="sd">            If you don&#39;t need the additional flexibility provided by this method,</span>
<span class="sd">            consider using :meth:`~ray.data.Dataset.to_tf` instead. It&#39;s easier</span>
<span class="sd">            to use.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import ray</span>
<span class="sd">            &gt;&gt;&gt; for batch in ray.data.range( # doctest: +SKIP</span>
<span class="sd">            ...     12,</span>
<span class="sd">            ... ).iter_tf_batches(batch_size=4):</span>
<span class="sd">            ...     print(batch.shape) # doctest: +SKIP</span>
<span class="sd">            (4, 1)</span>
<span class="sd">            (4, 1)</span>
<span class="sd">            (4, 1)</span>

<span class="sd">        Time complexity: O(1)</span>

<span class="sd">        Args:</span>
<span class="sd">            prefetch_batches: The number of batches to fetch ahead of the current batch</span>
<span class="sd">                to fetch. If set to greater than 0, a separate threadpool will be used</span>
<span class="sd">                to fetch the objects to the local node, format the batches, and apply</span>
<span class="sd">                the collate_fn. Defaults to 1. You can revert back to the old</span>
<span class="sd">                prefetching behavior that uses `prefetch_blocks` by setting</span>
<span class="sd">                `use_legacy_iter_batches` to True in the DataContext.</span>
<span class="sd">            batch_size: The number of rows in each batch, or None to use entire blocks</span>
<span class="sd">                as batches (blocks may contain different number of rows).</span>
<span class="sd">                The final batch may include fewer than ``batch_size`` rows if</span>
<span class="sd">                ``drop_last`` is ``False``. Defaults to 256.</span>
<span class="sd">            dtypes: The TensorFlow dtype(s) for the created tensor(s); if None, the</span>
<span class="sd">                dtype will be inferred from the tensor data.</span>
<span class="sd">            drop_last: Whether to drop the last batch if it&#39;s incomplete.</span>
<span class="sd">            local_shuffle_buffer_size: If non-None, the data will be randomly shuffled</span>
<span class="sd">                using a local in-memory shuffle buffer, and this value will serve as the</span>
<span class="sd">                minimum number of rows that must be in the local in-memory shuffle</span>
<span class="sd">                buffer in order to yield a batch. When there are no more rows to add to</span>
<span class="sd">                the buffer, the remaining rows in the buffer will be drained. This</span>
<span class="sd">                buffer size must be greater than or equal to ``batch_size``, and</span>
<span class="sd">                therefore ``batch_size`` must also be specified when using local</span>
<span class="sd">                shuffling.</span>
<span class="sd">            local_shuffle_seed: The seed to use for the local random shuffle.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An iterator over TensorFlow Tensor batches.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">ray.air._internal.tensorflow_utils</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">convert_ndarray_batch_to_tf_tensor_batch</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span>
            <span class="n">prefetch_batches</span><span class="o">=</span><span class="n">prefetch_batches</span><span class="p">,</span>
            <span class="n">prefetch_blocks</span><span class="o">=</span><span class="n">prefetch_blocks</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">batch_format</span><span class="o">=</span><span class="s2">&quot;numpy&quot;</span><span class="p">,</span>
            <span class="n">drop_last</span><span class="o">=</span><span class="n">drop_last</span><span class="p">,</span>
            <span class="n">local_shuffle_buffer_size</span><span class="o">=</span><span class="n">local_shuffle_buffer_size</span><span class="p">,</span>
            <span class="n">local_shuffle_seed</span><span class="o">=</span><span class="n">local_shuffle_seed</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="k">yield</span> <span class="n">convert_ndarray_batch_to_tf_tensor_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">=</span><span class="n">dtypes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_torch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">label_column</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">feature_columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">label_column_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;torch.dtype&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">feature_column_dtypes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.dtype&quot;</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;torch.dtype&quot;</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;torch.dtype&quot;</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">prefetch_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">local_shuffle_buffer_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">local_shuffle_seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">unsqueeze_label_tensor</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">unsqueeze_feature_tensors</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="c1"># Deprecated.</span>
        <span class="n">prefetch_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;torch.utils.data.IterableDataset&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return a Torch IterableDataset over this dataset.</span>

<span class="sd">        This is only supported for datasets convertible to Arrow records.</span>

<span class="sd">        It is recommended to use the returned ``IterableDataset`` directly</span>
<span class="sd">        instead of passing it into a torch ``DataLoader``.</span>

<span class="sd">        Each element in IterableDataset will be a tuple consisting of 2</span>
<span class="sd">        elements. The first item contains the feature tensor(s), and the</span>
<span class="sd">        second item is the label tensor. Those can take on different</span>
<span class="sd">        forms, depending on the specified arguments.</span>

<span class="sd">        For the features tensor (N is the ``batch_size`` and n, m, k</span>
<span class="sd">        are the number of features per tensor):</span>

<span class="sd">        * If ``feature_columns`` is a ``List[str]``, the features will be</span>
<span class="sd">          a tensor of shape (N, n), with columns corresponding to</span>
<span class="sd">          ``feature_columns``</span>

<span class="sd">        * If ``feature_columns`` is a ``List[List[str]]``, the features will be</span>
<span class="sd">          a list of tensors of shape [(N, m),...,(N, k)], with columns of each</span>
<span class="sd">          tensor corresponding to the elements of ``feature_columns``</span>

<span class="sd">        * If ``feature_columns`` is a ``Dict[str, List[str]]``, the features</span>
<span class="sd">          will be a dict of key-tensor pairs of shape</span>
<span class="sd">          {key1: (N, m),..., keyN: (N, k)}, with columns of each</span>
<span class="sd">          tensor corresponding to the value of ``feature_columns`` under the</span>
<span class="sd">          key.</span>

<span class="sd">        If ``unsqueeze_label_tensor=True`` (default), the label tensor will be</span>
<span class="sd">        of shape (N, 1). Otherwise, it will be of shape (N,).</span>
<span class="sd">        If ``label_column`` is specified as ``None``, then no column from the</span>
<span class="sd">        ``Dataset`` will be treated as the label, and the output label tensor</span>
<span class="sd">        will be ``None``.</span>

<span class="sd">        Note that you probably want to call ``.split()`` on this dataset if</span>
<span class="sd">        there are to be multiple Torch workers consuming the data.</span>

<span class="sd">        Time complexity: O(1)</span>

<span class="sd">        Args:</span>
<span class="sd">            label_column: The name of the column used as the</span>
<span class="sd">                label (second element of the output list). Can be None for</span>
<span class="sd">                prediction, in which case the second element of returned</span>
<span class="sd">                tuple will also be None.</span>
<span class="sd">            feature_columns: The names of the columns</span>
<span class="sd">                to use as the features. Can be a list of lists or</span>
<span class="sd">                a dict of string-list pairs for multi-tensor output.</span>
<span class="sd">                If None, then use all columns except the label column as</span>
<span class="sd">                the features.</span>
<span class="sd">            label_column_dtype: The torch dtype to</span>
<span class="sd">                use for the label column. If None, then automatically infer</span>
<span class="sd">                the dtype.</span>
<span class="sd">            feature_column_dtypes: The dtypes to use for the feature</span>
<span class="sd">                tensors. This should match the format of ``feature_columns``,</span>
<span class="sd">                or be a single dtype, in which case it will be applied to</span>
<span class="sd">                all tensors. If None, then automatically infer the dtype.</span>
<span class="sd">            batch_size: How many samples per batch to yield at a time.</span>
<span class="sd">                Defaults to 1.</span>
<span class="sd">            prefetch_batches: The number of batches to fetch ahead of the current batch</span>
<span class="sd">                to fetch. If set to greater than 0, a separate threadpool will be used</span>
<span class="sd">                to fetch the objects to the local node, format the batches, and apply</span>
<span class="sd">                the collate_fn. Defaults to 1. You can revert back to the old</span>
<span class="sd">                prefetching behavior that uses `prefetch_blocks` by setting</span>
<span class="sd">                `use_legacy_iter_batches` to True in the DataContext.</span>
<span class="sd">            drop_last: Set to True to drop the last incomplete batch,</span>
<span class="sd">                if the dataset size is not divisible by the batch size. If</span>
<span class="sd">                False and the size of dataset is not divisible by the batch</span>
<span class="sd">                size, then the last batch will be smaller. Defaults to False.</span>
<span class="sd">            local_shuffle_buffer_size: If non-None, the data will be randomly shuffled</span>
<span class="sd">                using a local in-memory shuffle buffer, and this value will serve as the</span>
<span class="sd">                minimum number of rows that must be in the local in-memory shuffle</span>
<span class="sd">                buffer in order to yield a batch. When there are no more rows to add to</span>
<span class="sd">                the buffer, the remaining rows in the buffer will be drained. This</span>
<span class="sd">                buffer size must be greater than or equal to ``batch_size``, and</span>
<span class="sd">                therefore ``batch_size`` must also be specified when using local</span>
<span class="sd">                shuffling.</span>
<span class="sd">            local_shuffle_seed: The seed to use for the local random shuffle.</span>
<span class="sd">            unsqueeze_label_tensor: If set to True, the label tensor</span>
<span class="sd">                will be unsqueezed (reshaped to (N, 1)). Otherwise, it will</span>
<span class="sd">                be left as is, that is (N, ). In general, regression loss</span>
<span class="sd">                functions expect an unsqueezed tensor, while classification</span>
<span class="sd">                loss functions expect a squeezed one. Defaults to True.</span>
<span class="sd">            unsqueeze_feature_tensors: If set to True, the features tensors</span>
<span class="sd">                will be unsqueezed (reshaped to (N, 1)) before being concatenated into</span>
<span class="sd">                the final features tensor. Otherwise, they will be left as is, that is</span>
<span class="sd">                (N, ). Defaults to True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A torch IterableDataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span> <span class="nn">torch</span>

        <span class="kn">from</span> <span class="nn">ray.air._internal.torch_utils</span> <span class="kn">import</span> <span class="n">convert_pandas_to_torch_tensor</span>
        <span class="kn">from</span> <span class="nn">ray.data._internal.torch_iterable_dataset</span> <span class="kn">import</span> <span class="n">TorchIterableDataset</span>

        <span class="c1"># If an empty collection is passed in, treat it the same as None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">feature_columns</span><span class="p">:</span>
            <span class="n">feature_columns</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">feature_column_dtypes</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">feature_column_dtypes</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">feature_columns</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">feature_column_dtypes</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                        <span class="s2">&quot;If `feature_columns` is a dict, &quot;</span>
                        <span class="s2">&quot;`feature_column_dtypes` must be None, `torch.dtype`,&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot; or dict, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">feature_column_dtypes</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="nb">set</span><span class="p">(</span><span class="n">feature_columns</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">set</span><span class="p">(</span><span class="n">feature_column_dtypes</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;`feature_columns` and `feature_column_dtypes` &quot;</span>
                        <span class="s2">&quot;must have the same keys.&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="ow">not</span> <span class="n">subcolumns</span> <span class="k">for</span> <span class="n">subcolumns</span> <span class="ow">in</span> <span class="n">feature_columns</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;column list may not be empty&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">feature_columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">feature_column_dtypes</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                        <span class="s2">&quot;If `feature_columns` is a list of lists, &quot;</span>
                        <span class="s2">&quot;`feature_column_dtypes` must be None, `torch.dtype`,&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot; or a sequence, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">feature_column_dtypes</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">feature_columns</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">feature_column_dtypes</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;`feature_columns` and `feature_column_dtypes` &quot;</span>
                        <span class="s2">&quot;must have the same length.&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="ow">not</span> <span class="n">subcolumns</span> <span class="k">for</span> <span class="n">subcolumns</span> <span class="ow">in</span> <span class="n">feature_columns</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;column list may not be empty&quot;</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">make_generator</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">batch_format</span><span class="o">=</span><span class="s2">&quot;pandas&quot;</span><span class="p">,</span>
                <span class="n">prefetch_blocks</span><span class="o">=</span><span class="n">prefetch_blocks</span><span class="p">,</span>
                <span class="n">prefetch_batches</span><span class="o">=</span><span class="n">prefetch_batches</span><span class="p">,</span>
                <span class="n">drop_last</span><span class="o">=</span><span class="n">drop_last</span><span class="p">,</span>
                <span class="n">local_shuffle_buffer_size</span><span class="o">=</span><span class="n">local_shuffle_buffer_size</span><span class="p">,</span>
                <span class="n">local_shuffle_seed</span><span class="o">=</span><span class="n">local_shuffle_seed</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="n">label_column</span><span class="p">:</span>
                    <span class="n">label_tensor</span> <span class="o">=</span> <span class="n">convert_pandas_to_torch_tensor</span><span class="p">(</span>
                        <span class="n">batch</span><span class="p">,</span>
                        <span class="p">[</span><span class="n">label_column</span><span class="p">],</span>
                        <span class="n">label_column_dtype</span><span class="p">,</span>
                        <span class="n">unsqueeze</span><span class="o">=</span><span class="n">unsqueeze_label_tensor</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">batch</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">label_column</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">label_tensor</span> <span class="o">=</span> <span class="kc">None</span>

                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">feature_columns</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="n">features_tensor</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="n">key</span><span class="p">:</span> <span class="n">convert_pandas_to_torch_tensor</span><span class="p">(</span>
                            <span class="n">batch</span><span class="p">,</span>
                            <span class="n">feature_columns</span><span class="p">[</span><span class="n">key</span><span class="p">],</span>
                            <span class="n">feature_column_dtypes</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
                            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">feature_column_dtypes</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
                            <span class="k">else</span> <span class="n">feature_column_dtypes</span><span class="p">,</span>
                            <span class="n">unsqueeze</span><span class="o">=</span><span class="n">unsqueeze_feature_tensors</span><span class="p">,</span>
                        <span class="p">)</span>
                        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">feature_columns</span>
                    <span class="p">}</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">features_tensor</span> <span class="o">=</span> <span class="n">convert_pandas_to_torch_tensor</span><span class="p">(</span>
                        <span class="n">batch</span><span class="p">,</span>
                        <span class="n">columns</span><span class="o">=</span><span class="n">feature_columns</span><span class="p">,</span>
                        <span class="n">column_dtypes</span><span class="o">=</span><span class="n">feature_column_dtypes</span><span class="p">,</span>
                        <span class="n">unsqueeze</span><span class="o">=</span><span class="n">unsqueeze_feature_tensors</span><span class="p">,</span>
                    <span class="p">)</span>

                <span class="k">yield</span> <span class="p">(</span><span class="n">features_tensor</span><span class="p">,</span> <span class="n">label_tensor</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">TorchIterableDataset</span><span class="p">(</span><span class="n">make_generator</span><span class="p">)</span>

<div class="viewcode-block" id="DataIterator.to_tf"><a class="viewcode-back" href="../../../data/api/doc/ray.data.DataIterator.to_tf.html#ray.data.DataIterator.to_tf">[docs]</a>    <span class="k">def</span> <span class="nf">to_tf</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">feature_columns</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
        <span class="n">label_columns</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prefetch_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">local_shuffle_buffer_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">local_shuffle_seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Deprecated.</span>
        <span class="n">prefetch_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;tf.data.Dataset&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return a TF Dataset over this dataset.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            If your dataset contains ragged tensors, this method errors. To prevent</span>
<span class="sd">            errors, :ref:`resize your tensors &lt;transforming_tensors&gt;`.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import ray</span>
<span class="sd">            &gt;&gt;&gt; ds = ray.data.read_csv(</span>
<span class="sd">            ...     &quot;s3://anonymous@air-example-data/iris.csv&quot;</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; it = ds.iterator(); it</span>
<span class="sd">            DataIterator(Dataset(</span>
<span class="sd">               num_blocks=1,</span>
<span class="sd">               num_rows=150,</span>
<span class="sd">               schema={</span>
<span class="sd">                  sepal length (cm): double,</span>
<span class="sd">                  sepal width (cm): double,</span>
<span class="sd">                  petal length (cm): double,</span>
<span class="sd">                  petal width (cm): double,</span>
<span class="sd">                  target: int64</span>
<span class="sd">               }</span>
<span class="sd">            ))</span>

<span class="sd">            If your model accepts a single tensor as input, specify a single feature column.</span>

<span class="sd">            &gt;&gt;&gt; it.to_tf(feature_columns=&quot;sepal length (cm)&quot;, label_columns=&quot;target&quot;)  # doctest: +SKIP</span>
<span class="sd">            &lt;_OptionsDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.float64, name=&#39;sepal length (cm)&#39;), TensorSpec(shape=(None,), dtype=tf.int64, name=&#39;target&#39;))&gt;</span>

<span class="sd">            If your model accepts a dictionary as input, specify a list of feature columns.</span>

<span class="sd">            &gt;&gt;&gt; it.to_tf([&quot;sepal length (cm)&quot;, &quot;sepal width (cm)&quot;], &quot;target&quot;)  # doctest: +SKIP</span>
<span class="sd">            &lt;_OptionsDataset element_spec=({&#39;sepal length (cm)&#39;: TensorSpec(shape=(None,), dtype=tf.float64, name=&#39;sepal length (cm)&#39;), &#39;sepal width (cm)&#39;: TensorSpec(shape=(None,), dtype=tf.float64, name=&#39;sepal width (cm)&#39;)}, TensorSpec(shape=(None,), dtype=tf.int64, name=&#39;target&#39;))&gt;</span>

<span class="sd">            If your dataset contains multiple features but your model accepts a single</span>
<span class="sd">            tensor as input, combine features with</span>
<span class="sd">            :class:`~ray.data.preprocessors.Concatenator`.</span>

<span class="sd">            &gt;&gt;&gt; from ray.data.preprocessors import Concatenator</span>
<span class="sd">            &gt;&gt;&gt; preprocessor = Concatenator(output_column_name=&quot;features&quot;, exclude=&quot;target&quot;)</span>
<span class="sd">            &gt;&gt;&gt; it = preprocessor.transform(ds).iterator()</span>
<span class="sd">            &gt;&gt;&gt; it</span>
<span class="sd">            DataIterator(Concatenator</span>
<span class="sd">            +- Dataset(</span>
<span class="sd">                  num_blocks=1,</span>
<span class="sd">                  num_rows=150,</span>
<span class="sd">                  schema={</span>
<span class="sd">                     sepal length (cm): double,</span>
<span class="sd">                     sepal width (cm): double,</span>
<span class="sd">                     petal length (cm): double,</span>
<span class="sd">                     petal width (cm): double,</span>
<span class="sd">                     target: int64</span>
<span class="sd">                  }</span>
<span class="sd">               ))</span>
<span class="sd">            &gt;&gt;&gt; it.to_tf(&quot;features&quot;, &quot;target&quot;)  # doctest: +SKIP</span>
<span class="sd">            &lt;_OptionsDataset element_spec=(TensorSpec(shape=(None, 4), dtype=tf.float64, name=&#39;features&#39;), TensorSpec(shape=(None,), dtype=tf.int64, name=&#39;target&#39;))&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            feature_columns: Columns that correspond to model inputs. If this is a</span>
<span class="sd">                string, the input data is a tensor. If this is a list, the input data</span>
<span class="sd">                is a ``dict`` that maps column names to their tensor representation.</span>
<span class="sd">            label_column: Columns that correspond to model targets. If this is a</span>
<span class="sd">                string, the target data is a tensor. If this is a list, the target data</span>
<span class="sd">                is a ``dict`` that maps column names to their tensor representation.</span>
<span class="sd">            prefetch_batches: The number of batches to fetch ahead of the current batch</span>
<span class="sd">                to fetch. If set to greater than 0, a separate threadpool will be used</span>
<span class="sd">                to fetch the objects to the local node, format the batches, and apply</span>
<span class="sd">                the collate_fn. Defaults to 1. You can revert back to the old</span>
<span class="sd">                prefetching behavior that uses `prefetch_blocks` by setting</span>
<span class="sd">                `use_legacy_iter_batches` to True in the DataContext.</span>
<span class="sd">            batch_size: Record batch size. Defaults to 1.</span>
<span class="sd">            drop_last: Set to True to drop the last incomplete batch,</span>
<span class="sd">                if the dataset size is not divisible by the batch size. If</span>
<span class="sd">                False and the size of dataset is not divisible by the batch</span>
<span class="sd">                size, then the last batch will be smaller. Defaults to False.</span>
<span class="sd">            local_shuffle_buffer_size: If non-None, the data will be randomly shuffled</span>
<span class="sd">                using a local in-memory shuffle buffer, and this value will serve as the</span>
<span class="sd">                minimum number of rows that must be in the local in-memory shuffle</span>
<span class="sd">                buffer in order to yield a batch. When there are no more rows to add to</span>
<span class="sd">                the buffer, the remaining rows in the buffer will be drained. This</span>
<span class="sd">                buffer size must be greater than or equal to ``batch_size``, and</span>
<span class="sd">                therefore ``batch_size`` must also be specified when using local</span>
<span class="sd">                shuffling.</span>
<span class="sd">            local_shuffle_seed: The seed to use for the local random shuffle.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A ``tf.data.Dataset`` that yields inputs and targets.</span>
<span class="sd">        &quot;&quot;&quot;</span>  <span class="c1"># noqa: E501</span>

        <span class="kn">from</span> <span class="nn">ray.air._internal.tensorflow_utils</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">convert_ndarray_to_tf_tensor</span><span class="p">,</span>
            <span class="n">get_type_spec</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;tensorflow must be installed!&quot;</span><span class="p">)</span>

        <span class="n">schema</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">()</span>
        <span class="n">valid_columns</span> <span class="o">=</span> <span class="n">schema</span><span class="o">.</span><span class="n">names</span>

        <span class="k">def</span> <span class="nf">validate_column</span><span class="p">(</span><span class="n">column</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">column</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_columns</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;You specified &#39;</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s2">&#39; in `feature_columns` or &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;`label_columns`, but there&#39;s no column named &#39;</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s2">&#39; in the &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;dataset. Valid column names are: </span><span class="si">{</span><span class="n">valid_columns</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>

        <span class="k">def</span> <span class="nf">validate_columns</span><span class="p">(</span><span class="n">columns</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
                    <span class="n">validate_column</span><span class="p">(</span><span class="n">column</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">validate_column</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span>

        <span class="n">validate_columns</span><span class="p">(</span><span class="n">feature_columns</span><span class="p">)</span>
        <span class="n">validate_columns</span><span class="p">(</span><span class="n">label_columns</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">convert_batch_to_tensors</span><span class="p">(</span>
            <span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
            <span class="o">*</span><span class="p">,</span>
            <span class="n">columns</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
            <span class="n">type_spec</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">TypeSpec</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">TypeSpec</span><span class="p">]],</span>
        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">convert_ndarray_to_tf_tensor</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="n">columns</span><span class="p">],</span> <span class="n">type_spec</span><span class="o">=</span><span class="n">type_spec</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">{</span>
                <span class="n">column</span><span class="p">:</span> <span class="n">convert_ndarray_to_tf_tensor</span><span class="p">(</span>
                    <span class="n">batch</span><span class="p">[</span><span class="n">column</span><span class="p">],</span> <span class="n">type_spec</span><span class="o">=</span><span class="n">type_spec</span><span class="p">[</span><span class="n">column</span><span class="p">]</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">columns</span>
            <span class="p">}</span>

        <span class="k">def</span> <span class="nf">generator</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span>
                <span class="n">prefetch_batches</span><span class="o">=</span><span class="n">prefetch_batches</span><span class="p">,</span>
                <span class="n">prefetch_blocks</span><span class="o">=</span><span class="n">prefetch_blocks</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">drop_last</span><span class="o">=</span><span class="n">drop_last</span><span class="p">,</span>
                <span class="n">local_shuffle_buffer_size</span><span class="o">=</span><span class="n">local_shuffle_buffer_size</span><span class="p">,</span>
                <span class="n">local_shuffle_seed</span><span class="o">=</span><span class="n">local_shuffle_seed</span><span class="p">,</span>
                <span class="n">batch_format</span><span class="o">=</span><span class="s2">&quot;numpy&quot;</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
                <span class="n">features</span> <span class="o">=</span> <span class="n">convert_batch_to_tensors</span><span class="p">(</span>
                    <span class="n">batch</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">feature_columns</span><span class="p">,</span> <span class="n">type_spec</span><span class="o">=</span><span class="n">feature_type_spec</span>
                <span class="p">)</span>
                <span class="n">labels</span> <span class="o">=</span> <span class="n">convert_batch_to_tensors</span><span class="p">(</span>
                    <span class="n">batch</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">label_columns</span><span class="p">,</span> <span class="n">type_spec</span><span class="o">=</span><span class="n">label_type_spec</span>
                <span class="p">)</span>
                <span class="k">yield</span> <span class="n">features</span><span class="p">,</span> <span class="n">labels</span>

        <span class="n">feature_type_spec</span> <span class="o">=</span> <span class="n">get_type_spec</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">feature_columns</span><span class="p">)</span>
        <span class="n">label_type_spec</span> <span class="o">=</span> <span class="n">get_type_spec</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">label_columns</span><span class="p">)</span>
        <span class="n">output_signature</span> <span class="o">=</span> <span class="p">(</span><span class="n">feature_type_spec</span><span class="p">,</span> <span class="n">label_type_spec</span><span class="p">)</span>

        <span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_generator</span><span class="p">(</span>
            <span class="n">generator</span><span class="p">,</span> <span class="n">output_signature</span><span class="o">=</span><span class="n">output_signature</span>
        <span class="p">)</span>

        <span class="n">options</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Options</span><span class="p">()</span>
        <span class="n">options</span><span class="o">.</span><span class="n">experimental_distribute</span><span class="o">.</span><span class="n">auto_shard_policy</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AutoShardPolicy</span><span class="o">.</span><span class="n">OFF</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">dataset</span><span class="o">.</span><span class="n">with_options</span><span class="p">(</span><span class="n">options</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">iter_epochs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">DeprecationWarning</span><span class="p">(</span>
            <span class="s2">&quot;If you are using AIR, note that session.get_dataset_shard() &quot;</span>
            <span class="s2">&quot;returns a ray.data.DataIterator instead of a &quot;</span>
            <span class="s2">&quot;DatasetPipeline as of Ray 2.3. &quot;</span>
            <span class="s2">&quot;To iterate over one epoch of data, use iter_batches(), &quot;</span>
            <span class="s2">&quot;iter_torch_batches(), or to_tf().&quot;</span>
        <span class="p">)</span></div>


<span class="c1"># Backwards compatibility alias.</span>
<span class="n">DatasetIterator</span> <span class="o">=</span> <span class="n">DataIterator</span>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>