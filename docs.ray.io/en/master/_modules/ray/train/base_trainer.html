
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ray.train.base_trainer &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/js/versionwarning.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../../_static/js/docsearch.js"></script>
    <script src="../../../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../../_static/js/custom.js"></script>
    <script defer="defer" src="../../../_static/js/top-navigation.js"></script>
    <script src="../../../_static/js/tags.js"></script>
    <script src="../../../_static/tabs.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script async="async" src="../../../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/_modules/ray/train/base_trainer.html" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "_modules/ray/train/base_trainer", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2F_modules/ray/train/base_trainer.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for ray.train.base_trainer</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">abc</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">import</span> <span class="nn">ray.cloudpickle</span> <span class="k">as</span> <span class="nn">pickle</span>
<span class="kn">from</span> <span class="nn">ray.air._internal.config</span> <span class="kn">import</span> <span class="n">ensure_only_allowed_dataclass_keys_updated</span>
<span class="kn">from</span> <span class="nn">ray.air._internal.remote_storage</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">download_from_uri</span><span class="p">,</span>
    <span class="n">is_non_local_path_uri</span><span class="p">,</span>
    <span class="n">list_at_uri</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.air._internal</span> <span class="kn">import</span> <span class="n">usage</span> <span class="k">as</span> <span class="n">air_usage</span>
<span class="kn">from</span> <span class="nn">ray.air._internal.usage</span> <span class="kn">import</span> <span class="n">AirEntrypoint</span>
<span class="kn">from</span> <span class="nn">ray.air.checkpoint</span> <span class="kn">import</span> <span class="n">Checkpoint</span>
<span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span>
<span class="kn">from</span> <span class="nn">ray.air.config</span> <span class="kn">import</span> <span class="n">RunConfig</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.air.result</span> <span class="kn">import</span> <span class="n">Result</span>
<span class="kn">from</span> <span class="nn">ray.train.constants</span> <span class="kn">import</span> <span class="n">TRAIN_DATASET_KEY</span>
<span class="kn">from</span> <span class="nn">ray.util</span> <span class="kn">import</span> <span class="n">PublicAPI</span>
<span class="kn">from</span> <span class="nn">ray.util.annotations</span> <span class="kn">import</span> <span class="n">DeveloperAPI</span>
<span class="kn">from</span> <span class="nn">ray._private.dict</span> <span class="kn">import</span> <span class="n">merge_dicts</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">ray.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
    <span class="kn">from</span> <span class="nn">ray.data.preprocessor</span> <span class="kn">import</span> <span class="n">Preprocessor</span>

    <span class="kn">from</span> <span class="nn">ray.tune</span> <span class="kn">import</span> <span class="n">Trainable</span>

<span class="n">_TRAINER_PKL</span> <span class="o">=</span> <span class="s2">&quot;trainer.pkl&quot;</span>

<span class="c1"># A type representing either a ray.data.Dataset or a function that returns a</span>
<span class="c1"># ray.data.Dataset and accepts no arguments.</span>
<span class="n">GenDataset</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;Dataset&quot;</span><span class="p">,</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="s2">&quot;Dataset&quot;</span><span class="p">]]</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="nd">@PublicAPI</span><span class="p">(</span><span class="n">stability</span><span class="o">=</span><span class="s2">&quot;beta&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TrainingFailedError</span><span class="p">(</span><span class="ne">RuntimeError</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An error indicating that training has failed.&quot;&quot;&quot;</span>

    <span class="n">_RESTORE_MSG</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;The Ray Train run failed. Please inspect the previous error messages for a &quot;</span>
        <span class="s2">&quot;cause. After fixing the issue (assuming that the error is not caused by &quot;</span>
        <span class="s2">&quot;your own application logic, but rather an error such as OOM), you can restart &quot;</span>
        <span class="s2">&quot;the run from scratch or continue this run.</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;To continue this run, you can use: &quot;</span>
        <span class="s1">&#39;`trainer = </span><span class="si">{trainer_cls_name}</span><span class="s1">.restore(&quot;</span><span class="si">{path}</span><span class="s1">&quot;)`.&#39;</span>
    <span class="p">)</span>

    <span class="n">_FAILURE_CONFIG_MSG</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;To start a new run that will retry on training failures, set &quot;</span>
        <span class="s2">&quot;`air.RunConfig(failure_config=air.FailureConfig(max_failures))` &quot;</span>
        <span class="s2">&quot;in the Trainer&#39;s `run_config` with `max_failures &gt; 0`, or `max_failures = -1` &quot;</span>
        <span class="s2">&quot;for unlimited retries.&quot;</span>
    <span class="p">)</span>


<div class="viewcode-block" id="BaseTrainer"><a class="viewcode-back" href="../../../train/api/doc/ray.train.trainer.BaseTrainer.html#ray.train.trainer.BaseTrainer">[docs]</a><span class="nd">@DeveloperAPI</span>
<span class="k">class</span> <span class="nc">BaseTrainer</span><span class="p">(</span><span class="n">abc</span><span class="o">.</span><span class="n">ABC</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Defines interface for distributed training on Ray.</span>

<span class="sd">    Note: The base ``BaseTrainer`` class cannot be instantiated directly. Only</span>
<span class="sd">    one of its subclasses can be used.</span>

<span class="sd">    Note to AIR developers: If a new AIR trainer is added, please update</span>
<span class="sd">    `air/_internal/usage.py`.</span>

<span class="sd">    **How does a trainer work?**</span>

<span class="sd">    - First, initialize the Trainer. The initialization runs locally,</span>
<span class="sd">      so heavyweight setup should not be done in ``__init__``.</span>
<span class="sd">    - Then, when you call ``trainer.fit()``, the Trainer is serialized</span>
<span class="sd">      and copied to a remote Ray actor. The following methods are then</span>
<span class="sd">      called in sequence on the remote actor.</span>
<span class="sd">    - ``trainer.setup()``: Any heavyweight Trainer setup should be</span>
<span class="sd">      specified here.</span>
<span class="sd">    - ``trainer.preprocess_datasets()``: The datasets passed to the Trainer will be</span>
<span class="sd">      setup here.</span>
<span class="sd">    - ``trainer.train_loop()``: Executes the main training logic.</span>
<span class="sd">    - Calling ``trainer.fit()`` will return a ``ray.result.Result``</span>
<span class="sd">      object where you can access metrics from your training run, as well</span>
<span class="sd">      as any checkpoints that may have been saved.</span>

<span class="sd">    **How do I create a new Trainer?**</span>

<span class="sd">    Subclass ``ray.train.trainer.BaseTrainer``, and override the ``training_loop``</span>
<span class="sd">    method, and optionally ``setup``.</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        import torch</span>

<span class="sd">        from ray.train.trainer import BaseTrainer</span>
<span class="sd">        from ray import tune</span>
<span class="sd">        from ray.air import session</span>


<span class="sd">        class MyPytorchTrainer(BaseTrainer):</span>
<span class="sd">            def setup(self):</span>
<span class="sd">                self.model = torch.nn.Linear(1, 1)</span>
<span class="sd">                self.optimizer = torch.optim.SGD(</span>
<span class="sd">                    self.model.parameters(), lr=0.1)</span>

<span class="sd">            def training_loop(self):</span>
<span class="sd">                # You can access any Trainer attributes directly in this method.</span>
<span class="sd">                # self.datasets[&quot;train&quot;] has already been</span>
<span class="sd">                dataset = self.datasets[&quot;train&quot;]</span>

<span class="sd">                torch_ds = dataset.iter_torch_batches(dtypes=torch.float)</span>
<span class="sd">                loss_fn = torch.nn.MSELoss()</span>

<span class="sd">                for epoch_idx in range(10):</span>
<span class="sd">                    loss = 0</span>
<span class="sd">                    num_batches = 0</span>
<span class="sd">                    for batch in torch_ds:</span>
<span class="sd">                        X, y = torch.unsqueeze(batch[&quot;x&quot;], 1), batch[&quot;y&quot;]</span>
<span class="sd">                        # Compute prediction error</span>
<span class="sd">                        pred = self.model(X)</span>
<span class="sd">                        batch_loss = loss_fn(pred, y)</span>

<span class="sd">                        # Backpropagation</span>
<span class="sd">                        self.optimizer.zero_grad()</span>
<span class="sd">                        batch_loss.backward()</span>
<span class="sd">                        self.optimizer.step()</span>

<span class="sd">                        loss += batch_loss.item()</span>
<span class="sd">                        num_batches += 1</span>
<span class="sd">                    loss /= num_batches</span>

<span class="sd">                    # Use Tune functions to report intermediate</span>
<span class="sd">                    # results.</span>
<span class="sd">                    session.report({&quot;loss&quot;: loss, &quot;epoch&quot;: epoch_idx})</span>

<span class="sd">    **How do I use an existing Trainer or one of my custom Trainers?**</span>

<span class="sd">    Initialize the Trainer, and call Trainer.fit()</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        import ray</span>
<span class="sd">        train_dataset = ray.data.from_items(</span>
<span class="sd">            [{&quot;x&quot;: i, &quot;y&quot;: i} for i in range(3)])</span>
<span class="sd">        my_trainer = MyPytorchTrainer(datasets={&quot;train&quot;: train_dataset})</span>
<span class="sd">        result = my_trainer.fit()</span>


<span class="sd">    Args:</span>
<span class="sd">        scaling_config: Configuration for how to scale training.</span>
<span class="sd">        run_config: Configuration for the execution of the training run.</span>
<span class="sd">        datasets: Any Datasets to use for training. Use the key &quot;train&quot;</span>
<span class="sd">            to denote which dataset is the training dataset.</span>
<span class="sd">        resume_from_checkpoint: A checkpoint to resume training from.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_scaling_config_allowed_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;trainer_resources&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_max_cpu_fraction_per_node&quot;</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="n">_handles_checkpoint_freq</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">_handles_checkpoint_at_end</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># fields to propagate to Tuner param_space.</span>
    <span class="c1"># See `BaseTrainer._extract_fields_for_tuner_param_space` for more details.</span>
    <span class="n">_fields_for_tuner_param_space</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">scaling_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ScalingConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">RunConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">datasets</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">GenDataset</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">resume_from_checkpoint</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Checkpoint</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Deprecated.</span>
        <span class="n">preprocessor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;Preprocessor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling_config</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">scaling_config</span> <span class="k">if</span> <span class="n">scaling_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">ScalingConfig</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_config</span> <span class="o">=</span> <span class="n">run_config</span> <span class="k">if</span> <span class="n">run_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">RunConfig</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">datasets</span> <span class="o">=</span> <span class="n">datasets</span> <span class="k">if</span> <span class="n">datasets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">preprocessor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resume_from_checkpoint</span> <span class="o">=</span> <span class="n">resume_from_checkpoint</span>

        <span class="c1"># This path should only be set through restore</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_restore_path</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_attributes</span><span class="p">()</span>

        <span class="n">air_usage</span><span class="o">.</span><span class="n">tag_air_trainer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">preprocessor</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;The `preprocessor` arg to Trainer is deprecated. Apply &quot;</span>
                <span class="s2">&quot;preprocessor transformations ahead of time by calling &quot;</span>
                <span class="s2">&quot;`preprocessor.transform(ds)`. Support for the preprocessor &quot;</span>
                <span class="s2">&quot;arg will be dropped in a future release.&quot;</span>
            <span class="p">)</span>

<div class="viewcode-block" id="BaseTrainer.restore"><a class="viewcode-back" href="../../../train/api/doc/ray.train.trainer.BaseTrainer.restore.html#ray.train.trainer.BaseTrainer.restore">[docs]</a>    <span class="nd">@PublicAPI</span><span class="p">(</span><span class="n">stability</span><span class="o">=</span><span class="s2">&quot;alpha&quot;</span><span class="p">)</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="s2">&quot;BaseTrainer&quot;</span><span class="p">],</span>
        <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">datasets</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">GenDataset</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">preprocessor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;Preprocessor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">scaling_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ScalingConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BaseTrainer&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Restores a Train experiment from a previously interrupted/failed run.</span>

<span class="sd">        Restore should be used for experiment-level fault tolerance in the event</span>
<span class="sd">        that the head node crashes (e.g., OOM or some other runtime error) or the</span>
<span class="sd">        entire cluster goes down (e.g., network error affecting all nodes).</span>

<span class="sd">        The following example can be paired with implementing job retry using</span>
<span class="sd">        :ref:`Ray Jobs &lt;jobs-overview&gt;` to produce a Train experiment that will</span>
<span class="sd">        attempt to resume on both experiment-level and trial-level failures:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            import os</span>
<span class="sd">            from ray import air</span>
<span class="sd">            from ray.data.preprocessors import BatchMapper</span>
<span class="sd">            from ray.train.trainer import BaseTrainer</span>

<span class="sd">            experiment_name = &quot;unique_experiment_name&quot;</span>
<span class="sd">            local_dir = &quot;~/ray_results&quot;</span>
<span class="sd">            experiment_dir = os.path.join(local_dir, experiment_name)</span>

<span class="sd">            # Define some dummy inputs for demonstration purposes</span>
<span class="sd">            datasets = {&quot;train&quot;: ray.data.from_items([{&quot;a&quot;: i} for i in range(10)])}</span>
<span class="sd">            preprocessor = BatchMapper(lambda x: x, batch_format=&quot;numpy&quot;)</span>

<span class="sd">            class CustomTrainer(BaseTrainer):</span>
<span class="sd">                def training_loop(self):</span>
<span class="sd">                    pass</span>

<span class="sd">            if CustomTrainer.can_restore(experiment_dir):</span>
<span class="sd">                trainer = CustomTrainer.restore(</span>
<span class="sd">                    experiment_dir,</span>
<span class="sd">                    datasets=datasets,</span>
<span class="sd">                )</span>
<span class="sd">            else:</span>
<span class="sd">                trainer = CustomTrainer(</span>
<span class="sd">                    datasets=datasets,</span>
<span class="sd">                    preprocessor=preprocessor,</span>
<span class="sd">                    run_config=air.RunConfig(</span>
<span class="sd">                        name=experiment_name,</span>
<span class="sd">                        local_dir=local_dir,</span>
<span class="sd">                        # Tip: You can also enable retries on failure for</span>
<span class="sd">                        # worker-level fault tolerance</span>
<span class="sd">                        failure_config=air.FailureConfig(max_failures=3),</span>
<span class="sd">                    ),</span>
<span class="sd">                )</span>

<span class="sd">            result = trainer.fit()</span>

<span class="sd">        Args:</span>
<span class="sd">            path: The path to the experiment directory of the training run to restore.</span>
<span class="sd">                This can be a local path or a remote URI if the experiment was</span>
<span class="sd">                uploaded to the cloud.</span>
<span class="sd">            datasets: Re-specified datasets used in the original training run.</span>
<span class="sd">                This must include all the datasets that were passed in the</span>
<span class="sd">                original trainer constructor.</span>
<span class="sd">            preprocessor: Optionally re-specified preprocessor that was passed in</span>
<span class="sd">                the original trainer constructor. This should be used to re-supply</span>
<span class="sd">                the preprocessor if it is not restorable in a new Ray cluster.</span>
<span class="sd">                This preprocessor will be fit at the start before resuming training.</span>
<span class="sd">                If no preprocessor is passed in restore, then the old preprocessor</span>
<span class="sd">                will be loaded from the latest checkpoint and will not be re-fit.</span>
<span class="sd">            scaling_config: Optionally re-specified scaling config. This can be</span>
<span class="sd">                modified to be different from the original spec.</span>
<span class="sd">            **kwargs: Other optionally re-specified arguments, passed in by subclasses.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If all datasets were not re-supplied on restore.</span>

<span class="sd">        Returns:</span>
<span class="sd">            BaseTrainer: A restored instance of the class that is calling this method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">cls</span><span class="o">.</span><span class="n">can_restore</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Invalid restore path: </span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">. Make sure that this path exists and &quot;</span>
                <span class="s2">&quot;is the experiment directory that results from a call to &quot;</span>
                <span class="s2">&quot;`trainer.fit()`.&quot;</span>
            <span class="p">)</span>
        <span class="n">trainer_state_path</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_maybe_sync_down_trainer_state</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">trainer_state_path</span><span class="o">.</span><span class="n">exists</span><span class="p">()</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">trainer_state_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">trainer_cls</span><span class="p">,</span> <span class="n">param_dict</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">trainer_cls</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">cls</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Invalid trainer type. You are attempting to restore a trainer of type&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">trainer_cls</span><span class="si">}</span><span class="s2"> with `</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.restore`, &quot;</span>
                <span class="s2">&quot;which will most likely fail. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Use `</span><span class="si">{</span><span class="n">trainer_cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.restore` instead.&quot;</span>
            <span class="p">)</span>

        <span class="n">original_datasets</span> <span class="o">=</span> <span class="n">param_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;datasets&quot;</span><span class="p">,</span> <span class="p">{})</span>
        <span class="k">if</span> <span class="n">original_datasets</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">datasets</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The following datasets need to be provided again on restore: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">original_datasets</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Use </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.restore(..., datasets=datasets) &quot;</span>
                <span class="s2">&quot;with the datasets that were provided to the original trainer.&quot;</span>
            <span class="p">)</span>
        <span class="n">datasets</span> <span class="o">=</span> <span class="n">datasets</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="nb">set</span><span class="p">(</span><span class="n">original_datasets</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">set</span><span class="p">(</span><span class="n">datasets</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The provided datasets don&#39;t match the original dataset keys.</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;  Expected datasets for the keys: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">original_datasets</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;  Actual datasets provided: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">datasets</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="n">param_dict</span><span class="p">[</span><span class="s2">&quot;datasets&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">datasets</span>

        <span class="c1"># If no preprocessor is re-specified, then it will be set to None</span>
        <span class="c1"># here and loaded from the latest checkpoint</span>
        <span class="n">param_dict</span><span class="p">[</span><span class="s2">&quot;preprocessor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocessor</span>

        <span class="k">if</span> <span class="n">scaling_config</span><span class="p">:</span>
            <span class="n">param_dict</span><span class="p">[</span><span class="s2">&quot;scaling_config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaling_config</span>

        <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Overwrite the old value if something is passed into restore</span>
            <span class="k">if</span> <span class="n">val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">param_dict</span><span class="p">[</span><span class="n">param_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">trainer</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="o">**</span><span class="n">param_dict</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Trainer restoration failed (see above for the stack trace). &quot;</span>
                <span class="s2">&quot;Make sure that you use the right trainer class to restore: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.restore`</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">_restore_path</span> <span class="o">=</span> <span class="n">path</span>
        <span class="k">return</span> <span class="n">trainer</span></div>

<div class="viewcode-block" id="BaseTrainer.can_restore"><a class="viewcode-back" href="../../../train/api/doc/ray.train.trainer.BaseTrainer.can_restore.html#ray.train.trainer.BaseTrainer.can_restore">[docs]</a>    <span class="nd">@PublicAPI</span><span class="p">(</span><span class="n">stability</span><span class="o">=</span><span class="s2">&quot;alpha&quot;</span><span class="p">)</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">can_restore</span><span class="p">(</span><span class="bp">cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="s2">&quot;BaseTrainer&quot;</span><span class="p">],</span> <span class="n">path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Checks whether a given directory contains a restorable Train experiment.</span>

<span class="sd">        Args:</span>
<span class="sd">            path: The path to the experiment directory of the Train experiment.</span>
<span class="sd">                This can be either a local directory (e.g., ~/ray_results/exp_name)</span>
<span class="sd">                or a remote URI (e.g., s3://bucket/exp_name).</span>

<span class="sd">        Returns:</span>
<span class="sd">            bool: Whether this path exists and contains the trainer state to resume from</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">_TRAINER_PKL</span> <span class="ow">in</span> <span class="n">list_at_uri</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">path</span><span class="p">))</span></div>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># A dictionary that maps parameters to their default values.</span>
        <span class="n">default_values</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;scaling_config&quot;</span><span class="p">:</span> <span class="n">ScalingConfig</span><span class="p">(),</span>
            <span class="s2">&quot;run_config&quot;</span><span class="p">:</span> <span class="n">RunConfig</span><span class="p">(),</span>
            <span class="s2">&quot;datasets&quot;</span><span class="p">:</span> <span class="p">{},</span>
            <span class="s2">&quot;preprocessor&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;resume_from_checkpoint&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">non_default_arguments</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">parameter</span><span class="p">,</span> <span class="n">default_value</span> <span class="ow">in</span> <span class="n">default_values</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameter</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">value</span> <span class="o">!=</span> <span class="n">default_value</span><span class="p">:</span>
                <span class="n">non_default_arguments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">parameter</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">non_default_arguments</span><span class="p">:</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;&lt;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">non_default_arguments</span><span class="p">)</span><span class="si">}</span><span class="s2">&gt;&quot;</span>

        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;&lt;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&gt;&quot;</span>

    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Store the init args as attributes so this can be merged with Tune hparams.</span>
        <span class="n">trainer</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">BaseTrainer</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="fm">__init__</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="c1"># Remove self.</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">arg_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">args</span><span class="p">))</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">_param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">arg_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">trainer</span>

    <span class="k">def</span> <span class="nf">_validate_attributes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Called on __init()__ to validate trainer attributes.&quot;&quot;&quot;</span>
        <span class="c1"># Run config</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">run_config</span><span class="p">,</span> <span class="n">RunConfig</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`run_config` should be an instance of `ray.air.RunConfig`, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;found </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">run_config</span><span class="p">)</span><span class="si">}</span><span class="s2"> with value `</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">run_config</span><span class="si">}</span><span class="s2">`.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Scaling config</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scaling_config</span><span class="p">,</span> <span class="n">ScalingConfig</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`scaling_config` should be an instance of `ScalingConfig`, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;found </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scaling_config</span><span class="p">)</span><span class="si">}</span><span class="s2"> with value `</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">scaling_config</span><span class="si">}</span><span class="s2">`.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Datasets</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">datasets</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`datasets` should be a dict mapping from a string to &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`ray.data.Dataset` objects, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;found </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">datasets</span><span class="p">)</span><span class="si">}</span><span class="s2"> with value `</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">datasets</span><span class="si">}</span><span class="s2">`.&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DatasetPipeline</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;The Dataset under &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39; key is a &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;`ray.data.DatasetPipeline`. Only `ray.data.Dataset` are &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;allowed to be passed in.  Pipelined/streaming ingest can be &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;configured via the `dataset_config` arg. See &quot;</span>
                        <span class="s2">&quot;https://docs.ray.io/en/latest/ray-air/check-ingest.html#enabling-streaming-ingest&quot;</span>  <span class="c1"># noqa: E501</span>
                        <span class="s2">&quot;for an example.&quot;</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span>
                    <span class="n">dataset</span>
                <span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;The Dataset under &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39; key is not a &quot;</span>
                        <span class="s2">&quot;`ray.data.Dataset`. &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;Received </span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="s2"> instead.&quot;</span>
                    <span class="p">)</span>

        <span class="c1"># Preprocessor</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span><span class="p">,</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Preprocessor</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`preprocessor` should be an instance of `ray.data.Preprocessor`, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;found </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span><span class="p">)</span><span class="si">}</span><span class="s2"> with value `</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span><span class="si">}</span><span class="s2">`.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">resume_from_checkpoint</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">resume_from_checkpoint</span><span class="p">,</span> <span class="n">ray</span><span class="o">.</span><span class="n">air</span><span class="o">.</span><span class="n">Checkpoint</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`resume_from_checkpoint` should be an instance of &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`ray.air.Checkpoint`, found </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">resume_from_checkpoint</span><span class="p">)</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;with value `</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">resume_from_checkpoint</span><span class="si">}</span><span class="s2">`.&quot;</span>
            <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_validate_scaling_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">scaling_config</span><span class="p">:</span> <span class="n">ScalingConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ScalingConfig</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return scaling config dataclass after validating updated keys.&quot;&quot;&quot;</span>
        <span class="n">ensure_only_allowed_dataclass_keys_updated</span><span class="p">(</span>
            <span class="n">dataclass</span><span class="o">=</span><span class="n">scaling_config</span><span class="p">,</span>
            <span class="n">allowed_keys</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">_scaling_config_allowed_keys</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">scaling_config</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_maybe_sync_down_trainer_state</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">restore_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sync down trainer state from remote storage.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: Local directory containing the trainer state</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_non_local_path_uri</span><span class="p">(</span><span class="n">restore_path</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">restore_path</span><span class="p">))</span> <span class="o">/</span> <span class="n">_TRAINER_PKL</span>

        <span class="n">tempdir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(</span><span class="s2">&quot;tmp_experiment_dir&quot;</span><span class="p">))</span>

        <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">restore_path</span><span class="p">)</span>
        <span class="n">download_from_uri</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="n">_TRAINER_PKL</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">tempdir</span> <span class="o">/</span> <span class="n">_TRAINER_PKL</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">tempdir</span> <span class="o">/</span> <span class="n">_TRAINER_PKL</span>

<div class="viewcode-block" id="BaseTrainer.setup"><a class="viewcode-back" href="../../../train/api/doc/ray.train.trainer.BaseTrainer.setup.html#ray.train.trainer.BaseTrainer.setup">[docs]</a>    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Called during fit() to perform initial setup on the Trainer.</span>

<span class="sd">        .. note:: This method is run on a remote process.</span>

<span class="sd">        This method will not be called on the driver, so any expensive setup</span>
<span class="sd">        operations should be placed here and not in ``__init__``.</span>

<span class="sd">        This method is called prior to ``preprocess_datasets`` and</span>
<span class="sd">        ``training_loop``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>

<div class="viewcode-block" id="BaseTrainer.preprocess_datasets"><a class="viewcode-back" href="../../../train/api/doc/ray.train.trainer.BaseTrainer.preprocess_datasets.html#ray.train.trainer.BaseTrainer.preprocess_datasets">[docs]</a>    <span class="k">def</span> <span class="nf">preprocess_datasets</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Called during fit() to preprocess dataset attributes with preprocessor.</span>

<span class="sd">        .. note:: This method is run on a remote process.</span>

<span class="sd">        This method is called prior to entering the training_loop.</span>

<span class="sd">        If the ``Trainer`` has both a datasets dict and</span>
<span class="sd">        a preprocessor, the datasets dict contains a training dataset (denoted by</span>
<span class="sd">        the &quot;train&quot; key), and the preprocessor has not yet</span>
<span class="sd">        been fit, then it will be fit on the train dataset.</span>

<span class="sd">        Then, all Trainer&#39;s datasets will be transformed by the preprocessor.</span>

<span class="sd">        The transformed datasets will be set back in the ``self.datasets`` attribute</span>
<span class="sd">        of the Trainer to be used when overriding ``training_loop``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Evaluate all datasets.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">datasets</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">d</span><span class="p">()</span> <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">else</span> <span class="n">d</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span><span class="p">:</span>
            <span class="n">train_dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">TRAIN_DATASET_KEY</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">train_dataset</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span><span class="o">.</span><span class="n">fit_status</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span>
                <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Preprocessor</span><span class="o">.</span><span class="n">FitStatus</span><span class="o">.</span><span class="n">NOT_FITTED</span><span class="p">,</span>
                <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Preprocessor</span><span class="o">.</span><span class="n">FitStatus</span><span class="o">.</span><span class="n">PARTIALLY_FITTED</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>

            <span class="c1"># Execute dataset transformations serially for now.</span>
            <span class="c1"># Cannot execute them in remote tasks due to dataset ownership model:</span>
            <span class="c1"># if datasets are created on a remote node, then if that node fails,</span>
            <span class="c1"># we cannot recover the dataset.</span>
            <span class="n">new_datasets</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">new_datasets</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">datasets</span> <span class="o">=</span> <span class="n">new_datasets</span></div>

<div class="viewcode-block" id="BaseTrainer.training_loop"><a class="viewcode-back" href="../../../train/api/doc/ray.train.trainer.BaseTrainer.training_loop.html#ray.train.trainer.BaseTrainer.training_loop">[docs]</a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">training_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Loop called by fit() to run training and report results to Tune.</span>

<span class="sd">        .. note:: This method runs on a remote process.</span>

<span class="sd">        ``self.datasets`` have already been preprocessed by ``self.preprocessor``.</span>

<span class="sd">        You can use the :ref:`Tune Function API functions &lt;tune-function-docstring&gt;`</span>
<span class="sd">        (``session.report()`` and ``session.get_checkpoint()``) inside</span>
<span class="sd">        this training loop.</span>

<span class="sd">        Example:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            from ray.train.trainer import BaseTrainer</span>

<span class="sd">            class MyTrainer(BaseTrainer):</span>
<span class="sd">                def training_loop(self):</span>
<span class="sd">                    for epoch_idx in range(5):</span>
<span class="sd">                        ...</span>
<span class="sd">                        session.report({&quot;epoch&quot;: epoch_idx})</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="BaseTrainer.fit"><a class="viewcode-back" href="../../../train/api/doc/ray.train.trainer.BaseTrainer.fit.html#ray.train.trainer.BaseTrainer.fit">[docs]</a>    <span class="nd">@PublicAPI</span><span class="p">(</span><span class="n">stability</span><span class="o">=</span><span class="s2">&quot;beta&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Result</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Runs training.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A Result object containing the training result.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TrainingFailedError: If any failures during the execution of</span>
<span class="sd">            ``self.as_trainable()``, or during the Tune execution loop.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">ray.tune.tuner</span> <span class="kn">import</span> <span class="n">Tuner</span><span class="p">,</span> <span class="n">TunerInternal</span>
        <span class="kn">from</span> <span class="nn">ray.tune</span> <span class="kn">import</span> <span class="n">TuneError</span>

        <span class="n">trainable</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">as_trainable</span><span class="p">()</span>
        <span class="n">param_space</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_fields_for_tuner_param_space</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_restore_path</span><span class="p">:</span>
            <span class="n">tuner</span> <span class="o">=</span> <span class="n">Tuner</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_restore_path</span><span class="p">,</span>
                <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
                <span class="n">param_space</span><span class="o">=</span><span class="n">param_space</span><span class="p">,</span>
                <span class="n">resume_unfinished</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">resume_errored</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tuner</span> <span class="o">=</span> <span class="n">Tuner</span><span class="p">(</span>
                <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
                <span class="n">param_space</span><span class="o">=</span><span class="n">param_space</span><span class="p">,</span>
                <span class="n">run_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">run_config</span><span class="p">,</span>
                <span class="n">_entrypoint</span><span class="o">=</span><span class="n">AirEntrypoint</span><span class="o">.</span><span class="n">TRAINER</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">experiment_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span>
            <span class="n">TunerInternal</span><span class="o">.</span><span class="n">setup_create_experiment_checkpoint_dir</span><span class="p">(</span>
                <span class="n">trainable</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_config</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_save</span><span class="p">(</span><span class="n">experiment_path</span><span class="p">)</span>

        <span class="n">restore_msg</span> <span class="o">=</span> <span class="n">TrainingFailedError</span><span class="o">.</span><span class="n">_RESTORE_MSG</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">trainer_cls_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
            <span class="n">path</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">experiment_path</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">result_grid</span> <span class="o">=</span> <span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
        <span class="k">except</span> <span class="n">TuneError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="c1"># Catch any `TuneError`s raised by the `Tuner.fit` call.</span>
            <span class="c1"># Unwrap the `TuneError` if needed.</span>
            <span class="n">parent_error</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">__cause__</span> <span class="ow">or</span> <span class="n">e</span>

            <span class="c1"># Raise it to the user as a `TrainingFailedError` with a message to restore.</span>
            <span class="k">raise</span> <span class="n">TrainingFailedError</span><span class="p">(</span><span class="n">restore_msg</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">parent_error</span>
        <span class="c1"># Other exceptions get passed through directly (ex: on `fail_fast=&#39;raise&#39;`)</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">result_grid</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">result_grid</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">result</span><span class="o">.</span><span class="n">error</span><span class="p">:</span>
            <span class="c1"># Raise trainable errors to the user with a message to restore</span>
            <span class="c1"># or configure `FailureConfig` in a new run.</span>
            <span class="k">raise</span> <span class="n">TrainingFailedError</span><span class="p">(</span>
                <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">restore_msg</span><span class="p">,</span> <span class="n">TrainingFailedError</span><span class="o">.</span><span class="n">_FAILURE_CONFIG_MSG</span><span class="p">])</span>
            <span class="p">)</span> <span class="kn">from</span> <span class="nn">result.error</span>
        <span class="k">return</span> <span class="n">result</span></div>

    <span class="k">def</span> <span class="nf">_save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experiment_path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;Saves the current trainer&#39;s class along with the `param_dict` of</span>
<span class="sd">        parameters passed to this trainer&#39;s constructor.</span>

<span class="sd">        This is used to recreate the trainer on restore.</span>
<span class="sd">        Unless a parameter is re-specified during restoration (only a subset</span>
<span class="sd">        of parameters can be passed in again), that parameter will be loaded</span>
<span class="sd">        from the saved copy.</span>

<span class="sd">        Datasets should not be saved as part of the state. Instead, we save the</span>
<span class="sd">        keys and replace the dataset values with dummy functions that will</span>
<span class="sd">        raise an error if invoked. The error only serves as a guardrail for</span>
<span class="sd">        misuse (e.g., manually unpickling and constructing the Trainer again)</span>
<span class="sd">        and is not typically surfaced, since datasets must be re-specified</span>
<span class="sd">        upon restoration.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">datasets</span> <span class="o">=</span> <span class="n">param_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;datasets&quot;</span><span class="p">,</span> <span class="p">{})</span>

        <span class="k">def</span> <span class="nf">raise_fn</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span>

        <span class="k">if</span> <span class="n">datasets</span><span class="p">:</span>
            <span class="n">param_dict</span><span class="p">[</span><span class="s2">&quot;datasets&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">dataset_name</span><span class="p">:</span> <span class="n">raise_fn</span> <span class="k">for</span> <span class="n">dataset_name</span> <span class="ow">in</span> <span class="n">datasets</span>
            <span class="p">}</span>

        <span class="n">cls_and_param_dict</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>

        <span class="n">experiment_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">experiment_path</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">experiment_path</span> <span class="o">/</span> <span class="n">_TRAINER_PKL</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">cls_and_param_dict</span><span class="p">,</span> <span class="n">fp</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_extract_fields_for_tuner_param_space</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Extracts fields to be included in `Tuner.param_space`.</span>

<span class="sd">        This is needed to leverage the full logging/integration offerings from Tune.</span>
<span class="sd">        For example, `param_space` is logged automatically to wandb integration.</span>

<span class="sd">        Currently only done for `train_loop_config`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary that should be passed to Tuner.param_space.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fields_for_tuner_param_space</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">result</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_param_dict</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">_generate_trainable_cls</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Type</span><span class="p">[</span><span class="s2">&quot;Trainable&quot;</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Generate the base Trainable class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A Trainable class to use for training.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="kn">from</span> <span class="nn">ray.tune.execution.placement_groups</span> <span class="kn">import</span> <span class="n">PlacementGroupFactory</span>
        <span class="kn">from</span> <span class="nn">ray.tune.trainable</span> <span class="kn">import</span> <span class="n">wrap_function</span>

        <span class="n">trainer_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span>
        <span class="n">scaling_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling_config</span>
        <span class="n">restored</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_restore_path</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
            <span class="c1"># config already contains merged values.</span>
            <span class="c1"># Instantiate new Trainer in Trainable.</span>
            <span class="n">trainer</span> <span class="o">=</span> <span class="n">trainer_cls</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">)</span>

            <span class="c1"># Get the checkpoint from the Tune session, and use it to initialize</span>
            <span class="c1"># the restored trainer.</span>
            <span class="c1"># This handles both worker-level and cluster-level restoration</span>
            <span class="c1"># of the Train experiment.</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get_checkpoint</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">checkpoint</span><span class="p">:</span>
                <span class="n">trainer</span><span class="o">.</span><span class="n">resume_from_checkpoint</span> <span class="o">=</span> <span class="n">checkpoint</span>
                <span class="c1"># Always load the preprocessor from an available checkpoint</span>
                <span class="c1"># Unless we are restoring the experiment and have explicitly</span>
                <span class="c1"># passed in a new preprocessor</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">restored</span> <span class="ow">and</span> <span class="n">trainer</span><span class="o">.</span><span class="n">preprocessor</span><span class="p">):</span>
                    <span class="n">trainer</span><span class="o">.</span><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">get_preprocessor</span><span class="p">()</span>

            <span class="n">trainer</span><span class="o">.</span><span class="n">setup</span><span class="p">()</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">preprocess_datasets</span><span class="p">()</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">training_loop</span><span class="p">()</span>

        <span class="c1"># Change the name of the training function to match the name of the Trainer</span>
        <span class="c1"># class. This will mean the Tune trial name will match the name of Trainer on</span>
        <span class="c1"># stdout messages and the results directory.</span>
        <span class="n">train_func</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="n">trainer_cls</span><span class="o">.</span><span class="vm">__name__</span>

        <span class="n">trainable_cls</span> <span class="o">=</span> <span class="n">wrap_function</span><span class="p">(</span><span class="n">train_func</span><span class="p">,</span> <span class="n">warn</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">has_base_dataset</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">datasets</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">has_base_dataset</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">ray.data.context</span> <span class="kn">import</span> <span class="n">DataContext</span>

            <span class="n">dataset_context</span> <span class="o">=</span> <span class="n">DataContext</span><span class="o">.</span><span class="n">get_current</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dataset_context</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">class</span> <span class="nc">TrainTrainable</span><span class="p">(</span><span class="n">trainable_cls</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;Add default resources to the Trainable.&quot;&quot;&quot;</span>

            <span class="n">_handles_checkpoint_freq</span> <span class="o">=</span> <span class="n">trainer_cls</span><span class="o">.</span><span class="n">_handles_checkpoint_freq</span>
            <span class="n">_handles_checkpoint_at_end</span> <span class="o">=</span> <span class="n">trainer_cls</span><span class="o">.</span><span class="n">_handles_checkpoint_at_end</span>

            <span class="nd">@classmethod</span>
            <span class="k">def</span> <span class="nf">has_base_dataset</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
                <span class="sd">&quot;&quot;&quot;Whether a dataset is provided through the Trainer.&quot;&quot;&quot;</span>
                <span class="k">return</span> <span class="n">has_base_dataset</span>

            <span class="nd">@classmethod</span>
            <span class="k">def</span> <span class="nf">base_scaling_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ScalingConfig</span><span class="p">:</span>
                <span class="sd">&quot;&quot;&quot;Returns the unchanged scaling config provided through the Trainer.&quot;&quot;&quot;</span>
                <span class="k">return</span> <span class="n">scaling_config</span>

            <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                <span class="n">base_config</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="c1"># Create a new config by merging the dicts.</span>
                <span class="c1"># run_config is not a tunable hyperparameter so it does not need to be</span>
                <span class="c1"># merged.</span>
                <span class="n">run_config</span> <span class="o">=</span> <span class="n">base_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;run_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_merged_config</span> <span class="o">=</span> <span class="n">merge_dicts</span><span class="p">(</span><span class="n">base_config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_merged_config</span><span class="p">[</span><span class="s2">&quot;run_config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">run_config</span>
                <span class="n">merged_scaling_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merged_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;scaling_config&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">merged_scaling_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="n">merged_scaling_config</span> <span class="o">=</span> <span class="n">ScalingConfig</span><span class="p">(</span><span class="o">**</span><span class="n">merged_scaling_config</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_merged_config</span><span class="p">[</span>
                    <span class="s2">&quot;scaling_config&quot;</span>
                <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reconcile_scaling_config_with_trial_resources</span><span class="p">(</span>
                    <span class="n">merged_scaling_config</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_base_dataset</span><span class="p">():</span>
                    <span class="c1"># Set the DataContext on the Trainer actor to the DataContext</span>
                    <span class="c1"># specified on the driver.</span>
                    <span class="n">DataContext</span><span class="o">.</span><span class="n">_set_current</span><span class="p">(</span><span class="n">dataset_context</span><span class="p">)</span>
                <span class="nb">super</span><span class="p">(</span><span class="n">TrainTrainable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

            <span class="k">def</span> <span class="nf">_reconcile_scaling_config_with_trial_resources</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span> <span class="n">scaling_config</span><span class="p">:</span> <span class="n">ScalingConfig</span>
            <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ScalingConfig</span><span class="p">:</span>
                <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">                ResourceChangingScheduler workaround.</span>

<span class="sd">                Ensures that the scaling config matches trial resources.</span>

<span class="sd">                This should be replaced with RCS returning a ScalingConfig</span>
<span class="sd">                in the future.</span>
<span class="sd">                &quot;&quot;&quot;</span>

                <span class="n">trial_resources</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trial_resources</span>
                <span class="c1"># This will be false if the resources are default</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">trial_resources</span><span class="p">,</span> <span class="n">PlacementGroupFactory</span><span class="p">):</span>
                    <span class="k">return</span> <span class="n">scaling_config</span>

                <span class="k">if</span> <span class="n">scaling_config</span><span class="p">:</span>
                    <span class="n">scaling_config</span> <span class="o">=</span> <span class="n">trainer_cls</span><span class="o">.</span><span class="n">_validate_scaling_config</span><span class="p">(</span>
                        <span class="n">scaling_config</span>
                    <span class="p">)</span>
                <span class="n">scaling_config_from_trial_resources</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">ScalingConfig</span><span class="o">.</span><span class="n">from_placement_group_factory</span><span class="p">(</span><span class="n">trial_resources</span><span class="p">)</span>
                <span class="p">)</span>

                <span class="c1"># This check should always pass if ResourceChangingScheduler is not</span>
                <span class="c1"># used.</span>
                <span class="k">if</span> <span class="n">scaling_config_from_trial_resources</span> <span class="o">!=</span> <span class="n">scaling_config</span><span class="p">:</span>
                    <span class="n">scaling_config</span> <span class="o">=</span> <span class="n">trainer_cls</span><span class="o">.</span><span class="n">_validate_scaling_config</span><span class="p">(</span>
                        <span class="n">scaling_config_from_trial_resources</span>
                    <span class="p">)</span>
                <span class="k">return</span> <span class="n">scaling_config</span>

            <span class="k">def</span> <span class="nf">_trainable_func</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">reporter</span><span class="p">,</span> <span class="n">checkpoint_dir</span><span class="p">):</span>
                <span class="c1"># We ignore the config passed by Tune and instead use the merged</span>
                <span class="c1"># config which includes the initial Trainer args.</span>
                <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_trainable_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_merged_config</span><span class="p">,</span> <span class="n">reporter</span><span class="p">,</span> <span class="n">checkpoint_dir</span><span class="p">)</span>

            <span class="nd">@classmethod</span>
            <span class="k">def</span> <span class="nf">default_resource_request</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
                <span class="c1"># `config[&quot;scaling_config&quot;] is a dataclass when passed via the</span>
                <span class="c1"># `scaling_config` argument in `Trainer` and is a dict when passed</span>
                <span class="c1"># via the `scaling_config` key of `param_spec`.</span>

                <span class="c1"># Conversion logic must be duplicated in `TrainTrainable.__init__`</span>
                <span class="c1"># because this is a class method.</span>
                <span class="n">updated_scaling_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;scaling_config&quot;</span><span class="p">,</span> <span class="n">scaling_config</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">updated_scaling_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="n">updated_scaling_config</span> <span class="o">=</span> <span class="n">ScalingConfig</span><span class="p">(</span><span class="o">**</span><span class="n">updated_scaling_config</span><span class="p">)</span>
                <span class="n">validated_scaling_config</span> <span class="o">=</span> <span class="n">trainer_cls</span><span class="o">.</span><span class="n">_validate_scaling_config</span><span class="p">(</span>
                    <span class="n">updated_scaling_config</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">validated_scaling_config</span><span class="o">.</span><span class="n">as_placement_group_factory</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">TrainTrainable</span>

<div class="viewcode-block" id="BaseTrainer.as_trainable"><a class="viewcode-back" href="../../../train/api/doc/ray.train.trainer.BaseTrainer.as_trainable.html#ray.train.trainer.BaseTrainer.as_trainable">[docs]</a>    <span class="k">def</span> <span class="nf">as_trainable</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Type</span><span class="p">[</span><span class="s2">&quot;Trainable&quot;</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Convert self to a ``tune.Trainable`` class.&quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>

        <span class="n">base_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_dict</span>
        <span class="n">trainable_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_trainable_cls</span><span class="p">()</span>

        <span class="c1"># Wrap with `tune.with_parameters` to handle very large values in base_config</span>
        <span class="k">return</span> <span class="n">tune</span><span class="o">.</span><span class="n">with_parameters</span><span class="p">(</span><span class="n">trainable_cls</span><span class="p">,</span> <span class="o">**</span><span class="n">base_config</span><span class="p">)</span></div></div>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>