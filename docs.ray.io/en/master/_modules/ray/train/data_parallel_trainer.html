
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ray.train.data_parallel_trainer &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/js/versionwarning.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../../_static/js/docsearch.js"></script>
    <script src="../../../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../../_static/js/custom.js"></script>
    <script defer="defer" src="../../../_static/js/top-navigation.js"></script>
    <script src="../../../_static/js/tags.js"></script>
    <script src="../../../_static/tabs.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script async="async" src="../../../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/_modules/ray/train/data_parallel_trainer.html" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "_modules/ray/train/data_parallel_trainer", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2F_modules/ray/train/data_parallel_trainer.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for ray.train.data_parallel_trainer</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">from</span> <span class="nn">ray._private.thirdparty.tabulate.tabulate</span> <span class="kn">import</span> <span class="n">tabulate</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span>
<span class="kn">from</span> <span class="nn">ray.air.checkpoint</span> <span class="kn">import</span> <span class="n">Checkpoint</span>
<span class="kn">from</span> <span class="nn">ray.air._internal.checkpointing</span> <span class="kn">import</span> <span class="n">add_preprocessor_to_checkpoint</span>
<span class="kn">from</span> <span class="nn">ray.air.config</span> <span class="kn">import</span> <span class="n">DatasetConfig</span><span class="p">,</span> <span class="n">RunConfig</span><span class="p">,</span> <span class="n">ScalingConfig</span><span class="p">,</span> <span class="n">CheckpointConfig</span>
<span class="kn">from</span> <span class="nn">ray.air.constants</span> <span class="kn">import</span> <span class="n">MODEL_KEY</span><span class="p">,</span> <span class="n">PREPROCESSOR_KEY</span><span class="p">,</span> <span class="n">LAZY_CHECKPOINT_MARKER_FILE</span>
<span class="kn">from</span> <span class="nn">ray.air._internal.checkpoint_manager</span> <span class="kn">import</span> <span class="n">_TrackedCheckpoint</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">BackendConfig</span><span class="p">,</span> <span class="n">TrainingIterator</span>
<span class="kn">from</span> <span class="nn">ray.train._internal.backend_executor</span> <span class="kn">import</span> <span class="n">BackendExecutor</span><span class="p">,</span> <span class="n">TrialInfo</span>
<span class="kn">from</span> <span class="nn">ray.train._internal.checkpoint</span> <span class="kn">import</span> <span class="n">TuneCheckpointManager</span>
<span class="kn">from</span> <span class="nn">ray.train.data_config</span> <span class="kn">import</span> <span class="n">DataConfig</span><span class="p">,</span> <span class="n">_LegacyDataConfigWrapper</span>
<span class="kn">from</span> <span class="nn">ray.train._internal.utils</span> <span class="kn">import</span> <span class="n">construct_train_func</span>
<span class="kn">from</span> <span class="nn">ray.train.constants</span> <span class="kn">import</span> <span class="n">TRAIN_DATASET_KEY</span><span class="p">,</span> <span class="n">WILDCARD_KEY</span>
<span class="kn">from</span> <span class="nn">ray.train.trainer</span> <span class="kn">import</span> <span class="n">BaseTrainer</span><span class="p">,</span> <span class="n">GenDataset</span>
<span class="kn">from</span> <span class="nn">ray.util.annotations</span> <span class="kn">import</span> <span class="n">DeveloperAPI</span>
<span class="kn">from</span> <span class="nn">ray.widgets</span> <span class="kn">import</span> <span class="n">Template</span>
<span class="kn">from</span> <span class="nn">ray.widgets.util</span> <span class="kn">import</span> <span class="n">repr_with_fallback</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">ray.data.preprocessor</span> <span class="kn">import</span> <span class="n">Preprocessor</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="c1"># TODO(team-ml): Refactor checkpoint management along with Tune.</span>
<span class="k">class</span> <span class="nc">_DataParallelCheckpointManager</span><span class="p">(</span><span class="n">TuneCheckpointManager</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">preprocessor</span><span class="p">:</span> <span class="s2">&quot;Preprocessor&quot;</span><span class="p">,</span>
        <span class="n">run_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Path</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">checkpoint_strategy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CheckpointConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">preprocessor</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_DataParallelCheckpointManager</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">run_dir</span><span class="o">=</span><span class="n">run_dir</span><span class="p">,</span>
            <span class="n">checkpoint_strategy</span><span class="o">=</span><span class="n">checkpoint_strategy</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_process_persistent_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">:</span> <span class="n">_TrackedCheckpoint</span><span class="p">):</span>
        <span class="n">air_checkpoint</span><span class="p">:</span> <span class="n">Checkpoint</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">dir_or_data</span>
        <span class="n">checkpoint</span><span class="o">.</span><span class="n">dir_or_data</span> <span class="o">=</span> <span class="n">add_preprocessor_to_checkpoint</span><span class="p">(</span>
            <span class="n">air_checkpoint</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span>
        <span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_DataParallelCheckpointManager</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_process_persistent_checkpoint</span><span class="p">(</span>
            <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span>
        <span class="p">)</span>


<div class="viewcode-block" id="DataParallelTrainer"><a class="viewcode-back" href="../../../train/api/doc/ray.train.data_parallel_trainer.DataParallelTrainer.html#ray.train.data_parallel_trainer.DataParallelTrainer">[docs]</a><span class="nd">@DeveloperAPI</span>
<span class="k">class</span> <span class="nc">DataParallelTrainer</span><span class="p">(</span><span class="n">BaseTrainer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A Trainer for data parallel training.</span>

<span class="sd">    You should subclass this Trainer if your Trainer follows SPMD (single program,</span>
<span class="sd">    multiple data) programming paradigm - you want multiple processes to run the same</span>
<span class="sd">    function, but on different data.</span>

<span class="sd">    This Trainer runs the function ``train_loop_per_worker`` on multiple Ray</span>
<span class="sd">    Actors.</span>

<span class="sd">    The ``train_loop_per_worker`` function is expected to take in either 0 or 1</span>
<span class="sd">    arguments:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        def train_loop_per_worker():</span>
<span class="sd">            ...</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        def train_loop_per_worker(config: Dict):</span>
<span class="sd">            ...</span>

<span class="sd">    If ``train_loop_per_worker`` accepts an argument, then</span>
<span class="sd">    ``train_loop_config`` will be passed in as the argument. This is useful if you</span>
<span class="sd">    want to tune the values in ``train_loop_config`` as hyperparameters.</span>

<span class="sd">    If the ``datasets`` dict contains a training dataset (denoted by</span>
<span class="sd">    the &quot;train&quot; key), then it will be split into multiple dataset</span>
<span class="sd">    shards that can then be accessed by ``session.get_dataset_shard(&quot;train&quot;)`` inside</span>
<span class="sd">    ``train_loop_per_worker``. All the other datasets will not be split and</span>
<span class="sd">    ``session.get_dataset_shard(...)`` will return the the entire Dataset.</span>

<span class="sd">    Inside the ``train_loop_per_worker`` function, you can use any of the</span>
<span class="sd">    :ref:`Ray AIR session methods &lt;air-session-ref&gt;`.</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        def train_loop_per_worker():</span>
<span class="sd">            # Report intermediate results for callbacks or logging and</span>
<span class="sd">            # checkpoint data.</span>
<span class="sd">            session.report(...)</span>

<span class="sd">            # Returns dict of last saved checkpoint.</span>
<span class="sd">            session.get_checkpoint()</span>

<span class="sd">            # Returns the Dataset shard for the given key.</span>
<span class="sd">            session.get_dataset_shard(&quot;my_dataset&quot;)</span>

<span class="sd">            # Returns the total number of workers executing training.</span>
<span class="sd">            session.get_world_size()</span>

<span class="sd">            # Returns the rank of this worker.</span>
<span class="sd">            session.get_world_rank()</span>

<span class="sd">            # Returns the rank of the worker on the current node.</span>
<span class="sd">            session.get_local_rank()</span>

<span class="sd">    Any returns from the ``train_loop_per_worker`` will be discarded and not</span>
<span class="sd">    used or persisted anywhere.</span>

<span class="sd">    **How do I use DataParallelTrainer or any of its subclasses?**</span>

<span class="sd">    Example:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        import ray</span>
<span class="sd">        from ray.air import session</span>

<span class="sd">        def train_loop_for_worker():</span>
<span class="sd">            dataset_shard_for_this_worker = session.get_dataset_shard(&quot;train&quot;)</span>

<span class="sd">            assert len(dataset_shard_for_this_worker) == 1</span>

<span class="sd">        train_dataset = ray.data.from_items([1, 2, 3])</span>
<span class="sd">        assert len(train_dataset) == 3</span>
<span class="sd">        trainer = DataParallelTrainer(</span>
<span class="sd">            ray.air.config.ScalingConfig(num_workers=3),</span>
<span class="sd">            datasets={&quot;train&quot;: train_dataset},</span>
<span class="sd">        )</span>
<span class="sd">        result = trainer.fit()</span>

<span class="sd">    **How do I develop on top of DataParallelTrainer?**</span>

<span class="sd">    In many cases, using DataParallelTrainer directly is sufficient to execute</span>
<span class="sd">    functions on multiple actors.</span>

<span class="sd">    However, you may want to subclass ``DataParallelTrainer`` and create a custom</span>
<span class="sd">    Trainer for the following 2 use cases:</span>

<span class="sd">      - **Use Case 1:** You want to do data parallel training, but want to have</span>
<span class="sd">        a predefined ``training_loop_per_worker``.</span>

<span class="sd">      - **Use Case 2:** You want to implement a custom</span>
<span class="sd">        :py:class:`~ray.train.backend.Backend` that automatically handles</span>
<span class="sd">        additional setup or teardown logic on each actor, so that the users of this</span>
<span class="sd">        new trainer do not have to implement this logic. For example, a</span>
<span class="sd">        ``TensorflowTrainer`` can be built on top of ``DataParallelTrainer``</span>
<span class="sd">        that automatically handles setting the proper environment variables for</span>
<span class="sd">        distributed Tensorflow on each actor.</span>

<span class="sd">    For 1, you can set a predefined training loop in __init__</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        from ray.train.data_parallel_trainer import DataParallelTrainer</span>

<span class="sd">        class MyDataParallelTrainer(DataParallelTrainer):</span>
<span class="sd">            def __init__(self, *args, **kwargs):</span>
<span class="sd">                predefined_train_loop_per_worker = lambda: 1</span>
<span class="sd">                super().__init__(predefined_train_loop_per_worker, *args, **kwargs)</span>


<span class="sd">    For 2, you can implement the ``ray.train.Backend`` and ``ray.train.BackendConfig``</span>
<span class="sd">    interfaces.</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        from dataclasses import dataclass</span>
<span class="sd">        from ray.train.backend import Backend, BackendConfig</span>

<span class="sd">        class MyBackend(Backend):</span>
<span class="sd">            def on_start(self, worker_group, backend_config):</span>
<span class="sd">                def set_env_var(env_var_value):</span>
<span class="sd">                    import os</span>
<span class="sd">                    os.environ[&quot;MY_ENV_VAR&quot;] = env_var_value</span>

<span class="sd">                worker_group.execute(set_env_var, backend_config.env_var)</span>

<span class="sd">        @dataclass</span>
<span class="sd">        class MyBackendConfig(BackendConfig):</span>
<span class="sd">            env_var: str = &quot;default_value&quot;</span>

<span class="sd">            def backend_cls(self):</span>
<span class="sd">                return MyBackend</span>

<span class="sd">        class MyTrainer(DataParallelTrainer):</span>
<span class="sd">            def __init__(self, train_loop_per_worker, my_backend_config:</span>
<span class="sd">                MyBackendConfig, **kwargs):</span>

<span class="sd">                super().__init__(</span>
<span class="sd">                    train_loop_per_worker,</span>
<span class="sd">                    backend_config=my_backend_config, **kwargs)</span>

<span class="sd">    Args:</span>
<span class="sd">        train_loop_per_worker: The training function to execute.</span>
<span class="sd">            This can either take in no arguments or a ``config`` dict.</span>
<span class="sd">        train_loop_config: Configurations to pass into</span>
<span class="sd">            ``train_loop_per_worker`` if it accepts an argument.</span>
<span class="sd">        backend_config: Configuration for setting up a Backend (e.g. Torch,</span>
<span class="sd">            Tensorflow, Horovod) on each worker to enable distributed</span>
<span class="sd">            communication. If no Backend should be set up, then set this to None.</span>
<span class="sd">        scaling_config: Configuration for how to scale data parallel training.</span>
<span class="sd">        dataset_config: Configuration for dataset ingest. This is merged with the</span>
<span class="sd">            default dataset config for the given trainer (`cls._dataset_config`).</span>
<span class="sd">        run_config: Configuration for the execution of the training run.</span>
<span class="sd">        datasets: Any Datasets to use for training. Use</span>
<span class="sd">            the key &quot;train&quot; to denote which dataset is the training</span>
<span class="sd">            dataset. If a ``preprocessor`` is provided and has not already been fit,</span>
<span class="sd">            it will be fit on the training dataset. All datasets will be transformed</span>
<span class="sd">            by the ``preprocessor`` if one is provided.</span>
<span class="sd">        preprocessor: A ray.data.Preprocessor to preprocess the</span>
<span class="sd">            provided datasets.</span>
<span class="sd">        resume_from_checkpoint: A checkpoint to resume training from.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_checkpoint_manager_cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span>
        <span class="n">TuneCheckpointManager</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">_DataParallelCheckpointManager</span>

    <span class="c1"># Exposed here for testing purposes. Should never need</span>
    <span class="c1"># to be overriden.</span>
    <span class="n">_backend_executor_cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">BackendExecutor</span><span class="p">]</span> <span class="o">=</span> <span class="n">BackendExecutor</span>
    <span class="n">_training_iterator_cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">TrainingIterator</span><span class="p">]</span> <span class="o">=</span> <span class="n">TrainingIterator</span>

    <span class="n">_scaling_config_allowed_keys</span> <span class="o">=</span> <span class="n">BaseTrainer</span><span class="o">.</span><span class="n">_scaling_config_allowed_keys</span> <span class="o">+</span> <span class="p">[</span>
        <span class="s2">&quot;num_workers&quot;</span><span class="p">,</span>
        <span class="s2">&quot;resources_per_worker&quot;</span><span class="p">,</span>
        <span class="s2">&quot;use_gpu&quot;</span><span class="p">,</span>
        <span class="s2">&quot;placement_strategy&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># For backwards compatibility with the legacy dataset config API.</span>
    <span class="n">_dataset_config</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">_fields_for_tuner_param_space</span> <span class="o">=</span> <span class="n">BaseTrainer</span><span class="o">.</span><span class="n">_fields_for_tuner_param_space</span> <span class="o">+</span> <span class="p">[</span>
        <span class="s2">&quot;train_loop_config&quot;</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">train_loop_per_worker</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="kc">None</span><span class="p">],</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Dict</span><span class="p">],</span> <span class="kc">None</span><span class="p">]],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">train_loop_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">backend_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BackendConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">scaling_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ScalingConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dataset_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DataConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">RunConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">datasets</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">GenDataset</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">resume_from_checkpoint</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Checkpoint</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Deprecated.</span>
        <span class="n">preprocessor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;Preprocessor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_loop_per_worker</span> <span class="o">=</span> <span class="n">train_loop_per_worker</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_loop_config</span> <span class="o">=</span> <span class="n">train_loop_config</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_config</span> <span class="ow">or</span> <span class="n">preprocessor</span><span class="p">:</span>
            <span class="c1"># Warn about deprecated cases (will raise error in future).</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;The dict form of `dataset_config` is deprecated. Use the &quot;</span>
                    <span class="s2">&quot;DataConfig class instead. Support for this will be dropped &quot;</span>
                    <span class="s2">&quot;in a future release.&quot;</span>
                <span class="p">)</span>
            <span class="c1"># If using the new API, hard-disallow deprecated features.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset_config</span><span class="p">,</span> <span class="n">DataConfig</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_config</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;The DataConfig class is not compatible with the &quot;</span>
                        <span class="s2">&quot;Trainer._dataset_config field. Remove `_dataset_config` &quot;</span>
                        <span class="s2">&quot;from your trainer subclass to use DataConfig.&quot;</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="n">preprocessor</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;The DataConfig class is not compatible with the &quot;</span>
                        <span class="s2">&quot;Trainer preprocessor arg. Remove the `preprocessor` arg &quot;</span>
                        <span class="s2">&quot;to use DataConfig.&quot;</span>
                    <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">base_dataset_config</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">TRAIN_DATASET_KEY</span><span class="p">:</span> <span class="n">DatasetConfig</span><span class="p">(</span><span class="n">fit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                    <span class="n">WILDCARD_KEY</span><span class="p">:</span> <span class="n">DatasetConfig</span><span class="p">(</span><span class="n">split</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                <span class="p">}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">base_dataset_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_config</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data_config</span> <span class="o">=</span> <span class="n">_LegacyDataConfigWrapper</span><span class="p">(</span>
                <span class="n">base_dataset_config</span><span class="p">,</span> <span class="n">dataset_config</span><span class="p">,</span> <span class="n">datasets</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset_config</span><span class="p">,</span> <span class="n">DataConfig</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data_config</span> <span class="o">=</span> <span class="n">dataset_config</span>
        <span class="k">elif</span> <span class="n">dataset_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data_config</span> <span class="o">=</span> <span class="n">DataConfig</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`dataset_config` must be an instance of ray.train.DataConfig, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;was: </span><span class="si">{</span><span class="n">dataset_config</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">backend_config</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">backend_config</span> <span class="k">if</span> <span class="n">backend_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">BackendConfig</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backend_config</span> <span class="o">=</span> <span class="n">backend_config</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">DataParallelTrainer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">scaling_config</span><span class="o">=</span><span class="n">scaling_config</span><span class="p">,</span>
            <span class="n">run_config</span><span class="o">=</span><span class="n">run_config</span><span class="p">,</span>
            <span class="n">datasets</span><span class="o">=</span><span class="n">datasets</span><span class="p">,</span>
            <span class="n">preprocessor</span><span class="o">=</span><span class="n">preprocessor</span><span class="p">,</span>
            <span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="n">resume_from_checkpoint</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="DataParallelTrainer.restore"><a class="viewcode-back" href="../../../train/api/doc/ray.train.data_parallel_trainer.DataParallelTrainer.restore.html#ray.train.data_parallel_trainer.DataParallelTrainer.restore">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="s2">&quot;DataParallelTrainer&quot;</span><span class="p">],</span>
        <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">train_loop_per_worker</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="kc">None</span><span class="p">],</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Dict</span><span class="p">],</span> <span class="kc">None</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">train_loop_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">datasets</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">GenDataset</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">preprocessor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;Preprocessor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">scaling_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ScalingConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DataParallelTrainer&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Restores a DataParallelTrainer from a previously interrupted/failed run.</span>

<span class="sd">        Args:</span>
<span class="sd">            train_loop_per_worker: Optionally re-specified train loop function.</span>
<span class="sd">                This should be used to re-specify a function that is not</span>
<span class="sd">                restorable in a new Ray cluster (e.g., it holds onto outdated</span>
<span class="sd">                object references). This should be the same training loop</span>
<span class="sd">                that was passed to the original trainer constructor.</span>
<span class="sd">            train_loop_config: Optionally re-specified train config.</span>
<span class="sd">                This should similarly be used if the original `train_loop_config`</span>
<span class="sd">                contained outdated object references, and it should not be modified</span>
<span class="sd">                from what was originally passed in.</span>

<span class="sd">        See :meth:`BaseTrainer.restore() &lt;ray.train.trainer.BaseTrainer.restore&gt;`</span>
<span class="sd">        for descriptions of the other arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            DataParallelTrainer: A restored instance of the `DataParallelTrainer`</span>
<span class="sd">            subclass that is calling this method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">DataParallelTrainer</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span>
            <span class="n">path</span><span class="o">=</span><span class="n">path</span><span class="p">,</span>
            <span class="n">train_loop_per_worker</span><span class="o">=</span><span class="n">train_loop_per_worker</span><span class="p">,</span>
            <span class="n">train_loop_config</span><span class="o">=</span><span class="n">train_loop_config</span><span class="p">,</span>
            <span class="n">datasets</span><span class="o">=</span><span class="n">datasets</span><span class="p">,</span>
            <span class="n">preprocessor</span><span class="o">=</span><span class="n">preprocessor</span><span class="p">,</span>
            <span class="n">scaling_config</span><span class="o">=</span><span class="n">scaling_config</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_validate_attributes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_validate_attributes</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_train_loop_per_worker</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_train_loop_per_worker</span><span class="p">,</span> <span class="s2">&quot;train_loop_per_worker&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">preprocess_datasets</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Evaluate all datasets.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">datasets</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">d</span><span class="p">()</span> <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">else</span> <span class="n">d</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">datasets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_config</span><span class="o">.</span><span class="n">_legacy_preprocessing</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">datasets</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_train_loop_per_worker</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">train_loop_per_worker</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">fn_name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_params</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">train_loop_per_worker</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_params</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fn_name</span><span class="si">}</span><span class="s2"> should take in 0 or 1 arguments, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but it accepts </span><span class="si">{</span><span class="n">num_params</span><span class="si">}</span><span class="s2"> arguments instead.&quot;</span>
            <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_validate_scaling_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">scaling_config</span><span class="p">:</span> <span class="n">ScalingConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ScalingConfig</span><span class="p">:</span>
        <span class="n">scaling_config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">DataParallelTrainer</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">_validate_scaling_config</span><span class="p">(</span>
            <span class="n">scaling_config</span>
        <span class="p">)</span>

        <span class="c1"># This validation happens after the scaling config is updated from</span>
        <span class="c1"># its specification in the Tuner `param_space`</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">scaling_config</span><span class="o">.</span><span class="n">use_gpu</span> <span class="ow">and</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">in</span> <span class="n">ray</span><span class="o">.</span><span class="n">available_resources</span><span class="p">():</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;GPUs are detected in your Ray cluster, but GPU &quot;</span>
                <span class="s2">&quot;training is not enabled for this trainer. To enable &quot;</span>
                <span class="s2">&quot;GPU training, make sure to set `use_gpu` to True &quot;</span>
                <span class="s2">&quot;in your scaling config.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">scaling_config</span><span class="o">.</span><span class="n">num_workers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You must specify the &#39;num_workers&#39; in `scaling_config` as either an &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;argument of `</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">` or through the `param_space` of a &quot;</span>
                <span class="s2">&quot;`Tuner` (if performing hyperparameter tuning).&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">scaling_config</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;&#39;num_workers&#39; in `scaling_config` must be a positive &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;integer. Received </span><span class="si">{</span><span class="n">scaling_config</span><span class="o">.</span><span class="n">num_workers</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">scaling_config</span>

    <span class="k">def</span> <span class="nf">_report</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_iterator</span><span class="p">:</span> <span class="n">TrainingIterator</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">results</span> <span class="ow">in</span> <span class="n">training_iterator</span><span class="p">:</span>
            <span class="c1"># TODO(ml-team): add ability to report results from multiple workers.</span>
            <span class="n">first_worker_results</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">tune</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="o">**</span><span class="n">first_worker_results</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">training_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scaling_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_scaling_config</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scaling_config</span><span class="p">)</span>

        <span class="n">train_loop_per_worker</span> <span class="o">=</span> <span class="n">construct_train_func</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_train_loop_per_worker</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_train_loop_config</span><span class="p">,</span>
            <span class="n">fn_arg_name</span><span class="o">=</span><span class="s2">&quot;train_loop_per_worker&quot;</span><span class="p">,</span>
            <span class="n">discard_returns</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">additional_resources_per_worker</span> <span class="o">=</span> <span class="n">scaling_config</span><span class="o">.</span><span class="n">additional_resources_per_worker</span>

        <span class="n">trial_info</span> <span class="o">=</span> <span class="n">TrialInfo</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="n">session</span><span class="o">.</span><span class="n">get_trial_name</span><span class="p">(),</span>
            <span class="nb">id</span><span class="o">=</span><span class="n">session</span><span class="o">.</span><span class="n">get_trial_id</span><span class="p">(),</span>
            <span class="n">resources</span><span class="o">=</span><span class="n">session</span><span class="o">.</span><span class="n">get_trial_resources</span><span class="p">(),</span>
            <span class="n">logdir</span><span class="o">=</span><span class="n">session</span><span class="o">.</span><span class="n">get_trial_dir</span><span class="p">(),</span>
            <span class="n">driver_ip</span><span class="o">=</span><span class="n">ray</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">get_node_ip_address</span><span class="p">(),</span>
            <span class="n">experiment_name</span><span class="o">=</span><span class="n">session</span><span class="o">.</span><span class="n">get_experiment_name</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="n">backend_executor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backend_executor_cls</span><span class="p">(</span>
            <span class="n">backend_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_backend_config</span><span class="p">,</span>
            <span class="n">trial_info</span><span class="o">=</span><span class="n">trial_info</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="n">scaling_config</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span>
            <span class="n">num_cpus_per_worker</span><span class="o">=</span><span class="n">scaling_config</span><span class="o">.</span><span class="n">num_cpus_per_worker</span><span class="p">,</span>
            <span class="n">num_gpus_per_worker</span><span class="o">=</span><span class="n">scaling_config</span><span class="o">.</span><span class="n">num_gpus_per_worker</span><span class="p">,</span>
            <span class="n">additional_resources_per_worker</span><span class="o">=</span><span class="n">additional_resources_per_worker</span><span class="p">,</span>
            <span class="n">max_retries</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">checkpoint_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">run_config</span><span class="o">.</span><span class="n">checkpoint_config</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">def</span> <span class="nf">clear_lazy_checkpoint_marker</span><span class="p">():</span>
            <span class="sd">&quot;&quot;&quot;Clear the stale lazy checkpointing marker on all worker nodes.</span>

<span class="sd">            After recovery, the trainer may be scheduled on another node.</span>
<span class="sd">            We should delete the marker files created earlier on each node to</span>
<span class="sd">            Avoid converting checkpoints to string paths.</span>

<span class="sd">            Please note that we need to clear the flag before the initialization</span>
<span class="sd">            of the checkpoint_manager, during which it will create a new lazy</span>
<span class="sd">            checkpointing marker file.</span>
<span class="sd">            &quot;&quot;&quot;</span>

            <span class="n">marker_file</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">trial_info</span><span class="o">.</span><span class="n">logdir</span><span class="p">)</span> <span class="o">/</span> <span class="n">LAZY_CHECKPOINT_MARKER_FILE</span>
            <span class="k">if</span> <span class="n">marker_file</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Deleting the stale lazy checkpoint marker file: </span><span class="si">{</span><span class="n">marker_file</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
                <span class="c1"># Multiple workers on the same node may delete this file at the</span>
                <span class="c1"># same time. Return if the marker file has been deleted.</span>
                <span class="c1"># TODO(ml-team): replace this try-except block with `missing_ok=True`</span>
                <span class="c1"># after we completely drop py37 support.</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">marker_file</span><span class="o">.</span><span class="n">unlink</span><span class="p">()</span>
                <span class="k">except</span> <span class="ne">FileNotFoundError</span><span class="p">:</span>
                    <span class="k">return</span>

        <span class="c1"># Start the remote actors.</span>
        <span class="n">backend_executor</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="n">initialization_hook</span><span class="o">=</span><span class="n">clear_lazy_checkpoint_marker</span><span class="p">)</span>

        <span class="n">checkpoint_manager</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint_manager_cls</span><span class="p">(</span>
            <span class="n">preprocessor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span>
        <span class="p">)</span>

        <span class="c1"># Disable TrainingIterator&#39;s CheckpointManager from handling</span>
        <span class="c1"># checkpoints itself by setting num_to_keep to None.</span>
        <span class="c1"># This is important because otherwise Trainer&#39;s CheckpointManager</span>
        <span class="c1"># may delete a checkpoint prematurely, before the next checkpoint</span>
        <span class="c1"># has been fully handled by Tune.</span>
        <span class="c1"># TODO(jungong, justinvyu) : Trainer should not own a</span>
        <span class="c1"># CheckpointManager.</span>
        <span class="n">checkpoint_strategy</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">run_config</span><span class="o">.</span><span class="n">checkpoint_config</span><span class="p">)</span>
        <span class="n">checkpoint_strategy</span><span class="o">.</span><span class="n">num_to_keep</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">checkpoint_strategy</span><span class="o">.</span><span class="n">checkpoint_score_attribute</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">training_iterator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_training_iterator_cls</span><span class="p">(</span>
            <span class="n">backend_executor</span><span class="o">=</span><span class="n">backend_executor</span><span class="p">,</span>
            <span class="n">backend_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_backend_config</span><span class="p">,</span>
            <span class="n">train_func</span><span class="o">=</span><span class="n">train_loop_per_worker</span><span class="p">,</span>
            <span class="n">datasets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">datasets</span><span class="p">,</span>
            <span class="n">data_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_config</span><span class="p">,</span>
            <span class="n">checkpoint_manager</span><span class="o">=</span><span class="n">checkpoint_manager</span><span class="p">,</span>
            <span class="n">checkpoint</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">resume_from_checkpoint</span><span class="p">,</span>
            <span class="n">checkpoint_strategy</span><span class="o">=</span><span class="n">checkpoint_strategy</span><span class="p">,</span>
            <span class="n">storage_path</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">run_config</span><span class="o">.</span><span class="n">storage_path</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_report</span><span class="p">(</span><span class="n">training_iterator</span><span class="p">)</span>

        <span class="c1"># Shutdown workers.</span>
        <span class="n">backend_executor</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>

<div class="viewcode-block" id="DataParallelTrainer.get_dataset_config"><a class="viewcode-back" href="../../../train/api/doc/ray.train.data_parallel_trainer.DataParallelTrainer.get_dataset_config.html#ray.train.data_parallel_trainer.DataParallelTrainer.get_dataset_config">[docs]</a>    <span class="k">def</span> <span class="nf">get_dataset_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataConfig</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return a copy of this Trainer&#39;s final dataset configs.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The merged default + user-supplied dataset config.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_config</span><span class="p">,</span> <span class="n">_LegacyDataConfigWrapper</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_config</span><span class="o">.</span><span class="n">_dataset_config</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_config</span></div>

    <span class="nd">@repr_with_fallback</span><span class="p">([</span><span class="s2">&quot;ipywidgets&quot;</span><span class="p">,</span> <span class="s2">&quot;8&quot;</span><span class="p">])</span>
    <span class="k">def</span> <span class="nf">_repr_mimebundle_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return a mimebundle with an ipywidget repr and a simple text repr.</span>

<span class="sd">        Depending on the frontend where the data is being displayed,</span>
<span class="sd">        different mimetypes will be used from this bundle.</span>
<span class="sd">        See https://ipython.readthedocs.io/en/stable/config/integrating.html</span>
<span class="sd">        for information about this method, and</span>
<span class="sd">        https://ipywidgets.readthedocs.io/en/latest/embedding.html</span>
<span class="sd">        for more information about the jupyter widget mimetype.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A mimebundle containing an ipywidget repr and a simple text repr.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">HTML</span><span class="p">,</span> <span class="n">VBox</span><span class="p">,</span> <span class="n">Tab</span><span class="p">,</span> <span class="n">Layout</span>

        <span class="n">title</span> <span class="o">=</span> <span class="n">HTML</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&lt;h2&gt;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&lt;/h2&gt;&quot;</span><span class="p">)</span>

        <span class="n">children</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">titles</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">datasets</span><span class="p">:</span>
            <span class="n">children</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_datasets_repr_</span><span class="p">())</span>
            <span class="n">titles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Datasets&quot;</span><span class="p">)</span>

            <span class="n">children</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_config_repr_html_</span><span class="p">()))</span>
            <span class="n">titles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Data Config&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_loop_config</span><span class="p">:</span>
            <span class="n">children</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train_loop_config_repr_html_</span><span class="p">()))</span>
            <span class="n">titles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Train Loop Config&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling_config</span><span class="p">:</span>
            <span class="n">children</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scaling_config</span><span class="o">.</span><span class="n">_repr_html_</span><span class="p">()))</span>
            <span class="n">titles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Scaling Config&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_config</span><span class="p">:</span>
            <span class="n">children</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">run_config</span><span class="o">.</span><span class="n">_repr_html_</span><span class="p">()))</span>
            <span class="n">titles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Run Config&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backend_config</span><span class="p">:</span>
            <span class="n">children</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backend_config</span><span class="o">.</span><span class="n">_repr_html_</span><span class="p">()))</span>
            <span class="n">titles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Backend Config&quot;</span><span class="p">)</span>

        <span class="n">tab</span> <span class="o">=</span> <span class="n">Tab</span><span class="p">(</span><span class="n">children</span><span class="p">,</span> <span class="n">titles</span><span class="o">=</span><span class="n">titles</span><span class="p">)</span>
        <span class="n">widget</span> <span class="o">=</span> <span class="n">VBox</span><span class="p">([</span><span class="n">title</span><span class="p">,</span> <span class="n">tab</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">Layout</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="s2">&quot;100%&quot;</span><span class="p">))</span>
        <span class="n">bundle</span> <span class="o">=</span> <span class="n">widget</span><span class="o">.</span><span class="n">_repr_mimebundle_</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">bundle</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;text/plain&quot;</span><span class="p">:</span> <span class="nb">repr</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">bundle</span>

    <span class="k">def</span> <span class="nf">_train_loop_config_repr_html_</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_loop_config</span><span class="p">:</span>
            <span class="n">table_data</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_loop_config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="o">.</span><span class="n">isnumeric</span><span class="p">():</span>
                    <span class="n">table_data</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
                <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s2">&quot;_repr_html_&quot;</span><span class="p">):</span>
                    <span class="n">table_data</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">_repr_html_</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">table_data</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">Template</span><span class="p">(</span><span class="s2">&quot;title_data.html.j2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">render</span><span class="p">(</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Train Loop Config&quot;</span><span class="p">,</span>
                <span class="n">data</span><span class="o">=</span><span class="n">Template</span><span class="p">(</span><span class="s2">&quot;scrollableTable.html.j2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">render</span><span class="p">(</span>
                    <span class="n">table</span><span class="o">=</span><span class="n">tabulate</span><span class="p">(</span>
                        <span class="n">table_data</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
                        <span class="n">headers</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Setting&quot;</span><span class="p">,</span> <span class="s2">&quot;Value&quot;</span><span class="p">],</span>
                        <span class="n">showindex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">tablefmt</span><span class="o">=</span><span class="s2">&quot;unsafehtml&quot;</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">max_height</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_data_config_repr_html_</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="c1"># TODO make this rendering nicer.</span>
        <span class="n">content</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_config</span><span class="p">)]</span>
        <span class="k">return</span> <span class="n">Template</span><span class="p">(</span><span class="s2">&quot;rendered_html_common.html.j2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_datasets_repr_</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">HTML</span><span class="p">,</span> <span class="n">VBox</span><span class="p">,</span> <span class="n">Layout</span>

        <span class="n">content</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">datasets</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">config</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">tab</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">_tab_repr_</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">tab</span><span class="p">:</span>
                    <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">HTML</span><span class="p">(</span>
                            <span class="n">Template</span><span class="p">(</span><span class="s2">&quot;title_data.html.j2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">render</span><span class="p">(</span>
                                <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Dataset - &lt;code&gt;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&lt;/code&gt;&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="kc">None</span>
                            <span class="p">)</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">_tab_repr_</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">VBox</span><span class="p">(</span><span class="n">content</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">Layout</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="s2">&quot;100%&quot;</span><span class="p">))</span></div>


<span class="k">def</span> <span class="nf">_load_checkpoint_dict</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">:</span> <span class="n">Checkpoint</span><span class="p">,</span> <span class="n">trainer_name</span><span class="p">:</span> <span class="nb">str</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;Preprocessor&quot;</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Load a Ray Train Checkpoint (dict based).</span>

<span class="sd">    This is a private API.</span>

<span class="sd">    Args:</span>
<span class="sd">        checkpoint: The checkpoint to load the weights and</span>
<span class="sd">            preprocessor from.</span>
<span class="sd">        trainer_name: Trainer class name to use in error</span>
<span class="sd">            message.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The model or weights and AIR preprocessor contained within.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">checkpoint_dict</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
    <span class="n">preprocessor</span> <span class="o">=</span> <span class="n">checkpoint_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">PREPROCESSOR_KEY</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">MODEL_KEY</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">checkpoint_dict</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;No item with key: </span><span class="si">{</span><span class="n">MODEL_KEY</span><span class="si">}</span><span class="s2"> is found in the &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Checkpoint. Make sure this key exists when saving the &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;checkpoint in ``</span><span class="si">{</span><span class="n">trainer_name</span><span class="si">}</span><span class="s2">``.&quot;</span>
        <span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">checkpoint_dict</span><span class="p">[</span><span class="n">MODEL_KEY</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">preprocessor</span>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>