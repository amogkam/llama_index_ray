
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ray.train.huggingface.transformers.transformers_trainer &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../../../../../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../../../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../../../../../" id="documentation_options" src="../../../../../_static/documentation_options.js"></script>
    <script src="../../../../../_static/jquery.js"></script>
    <script src="../../../../../_static/underscore.js"></script>
    <script src="../../../../../_static/doctools.js"></script>
    <script src="../../../../../_static/clipboard.min.js"></script>
    <script src="../../../../../_static/copybutton.js"></script>
    <script src="../../../../../_static/js/versionwarning.js"></script>
    <script src="../../../../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../../../../_static/js/docsearch.js"></script>
    <script src="../../../../../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../../../../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../../../../_static/js/custom.js"></script>
    <script defer="defer" src="../../../../../_static/js/top-navigation.js"></script>
    <script src="../../../../../_static/js/tags.js"></script>
    <script src="../../../../../_static/tabs.js"></script>
    <script src="../../../../../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../../../_static/design-tabs.js"></script>
    <script async="async" src="../../../../../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/_modules/ray/train/huggingface/transformers/transformers_trainer.html" />
    <link rel="shortcut icon" href="../../../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../../../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "_modules/ray/train/huggingface/transformers/transformers_trainer", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../../../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../../../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2F_modules/ray/train/huggingface/transformers/transformers_trainer.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for ray.train.huggingface.transformers.transformers_trainer</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">importlib.util</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">packaging.version</span> <span class="kn">import</span> <span class="n">Version</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Type</span>

<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span> <span class="k">as</span> <span class="n">TorchDataset</span>

<span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span>
<span class="kn">from</span> <span class="nn">ray.air.checkpoint</span> <span class="kn">import</span> <span class="n">Checkpoint</span>
<span class="kn">from</span> <span class="nn">ray.air.config</span> <span class="kn">import</span> <span class="n">DatasetConfig</span><span class="p">,</span> <span class="n">RunConfig</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.constants</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">EVALUATION_DATASET_KEY</span><span class="p">,</span>
    <span class="n">TRAIN_DATASET_KEY</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.train.data_parallel_trainer</span> <span class="kn">import</span> <span class="n">DataParallelTrainer</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchConfig</span><span class="p">,</span> <span class="n">TorchTrainer</span>
<span class="kn">from</span> <span class="nn">ray.train.trainer</span> <span class="kn">import</span> <span class="n">GenDataset</span>
<span class="kn">from</span> <span class="nn">ray.util</span> <span class="kn">import</span> <span class="n">PublicAPI</span>


<span class="n">TRANSFORMERS_IMPORT_ERROR</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">ImportError</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">transformers</span>
    <span class="kn">import</span> <span class="nn">transformers.modeling_utils</span>
    <span class="kn">import</span> <span class="nn">transformers.trainer</span>
    <span class="kn">import</span> <span class="nn">transformers.training_args</span>
    <span class="kn">from</span> <span class="nn">transformers.trainer_utils</span> <span class="kn">import</span> <span class="n">IntervalStrategy</span>
    <span class="kn">from</span> <span class="nn">transformers.utils</span> <span class="kn">import</span> <span class="n">is_datasets_available</span>

    <span class="kn">from</span> <span class="nn">ray.train.huggingface.transformers._transformers_utils</span> <span class="kn">import</span> <span class="p">(</span>
        <span class="n">TrainReportCallback</span><span class="p">,</span>
        <span class="n">process_datasets</span><span class="p">,</span>
        <span class="n">wrap_transformers_trainer</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Due to HF Dataset&#39;s dynamic module system, we need to dynamically import the</span>
    <span class="c1"># datasets_modules module on every actor when training.</span>
    <span class="c1"># We accomplish this by simply running the following bit of code directly</span>
    <span class="c1"># in module you are currently viewing. This ensures that when we</span>
    <span class="c1"># unpickle the TransformersTrainer, it will be ran before pickle tries to</span>
    <span class="c1"># import datasets_modules and prevents an exception from being thrown.</span>
    <span class="c1"># Same logic is present inside HF Transformers Ray integration:</span>
    <span class="c1"># https://github.com/huggingface/transformers/blob/\</span>
    <span class="c1"># 7d5fde991d598370d961be8cb7add6541e2b59ce/src/transformers/integrations.py#L271</span>
    <span class="c1"># Also see https://github.com/ray-project/ray/issues/28084</span>
    <span class="k">if</span> <span class="s2">&quot;datasets_modules&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span> <span class="ow">and</span> <span class="n">is_datasets_available</span><span class="p">():</span>
        <span class="kn">import</span> <span class="nn">datasets.load</span>

        <span class="n">dynamic_modules_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">datasets</span><span class="o">.</span><span class="n">load</span><span class="o">.</span><span class="n">init_dynamic_modules</span><span class="p">(),</span> <span class="s2">&quot;__init__.py&quot;</span>
        <span class="p">)</span>
        <span class="c1"># load dynamic_modules from path</span>
        <span class="n">spec</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">spec_from_file_location</span><span class="p">(</span>
            <span class="s2">&quot;datasets_modules&quot;</span><span class="p">,</span> <span class="n">dynamic_modules_path</span>
        <span class="p">)</span>
        <span class="n">datasets_modules</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">module_from_spec</span><span class="p">(</span><span class="n">spec</span><span class="p">)</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="n">spec</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">datasets_modules</span>
        <span class="n">spec</span><span class="o">.</span><span class="n">loader</span><span class="o">.</span><span class="n">exec_module</span><span class="p">(</span><span class="n">datasets_modules</span><span class="p">)</span>

<span class="k">except</span> <span class="ne">ImportError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="n">TRANSFORMERS_IMPORT_ERROR</span> <span class="o">=</span> <span class="n">e</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">ray.data.preprocessor</span> <span class="kn">import</span> <span class="n">Preprocessor</span>


<span class="n">TRAINER_INIT_FN_KEY</span> <span class="o">=</span> <span class="s2">&quot;_trainer_init_per_worker&quot;</span>


<div class="viewcode-block" id="TransformersTrainer"><a class="viewcode-back" href="../../../../../train/api/doc/ray.train.huggingface.TransformersTrainer.html#ray.train.huggingface.TransformersTrainer">[docs]</a><span class="nd">@PublicAPI</span><span class="p">(</span><span class="n">stability</span><span class="o">=</span><span class="s2">&quot;alpha&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TransformersTrainer</span><span class="p">(</span><span class="n">TorchTrainer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A Trainer for data parallel HuggingFace Transformers on PyTorch training.</span>

<span class="sd">    This Trainer runs the ``transformers.Trainer.train()`` method on multiple</span>
<span class="sd">    Ray Actors. The training is carried out in a distributed fashion through PyTorch</span>
<span class="sd">    DDP. These actors already have the necessary torch process group already</span>
<span class="sd">    configured for distributed PyTorch training. If you have PyTorch &gt;= 1.12.0</span>
<span class="sd">    installed, you can also run FSDP training by specifying the ``fsdp`` argument</span>
<span class="sd">    in ``TrainingArguments``. DeepSpeed is</span>
<span class="sd">    also supported - see :doc:`/ray-air/examples/gptj_deepspeed_fine_tuning`.</span>
<span class="sd">    For more information on configuring FSDP or DeepSpeed, refer to `Hugging Face</span>
<span class="sd">    documentation &lt;https://huggingface.co/docs/transformers/\</span>
<span class="sd">main/en/main_classes/trainer#transformers.TrainingArguments&gt;`__.</span>

<span class="sd">    The training function ran on every Actor will first run the</span>
<span class="sd">    specified ``trainer_init_per_worker`` function to obtain an instantiated</span>
<span class="sd">    ``transformers.Trainer`` object. The ``trainer_init_per_worker`` function</span>
<span class="sd">    will have access to preprocessed train and evaluation datasets.</span>

<span class="sd">    If the ``datasets`` dict contains a training dataset (denoted by</span>
<span class="sd">    the &quot;train&quot; key), then it will be split into multiple dataset</span>
<span class="sd">    shards, with each Actor training on a single shard.</span>
<span class="sd">    All the other datasets will not be split.</span>

<span class="sd">    Please note that if you use a custom ``transformers.Trainer`` subclass,</span>
<span class="sd">    the ``get_train_dataloader`` method will be wrapped around to disable</span>
<span class="sd">    sharding by ``transformers.IterableDatasetShard``, as the dataset will</span>
<span class="sd">    already be sharded on the Ray AIR side.</span>

<span class="sd">    You can also provide ``datasets.Dataset`` object or other dataset objects</span>
<span class="sd">    allowed by ``transformers.Trainer`` directly in the ``trainer_init_per_worker``</span>
<span class="sd">    function, without specifying the ``datasets`` dict. It is recommended to initialize</span>
<span class="sd">    those objects inside the function, as otherwise they will be serialized and passed</span>
<span class="sd">    to the function, which may lead to long runtime and memory issues with large</span>
<span class="sd">    amounts of data. In this case, the training dataset will be split</span>
<span class="sd">    automatically by Transformers.</span>

<span class="sd">    HuggingFace loggers will be automatically disabled, and the ``local_rank``</span>
<span class="sd">    argument in ``TrainingArguments`` will be automatically set. Please note</span>
<span class="sd">    that if you want to use CPU training, you will need to set the ``no_cuda``</span>
<span class="sd">    argument in ``TrainingArguments`` manually - otherwise, an exception</span>
<span class="sd">    (segfault) may be thrown.</span>

<span class="sd">    This Trainer requires ``transformers&gt;=4.19.0`` package.</span>
<span class="sd">    It is tested with ``transformers==4.19.1``.</span>

<span class="sd">    Example:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            # Based on</span>
<span class="sd">            # huggingface/notebooks/examples/language_modeling_from_scratch.ipynb</span>

<span class="sd">            # Hugging Face imports</span>
<span class="sd">            from datasets import load_dataset</span>
<span class="sd">            import transformers</span>
<span class="sd">            from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer</span>

<span class="sd">            import ray</span>
<span class="sd">            from ray.train.huggingface import TransformersTrainer</span>
<span class="sd">            from ray.air.config import ScalingConfig</span>

<span class="sd">            # If using GPUs, set this to True.</span>
<span class="sd">            use_gpu = False</span>

<span class="sd">            model_checkpoint = &quot;gpt2&quot;</span>
<span class="sd">            tokenizer_checkpoint = &quot;sgugger/gpt2-like-tokenizer&quot;</span>
<span class="sd">            block_size = 128</span>

<span class="sd">            datasets = load_dataset(&quot;wikitext&quot;, &quot;wikitext-2-raw-v1&quot;)</span>
<span class="sd">            tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)</span>

<span class="sd">            def tokenize_function(examples):</span>
<span class="sd">                return tokenizer(examples[&quot;text&quot;])</span>

<span class="sd">            tokenized_datasets = datasets.map(</span>
<span class="sd">                tokenize_function, batched=True, num_proc=1, remove_columns=[&quot;text&quot;]</span>
<span class="sd">            )</span>

<span class="sd">            def group_texts(examples):</span>
<span class="sd">                # Concatenate all texts.</span>
<span class="sd">                concatenated_examples = {</span>
<span class="sd">                    k: sum(examples[k], []) for k in examples.keys()</span>
<span class="sd">                }</span>
<span class="sd">                total_length = len(concatenated_examples[list(examples.keys())[0]])</span>
<span class="sd">                # We drop the small remainder, we could add padding if the model</span>
<span class="sd">                # supported it.</span>
<span class="sd">                # instead of this drop, you can customize this part to your needs.</span>
<span class="sd">                total_length = (total_length // block_size) * block_size</span>
<span class="sd">                # Split by chunks of max_len.</span>
<span class="sd">                result = {</span>
<span class="sd">                    k: [</span>
<span class="sd">                        t[i : i + block_size]</span>
<span class="sd">                        for i in range(0, total_length, block_size)</span>
<span class="sd">                    ]</span>
<span class="sd">                    for k, t in concatenated_examples.items()</span>
<span class="sd">                }</span>
<span class="sd">                result[&quot;labels&quot;] = result[&quot;input_ids&quot;].copy()</span>
<span class="sd">                return result</span>

<span class="sd">            lm_datasets = tokenized_datasets.map(</span>
<span class="sd">                group_texts,</span>
<span class="sd">                batched=True,</span>
<span class="sd">                batch_size=1000,</span>
<span class="sd">                num_proc=1,</span>
<span class="sd">            )</span>
<span class="sd">            ray_train_ds = ray.data.from_huggingface(lm_datasets[&quot;train&quot;])</span>
<span class="sd">            ray_evaluation_ds = ray.data.from_huggingface(</span>
<span class="sd">                lm_datasets[&quot;validation&quot;]</span>
<span class="sd">            )</span>

<span class="sd">            def trainer_init_per_worker(train_dataset, eval_dataset, **config):</span>
<span class="sd">                model_config = AutoConfig.from_pretrained(model_checkpoint)</span>
<span class="sd">                model = AutoModelForCausalLM.from_config(model_config)</span>
<span class="sd">                args = transformers.TrainingArguments(</span>
<span class="sd">                    output_dir=f&quot;{model_checkpoint}-wikitext2&quot;,</span>
<span class="sd">                    evaluation_strategy=&quot;epoch&quot;,</span>
<span class="sd">                    save_strategy=&quot;epoch&quot;,</span>
<span class="sd">                    logging_strategy=&quot;epoch&quot;,</span>
<span class="sd">                    learning_rate=2e-5,</span>
<span class="sd">                    weight_decay=0.01,</span>
<span class="sd">                    no_cuda=(not use_gpu),</span>
<span class="sd">                )</span>
<span class="sd">                return transformers.Trainer(</span>
<span class="sd">                    model=model,</span>
<span class="sd">                    args=args,</span>
<span class="sd">                    train_dataset=train_dataset,</span>
<span class="sd">                    eval_dataset=eval_dataset,</span>
<span class="sd">                )</span>

<span class="sd">            scaling_config = ScalingConfig(num_workers=3, use_gpu=use_gpu)</span>
<span class="sd">            trainer = TransformersTrainer(</span>
<span class="sd">                trainer_init_per_worker=trainer_init_per_worker,</span>
<span class="sd">                scaling_config=scaling_config,</span>
<span class="sd">                datasets={&quot;train&quot;: ray_train_ds, &quot;evaluation&quot;: ray_evaluation_ds},</span>
<span class="sd">            )</span>
<span class="sd">            result = trainer.fit()</span>

<span class="sd">    Args:</span>
<span class="sd">        trainer_init_per_worker: The function that returns an instantiated</span>
<span class="sd">            ``transformers.Trainer`` object and takes in the following arguments:</span>
<span class="sd">            train ``Torch.Dataset``, optional evaluation ``Torch.Dataset``</span>
<span class="sd">            and config as kwargs. The Torch Datasets are automatically</span>
<span class="sd">            created by converting the Ray Datasets internally before</span>
<span class="sd">            they are passed into the function.</span>
<span class="sd">        trainer_init_config: Configurations to pass into</span>
<span class="sd">            ``trainer_init_per_worker`` as kwargs.</span>
<span class="sd">        torch_config: Configuration for setting up the PyTorch backend. If set to</span>
<span class="sd">            None, use the default configuration. This replaces the ``backend_config``</span>
<span class="sd">            arg of ``DataParallelTrainer``. Same as in ``TorchTrainer``.</span>
<span class="sd">        scaling_config: Configuration for how to scale data parallel training.</span>
<span class="sd">        dataset_config: Configuration for dataset ingest.</span>
<span class="sd">        run_config: Configuration for the execution of the training run.</span>
<span class="sd">        datasets: Any Ray Datasets to use for training. Use</span>
<span class="sd">            the key &quot;train&quot; to denote which dataset is the training</span>
<span class="sd">            dataset and key &quot;evaluation&quot; to denote the evaluation</span>
<span class="sd">            dataset. Can only contain a training dataset</span>
<span class="sd">            and up to one extra dataset to be used for evaluation.</span>
<span class="sd">            If a ``preprocessor`` is provided and has not already been fit,</span>
<span class="sd">            it will be fit on the training dataset. All datasets will be</span>
<span class="sd">            transformed by the ``preprocessor`` if one is provided.</span>
<span class="sd">        preprocessor: A ray.data.Preprocessor to preprocess the</span>
<span class="sd">            provided datasets.</span>
<span class="sd">        resume_from_checkpoint: A checkpoint to resume training from.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">trainer_init_per_worker</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
            <span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">TorchDataset</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchDataset</span><span class="p">],</span> <span class="n">Any</span><span class="p">],</span>
            <span class="s2">&quot;transformers.trainer.Trainer&quot;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">trainer_init_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">torch_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">scaling_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ScalingConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dataset_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">DatasetConfig</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">RunConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">datasets</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">GenDataset</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">preprocessor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;Preprocessor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">resume_from_checkpoint</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Checkpoint</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="k">if</span> <span class="n">TRANSFORMERS_IMPORT_ERROR</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">TRANSFORMERS_IMPORT_ERROR</span>

        <span class="c1"># Functionality required for TransformersTrainer only added in this</span>
        <span class="c1"># version</span>
        <span class="k">if</span> <span class="n">Version</span><span class="p">(</span><span class="n">transformers</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">Version</span><span class="p">(</span><span class="s2">&quot;4.19.0&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;TransformersTrainer requires transformers&gt;=4.19.0, but you &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;have </span><span class="si">{</span><span class="n">transformers</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2"> which is incompatible. &quot;</span>
                <span class="s2">&quot;Update on all nodes with `pip install -U &#39;transformers&gt;=4.19.0&#39;`.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_trainer_init_per_worker</span><span class="p">(</span>
            <span class="n">trainer_init_per_worker</span><span class="p">,</span> <span class="s2">&quot;trainer_init_per_worker&quot;</span>
        <span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">train_loop_per_worker</span><span class="o">=</span><span class="n">_huggingface_train_loop_per_worker</span><span class="p">,</span>
            <span class="n">train_loop_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_create_trainer_init_config</span><span class="p">(</span>
                <span class="n">trainer_init_per_worker</span><span class="p">,</span> <span class="n">trainer_init_config</span>
            <span class="p">),</span>
            <span class="n">torch_config</span><span class="o">=</span><span class="n">torch_config</span><span class="p">,</span>
            <span class="n">scaling_config</span><span class="o">=</span><span class="n">scaling_config</span><span class="p">,</span>
            <span class="n">dataset_config</span><span class="o">=</span><span class="n">dataset_config</span><span class="p">,</span>
            <span class="n">run_config</span><span class="o">=</span><span class="n">run_config</span><span class="p">,</span>
            <span class="n">datasets</span><span class="o">=</span><span class="n">datasets</span><span class="p">,</span>
            <span class="n">preprocessor</span><span class="o">=</span><span class="n">preprocessor</span><span class="p">,</span>
            <span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="n">resume_from_checkpoint</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_create_trainer_init_config</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">trainer_init_per_worker</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
            <span class="p">[</span><span class="n">TorchDataset</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchDataset</span><span class="p">],</span> <span class="n">Any</span><span class="p">],</span>
            <span class="s2">&quot;transformers.trainer.Trainer&quot;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="n">trainer_init_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="n">trainer_init_config</span> <span class="o">=</span> <span class="n">trainer_init_config</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="k">if</span> <span class="n">trainer_init_config</span> <span class="k">else</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">TRAINER_INIT_FN_KEY</span> <span class="ow">in</span> <span class="n">trainer_init_config</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="n">TRAINER_INIT_FN_KEY</span><span class="si">}</span><span class="s2">&#39; is a reserved key in `trainer_init_config`.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">trainer_init_per_worker</span><span class="p">:</span>
            <span class="n">trainer_init_config</span><span class="p">[</span><span class="n">TRAINER_INIT_FN_KEY</span><span class="p">]</span> <span class="o">=</span> <span class="n">trainer_init_per_worker</span>
        <span class="k">return</span> <span class="n">trainer_init_config</span>

<div class="viewcode-block" id="TransformersTrainer.restore"><a class="viewcode-back" href="../../../../../train/api/doc/ray.train.huggingface.TransformersTrainer.restore.html#ray.train.huggingface.TransformersTrainer.restore">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="s2">&quot;TransformersTrainer&quot;</span><span class="p">],</span>
        <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">trainer_init_per_worker</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Callable</span><span class="p">[</span>
                <span class="p">[</span><span class="n">TorchDataset</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchDataset</span><span class="p">],</span> <span class="n">Any</span><span class="p">],</span>
                <span class="s2">&quot;transformers.trainer.Trainer&quot;</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trainer_init_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">datasets</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">GenDataset</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">preprocessor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;Preprocessor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">scaling_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ScalingConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;TransformersTrainer&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Restores a TransformersTrainer from a previously interrupted/failed run.</span>

<span class="sd">        Args:</span>
<span class="sd">            trainer_init_per_worker: Optionally re-specified trainer init function.</span>
<span class="sd">                This should be used to re-specify a function that is not</span>
<span class="sd">                restorable in a new Ray cluster (e.g., it holds onto outdated</span>
<span class="sd">                object references). This should be the same trainer init</span>
<span class="sd">                that was passed to the original trainer constructor.</span>
<span class="sd">            trainer_init_config: Optionally re-specified trainer init config.</span>
<span class="sd">                This should similarly be used if the original `train_loop_config`</span>
<span class="sd">                contained outdated object references, and it should not be modified</span>
<span class="sd">                from what was originally passed in.</span>

<span class="sd">        See :meth:`BaseTrainer.restore() &lt;ray.train.trainer.BaseTrainer.restore&gt;`</span>
<span class="sd">        for descriptions of the other arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            TransformersTrainer: A restored instance of `TransformersTrainer`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">DataParallelTrainer</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span>
            <span class="n">path</span><span class="o">=</span><span class="n">path</span><span class="p">,</span>
            <span class="n">trainer_init_per_worker</span><span class="o">=</span><span class="n">trainer_init_per_worker</span><span class="p">,</span>
            <span class="n">trainer_init_config</span><span class="o">=</span><span class="n">trainer_init_config</span><span class="p">,</span>
            <span class="n">datasets</span><span class="o">=</span><span class="n">datasets</span><span class="p">,</span>
            <span class="n">preprocessor</span><span class="o">=</span><span class="n">preprocessor</span><span class="p">,</span>
            <span class="n">scaling_config</span><span class="o">=</span><span class="n">scaling_config</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_validate_trainer_init_per_worker</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">trainer_init_per_worker</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">fn_name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_params</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">trainer_init_per_worker</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_params</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fn_name</span><span class="si">}</span><span class="s2"> should take in at least 3 arguments, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but it accepts </span><span class="si">{</span><span class="n">num_params</span><span class="si">}</span><span class="s2"> arguments instead.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_attributes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_config</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">conf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">conf</span><span class="o">.</span><span class="n">use_stream_api</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;TransformersTrainer does not support `use_stream_api`.&quot;</span>
                    <span class="p">)</span>
        <span class="n">gpus_per_worker</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling_config</span><span class="o">.</span><span class="n">num_gpus_per_worker</span>
        <span class="k">if</span> <span class="n">gpus_per_worker</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You have assigned </span><span class="si">{</span><span class="n">gpus_per_worker</span><span class="si">}</span><span class="s2"> GPUs per worker. &quot;</span>
                <span class="s2">&quot;This is not supported by HuggingFace, which expects &quot;</span>
                <span class="s2">&quot;one GPU per worker in DDP mode and will fail &quot;</span>
                <span class="s2">&quot;if more are assigned.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">gpus_per_worker</span> <span class="o">!=</span> <span class="nb">int</span><span class="p">(</span><span class="n">gpus_per_worker</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You have assigned </span><span class="si">{</span><span class="n">gpus_per_worker</span><span class="si">}</span><span class="s2"> GPUs per worker, &quot;</span>
                <span class="s2">&quot;but fractional GPUs are not supported by HuggingFace.&quot;</span>
            <span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_validate_attributes</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">_huggingface_train_loop_per_worker</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Per-worker training loop for HuggingFace Transformers.&quot;&quot;&quot;</span>
    <span class="n">trainer_init_per_worker</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_trainer_init_per_worker&quot;</span><span class="p">)</span>

    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="n">TRAIN_DATASET_KEY</span><span class="p">)</span>
    <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="n">EVALUATION_DATASET_KEY</span><span class="p">)</span>

    <span class="n">train_torch_dataset</span><span class="p">,</span> <span class="n">eval_torch_dataset</span> <span class="o">=</span> <span class="n">process_datasets</span><span class="p">(</span>
        <span class="n">train_dataset</span><span class="p">,</span>
        <span class="n">eval_dataset</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">trainer</span><span class="p">:</span> <span class="n">transformers</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">Trainer</span> <span class="o">=</span> <span class="n">trainer_init_per_worker</span><span class="p">(</span>
        <span class="n">train_torch_dataset</span><span class="p">,</span> <span class="n">eval_torch_dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">config</span>
    <span class="p">)</span>

    <span class="n">strategies</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">strategy</span>
        <span class="k">for</span> <span class="n">strategy</span> <span class="ow">in</span> <span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">evaluation_strategy</span><span class="p">,</span> <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_strategy</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">strategy</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;no&quot;</span><span class="p">,</span> <span class="n">IntervalStrategy</span><span class="o">.</span><span class="n">NO</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">strategies</span> <span class="o">=</span> <span class="p">[</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">logging_strategy</span><span class="p">]</span> <span class="o">+</span> <span class="n">strategies</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">strategy</span> <span class="o">==</span> <span class="n">strategies</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">strategy</span> <span class="ow">in</span> <span class="n">strategies</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;When using Ray AIR,`logging_strategy`, `evaluation_strategy` &quot;</span>
            <span class="s2">&quot;and `save_strategy` must all be set to the same value. &quot;</span>
            <span class="s2">&quot;`evaluation_strategy` or `save_strategy` may also be set to &#39;no&#39;.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Got `logging_strategy`=</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">logging_strategy</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`evaluation_strategy`=</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">evaluation_strategy</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;`save_strategy`=</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_strategy</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_strategy</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">,</span> <span class="n">IntervalStrategy</span><span class="o">.</span><span class="n">STEPS</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_steps</span> <span class="o">&lt;</span> <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">logging_steps</span>
            <span class="ow">or</span> <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_steps</span> <span class="o">%</span> <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">logging_steps</span> <span class="o">!=</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;When using &#39;steps&#39; `save_strategy`, `save_steps` must be &quot;</span>
                <span class="s2">&quot;equal or bigger to `logging_steps`, and must be divisible &quot;</span>
                <span class="s2">&quot;by `logging_steps` (so that saving occurs at the same time &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;logging does). Got `save_steps`=</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">save_steps</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`logging_steps`=</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">logging_steps</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">evaluation_strategy</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">,</span> <span class="n">IntervalStrategy</span><span class="o">.</span><span class="n">STEPS</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">logging_steps</span> <span class="o">!=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">eval_steps</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`logging_steps` must be equal to `eval_steps`. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Got `logging_steps`=</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">logging_steps</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`eval_steps`=</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">eval_steps</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">load_best_model_at_end</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;As Ray AIR replaces Transformers checkpointing, &quot;</span>
            <span class="s2">&quot;`load_best_model_at_end` must be set to False.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;You can obtain the AIR Checkpoint with &quot;</span>
            <span class="s2">&quot;`Result.checkpoint` returned by the `fit()` method &quot;</span>
            <span class="s2">&quot;of this Trainer, and the model itself by calling &quot;</span>
            <span class="s2">&quot;`Checkpoint.get_model()`.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;You can configure the checkpointing by setting &quot;</span>
            <span class="s2">&quot;`run_config.checkpoint_config`.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">push_to_hub</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">hub_token</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;You have set `push_to_hub=True` but didn&#39;t specify `hub_token`. &quot;</span>
            <span class="s2">&quot;Pushing to hub will most likely fail, as the credentials will not &quot;</span>
            <span class="s2">&quot;be automatically propagated from the local enviroment to the Ray Actors. &quot;</span>
            <span class="s2">&quot;If that happens, specify `hub_token` in `TrainingArguments`.&quot;</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">wrap_transformers_trainer</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>

    <span class="c1"># ensure no HF logging callbacks are added</span>
    <span class="c1"># aside from doubling functionality with our callbacks,</span>
    <span class="c1"># the Wandb callbacks causes training to freeze</span>
    <span class="n">integration_callbacks</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">get_reporting_integration_callbacks</span><span class="p">(</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">report_to</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">callback</span> <span class="ow">in</span> <span class="n">integration_callbacks</span><span class="p">:</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">pop_callback</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">add_callback</span><span class="p">(</span><span class="n">TrainReportCallback</span><span class="p">)</span>

    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get_checkpoint</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">checkpoint</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">as_directory</span><span class="p">()</span> <span class="k">as</span> <span class="n">checkpoint_path</span><span class="p">:</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="n">checkpoint_path</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>