
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>How To Customize Policies &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/versionwarning.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../_static/js/docsearch.js"></script>
    <script src="../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../_static/js/termynal.js"></script>
    <script defer="defer" src="../_static/js/custom.js"></script>
    <script defer="defer" src="../_static/js/top-navigation.js"></script>
    <script src="../_static/js/tags.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/rllib/rllib-concepts.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sample Collections and Trajectory Views" href="rllib-sample-collection.html" />
    <link rel="prev" title="Saving and Loading your RL Algorithms and Policies" href="rllib-saving-and-loading-algos-and-policies.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "rllib/rllib-concepts", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Ray RLlib
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-training.html">
     Getting Started with RLlib
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="key-concepts.html">
     Key Concepts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../rllib-env.html">
     Environments
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-algorithms.html">
     Algorithms
    </a>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="user-guides.html">
     User Guides
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-advanced-api.html">
       Advanced Python APIs
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-models.html">
       Models, Preprocessors, and Action Distributions
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-saving-and-loading-algos-and-policies.html">
       Saving and Loading your RL Algorithms and Policies
      </a>
     </li>
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="rllib-concepts.html#">
       How To Customize Policies
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-sample-collection.html">
       Sample Collections and Trajectory Views
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-replay-buffers.html">
       Replay Buffers
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-offline.html">
       Working With Offline Data
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-catalogs.html">
       Catalog (Alpha)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-connector.html">
       Connectors (Alpha)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-rlmodule.html">
       RL Modules (Alpha)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-fault-tolerance.html">
       Fault Tolerance And Elastic Training
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-dev.html">
       How To Contribute to RLlib
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-cli.html">
       Working with the RLlib CLI
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-examples.html">
     Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="package_ref/index.html">
     Ray RLlib API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Frllib/rllib-concepts.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/rllib/rllib-concepts.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/rllib/rllib-concepts.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-concepts.html#policies-in-multi-agent">
   Policies in Multi-Agent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-concepts.html#building-policies-in-tensorflow">
   Building Policies in TensorFlow
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-concepts.html#building-policies-in-tensorflow-eager">
   Building Policies in TensorFlow Eager
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-concepts.html#building-policies-in-pytorch">
   Building Policies in PyTorch
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-concepts.html#extending-existing-policies">
   Extending Existing Policies
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>How To Customize Policies</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-concepts.html#policies-in-multi-agent">
   Policies in Multi-Agent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-concepts.html#building-policies-in-tensorflow">
   Building Policies in TensorFlow
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-concepts.html#building-policies-in-tensorflow-eager">
   Building Policies in TensorFlow Eager
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-concepts.html#building-policies-in-pytorch">
   Building Policies in PyTorch
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-concepts.html#extending-existing-policies">
   Extending Existing Policies
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <a class="reference external image-reference" href="https://ray-docs-promo.netlify.app/rllib"><img alt="" src="https://ray-docs-promo.netlify.app/assets/img/rllib/top.png" /></a>
<div class="bottom-right-promo-banner docutils">
<a class="reference external image-reference" href="https://ray-docs-promo.netlify.app/rllib"><img alt="" src="https://ray-docs-promo.netlify.app/assets/img/rllib/square.png" /></a>
</div>
<section id="how-to-customize-policies">
<span id="rllib-policy-walkthrough"></span><h1>How To Customize Policies<a class="headerlink" href="rllib-concepts.html#how-to-customize-policies" title="Permalink to this headline">#</a></h1>
<p>This page describes the internal concepts used to implement algorithms in RLlib.
You might find this useful if modifying or adding new algorithms to RLlib.</p>
<p>Policy classes encapsulate the core numerical components of RL algorithms.
This typically includes the policy model that determines actions to take, a trajectory postprocessor for experiences, and a loss function to improve the policy given post-processed experiences.
For a simple example, see the policy gradients <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/pg/pg_tf_policy.py">policy definition</a>.</p>
<p>Most interaction with deep learning frameworks is isolated to the <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/policy/policy.py">Policy interface</a>, allowing RLlib to support multiple frameworks.
To simplify the definition of policies, RLlib includes <a class="reference external" href="rllib-concepts.html#building-policies-in-tensorflow">Tensorflow</a> and <a class="reference external" href="rllib-concepts.html#building-policies-in-pytorch">PyTorch-specific</a> templates.
You can also write your own from scratch. Here is an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CustomPolicy</span><span class="p">(</span><span class="n">Policy</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Example of a custom policy written from scratch.</span>

<span class="sd">    You might find it more convenient to use the `build_tf_policy` and</span>
<span class="sd">    `build_torch_policy` helpers instead for a real policy, which are</span>
<span class="sd">    described in the next sections.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="n">Policy</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
        <span class="c1"># example parameter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="k">def</span> <span class="nf">compute_actions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">obs_batch</span><span class="p">,</span>
                        <span class="n">state_batches</span><span class="p">,</span>
                        <span class="n">prev_action_batch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">prev_reward_batch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">info_batch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">episodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># return action batch, RNN states, extra values to include in batch</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">obs_batch</span><span class="p">],</span> <span class="p">[],</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">learn_on_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">samples</span><span class="p">):</span>
        <span class="c1"># implement your learning code here</span>
        <span class="k">return</span> <span class="p">{}</span>  <span class="c1"># return stats</span>

    <span class="k">def</span> <span class="nf">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>The above basic policy, when run, will produce batches of observations with the basic <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">new_obs</span></code>, <code class="docutils literal notranslate"><span class="pre">actions</span></code>, <code class="docutils literal notranslate"><span class="pre">rewards</span></code>, <code class="docutils literal notranslate"><span class="pre">dones</span></code>, and <code class="docutils literal notranslate"><span class="pre">infos</span></code> columns.
There are two more mechanisms to pass along and emit extra information:</p>
<p><strong>Policy recurrent state</strong>: Suppose you want to compute actions based on the current timestep of the episode.
While it is possible to have the environment provide this as part of the observation, we can instead compute and store it as part of the Policy recurrent state:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_initial_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns initial RNN state for the current policy.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># list of single state element (t=0)</span>
                <span class="c1"># you could also return multiple values, e.g., [0, &quot;foo&quot;]</span>

<span class="k">def</span> <span class="nf">compute_actions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">obs_batch</span><span class="p">,</span>
                    <span class="n">state_batches</span><span class="p">,</span>
                    <span class="n">prev_action_batch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">prev_reward_batch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">info_batch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">episodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">state_batches</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_initial_state</span><span class="p">())</span>
    <span class="n">new_state_batches</span> <span class="o">=</span> <span class="p">[[</span>
        <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">state_batches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">]]</span>
    <span class="k">return</span> <span class="o">...</span><span class="p">,</span> <span class="n">new_state_batches</span><span class="p">,</span> <span class="p">{}</span>

<span class="k">def</span> <span class="nf">learn_on_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">samples</span><span class="p">):</span>
    <span class="c1"># can access array of the state elements at each timestep</span>
    <span class="c1"># or state_in_1, 2, etc. if there are multiple state elements</span>
    <span class="k">assert</span> <span class="s2">&quot;state_in_0&quot;</span> <span class="ow">in</span> <span class="n">samples</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
    <span class="k">assert</span> <span class="s2">&quot;state_out_0&quot;</span> <span class="ow">in</span> <span class="n">samples</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Extra action info output</strong>: You can also emit extra outputs at each step which will be available for learning on. For example, you might want to output the behaviour policy logits as extra action info, which can be used for importance weighting, but in general arbitrary values can be stored here (as long as they are convertible to numpy arrays):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_actions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">obs_batch</span><span class="p">,</span>
                    <span class="n">state_batches</span><span class="p">,</span>
                    <span class="n">prev_action_batch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">prev_reward_batch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">info_batch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">episodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">action_info_batch</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;some_value&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;foo&quot;</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">obs_batch</span><span class="p">],</span>
        <span class="s2">&quot;other_value&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">12345</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">obs_batch</span><span class="p">],</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="o">...</span><span class="p">,</span> <span class="p">[],</span> <span class="n">action_info_batch</span>

<span class="k">def</span> <span class="nf">learn_on_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">samples</span><span class="p">):</span>
    <span class="c1"># can access array of the extra values at each timestep</span>
    <span class="k">assert</span> <span class="s2">&quot;some_value&quot;</span> <span class="ow">in</span> <span class="n">samples</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
    <span class="k">assert</span> <span class="s2">&quot;other_value&quot;</span> <span class="ow">in</span> <span class="n">samples</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
<section id="policies-in-multi-agent">
<h2>Policies in Multi-Agent<a class="headerlink" href="rllib-concepts.html#policies-in-multi-agent" title="Permalink to this headline">#</a></h2>
<p>Beyond being agnostic of framework implementation, one of the main reasons to have a Policy abstraction is for use in multi-agent environments. For example, the <a class="reference external" href="../rllib-env.html#rock-paper-scissors-example">rock-paper-scissors example</a> shows how you can leverage the Policy abstraction to evaluate heuristic policies against learned policies.</p>
</section>
<section id="building-policies-in-tensorflow">
<h2>Building Policies in TensorFlow<a class="headerlink" href="rllib-concepts.html#building-policies-in-tensorflow" title="Permalink to this headline">#</a></h2>
<p>This section covers how to build a TensorFlow RLlib policy using <code class="docutils literal notranslate"><span class="pre">tf_policy_template.build_tf_policy()</span></code>.</p>
<p>To start, you first have to define a loss function. In RLlib, loss functions are defined over batches of trajectory data produced by policy evaluation. A basic policy gradient loss that only tries to maximize the 1-step reward can be defined as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.sample_batch</span> <span class="kn">import</span> <span class="n">SampleBatch</span>

<span class="k">def</span> <span class="nf">policy_gradient_loss</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">dist_class</span><span class="p">,</span> <span class="n">train_batch</span><span class="p">):</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="n">train_batch</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">]</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">train_batch</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">REWARDS</span><span class="p">]</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">from_batch</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
    <span class="n">action_dist</span> <span class="o">=</span> <span class="n">dist_class</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">action_dist</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span> <span class="o">*</span> <span class="n">rewards</span><span class="p">)</span>
</pre></div>
</div>
<p>In the above snippet, <code class="docutils literal notranslate"><span class="pre">actions</span></code> is a Tensor placeholder of shape <code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">action_dim...]</span></code>, and <code class="docutils literal notranslate"><span class="pre">rewards</span></code> is a placeholder of shape <code class="docutils literal notranslate"><span class="pre">[batch_size]</span></code>. The <code class="docutils literal notranslate"><span class="pre">action_dist</span></code> object is an <a class="reference internal" href="rllib-models.html#rllib-models-walkthrough"><span class="std std-ref">ActionDistribution</span></a> that is parameterized by the output of the neural network policy model. Passing this loss function to <code class="docutils literal notranslate"><span class="pre">build_tf_policy</span></code> is enough to produce a very basic TF policy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.policy.tf_policy_template</span> <span class="kn">import</span> <span class="n">build_tf_policy</span>

<span class="c1"># &lt;class &#39;ray.rllib.policy.tf_policy_template.MyTFPolicy&#39;&gt;</span>
<span class="n">MyTFPolicy</span> <span class="o">=</span> <span class="n">build_tf_policy</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;MyTFPolicy&quot;</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">policy_gradient_loss</span><span class="p">)</span>
</pre></div>
</div>
<p>We can create an <a class="reference external" href="rllib-concepts.html#algorithms">Algorithm</a> and try running this policy on a toy env with two parallel rollout workers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm</span> <span class="kn">import</span> <span class="n">Algorithm</span>

<span class="k">class</span> <span class="nc">MyAlgo</span><span class="p">(</span><span class="n">Algorithm</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">get_default_policy_class</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">MyTFPolicy</span>

<span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span><span class="n">MyAlgo</span><span class="p">,</span> <span class="n">param_space</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;env&quot;</span><span class="p">:</span> <span class="s2">&quot;CartPole-v1&quot;</span><span class="p">,</span> <span class="s2">&quot;num_workers&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">})</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>If you run the above snippet <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/custom_tf_policy.py">(runnable file here)</a>, you’ll probably notice that CartPole doesn’t learn so well:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">==</span> <span class="nv">Status</span> <span class="o">==</span>
Using FIFO scheduling algorithm.
Resources requested: <span class="m">3</span>/4 CPUs, <span class="m">0</span>/0 GPUs
Memory usage on this node: <span class="m">4</span>.6/12.3 GB
Result logdir: /home/ubuntu/ray_results/MyAlgTrainer
Number of trials: <span class="m">1</span> <span class="o">({</span><span class="s1">&#39;RUNNING&#39;</span>: <span class="m">1</span><span class="o">})</span>
RUNNING trials:
 - MyAlgTrainer_CartPole-v0_0:      RUNNING, <span class="o">[</span><span class="m">3</span> CPUs, <span class="m">0</span> GPUs<span class="o">]</span>, <span class="o">[</span><span class="nv">pid</span><span class="o">=</span><span class="m">26784</span><span class="o">]</span>,
                                    <span class="m">32</span> s, <span class="m">156</span> iter, <span class="m">62400</span> ts, <span class="m">23</span>.1 rew
</pre></div>
</div>
<p>Let’s modify our policy loss to include rewards summed over time. To enable this advantage calculation, we need to define a <em>trajectory postprocessor</em> for the policy. This can be done by defining <code class="docutils literal notranslate"><span class="pre">postprocess_fn</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.evaluation.postprocessing</span> <span class="kn">import</span> <span class="n">compute_advantages</span><span class="p">,</span> \
    <span class="n">Postprocessing</span>

<span class="k">def</span> <span class="nf">postprocess_advantages</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span>
                           <span class="n">sample_batch</span><span class="p">,</span>
                           <span class="n">other_agent_batches</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">episode</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">compute_advantages</span><span class="p">(</span>
        <span class="n">sample_batch</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;gamma&quot;</span><span class="p">],</span> <span class="n">use_gae</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">policy_gradient_loss</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">dist_class</span><span class="p">,</span> <span class="n">train_batch</span><span class="p">):</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">from_batch</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
    <span class="n">action_dist</span> <span class="o">=</span> <span class="n">dist_class</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
        <span class="n">action_dist</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="n">train_batch</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">])</span> <span class="o">*</span>
        <span class="n">train_batch</span><span class="p">[</span><span class="n">Postprocessing</span><span class="o">.</span><span class="n">ADVANTAGES</span><span class="p">])</span>

<span class="n">MyTFPolicy</span> <span class="o">=</span> <span class="n">build_tf_policy</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;MyTFPolicy&quot;</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">policy_gradient_loss</span><span class="p">,</span>
    <span class="n">postprocess_fn</span><span class="o">=</span><span class="n">postprocess_advantages</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">postprocess_advantages()</span></code> function above uses calls RLlib’s <code class="docutils literal notranslate"><span class="pre">compute_advantages</span></code> function to compute advantages for each timestep. If you re-run the algorithm with this improved policy, you’ll find that it quickly achieves the max reward of 200.</p>
<p>You might be wondering how RLlib makes the advantages placeholder automatically available as <code class="docutils literal notranslate"><span class="pre">train_batch[Postprocessing.ADVANTAGES]</span></code>. When building your policy, RLlib will create a “dummy” trajectory batch where all observations, actions, rewards, etc. are zeros. It then calls your <code class="docutils literal notranslate"><span class="pre">postprocess_fn</span></code>, and generates TF placeholders based on the numpy shapes of the postprocessed batch. RLlib tracks which placeholders that <code class="docutils literal notranslate"><span class="pre">loss_fn</span></code> and <code class="docutils literal notranslate"><span class="pre">stats_fn</span></code> access, and then feeds the corresponding sample data into those placeholders during loss optimization. You can also access these placeholders via <code class="docutils literal notranslate"><span class="pre">policy.get_placeholder(&lt;name&gt;)</span></code> after loss initialization.</p>
<p><strong>Example 1: Proximal Policy Optimization</strong></p>
<p>In the above section you saw how to compose a simple policy gradient algorithm with RLlib.
In this example, we’ll dive into how PPO is defined within RLlib and how you can modify it.
First, check out the <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py">PPO definition</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PPO</span><span class="p">(</span><span class="n">Algorithm</span><span class="p">):</span>
    <span class="nd">@classmethod</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Algorithm</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_default_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AlgorithmConfigDict</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">DEFAULT_CONFIG</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Algorithm</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">validate_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">AlgorithmConfigDict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="o">...</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Algorithm</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_default_policy_class</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">PPOTFPolicy</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Algorithm</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>Besides some boilerplate for defining the PPO configuration and some warnings, the most important method to take note of is the <code class="docutils literal notranslate"><span class="pre">training_step</span></code>.</p>
<p>The algorithm’s <a class="reference external" href="https://docs.ray.io/en/master/rllib/core-concepts.html#training-step-method">training step method</a> defines the distributed training workflow.
Depending on the <code class="docutils literal notranslate"><span class="pre">simple_optimizer</span></code> config setting,
PPO can switch between a simple, synchronous optimizer, or a multi-GPU one that implements
pre-loading of the batch to the GPU for higher performance on repeated minibatch updates utilizing
the same pre-loaded batch:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ResultDict</span><span class="p">:</span>
<span class="c1"># Collect SampleBatches from sample workers until we have a full batch.</span>
<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_by_agent_steps</span><span class="p">:</span>
    <span class="n">train_batch</span> <span class="o">=</span> <span class="n">synchronous_parallel_sample</span><span class="p">(</span>
        <span class="n">worker_set</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">,</span> <span class="n">max_agent_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;train_batch_size&quot;</span><span class="p">]</span>
    <span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">train_batch</span> <span class="o">=</span> <span class="n">synchronous_parallel_sample</span><span class="p">(</span>
        <span class="n">worker_set</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">,</span> <span class="n">max_env_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;train_batch_size&quot;</span><span class="p">]</span>
    <span class="p">)</span>
<span class="n">train_batch</span> <span class="o">=</span> <span class="n">train_batch</span><span class="o">.</span><span class="n">as_multi_agent</span><span class="p">()</span>
<span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">]</span> <span class="o">+=</span> <span class="n">train_batch</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span>
<span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">]</span> <span class="o">+=</span> <span class="n">train_batch</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span>

<span class="c1"># Standardize advantages</span>
<span class="n">train_batch</span> <span class="o">=</span> <span class="n">standardize_fields</span><span class="p">(</span><span class="n">train_batch</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;advantages&quot;</span><span class="p">])</span>
<span class="c1"># Train</span>
<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;simple_optimizer&quot;</span><span class="p">]:</span>
    <span class="n">train_results</span> <span class="o">=</span> <span class="n">train_one_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_batch</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">train_results</span> <span class="o">=</span> <span class="n">multi_gpu_train_one_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_batch</span><span class="p">)</span>

<span class="n">global_vars</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;timestep&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">],</span>
<span class="p">}</span>

<span class="c1"># Update weights - after learning on the local worker - on all remote</span>
<span class="c1"># workers.</span>
<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">remote_workers</span><span class="p">():</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">WORKER_UPDATE_TIMER</span><span class="p">]:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span><span class="n">global_vars</span><span class="o">=</span><span class="n">global_vars</span><span class="p">)</span>

<span class="c1"># For each policy: update KL scale and warn about possible issues</span>
<span class="k">for</span> <span class="n">policy_id</span><span class="p">,</span> <span class="n">policy_info</span> <span class="ow">in</span> <span class="n">train_results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="c1"># Update KL loss with dynamic scaling</span>
    <span class="c1"># for each (possibly multiagent) policy we are training</span>
    <span class="n">kl_divergence</span> <span class="o">=</span> <span class="n">policy_info</span><span class="p">[</span><span class="n">LEARNER_STATS_KEY</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;kl&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span><span class="o">.</span><span class="n">update_kl</span><span class="p">(</span><span class="n">kl_divergence</span><span class="p">)</span>

<span class="c1"># Update global vars on local worker as well.</span>
<span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span><span class="o">.</span><span class="n">set_global_vars</span><span class="p">(</span><span class="n">global_vars</span><span class="p">)</span>

<span class="k">return</span> <span class="n">train_results</span>
</pre></div>
</div>
<p>Now let’s look at each PPO policy definition:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">PPOTFPolicy</span> <span class="o">=</span> <span class="n">build_tf_policy</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;PPOTFPolicy&quot;</span><span class="p">,</span>
    <span class="n">get_default_config</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">rllib</span><span class="o">.</span><span class="n">algorithms</span><span class="o">.</span><span class="n">ppo</span><span class="o">.</span><span class="n">ppo</span><span class="o">.</span><span class="n">PPOConfig</span><span class="p">()</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">ppo_surrogate_loss</span><span class="p">,</span>
    <span class="n">stats_fn</span><span class="o">=</span><span class="n">kl_and_loss_stats</span><span class="p">,</span>
    <span class="n">extra_action_out_fn</span><span class="o">=</span><span class="n">vf_preds_and_logits_fetches</span><span class="p">,</span>
    <span class="n">postprocess_fn</span><span class="o">=</span><span class="n">postprocess_ppo_gae</span><span class="p">,</span>
    <span class="n">gradients_fn</span><span class="o">=</span><span class="n">clip_gradients</span><span class="p">,</span>
    <span class="n">before_loss_init</span><span class="o">=</span><span class="n">setup_mixins</span><span class="p">,</span>
    <span class="n">mixins</span><span class="o">=</span><span class="p">[</span><span class="n">LearningRateSchedule</span><span class="p">,</span> <span class="n">KLCoeffMixin</span><span class="p">,</span> <span class="n">ValueNetworkMixin</span><span class="p">])</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">stats_fn</span></code>: The stats function returns a dictionary of Tensors that will be reported with the training results. This also includes the <code class="docutils literal notranslate"><span class="pre">kl</span></code> metric which is used by the algorithm to adjust the KL penalty. Note that many of the values below reference <code class="docutils literal notranslate"><span class="pre">policy.loss_obj</span></code>, which is assigned by <code class="docutils literal notranslate"><span class="pre">loss_fn</span></code> (not shown here since the PPO loss is quite complex). RLlib will always call <code class="docutils literal notranslate"><span class="pre">stats_fn</span></code> after <code class="docutils literal notranslate"><span class="pre">loss_fn</span></code>, so you can rely on using values saved by <code class="docutils literal notranslate"><span class="pre">loss_fn</span></code> as part of your statistics:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">kl_and_loss_stats</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">train_batch</span><span class="p">):</span>
    <span class="n">policy</span><span class="o">.</span><span class="n">explained_variance</span> <span class="o">=</span> <span class="n">explained_variance</span><span class="p">(</span>
        <span class="n">train_batch</span><span class="p">[</span><span class="n">Postprocessing</span><span class="o">.</span><span class="n">VALUE_TARGETS</span><span class="p">],</span> <span class="n">policy</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">value_function</span><span class="p">())</span>

    <span class="n">stats_fetches</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;cur_kl_coeff&quot;</span><span class="p">:</span> <span class="n">policy</span><span class="o">.</span><span class="n">kl_coeff</span><span class="p">,</span>
        <span class="s2">&quot;cur_lr&quot;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">cur_lr</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">),</span>
        <span class="s2">&quot;total_loss&quot;</span><span class="p">:</span> <span class="n">policy</span><span class="o">.</span><span class="n">loss_obj</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span>
        <span class="s2">&quot;policy_loss&quot;</span><span class="p">:</span> <span class="n">policy</span><span class="o">.</span><span class="n">loss_obj</span><span class="o">.</span><span class="n">mean_policy_loss</span><span class="p">,</span>
        <span class="s2">&quot;vf_loss&quot;</span><span class="p">:</span> <span class="n">policy</span><span class="o">.</span><span class="n">loss_obj</span><span class="o">.</span><span class="n">mean_vf_loss</span><span class="p">,</span>
        <span class="s2">&quot;vf_explained_var&quot;</span><span class="p">:</span> <span class="n">policy</span><span class="o">.</span><span class="n">explained_variance</span><span class="p">,</span>
        <span class="s2">&quot;kl&quot;</span><span class="p">:</span> <span class="n">policy</span><span class="o">.</span><span class="n">loss_obj</span><span class="o">.</span><span class="n">mean_kl</span><span class="p">,</span>
        <span class="s2">&quot;entropy&quot;</span><span class="p">:</span> <span class="n">policy</span><span class="o">.</span><span class="n">loss_obj</span><span class="o">.</span><span class="n">mean_entropy</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">stats_fetches</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">extra_actions_fetches_fn</span></code>: This function defines extra outputs that will be recorded when generating actions with the policy. For example, this enables saving the raw policy logits in the experience batch, which e.g. means it can be referenced in the PPO loss function via <code class="docutils literal notranslate"><span class="pre">batch[BEHAVIOUR_LOGITS]</span></code>. Other values such as the current value prediction can also be emitted for debugging or optimization purposes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">vf_preds_and_logits_fetches</span><span class="p">(</span><span class="n">policy</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="n">SampleBatch</span><span class="o">.</span><span class="n">VF_PREDS</span><span class="p">:</span> <span class="n">policy</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">value_function</span><span class="p">(),</span>
        <span class="n">BEHAVIOUR_LOGITS</span><span class="p">:</span> <span class="n">policy</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">last_output</span><span class="p">(),</span>
    <span class="p">}</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">gradients_fn</span></code>: If defined, this function returns TF gradients for the loss function. You’d typically only want to override this to apply transformations such as gradient clipping:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">clip_gradients</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">policy</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;grad_clip&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">())</span>
        <span class="n">policy</span><span class="o">.</span><span class="n">grads</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_global_norm</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span>
                                                 <span class="n">policy</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;grad_clip&quot;</span><span class="p">])</span>
        <span class="n">clipped_grads</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">grads</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">()))</span>
        <span class="k">return</span> <span class="n">clipped_grads</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">colocate_gradients_with_ops</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">mixins</span></code>: To add arbitrary stateful components, you can add mixin classes to the policy. Methods defined by these mixins will have higher priority than the base policy class, so you can use these to override methods (as in the case of <code class="docutils literal notranslate"><span class="pre">LearningRateSchedule</span></code>), or define extra methods and attributes (e.g., <code class="docutils literal notranslate"><span class="pre">KLCoeffMixin</span></code>, <code class="docutils literal notranslate"><span class="pre">ValueNetworkMixin</span></code>). Like any other Python superclass, these should be initialized at some point, which is what the <code class="docutils literal notranslate"><span class="pre">setup_mixins</span></code> function does:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">setup_mixins</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">obs_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="n">ValueNetworkMixin</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">obs_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
    <span class="n">KLCoeffMixin</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
    <span class="n">LearningRateSchedule</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;lr_schedule&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>In PPO we run <code class="docutils literal notranslate"><span class="pre">setup_mixins</span></code> before the loss function is called (i.e., <code class="docutils literal notranslate"><span class="pre">before_loss_init</span></code>), but other callbacks you can use include <code class="docutils literal notranslate"><span class="pre">before_init</span></code> and <code class="docutils literal notranslate"><span class="pre">after_init</span></code>.</p>
<p><strong>Example 2: Deep Q Networks</strong></p>
<p>Let’s look at how to implement a different family of policies, by looking at the <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/simple_q/simple_q_tf_policy.py">SimpleQ policy definition</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">SimpleQPolicy</span> <span class="o">=</span> <span class="n">build_tf_policy</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;SimpleQPolicy&quot;</span><span class="p">,</span>
    <span class="n">get_default_config</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">rllib</span><span class="o">.</span><span class="n">algorithms</span><span class="o">.</span><span class="n">dqn</span><span class="o">.</span><span class="n">dqn</span><span class="o">.</span><span class="n">DEFAULT_CONFIG</span><span class="p">,</span>
    <span class="n">make_model</span><span class="o">=</span><span class="n">build_q_models</span><span class="p">,</span>
    <span class="n">action_sampler_fn</span><span class="o">=</span><span class="n">build_action_sampler</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">build_q_losses</span><span class="p">,</span>
    <span class="n">extra_action_feed_fn</span><span class="o">=</span><span class="n">exploration_setting_inputs</span><span class="p">,</span>
    <span class="n">extra_action_out_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">policy</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;q_values&quot;</span><span class="p">:</span> <span class="n">policy</span><span class="o">.</span><span class="n">q_values</span><span class="p">},</span>
    <span class="n">extra_learn_fetches_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">policy</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;td_error&quot;</span><span class="p">:</span> <span class="n">policy</span><span class="o">.</span><span class="n">td_error</span><span class="p">},</span>
    <span class="n">before_init</span><span class="o">=</span><span class="n">setup_early_mixins</span><span class="p">,</span>
    <span class="n">after_init</span><span class="o">=</span><span class="n">setup_late_mixins</span><span class="p">,</span>
    <span class="n">obs_include_prev_action_reward</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mixins</span><span class="o">=</span><span class="p">[</span>
        <span class="n">ExplorationStateMixin</span><span class="p">,</span>
        <span class="n">TargetNetworkMixin</span><span class="p">,</span>
    <span class="p">])</span>
</pre></div>
</div>
<p>The biggest difference from the policy gradient policies you saw previously is that SimpleQPolicy defines its own <code class="docutils literal notranslate"><span class="pre">make_model</span></code> and <code class="docutils literal notranslate"><span class="pre">action_sampler_fn</span></code>. This means that the policy builder will not internally create a model and action distribution, rather it will call <code class="docutils literal notranslate"><span class="pre">build_q_models</span></code> and <code class="docutils literal notranslate"><span class="pre">build_action_sampler</span></code> to get the output action tensors.</p>
<p>The model creation function actually creates two different models for DQN: the base Q network, and also a target network. It requires each model to be of type <code class="docutils literal notranslate"><span class="pre">SimpleQModel</span></code>, which implements a <code class="docutils literal notranslate"><span class="pre">get_q_values()</span></code> method. The model catalog will raise an error if you try to use a custom ModelV2 model that isn’t a subclass of SimpleQModel. Similarly, the full DQN policy requires models to subclass <code class="docutils literal notranslate"><span class="pre">DistributionalQModel</span></code>, which implements <code class="docutils literal notranslate"><span class="pre">get_q_value_distributions()</span></code> and <code class="docutils literal notranslate"><span class="pre">get_state_value()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">build_q_models</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">obs_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="o">...</span>

    <span class="n">policy</span><span class="o">.</span><span class="n">q_model</span> <span class="o">=</span> <span class="n">ModelCatalog</span><span class="o">.</span><span class="n">get_model_v2</span><span class="p">(</span>
        <span class="n">obs_space</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">,</span>
        <span class="n">num_outputs</span><span class="p">,</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">],</span>
        <span class="n">framework</span><span class="o">=</span><span class="s2">&quot;tf&quot;</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">Q_SCOPE</span><span class="p">,</span>
        <span class="n">model_interface</span><span class="o">=</span><span class="n">SimpleQModel</span><span class="p">,</span>
        <span class="n">q_hiddens</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;hiddens&quot;</span><span class="p">])</span>

    <span class="n">policy</span><span class="o">.</span><span class="n">target_q_model</span> <span class="o">=</span> <span class="n">ModelCatalog</span><span class="o">.</span><span class="n">get_model_v2</span><span class="p">(</span>
        <span class="n">obs_space</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">,</span>
        <span class="n">num_outputs</span><span class="p">,</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">],</span>
        <span class="n">framework</span><span class="o">=</span><span class="s2">&quot;tf&quot;</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">Q_TARGET_SCOPE</span><span class="p">,</span>
        <span class="n">model_interface</span><span class="o">=</span><span class="n">SimpleQModel</span><span class="p">,</span>
        <span class="n">q_hiddens</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;hiddens&quot;</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">policy</span><span class="o">.</span><span class="n">q_model</span>
</pre></div>
</div>
<p>The action sampler is straightforward, it just takes the q_model, runs a forward pass, and returns the argmax over the actions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">build_action_sampler</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">q_model</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">obs_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">,</span>
                         <span class="n">config</span><span class="p">):</span>
    <span class="c1"># do max over Q values...</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">action</span><span class="p">,</span> <span class="n">action_logp</span>
</pre></div>
</div>
<p>The remainder of DQN is similar to other algorithms. Target updates are handled by a <code class="docutils literal notranslate"><span class="pre">after_optimizer_step</span></code> callback that periodically copies the weights of the Q network to the target.</p>
<p>Finally, note that you do not have to use <code class="docutils literal notranslate"><span class="pre">build_tf_policy</span></code> to define a TensorFlow policy. You can alternatively subclass <code class="docutils literal notranslate"><span class="pre">Policy</span></code>, <code class="docutils literal notranslate"><span class="pre">TFPolicy</span></code>, or <code class="docutils literal notranslate"><span class="pre">DynamicTFPolicy</span></code> as convenient.</p>
</section>
<section id="building-policies-in-tensorflow-eager">
<h2>Building Policies in TensorFlow Eager<a class="headerlink" href="rllib-concepts.html#building-policies-in-tensorflow-eager" title="Permalink to this headline">#</a></h2>
<p>Policies built with <code class="docutils literal notranslate"><span class="pre">build_tf_policy</span></code> (most of the reference algorithms are)
can be run in eager mode by setting
the <code class="docutils literal notranslate"><span class="pre">&quot;framework&quot;:</span> <span class="pre">&quot;tf2&quot;</span></code> / <code class="docutils literal notranslate"><span class="pre">&quot;eager_tracing&quot;:</span> <span class="pre">true</span></code> config options or
using <code class="docutils literal notranslate"><span class="pre">rllib</span> <span class="pre">train</span> <span class="pre">'{&quot;framework&quot;:</span> <span class="pre">&quot;tf2&quot;,</span> <span class="pre">&quot;eager_tracing&quot;:</span> <span class="pre">true}'</span></code>.
This will tell RLlib to execute the model forward pass, action distribution,
loss, and stats functions in eager mode.</p>
<p>Eager mode makes debugging much easier, since you can now use line-by-line
debugging with breakpoints or Python <code class="docutils literal notranslate"><span class="pre">print()</span></code> to inspect
intermediate tensor values.
However, eager can be slower than graph mode unless tracing is enabled.</p>
<p>You can also selectively leverage eager operations within graph mode
execution with <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/py_function">tf.py_function</a>.
Here’s an example of using eager ops embedded
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/eager_execution.py">within a loss function</a>.</p>
</section>
<section id="building-policies-in-pytorch">
<h2>Building Policies in PyTorch<a class="headerlink" href="rllib-concepts.html#building-policies-in-pytorch" title="Permalink to this headline">#</a></h2>
<p>Defining a policy in PyTorch is quite similar to that for TensorFlow (and the process of defining a algorithm given a Torch policy is exactly the same).
Here’s a simple example of a trivial torch policy <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/custom_torch_policy.py">(runnable file here)</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.policy.sample_batch</span> <span class="kn">import</span> <span class="n">SampleBatch</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.torch_policy_template</span> <span class="kn">import</span> <span class="n">build_torch_policy</span>

<span class="k">def</span> <span class="nf">policy_gradient_loss</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">dist_class</span><span class="p">,</span> <span class="n">train_batch</span><span class="p">):</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">from_batch</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
    <span class="n">action_dist</span> <span class="o">=</span> <span class="n">dist_class</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="n">train_batch</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">])</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">train_batch</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">REWARDS</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span>

<span class="c1"># &lt;class &#39;ray.rllib.policy.torch_policy_template.MyTorchPolicy&#39;&gt;</span>
<span class="n">MyTorchPolicy</span> <span class="o">=</span> <span class="n">build_torch_policy</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;MyTorchPolicy&quot;</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">policy_gradient_loss</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, building on the TF examples above, let’s look at how the <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/a3c/a3c_torch_policy.py">A3C torch policy</a> is defined:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A3CTorchPolicy</span> <span class="o">=</span> <span class="n">build_torch_policy</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;A3CTorchPolicy&quot;</span><span class="p">,</span>
    <span class="n">get_default_config</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">rllib</span><span class="o">.</span><span class="n">algorithms</span><span class="o">.</span><span class="n">a3c</span><span class="o">.</span><span class="n">a3c</span><span class="o">.</span><span class="n">DEFAULT_CONFIG</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">actor_critic_loss</span><span class="p">,</span>
    <span class="n">stats_fn</span><span class="o">=</span><span class="n">loss_and_entropy_stats</span><span class="p">,</span>
    <span class="n">postprocess_fn</span><span class="o">=</span><span class="n">add_advantages</span><span class="p">,</span>
    <span class="n">extra_action_out_fn</span><span class="o">=</span><span class="n">model_value_predictions</span><span class="p">,</span>
    <span class="n">extra_grad_process_fn</span><span class="o">=</span><span class="n">apply_grad_clipping</span><span class="p">,</span>
    <span class="n">optimizer_fn</span><span class="o">=</span><span class="n">torch_optimizer</span><span class="p">,</span>
    <span class="n">mixins</span><span class="o">=</span><span class="p">[</span><span class="n">ValueNetworkMixin</span><span class="p">])</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">loss_fn</span></code>: Similar to the TF example, the actor critic loss is defined over <code class="docutils literal notranslate"><span class="pre">batch</span></code>. We imperatively execute the forward pass by calling <code class="docutils literal notranslate"><span class="pre">model()</span></code> on the observations followed by <code class="docutils literal notranslate"><span class="pre">dist_class()</span></code> on the output logits. The output Tensors are saved as attributes of the policy object (e.g., <code class="docutils literal notranslate"><span class="pre">policy.entropy</span> <span class="pre">=</span> <span class="pre">dist.entropy.mean()</span></code>), and we return the scalar loss:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">actor_critic_loss</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">dist_class</span><span class="p">,</span> <span class="n">train_batch</span><span class="p">):</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">from_batch</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">value_function</span><span class="p">()</span>
    <span class="n">action_dist</span> <span class="o">=</span> <span class="n">dist_class</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="n">train_batch</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">])</span>
    <span class="n">policy</span><span class="o">.</span><span class="n">entropy</span> <span class="o">=</span> <span class="n">action_dist</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">overall_err</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">stats_fn</span></code>: The stats function references <code class="docutils literal notranslate"><span class="pre">entropy</span></code>, <code class="docutils literal notranslate"><span class="pre">pi_err</span></code>, and <code class="docutils literal notranslate"><span class="pre">value_err</span></code> saved from the call to the loss function, similar in the PPO TF example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss_and_entropy_stats</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">train_batch</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;policy_entropy&quot;</span><span class="p">:</span> <span class="n">policy</span><span class="o">.</span><span class="n">entropy</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
        <span class="s2">&quot;policy_loss&quot;</span><span class="p">:</span> <span class="n">policy</span><span class="o">.</span><span class="n">pi_err</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
        <span class="s2">&quot;vf_loss&quot;</span><span class="p">:</span> <span class="n">policy</span><span class="o">.</span><span class="n">value_err</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
    <span class="p">}</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">extra_action_out_fn</span></code>: We save value function predictions given model outputs. This makes the value function predictions of the model available in the trajectory as <code class="docutils literal notranslate"><span class="pre">batch[SampleBatch.VF_PREDS]</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model_value_predictions</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">state_batches</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">VF_PREDS</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">value_function</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()}</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">postprocess_fn</span></code> and <code class="docutils literal notranslate"><span class="pre">mixins</span></code>: Similar to the PPO example, we need access to the value function during postprocessing (i.e., <code class="docutils literal notranslate"><span class="pre">add_advantages</span></code> below calls <code class="docutils literal notranslate"><span class="pre">policy._value()</span></code>. The value function is exposed through a mixin class that defines the method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">add_advantages</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span>
                   <span class="n">sample_batch</span><span class="p">,</span>
                   <span class="n">other_agent_batches</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">episode</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">completed</span> <span class="o">=</span> <span class="n">sample_batch</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">DONES</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">completed</span><span class="p">:</span>
        <span class="n">last_r</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">last_r</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">_value</span><span class="p">(</span><span class="n">sample_batch</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">NEXT_OBS</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">compute_advantages</span><span class="p">(</span><span class="n">sample_batch</span><span class="p">,</span> <span class="n">last_r</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;gamma&quot;</span><span class="p">],</span>
                              <span class="n">policy</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;lambda&quot;</span><span class="p">])</span>

<span class="k">class</span> <span class="nc">ValueNetworkMixin</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">lock</span><span class="p">:</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">vf</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">({</span><span class="s2">&quot;obs&quot;</span><span class="p">:</span> <span class="n">obs</span><span class="p">},</span> <span class="p">[])</span>
            <span class="k">return</span> <span class="n">vf</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</pre></div>
</div>
<p>You can find the full policy definition in <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/a3c/a3c_torch_policy.py">a3c_torch_policy.py</a>.</p>
<p>In summary, the main differences between the PyTorch and TensorFlow policy builder functions is that the TF loss and stats functions are built symbolically when the policy is initialized, whereas for PyTorch (or TensorFlow Eager) these functions are called imperatively each time they are used.</p>
</section>
<section id="extending-existing-policies">
<h2>Extending Existing Policies<a class="headerlink" href="rllib-concepts.html#extending-existing-policies" title="Permalink to this headline">#</a></h2>
<p>You can use the <code class="docutils literal notranslate"><span class="pre">with_updates</span></code> method on Trainers and Policy objects built with <code class="docutils literal notranslate"><span class="pre">make_*</span></code> to create a copy of the object with some changes, for example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPO</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo.ppo_tf_policy</span> <span class="kn">import</span> <span class="n">PPOTFPolicy</span>

<span class="n">CustomPolicy</span> <span class="o">=</span> <span class="n">PPOTFPolicy</span><span class="o">.</span><span class="n">with_updates</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;MyCustomPPOTFPolicy&quot;</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">some_custom_loss_fn</span><span class="p">)</span>

<span class="n">CustomTrainer</span> <span class="o">=</span> <span class="n">PPOTrainer</span><span class="o">.</span><span class="n">with_updates</span><span class="p">(</span>
    <span class="n">default_policy</span><span class="o">=</span><span class="n">CustomPolicy</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="rllib-saving-and-loading-algos-and-policies.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Saving and Loading your RL Algorithms and Policies</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="rllib-sample-collection.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Sample Collections and Trajectory Views</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>