
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Working With Offline Data &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/versionwarning.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../_static/js/docsearch.js"></script>
    <script src="../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../_static/js/termynal.js"></script>
    <script defer="defer" src="../_static/js/custom.js"></script>
    <script defer="defer" src="../_static/js/top-navigation.js"></script>
    <script src="../_static/js/tags.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/rllib/rllib-offline.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Catalog (Alpha)" href="rllib-catalogs.html" />
    <link rel="prev" title="Replay Buffers" href="rllib-replay-buffers.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "rllib/rllib-offline", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Ray RLlib
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-training.html">
     Getting Started with RLlib
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="key-concepts.html">
     Key Concepts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../rllib-env.html">
     Environments
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-algorithms.html">
     Algorithms
    </a>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="user-guides.html">
     User Guides
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-advanced-api.html">
       Advanced Python APIs
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-models.html">
       Models, Preprocessors, and Action Distributions
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-saving-and-loading-algos-and-policies.html">
       Saving and Loading your RL Algorithms and Policies
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-concepts.html">
       How To Customize Policies
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-sample-collection.html">
       Sample Collections and Trajectory Views
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-replay-buffers.html">
       Replay Buffers
      </a>
     </li>
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="rllib-offline.html#">
       Working With Offline Data
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-catalogs.html">
       Catalog (Alpha)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-connector.html">
       Connectors (Alpha)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-rlmodule.html">
       RL Modules (Alpha)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-fault-tolerance.html">
       Fault Tolerance And Elastic Training
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-dev.html">
       How To Contribute to RLlib
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-cli.html">
       Working with the RLlib CLI
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-examples.html">
     Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="package_ref/index.html">
     Ray RLlib API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Frllib/rllib-offline.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/rllib/rllib-offline.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/rllib/rllib-offline.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#getting-started">
   Getting started
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#example-training-on-previously-saved-experiences">
   Example: Training on previously saved experiences
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#off-policy-estimation-ope">
   Off-Policy Estimation (OPE)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#example-converting-external-experiences-to-batch-format">
   Example: Converting external experiences to batch format
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#on-policy-algorithms-and-experience-postprocessing">
   On-policy algorithms and experience postprocessing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#mixing-simulation-and-offline-data">
   Mixing simulation and offline data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#scaling-i-o-throughput">
   Scaling I/O throughput
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#ray-data-integration">
   Ray Data Integration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#writing-environment-data">
   Writing Environment Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#input-pipeline-for-supervised-losses">
   Input Pipeline for Supervised Losses
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#input-api">
   Input API
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#example-custom-input-api">
   Example Custom Input API
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#output-api">
   Output API
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Working With Offline Data</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#getting-started">
   Getting started
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#example-training-on-previously-saved-experiences">
   Example: Training on previously saved experiences
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#off-policy-estimation-ope">
   Off-Policy Estimation (OPE)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#example-converting-external-experiences-to-batch-format">
   Example: Converting external experiences to batch format
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#on-policy-algorithms-and-experience-postprocessing">
   On-policy algorithms and experience postprocessing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#mixing-simulation-and-offline-data">
   Mixing simulation and offline data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#scaling-i-o-throughput">
   Scaling I/O throughput
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#ray-data-integration">
   Ray Data Integration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#writing-environment-data">
   Writing Environment Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#input-pipeline-for-supervised-losses">
   Input Pipeline for Supervised Losses
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#input-api">
   Input API
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#example-custom-input-api">
   Example Custom Input API
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-offline.html#output-api">
   Output API
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <a class="reference external image-reference" href="https://ray-docs-promo.netlify.app/rllib"><img alt="" src="https://ray-docs-promo.netlify.app/assets/img/rllib/top.png" /></a>
<div class="bottom-right-promo-banner docutils">
<a class="reference external image-reference" href="https://ray-docs-promo.netlify.app/rllib"><img alt="" src="https://ray-docs-promo.netlify.app/assets/img/rllib/square.png" /></a>
</div>
<section id="working-with-offline-data">
<h1>Working With Offline Data<a class="headerlink" href="rllib-offline.html#working-with-offline-data" title="Permalink to this headline">#</a></h1>
<section id="getting-started">
<h2>Getting started<a class="headerlink" href="rllib-offline.html#getting-started" title="Permalink to this headline">#</a></h2>
<p>RLlib’s offline dataset APIs enable working with experiences read from offline storage (e.g., disk, cloud storage, streaming systems, HDFS). For example, you might want to read experiences saved from previous training runs, or gathered from policies deployed in <a class="reference external" href="https://arxiv.org/abs/1811.00260">web applications</a>. You can also log new agent experiences produced during online training for future use.</p>
<p>RLlib represents trajectory sequences (i.e., <code class="docutils literal notranslate"><span class="pre">(s,</span> <span class="pre">a,</span> <span class="pre">r,</span> <span class="pre">s',</span> <span class="pre">...)</span></code> tuples) with <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py">SampleBatch</a> objects. Using a batch format enables efficient encoding and compression of experiences. During online training, RLlib uses <a class="reference external" href="rllib-concepts.html#policy-evaluation">policy evaluation</a> actors to generate batches of experiences in parallel using the current policy. RLlib also uses this same batch format for reading and writing experiences to offline storage.</p>
</section>
<section id="example-training-on-previously-saved-experiences">
<h2>Example: Training on previously saved experiences<a class="headerlink" href="rllib-offline.html#example-training-on-previously-saved-experiences" title="Permalink to this headline">#</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For custom models and enviroments, you’ll need to use the <a class="reference external" href="rllib-training.html#basic-python-api">Python API</a>.</p>
</div>
<p>In this example, we will save batches of experiences generated during online training to disk, and then leverage this saved data to train a policy offline using DQN. First, we run a simple policy gradient algorithm for 100k steps with <code class="docutils literal notranslate"><span class="pre">&quot;output&quot;:</span> <span class="pre">&quot;/tmp/cartpole-out&quot;</span></code> to tell RLlib to write simulation outputs to the <code class="docutils literal notranslate"><span class="pre">/tmp/cartpole-out</span></code> directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ rllib train <span class="se">\</span>
    --run<span class="o">=</span>PG <span class="se">\</span>
    --env<span class="o">=</span>CartPole-v1 <span class="se">\</span>
    --config<span class="o">=</span><span class="s1">&#39;{&quot;output&quot;: &quot;/tmp/cartpole-out&quot;, &quot;output_max_file_size&quot;: 5000000}&#39;</span> <span class="se">\</span>
    --stop<span class="o">=</span><span class="s1">&#39;{&quot;timesteps_total&quot;: 100000}&#39;</span>
</pre></div>
</div>
<p>The experiences will be saved in compressed JSON batch format:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ ls -l /tmp/cartpole-out
total 11636
-rw-rw-r-- 1 eric eric 5022257 output-2019-01-01_15-58-57_worker-0_0.json
-rw-rw-r-- 1 eric eric 5002416 output-2019-01-01_15-59-22_worker-0_1.json
-rw-rw-r-- 1 eric eric 1881666 output-2019-01-01_15-59-47_worker-0_2.json
</pre></div>
</div>
<p>Then, we can tell DQN to train using these previously generated experiences with <code class="docutils literal notranslate"><span class="pre">&quot;input&quot;:</span> <span class="pre">&quot;/tmp/cartpole-out&quot;</span></code>. We disable exploration since it has no effect on the input:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ rllib train <span class="se">\</span>
    --run<span class="o">=</span>DQN <span class="se">\</span>
    --env<span class="o">=</span>CartPole-v1 <span class="se">\</span>
    --config<span class="o">=</span><span class="s1">&#39;{</span>
<span class="s1">        &quot;input&quot;: &quot;/tmp/cartpole-out&quot;,</span>
<span class="s1">        &quot;explore&quot;: false}&#39;</span>
</pre></div>
</div>
</section>
<section id="off-policy-estimation-ope">
<h2>Off-Policy Estimation (OPE)<a class="headerlink" href="rllib-offline.html#off-policy-estimation-ope" title="Permalink to this headline">#</a></h2>
<p>In practice, when training on offline data, it is usually not straightforward to evaluate the trained policies using a simulator as in online RL. For example, in recommeder systems, rolling out a policy trained on offline data in a real-world environment can jeopardize your business if the policy is suboptimal. For these situations we can use <a class="reference external" href="https://arxiv.org/abs/1911.06854">off-policy estimation</a> methods which avoid the risk of evaluating a possibly sub-optimal policy in a real-world environment.</p>
<p>With RLlib’s evaluation framework you can:</p>
<ul class="simple">
<li><p>Evaluate policies on a simulated environement, if available, using <code class="docutils literal notranslate"><span class="pre">evaluation_config[&quot;input&quot;]</span> <span class="pre">=</span> <span class="pre">&quot;sampler&quot;</span></code>. You can then monitor your policy’s performance on tensorboard as it is getting trained (by using <code class="docutils literal notranslate"><span class="pre">tensorboard</span> <span class="pre">--logdir=~/ray_results</span></code>).</p></li>
<li><p>Use RLlib’s off-policy estimation methods, which estimate the policy’s performance on a separate offline dataset. To be able to use this feature, the evaluation dataset should contain <code class="docutils literal notranslate"><span class="pre">action_prob</span></code> key that represents the action probability distribution of the collected data so that we can do counterfactual evaluation.</p></li>
</ul>
<p>RLlib supports the following off-policy estimators:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/offline/estimators/importance_sampling.py">Importance Sampling (IS)</a></p></li>
<li><p><a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/offline/estimators/weighted_importance_sampling.py">Weighted Importance Sampling (WIS)</a></p></li>
<li><p><a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/offline/estimators/direct_method.py">Direct Method (DM)</a></p></li>
<li><p><a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/offline/estimators/doubly_robust.py">Doubly Robust (DR)</a></p></li>
</ul>
<p>IS and WIS compute the ratio between the action probabilities under the behavior policy (from the dataset) and the target policy (the policy under evaluation), and use this ratio to estimate the policy’s return. More details on this can be found in their respective papers.</p>
<p>DM and DR train a Q-model to compute the estimated return. By default, RLlib uses <a class="reference external" href="https://arxiv.org/abs/1911.06854">Fitted-Q Evaluation (FQE)</a> to train the Q-model. See <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/offline/estimators/fqe_torch_model.py">fqe_torch_model.py</a> for more details.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For a contextual bandit dataset, the <code class="docutils literal notranslate"><span class="pre">dones</span></code> key should always be set to <code class="docutils literal notranslate"><span class="pre">True</span></code>. In this case, FQE reduces to fitting a reward model to the data.</p>
</div>
<p>RLlib’s OPE estimators output six metrics:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">v_behavior</span></code>: The discounted sum over rewards in the offline episode, averaged over episodes in the batch.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">v_behavior_std</span></code>: The standard deviation corresponding to v_behavior.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">v_target</span></code>: The OPE’s estimated discounted return for the target policy, averaged over episodes in the batch.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">v_target_std</span></code>: The standard deviation corresponding to v_target.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">v_gain</span></code>: <code class="docutils literal notranslate"><span class="pre">v_target</span> <span class="pre">/</span> <span class="pre">max(v_behavior,</span> <span class="pre">1e-8)</span></code>. <code class="docutils literal notranslate"><span class="pre">v_gain</span> <span class="pre">&gt;</span> <span class="pre">1.0</span></code> indicates that the policy is better than the policy that generated the behavior data. In case, <code class="docutils literal notranslate"><span class="pre">v_behavior</span> <span class="pre">&lt;=</span> <span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">v_delta</span></code> should be used instead for comparison.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">v_delta</span></code>: The difference between v_target and v_behavior.</p></li>
</ul>
<p>As an example, we generate an evaluation dataset for off-policy estimation:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ rllib train <span class="se">\</span>
    --run<span class="o">=</span>PG <span class="se">\</span>
    --env<span class="o">=</span>CartPole-v1 <span class="se">\</span>
    --config<span class="o">=</span><span class="s1">&#39;{&quot;output&quot;: &quot;/tmp/cartpole-eval&quot;, &quot;output_max_file_size&quot;: 5000000}&#39;</span> <span class="se">\</span>
    --stop<span class="o">=</span><span class="s1">&#39;{&quot;timesteps_total&quot;: 10000}&#39;</span>
</pre></div>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>You should use separate datasets for algorithm training and OPE, as shown here.</p>
</div>
<p>We can now train a DQN algorithm offline and evaluate it using OPE:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.dqn</span> <span class="kn">import</span> <span class="n">DQNConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline.estimators</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ImportanceSampling</span><span class="p">,</span>
    <span class="n">WeightedImportanceSampling</span><span class="p">,</span>
    <span class="n">DirectMethod</span><span class="p">,</span>
    <span class="n">DoublyRobust</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline.estimators.fqe_torch_model</span> <span class="kn">import</span> <span class="n">FQETorchModel</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">DQNConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">framework</span><span class="p">(</span><span class="s2">&quot;torch&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span><span class="n">input_</span><span class="o">=</span><span class="s2">&quot;/tmp/cartpole-out&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">evaluation</span><span class="p">(</span>
        <span class="n">evaluation_interval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">evaluation_duration</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">evaluation_num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">evaluation_duration_unit</span><span class="o">=</span><span class="s2">&quot;episodes&quot;</span><span class="p">,</span>
        <span class="n">evaluation_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;/tmp/cartpole-eval&quot;</span><span class="p">},</span>
        <span class="n">off_policy_estimation_methods</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;is&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">ImportanceSampling</span><span class="p">},</span>
            <span class="s2">&quot;wis&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">WeightedImportanceSampling</span><span class="p">},</span>
            <span class="s2">&quot;dm_fqe&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">DirectMethod</span><span class="p">,</span>
                <span class="s2">&quot;q_model_config&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">FQETorchModel</span><span class="p">,</span> <span class="s2">&quot;polyak_coef&quot;</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">},</span>
            <span class="p">},</span>
            <span class="s2">&quot;dr_fqe&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">DoublyRobust</span><span class="p">,</span>
                <span class="s2">&quot;q_model_config&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">FQETorchModel</span><span class="p">,</span> <span class="s2">&quot;polyak_coef&quot;</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">},</span>
            <span class="p">},</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../_images/rllib-offline.png" src="../_images/rllib-offline.png" />
<p><strong>Estimator Python API:</strong> For greater control over the evaluation process, you can create off-policy estimators in your Python code and call <code class="docutils literal notranslate"><span class="pre">estimator.train(batch)</span></code> to perform any neccessary training and <code class="docutils literal notranslate"><span class="pre">estimator.estimate(batch)</span></code> to perform counterfactual estimation. The estimators take in an RLlib Policy object and gamma value for the environment, along with additional estimator-specific arguments (e.g. <code class="docutils literal notranslate"><span class="pre">q_model_config</span></code> for DM and DR). You can take a look at the example config parameters of the q_model_config <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/offline/estimators/fqe_torch_model.py">here</a>. You can also write your own off-policy estimator by subclassing from the <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/offline/estimators/off_policy_estimator.py">OffPolicyEstimator</a> base class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">algo</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="o">...</span>  <span class="c1"># train policy offline</span>

<span class="kn">from</span> <span class="nn">ray.rllib.offline.json_reader</span> <span class="kn">import</span> <span class="n">JsonReader</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline.estimators</span> <span class="kn">import</span> <span class="n">DoublyRobust</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline.estimators.fqe_torch_model</span> <span class="kn">import</span> <span class="n">FQETorchModel</span>

<span class="n">estimator</span> <span class="o">=</span> <span class="n">DoublyRobust</span><span class="p">(</span>
    <span class="n">policy</span><span class="o">=</span><span class="n">algo</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(),</span>
    <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
    <span class="n">q_model_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">FQETorchModel</span><span class="p">,</span> <span class="s2">&quot;n_iters&quot;</span><span class="p">:</span> <span class="mi">160</span><span class="p">},</span>
<span class="p">)</span>

<span class="c1"># Train estimator&#39;s Q-model; only required for DM and DR estimators</span>
<span class="n">reader</span> <span class="o">=</span> <span class="n">JsonReader</span><span class="p">(</span><span class="s2">&quot;/tmp/cartpole-out&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
    <span class="c1"># {&#39;loss&#39;: ...}</span>

<span class="n">reader</span> <span class="o">=</span> <span class="n">JsonReader</span><span class="p">(</span><span class="s2">&quot;/tmp/cartpole-eval&quot;</span><span class="p">)</span>
<span class="c1"># Compute off-policy estimates</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">estimate</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
    <span class="c1"># {&#39;v_behavior&#39;: ..., &#39;v_target&#39;: ..., &#39;v_gain&#39;: ...,</span>
    <span class="c1"># &#39;v_behavior_std&#39;: ..., &#39;v_target_std&#39;: ..., &#39;v_delta&#39;: ...}</span>
</pre></div>
</div>
</section>
<section id="example-converting-external-experiences-to-batch-format">
<h2>Example: Converting external experiences to batch format<a class="headerlink" href="rllib-offline.html#example-converting-external-experiences-to-batch-format" title="Permalink to this headline">#</a></h2>
<p>When the env does not support simulation (e.g., it is a web application), it is necessary to generate the <code class="docutils literal notranslate"><span class="pre">*.json</span></code> experience batch files outside of RLlib. This can be done by using the <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/offline/json_writer.py">JsonWriter</a> class to write out batches.
This <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/saving_experiences.py">runnable example</a> shows how to generate and save experience batches for CartPole-v1 to disk:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">ray._private.utils</span>

<span class="kn">from</span> <span class="nn">ray.rllib.models.preprocessors</span> <span class="kn">import</span> <span class="n">get_preprocessor</span>
<span class="kn">from</span> <span class="nn">ray.rllib.evaluation.sample_batch_builder</span> <span class="kn">import</span> <span class="n">SampleBatchBuilder</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline.json_writer</span> <span class="kn">import</span> <span class="n">JsonWriter</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">batch_builder</span> <span class="o">=</span> <span class="n">SampleBatchBuilder</span><span class="p">()</span>  <span class="c1"># or MultiAgentSampleBatchBuilder</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">JsonWriter</span><span class="p">(</span>
        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ray</span><span class="o">.</span><span class="n">_private</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_user_temp_dir</span><span class="p">(),</span> <span class="s2">&quot;demo-out&quot;</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># You normally wouldn&#39;t want to manually create sample batches if a</span>
    <span class="c1"># simulator is available, but let&#39;s do it anyways for example purposes:</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>

    <span class="c1"># RLlib uses preprocessors to implement transforms such as one-hot encoding</span>
    <span class="c1"># and flattening of tuple and dict observations. For CartPole a no-op</span>
    <span class="c1"># preprocessor is used, but this may be relevant for more complex envs.</span>
    <span class="n">prep</span> <span class="o">=</span> <span class="n">get_preprocessor</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">)(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The preprocessor is&quot;</span><span class="p">,</span> <span class="n">prep</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">eps_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">obs</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">prev_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">())</span>
        <span class="n">prev_reward</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">terminated</span> <span class="o">=</span> <span class="n">truncated</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">truncated</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="n">new_obs</span><span class="p">,</span> <span class="n">rew</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">batch_builder</span><span class="o">.</span><span class="n">add_values</span><span class="p">(</span>
                <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span>
                <span class="n">eps_id</span><span class="o">=</span><span class="n">eps_id</span><span class="p">,</span>
                <span class="n">agent_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">obs</span><span class="o">=</span><span class="n">prep</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">obs</span><span class="p">),</span>
                <span class="n">actions</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
                <span class="n">action_prob</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>  <span class="c1"># put the true action probability here</span>
                <span class="n">action_logp</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                <span class="n">rewards</span><span class="o">=</span><span class="n">rew</span><span class="p">,</span>
                <span class="n">prev_actions</span><span class="o">=</span><span class="n">prev_action</span><span class="p">,</span>
                <span class="n">prev_rewards</span><span class="o">=</span><span class="n">prev_reward</span><span class="p">,</span>
                <span class="n">terminateds</span><span class="o">=</span><span class="n">terminated</span><span class="p">,</span>
                <span class="n">truncateds</span><span class="o">=</span><span class="n">truncated</span><span class="p">,</span>
                <span class="n">infos</span><span class="o">=</span><span class="n">info</span><span class="p">,</span>
                <span class="n">new_obs</span><span class="o">=</span><span class="n">prep</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">new_obs</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">new_obs</span>
            <span class="n">prev_action</span> <span class="o">=</span> <span class="n">action</span>
            <span class="n">prev_reward</span> <span class="o">=</span> <span class="n">rew</span>
            <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">batch_builder</span><span class="o">.</span><span class="n">build_and_reset</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section id="on-policy-algorithms-and-experience-postprocessing">
<h2>On-policy algorithms and experience postprocessing<a class="headerlink" href="rllib-offline.html#on-policy-algorithms-and-experience-postprocessing" title="Permalink to this headline">#</a></h2>
<p>RLlib assumes that input batches are of
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/policy/policy.py#L434">postprocessed experiences</a>.
This isn’t typically critical for off-policy algorithms
(e.g., DQN’s <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/dqn/dqn_tf_policy.py#L434">post-processing</a>
is only needed if <code class="docutils literal notranslate"><span class="pre">n_step</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> or <code class="docutils literal notranslate"><span class="pre">replay_buffer_config.worker_side_prioritization:</span> <span class="pre">True</span></code>).
For off-policy algorithms, you can also safely set the <code class="docutils literal notranslate"><span class="pre">postprocess_inputs:</span> <span class="pre">True</span></code> config to auto-postprocess data.</p>
<p>However, for on-policy algorithms like PPO, you’ll need to pass in the extra values added during policy evaluation and postprocessing to <code class="docutils literal notranslate"><span class="pre">batch_builder.add_values()</span></code>, e.g., <code class="docutils literal notranslate"><span class="pre">logits</span></code>, <code class="docutils literal notranslate"><span class="pre">vf_preds</span></code>, <code class="docutils literal notranslate"><span class="pre">value_target</span></code>, and <code class="docutils literal notranslate"><span class="pre">advantages</span></code> for PPO. This is needed since the calculation of these values depends on the parameters of the <em>behaviour</em> policy, which RLlib does not have access to in the offline setting (in online training, these values are automatically added during policy evaluation).</p>
<p>Note that for on-policy algorithms, you’ll also have to throw away experiences generated by prior versions of the policy. This greatly reduces sample efficiency, which is typically undesirable for offline training, but can make sense for certain applications.</p>
</section>
<section id="mixing-simulation-and-offline-data">
<h2>Mixing simulation and offline data<a class="headerlink" href="rllib-offline.html#mixing-simulation-and-offline-data" title="Permalink to this headline">#</a></h2>
<p>RLlib supports multiplexing inputs from multiple input sources, including simulation. For example, in the following example we read 40% of our experiences from <code class="docutils literal notranslate"><span class="pre">/tmp/cartpole-out</span></code>, 30% from <code class="docutils literal notranslate"><span class="pre">hdfs:/archive/cartpole</span></code>, and the last 30% is produced via policy evaluation. Input sources are multiplexed using <a class="reference external" href="https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.choice.html">np.random.choice</a>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ rllib train <span class="se">\</span>
    --run<span class="o">=</span>DQN <span class="se">\</span>
    --env<span class="o">=</span>CartPole-v1 <span class="se">\</span>
    --config<span class="o">=</span><span class="s1">&#39;{</span>
<span class="s1">        &quot;input&quot;: {</span>
<span class="s1">            &quot;/tmp/cartpole-out&quot;: 0.4,</span>
<span class="s1">            &quot;hdfs:/archive/cartpole&quot;: 0.3,</span>
<span class="s1">            &quot;sampler&quot;: 0.3,</span>
<span class="s1">        },</span>
<span class="s1">        &quot;explore&quot;: false}&#39;</span>
</pre></div>
</div>
</section>
<section id="scaling-i-o-throughput">
<h2>Scaling I/O throughput<a class="headerlink" href="rllib-offline.html#scaling-i-o-throughput" title="Permalink to this headline">#</a></h2>
<p>Similar to scaling online training, you can scale offline I/O throughput by increasing the number of RLlib workers via the <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> config. Each worker accesses offline storage independently in parallel, for linear scaling of I/O throughput. Within each read worker, files are chosen in random order for reads, but file contents are read sequentially.</p>
</section>
<section id="ray-data-integration">
<h2>Ray Data Integration<a class="headerlink" href="rllib-offline.html#ray-data-integration" title="Permalink to this headline">#</a></h2>
<p>RLlib has experimental support for reading/writing training samples from/to large offline datasets using
<a class="reference internal" href="../data/data.html#data"><span class="std std-ref">Ray Data</span></a>.
We support JSON and Parquet files today. Other file formats supported by Ray Data can also be easily added.</p>
<p>Unlike JSON input, a single dataset can be automatically sharded and replayed by multiple rollout workers
by simply specifying the desired num_workers config.</p>
<p>To load sample data using Dataset, specify input and input_config keys like the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="o">...</span>
    <span class="s2">&quot;input&quot;</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
    <span class="s2">&quot;input_config&quot;</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;json&quot;</span><span class="p">,</span>  <span class="c1"># json or parquet</span>
        <span class="c1"># Path to data file or directory.</span>
        <span class="s2">&quot;path&quot;</span><span class="p">:</span> <span class="s2">&quot;/path/to/json_dir/&quot;</span><span class="p">,</span>
        <span class="c1"># Num of tasks reading dataset in parallel, default is num_workers.</span>
        <span class="s2">&quot;parallelism&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="c1"># Dataset allocates 0.5 CPU for each reader by default.</span>
        <span class="c1"># Adjust this value based on the size of your offline dataset.</span>
        <span class="s2">&quot;num_cpus_per_read_task&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="o">...</span>
<span class="p">}</span>
</pre></div>
</div>
<p>To write sample data to JSON or Parquet files using Dataset, specify output and output_config keys like the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
    <span class="s2">&quot;output_config&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;json&quot;</span><span class="p">,</span>  <span class="c1"># json or parquet</span>
        <span class="c1"># Directory to write data files.</span>
        <span class="s2">&quot;path&quot;</span><span class="p">:</span> <span class="s2">&quot;/tmp/test_samples/&quot;</span><span class="p">,</span>
        <span class="c1"># Break samples into multiple files, each containing about this many records.</span>
        <span class="s2">&quot;max_num_samples_per_file&quot;</span><span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="writing-environment-data">
<h2>Writing Environment Data<a class="headerlink" href="rllib-offline.html#writing-environment-data" title="Permalink to this headline">#</a></h2>
<p>To include environment data in the training sample datasets you can use the optional
<code class="docutils literal notranslate"><span class="pre">store_infos</span></code> parameter that is part of the <code class="docutils literal notranslate"><span class="pre">output_config</span></code> dictionary. This parameter
ensures that the <code class="docutils literal notranslate"><span class="pre">infos</span></code> dictionary, as returned by the RL environment, is included in the output files.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is the responsibility of the user to ensure that the content of <code class="docutils literal notranslate"><span class="pre">infos</span></code> can be serialized to file.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This setting is only relevant for the TensorFlow based agents, for PyTorch agents the <code class="docutils literal notranslate"><span class="pre">infos</span></code> data is always stored.</p>
</div>
<p>To write the <code class="docutils literal notranslate"><span class="pre">infos</span></code> data to JSON or Parquet files using Dataset, specify output and output_config keys like the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
    <span class="s2">&quot;output_config&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;json&quot;</span><span class="p">,</span>  <span class="c1"># json or parquet</span>
        <span class="c1"># Directory to write data files.</span>
        <span class="s2">&quot;path&quot;</span><span class="p">:</span> <span class="s2">&quot;/tmp/test_samples/&quot;</span><span class="p">,</span>
        <span class="c1"># Write the infos dict data</span>
        <span class="s2">&quot;store_infos&quot;</span> <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="input-pipeline-for-supervised-losses">
<h2>Input Pipeline for Supervised Losses<a class="headerlink" href="rllib-offline.html#input-pipeline-for-supervised-losses" title="Permalink to this headline">#</a></h2>
<p>You can also define supervised model losses over offline data. This requires defining a <a class="reference external" href="rllib-models.html#supervised-model-losses">custom model loss</a>. We provide a convenience function, <code class="docutils literal notranslate"><span class="pre">InputReader.tf_input_ops()</span></code>, that can be used to convert any input reader to a TF input pipeline. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">custom_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy_loss</span><span class="p">):</span>
    <span class="n">input_reader</span> <span class="o">=</span> <span class="n">JsonReader</span><span class="p">(</span><span class="s2">&quot;/tmp/cartpole-out&quot;</span><span class="p">)</span>
    <span class="c1"># print(input_reader.next())  # if you want to access imperatively</span>

    <span class="n">input_ops</span> <span class="o">=</span> <span class="n">input_reader</span><span class="o">.</span><span class="n">tf_input_ops</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">input_ops</span><span class="p">[</span><span class="s2">&quot;obs&quot;</span><span class="p">])</span>  <span class="c1"># -&gt; output Tensor shape=[None, 4]</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">input_ops</span><span class="p">[</span><span class="s2">&quot;actions&quot;</span><span class="p">])</span>  <span class="c1"># -&gt; output Tensor shape=[None]</span>

    <span class="n">supervised_loss</span> <span class="o">=</span> <span class="n">some_function_of</span><span class="p">(</span><span class="n">input_ops</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">policy_loss</span> <span class="o">+</span> <span class="n">supervised_loss</span>
</pre></div>
</div>
<p>See <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/custom_model_loss_and_metrics.py">custom_model_loss_and_metrics.py</a> for a runnable example of using these TF input ops in a custom loss.</p>
</section>
<section id="input-api">
<h2>Input API<a class="headerlink" href="rllib-offline.html#input-api" title="Permalink to this headline">#</a></h2>
<p>You can configure experience input for an agent using the following options:</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Plain python config dicts will soon be replaced by <a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlgorithmConfig</span></code></a>
objects, which have the advantage of being type safe, allowing users to set different config settings within
meaningful sub-categories (e.g. <code class="docutils literal notranslate"><span class="pre">my_config.offline_data(input_=[xyz])</span></code>), and offer the ability to
construct an Algorithm instance from these config objects (via their <code class="docutils literal notranslate"><span class="pre">.build()</span></code> method).</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Specify how to generate experiences:</span>
<span class="c1">#  - &quot;sampler&quot;: Generate experiences via online (env) simulation (default).</span>
<span class="c1">#  - A local directory or file glob expression (e.g., &quot;/tmp/*.json&quot;).</span>
<span class="c1">#  - A list of individual file paths/URIs (e.g., [&quot;/tmp/1.json&quot;,</span>
<span class="c1">#    &quot;s3://bucket/2.json&quot;]).</span>
<span class="c1">#  - A dict with string keys and sampling probabilities as values (e.g.,</span>
<span class="c1">#    {&quot;sampler&quot;: 0.4, &quot;/tmp/*.json&quot;: 0.4, &quot;s3://bucket/expert.json&quot;: 0.2}).</span>
<span class="c1">#  - A callable that takes an `IOContext` object as only arg and returns a</span>
<span class="c1">#    ray.rllib.offline.InputReader.</span>
<span class="c1">#  - A string key that indexes a callable with tune.registry.register_input</span>
<span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;sampler&quot;</span><span class="p">,</span>
<span class="c1"># Arguments accessible from the IOContext for configuring custom input</span>
<span class="s2">&quot;input_config&quot;</span><span class="p">:</span> <span class="p">{},</span>
<span class="c1"># True, if the actions in a given offline &quot;input&quot; are already normalized</span>
<span class="c1"># (between -1.0 and 1.0). This is usually the case when the offline</span>
<span class="c1"># file has been generated by another RLlib algorithm (e.g. PPO or SAC),</span>
<span class="c1"># while &quot;normalize_actions&quot; was set to True.</span>
<span class="s2">&quot;actions_in_input_normalized&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span class="c1"># Specify how to evaluate the current policy. This only has an effect when</span>
<span class="c1"># reading offline experiences (&quot;input&quot; is not &quot;sampler&quot;).</span>
<span class="c1"># Available options:</span>
<span class="c1">#  - &quot;simulation&quot;: Run the environment in the background, but use</span>
<span class="c1">#    this data for evaluation only and not for learning.</span>
<span class="c1">#  - Any subclass of OffPolicyEstimator, e.g.</span>
<span class="c1">#    ray.rllib.offline.estimators.is::ImportanceSampling or your own custom</span>
<span class="c1">#    subclass.</span>
<span class="s2">&quot;off_policy_estimation_methods&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="s2">&quot;is&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">ImportanceSampling</span><span class="p">},</span>
    <span class="s2">&quot;wis&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">WeightedImportanceSampling</span><span class="p">}</span>
<span class="p">},</span>
<span class="c1"># Whether to run postprocess_trajectory() on the trajectory fragments from</span>
<span class="c1"># offline inputs. Note that postprocessing will be done using the *current*</span>
<span class="c1"># policy, not the *behavior* policy, which is typically undesirable for</span>
<span class="c1"># on-policy algorithms.</span>
<span class="s2">&quot;postprocess_inputs&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span class="c1"># If positive, input batches will be shuffled via a sliding window buffer</span>
<span class="c1"># of this number of batches. Use this if the input data is not in random</span>
<span class="c1"># enough order. Input is delayed until the shuffle buffer is filled.</span>
<span class="s2">&quot;shuffle_buffer_size&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
</pre></div>
</div>
<p>The interface for a custom input reader is as follows:</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.offline.</span></span><span class="sig-name descname"><span class="pre">InputReader</span></span><a class="reference internal" href="../_modules/ray/rllib/offline/input_reader.html#InputReader"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>API for collecting and returning experiences during policy evaluation.</p>
<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">next</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">SampleBatch</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">MultiAgentBatch</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ray/rllib/offline/input_reader.html#InputReader.next"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Returns the next batch of read experiences.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The experience read (SampleBatch or MultiAgentBatch).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">tf_input_ops</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">queue_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ray/rllib/offline/input_reader.html#InputReader.tf_input_ops"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Returns TensorFlow queue ops for reading inputs from this reader.</p>
<p>The main use of these ops is for integration into custom model losses.
For example, you can use tf_input_ops() to read from files of external
experiences to add an imitation learning loss to your model.</p>
<p>This method creates a queue runner thread that will call next() on this
reader repeatedly to feed the TensorFlow queue.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>queue_size</strong> – Max elements to allow in the TF queue.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.models.modelv2</span> <span class="kn">import</span> <span class="n">ModelV2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.offline.json_reader</span> <span class="kn">import</span> <span class="n">JsonReader</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">imitation_loss</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># doctest +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">ModelV2</span><span class="p">):</span> <span class="c1"># doctest +SKIP</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">custom_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy_loss</span><span class="p">,</span> <span class="n">loss_inputs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">reader</span> <span class="o">=</span> <span class="n">JsonReader</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">input_ops</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">tf_input_ops</span><span class="p">()</span>
<span class="gp">... </span>        <span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_layers_v2</span><span class="p">(</span>
<span class="gp">... </span>            <span class="p">{</span><span class="s2">&quot;obs&quot;</span><span class="p">:</span> <span class="n">input_ops</span><span class="p">[</span><span class="s2">&quot;obs&quot;</span><span class="p">]},</span>
<span class="gp">... </span>            <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">il_loss</span> <span class="o">=</span> <span class="n">imitation_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">input_ops</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">])</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">policy_loss</span> <span class="o">+</span> <span class="n">il_loss</span>
</pre></div>
</div>
<p>You can find a runnable version of this in examples/custom_loss.py.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Dict of Tensors, one for each column of the read SampleBatch.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="example-custom-input-api">
<h2>Example Custom Input API<a class="headerlink" href="rllib-offline.html#example-custom-input-api" title="Permalink to this headline">#</a></h2>
<p>You can create a custom input reader like the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.offline</span> <span class="kn">import</span> <span class="n">InputReader</span><span class="p">,</span> <span class="n">IOContext</span><span class="p">,</span> <span class="n">ShuffledInput</span>
<span class="kn">from</span> <span class="nn">ray.tune.registry</span> <span class="kn">import</span> <span class="n">register_input</span>

<span class="k">class</span> <span class="nc">CustomInputReader</span><span class="p">(</span><span class="n">InputReader</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ioctx</span><span class="p">:</span> <span class="n">IOContext</span><span class="p">):</span> <span class="o">...</span>
    <span class="k">def</span> <span class="nf">next</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="o">...</span>

<span class="k">def</span> <span class="nf">input_creator</span><span class="p">(</span><span class="n">ioctx</span><span class="p">:</span> <span class="n">IOContext</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputReader</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">ShuffledInput</span><span class="p">(</span><span class="n">CustomInputReader</span><span class="p">(</span><span class="n">ioctx</span><span class="p">))</span>

<span class="n">register_input</span><span class="p">(</span><span class="s2">&quot;custom_input&quot;</span><span class="p">,</span> <span class="n">input_creator</span><span class="p">)</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;custom_input&quot;</span><span class="p">,</span>
    <span class="s2">&quot;input_config&quot;</span><span class="p">:</span> <span class="p">{},</span>
    <span class="o">...</span>
<span class="p">}</span>
</pre></div>
</div>
<p>You can pass arguments from the config to the custom input api through the
<code class="docutils literal notranslate"><span class="pre">input_config</span></code> option which can be accessed with the <code class="docutils literal notranslate"><span class="pre">IOContext</span></code>.
The interface for the <code class="docutils literal notranslate"><span class="pre">IOContext</span></code> is the following:</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.offline.</span></span><span class="sig-name descname"><span class="pre">IOContext</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">log_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">AlgorithmConfig</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">RolloutWorker</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/offline/io_context.html#IOContext"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Class containing attributes to pass to input/output class constructors.</p>
<p>RLlib auto-sets these attributes when constructing input/output classes,
such as InputReaders and OutputWriters.</p>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">default_sampler_input</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">SamplerInput</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ray/rllib/offline/io_context.html#IOContext.default_sampler_input"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Returns the RolloutWorker’s SamplerInput object, if any.</p>
<p>Returns None if the RolloutWorker has no SamplerInput. Note that local
workers in case there are also one or more remote workers by default
do not create a SamplerInput object.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The RolloutWorkers’ SamplerInput object or None if none exists.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<p>See <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/custom_input_api.py">custom_input_api.py</a> for a runnable example.</p>
</section>
<section id="output-api">
<h2>Output API<a class="headerlink" href="rllib-offline.html#output-api" title="Permalink to this headline">#</a></h2>
<p>You can configure experience output for an agent using the following options:</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Plain python config dicts will soon be replaced by <a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlgorithmConfig</span></code></a>
objects, which have the advantage of being type safe, allowing users to set different config settings within
meaningful sub-categories (e.g. <code class="docutils literal notranslate"><span class="pre">my_config.offline_data(input_=[xyz])</span></code>), and offer the ability to
construct an Algorithm instance from these config objects (via their <code class="docutils literal notranslate"><span class="pre">.build()</span></code> method).</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Specify where experiences should be saved:</span>
<span class="c1">#  - None: don&#39;t save any experiences</span>
<span class="c1">#  - &quot;logdir&quot; to save to the agent log dir</span>
<span class="c1">#  - a path/URI to save to a custom output directory (e.g., &quot;s3://bucket/&quot;)</span>
<span class="c1">#  - a function that returns a rllib.offline.OutputWriter</span>
<span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="c1"># Arguments accessible from the IOContext for configuring custom output</span>
<span class="s2">&quot;output_config&quot;</span><span class="p">:</span> <span class="p">{},</span>
<span class="c1"># What sample batch columns to LZ4 compress in the output data.</span>
<span class="s2">&quot;output_compress_columns&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;obs&quot;</span><span class="p">,</span> <span class="s2">&quot;new_obs&quot;</span><span class="p">],</span>
<span class="c1"># Max output file size (in bytes) before rolling over to a new file.</span>
<span class="s2">&quot;output_max_file_size&quot;</span><span class="p">:</span> <span class="mi">64</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">,</span>
</pre></div>
</div>
<p>The interface for a custom output writer is as follows:</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.offline.</span></span><span class="sig-name descname"><span class="pre">OutputWriter</span></span><a class="reference internal" href="../_modules/ray/rllib/offline/output_writer.html#OutputWriter"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Writer API for saving experiences from policy evaluation.</p>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">write</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">SampleBatch</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">MultiAgentBatch</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/offline/output_writer.html#OutputWriter.write"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Saves a batch of experiences.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sample_batch</strong> – SampleBatch or MultiAgentBatch to save.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="rllib-replay-buffers.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Replay Buffers</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="rllib-catalogs.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Catalog (Alpha)</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>