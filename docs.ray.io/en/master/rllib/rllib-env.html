
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Environments &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/versionwarning.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../_static/js/docsearch.js"></script>
    <script src="../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../_static/js/termynal.js"></script>
    <script defer="defer" src="../_static/js/custom.js"></script>
    <script defer="defer" src="../_static/js/top-navigation.js"></script>
    <script src="../_static/js/tags.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/rllib/rllib-env.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Algorithms" href="rllib-algorithms.html" />
    <link rel="prev" title="Key Concepts" href="key-concepts.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "rllib/rllib-env", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Ray RLlib
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-training.html">
     Getting Started with RLlib
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="key-concepts.html">
     Key Concepts
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="../rllib-env.html#">
     Environments
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-algorithms.html">
     Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="user-guides.html">
     User Guides
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-examples.html">
     Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="package_ref/index.html">
     Ray RLlib API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Frllib/rllib-env.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/rllib/rllib-env.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/rllib/rllib-env.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="../rllib-env.html#configuring-environments">
   Configuring Environments
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="../rllib-env.html#gymnasium">
   Gymnasium
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#performance">
     Performance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#expensive-environments">
     Expensive Environments
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="../rllib-env.html#vectorized">
   Vectorized
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="../rllib-env.html#multi-agent-and-hierarchical">
   Multi-Agent and Hierarchical
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#pettingzoo-multi-agent-environments">
     PettingZoo Multi-Agent Environments
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#rock-paper-scissors-example">
     Rock Paper Scissors Example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#variable-sharing-between-policies">
     Variable-Sharing Between Policies
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#implementing-a-centralized-critic">
     Implementing a Centralized Critic
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#grouping-agents">
     Grouping Agents
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#hierarchical-environments">
     Hierarchical Environments
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="../rllib-env.html#external-agents-and-applications">
   External Agents and Applications
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#logging-off-policy-actions">
     Logging off-policy actions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#external-application-clients">
     External Application Clients
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="../rllib-env.html#advanced-integrations">
   Advanced Integrations
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Environments</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="../rllib-env.html#configuring-environments">
   Configuring Environments
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="../rllib-env.html#gymnasium">
   Gymnasium
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#performance">
     Performance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#expensive-environments">
     Expensive Environments
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="../rllib-env.html#vectorized">
   Vectorized
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="../rllib-env.html#multi-agent-and-hierarchical">
   Multi-Agent and Hierarchical
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#pettingzoo-multi-agent-environments">
     PettingZoo Multi-Agent Environments
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#rock-paper-scissors-example">
     Rock Paper Scissors Example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#variable-sharing-between-policies">
     Variable-Sharing Between Policies
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#implementing-a-centralized-critic">
     Implementing a Centralized Critic
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#grouping-agents">
     Grouping Agents
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#hierarchical-environments">
     Hierarchical Environments
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="../rllib-env.html#external-agents-and-applications">
   External Agents and Applications
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#logging-off-policy-actions">
     Logging off-policy actions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="../rllib-env.html#external-application-clients">
     External Application Clients
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="../rllib-env.html#advanced-integrations">
   Advanced Integrations
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <a class="reference external image-reference" href="https://ray-docs-promo.netlify.app/rllib"><img alt="" src="https://ray-docs-promo.netlify.app/assets/img/rllib/top.png" /></a>
<div class="bottom-right-promo-banner docutils">
<a class="reference external image-reference" href="https://ray-docs-promo.netlify.app/rllib"><img alt="" src="https://ray-docs-promo.netlify.app/assets/img/rllib/square.png" /></a>
</div>
<section id="environments">
<span id="rllib-environments-doc"></span><h1>Environments<a class="headerlink" href="../rllib-env.html#environments" title="Permalink to this headline">#</a></h1>
<p>RLlib works with several different types of environments, including <a class="reference external" href="https://gymnasium.farama.org/">Farama-Foundation Gymnasium</a>, user-defined, multi-agent, and also batched environments.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Not all environments work with all algorithms. Check out the <a class="reference external" href="rllib-algorithms.html#available-algorithms-overview">algorithm overview</a> for more information.</p>
</div>
<img alt="../_images/rllib-envs.svg" src="../_images/rllib-envs.svg" /><section id="configuring-environments">
<span id="id1"></span><h2>Configuring Environments<a class="headerlink" href="../rllib-env.html#configuring-environments" title="Permalink to this headline">#</a></h2>
<p>You can pass either a string name or a Python class to specify an environment. By default, strings will be interpreted as a gym <a class="reference external" href="https://www.gymlibrary.dev/">environment name</a>.
Custom env classes passed directly to the algorithm must take a single <code class="docutils literal notranslate"><span class="pre">env_config</span></code> parameter in their constructor:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms</span> <span class="kn">import</span> <span class="n">ppo</span>

<span class="k">class</span> <span class="nc">MyEnv</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env_config</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">gym</span><span class="o">.</span><span class="n">Space</span><span class="o">&gt;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">gym</span><span class="o">.</span><span class="n">Space</span><span class="o">&gt;</span>
    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">options</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">&lt;</span><span class="n">obs</span><span class="o">&gt;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">info</span><span class="o">&gt;</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">&lt;</span><span class="n">obs</span><span class="o">&gt;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="o">&gt;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">terminated</span><span class="p">:</span> <span class="nb">bool</span><span class="o">&gt;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">truncated</span><span class="p">:</span> <span class="nb">bool</span><span class="o">&gt;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">info</span><span class="p">:</span> <span class="nb">dict</span><span class="o">&gt;</span>

<span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">ppo</span><span class="o">.</span><span class="n">PPO</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">MyEnv</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span>
    <span class="s2">&quot;env_config&quot;</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># config to pass to env class</span>
<span class="p">})</span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">())</span>
</pre></div>
</div>
<p>You can also register a custom env creator function with a string name. This function must take a single <code class="docutils literal notranslate"><span class="pre">env_config</span></code> (dict) parameter and return an env instance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.tune.registry</span> <span class="kn">import</span> <span class="n">register_env</span>

<span class="k">def</span> <span class="nf">env_creator</span><span class="p">(</span><span class="n">env_config</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">MyEnv</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># return an env instance</span>

<span class="n">register_env</span><span class="p">(</span><span class="s2">&quot;my_env&quot;</span><span class="p">,</span> <span class="n">env_creator</span><span class="p">)</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">ppo</span><span class="o">.</span><span class="n">PPO</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;my_env&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>For a full runnable code example using the custom environment API, see <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/custom_env.py">custom_env.py</a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The gymnasium registry is not compatible with Ray. Instead, always use the registration flows documented above to ensure Ray workers can access the environment.</p>
</div>
<p>In the above example, note that the <code class="docutils literal notranslate"><span class="pre">env_creator</span></code> function takes in an <code class="docutils literal notranslate"><span class="pre">env_config</span></code> object.
This is a dict containing options passed in through your algorithm.
You can also access <code class="docutils literal notranslate"><span class="pre">env_config.worker_index</span></code> and <code class="docutils literal notranslate"><span class="pre">env_config.vector_index</span></code> to get the worker id and env id within the worker (if <code class="docutils literal notranslate"><span class="pre">num_envs_per_worker</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>).
This can be useful if you want to train over an ensemble of different environments, for example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MultiEnv</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env_config</span><span class="p">):</span>
        <span class="c1"># pick actual env based on worker and env indexes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span>
            <span class="n">choose_env_for</span><span class="p">(</span><span class="n">env_config</span><span class="o">.</span><span class="n">worker_index</span><span class="p">,</span> <span class="n">env_config</span><span class="o">.</span><span class="n">vector_index</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span>
    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">options</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">options</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

<span class="n">register_env</span><span class="p">(</span><span class="s2">&quot;multienv&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">config</span><span class="p">:</span> <span class="n">MultiEnv</span><span class="p">(</span><span class="n">config</span><span class="p">))</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When using logging in an environment, the logging configuration needs to be done inside the environment, which runs inside Ray workers. Any configurations outside the environment, e.g., before starting Ray will be ignored.</p>
</div>
</section>
<section id="gymnasium">
<h2>Gymnasium<a class="headerlink" href="../rllib-env.html#gymnasium" title="Permalink to this headline">#</a></h2>
<p>RLlib uses Gymnasium as its environment interface for single-agent training. For more information on how to implement a custom Gymnasium environment, see the <a class="reference external" href="https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/core.py">gymnasium.Env class definition</a>. You may find the <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/custom_env.py">SimpleCorridor</a> example useful as a reference.</p>
<section id="performance">
<h3>Performance<a class="headerlink" href="../rllib-env.html#performance" title="Permalink to this headline">#</a></h3>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Also check out the <a class="reference external" href="rllib-training.html#scaling-guide">scaling guide</a> for RLlib training.</p>
</div>
<p>There are two ways to scale experience collection with Gym environments:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>Vectorization within a single process:</strong> Though many envs can achieve high frame rates per core, their throughput is limited in practice by policy evaluation between steps. For example, even small TensorFlow models incur a couple milliseconds of latency to evaluate. This can be worked around by creating multiple envs per process and batching policy evaluations across these envs.</p></li>
</ol>
<blockquote>
<div><p>You can configure <code class="docutils literal notranslate"><span class="pre">{&quot;num_envs_per_worker&quot;:</span> <span class="pre">M}</span></code> to have RLlib create <code class="docutils literal notranslate"><span class="pre">M</span></code> concurrent environments per worker. RLlib auto-vectorizes Gym environments via <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/env/vector_env.py">VectorEnv.wrap()</a>.</p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p><strong>Distribute across multiple processes:</strong> You can also have RLlib create multiple processes (Ray actors) for experience collection. In most algorithms this can be controlled by setting the <code class="docutils literal notranslate"><span class="pre">{&quot;num_workers&quot;:</span> <span class="pre">N}</span></code> config.</p></li>
</ol>
</div></blockquote>
<img alt="../_images/throughput.png" src="../_images/throughput.png" />
<p>You can also combine vectorization and distributed execution, as shown in the above figure. Here we plot just the throughput of RLlib policy evaluation from 1 to 128 CPUs. PongNoFrameskip-v4 on GPU scales from 2.4k to ∼200k actions/s, and Pendulum-v1 on CPU from 15k to 1.5M actions/s. One machine was used for 1-16 workers, and a Ray cluster of four machines for 32-128 workers. Each worker was configured with <code class="docutils literal notranslate"><span class="pre">num_envs_per_worker=64</span></code>.</p>
</section>
<section id="expensive-environments">
<h3>Expensive Environments<a class="headerlink" href="../rllib-env.html#expensive-environments" title="Permalink to this headline">#</a></h3>
<p>Some environments may be very resource-intensive to create. RLlib will create <code class="docutils literal notranslate"><span class="pre">num_workers</span> <span class="pre">+</span> <span class="pre">1</span></code> copies of the environment since one copy is needed for the driver process. To avoid paying the extra overhead of the driver copy, which is needed to access the env’s action and observation spaces, you can defer environment initialization until <code class="docutils literal notranslate"><span class="pre">reset()</span></code> is called.</p>
</section>
</section>
<section id="vectorized">
<h2>Vectorized<a class="headerlink" href="../rllib-env.html#vectorized" title="Permalink to this headline">#</a></h2>
<p>RLlib will auto-vectorize Gym envs for batch evaluation if the <code class="docutils literal notranslate"><span class="pre">num_envs_per_worker</span></code> config is set, or you can define a custom environment class that subclasses <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/env/vector_env.py">VectorEnv</a> to implement <code class="docutils literal notranslate"><span class="pre">vector_step()</span></code> and <code class="docutils literal notranslate"><span class="pre">vector_reset()</span></code>.</p>
<p>Note that auto-vectorization only applies to policy inference by default. This means that policy inference will be batched, but your envs will still be stepped one at a time. If you would like your envs to be stepped in parallel, you can set <code class="docutils literal notranslate"><span class="pre">&quot;remote_worker_envs&quot;:</span> <span class="pre">True</span></code>. This will create env instances in Ray actors and step them in parallel. These remote processes introduce communication overheads, so this only helps if your env is very expensive to step / reset.</p>
<p>When using remote envs, you can control the batching level for inference with <code class="docutils literal notranslate"><span class="pre">remote_env_batch_wait_ms</span></code>. The default value of 0ms means envs execute asynchronously and inference is only batched opportunistically. Setting the timeout to a large value will result in fully batched inference and effectively synchronous environment stepping. The optimal value depends on your environment step / reset time, and model inference speed.</p>
</section>
<section id="multi-agent-and-hierarchical">
<h2>Multi-Agent and Hierarchical<a class="headerlink" href="../rllib-env.html#multi-agent-and-hierarchical" title="Permalink to this headline">#</a></h2>
<p>In a multi-agent environment, there are more than one “agent” acting simultaneously, in a turn-based fashion, or in a combination of these two.</p>
<p>For example, in a traffic simulation, there may be multiple “car” and “traffic light” agents in the environment,
acting simultaneously. Whereas in a board game, you may have two or more agents acting in a turn-base fashion.</p>
<p>The mental model for multi-agent in RLlib is as follows:
(1) Your environment (a sub-class of <a class="reference internal" href="package_ref/env/multi_agent_env.html#ray.rllib.env.multi_agent_env.MultiAgentEnv" title="ray.rllib.env.multi_agent_env.MultiAgentEnv"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiAgentEnv</span></code></a>) returns dictionaries mapping agent IDs (e.g. strings; the env can chose these arbitrarily) to individual agents’ observations, rewards, and done-flags.
(2) You define (some of) the policies that are available up front (you can also add new policies on-the-fly throughout training), and
(3) You define a function that maps an env-produced agent ID to any available policy ID, which is then to be used for computing actions for this particular agent.</p>
<p>This is summarized by the below figure:</p>
<img alt="../_images/multi-agent.svg" src="../_images/multi-agent.svg" /><p>When implementing your own <a class="reference internal" href="package_ref/env/multi_agent_env.html#ray.rllib.env.multi_agent_env.MultiAgentEnv" title="ray.rllib.env.multi_agent_env.MultiAgentEnv"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiAgentEnv</span></code></a>, note that you should only return those
agent IDs in an observation dict, for which you expect to receive actions in the next call to <code class="xref py py-obj docutils literal notranslate"><span class="pre">step()</span></code>.</p>
<p>This API allows you to implement any type of multi-agent environment, from <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/self_play_with_open_spiel.py">turn-based games</a>
over environments, in which <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/env/multi_agent.py">all agents always act simultaneously</a>, to anything in between.</p>
<p>Here is an example of an env, in which all agents always step simultaneously:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Env, in which all agents (whose IDs are entirely determined by the env</span>
<span class="c1"># itself via the returned multi-agent obs/reward/dones-dicts) step</span>
<span class="c1"># simultaneously.</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">MultiAgentTrafficEnv</span><span class="p">(</span><span class="n">num_cars</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_traffic_lights</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Observations are a dict mapping agent names to their obs. Only those</span>
<span class="c1"># agents&#39; names that require actions in the next call to `step()` should</span>
<span class="c1"># be present in the returned observation dict (here: all, as we always step</span>
<span class="c1"># simultaneously).</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">())</span>
<span class="c1"># ... {</span>
<span class="c1"># ...   &quot;car_1&quot;: [[...]],</span>
<span class="c1"># ...   &quot;car_2&quot;: [[...]],</span>
<span class="c1"># ...   &quot;traffic_light_1&quot;: [[...]],</span>
<span class="c1"># ... }</span>

<span class="c1"># In the following call to `step`, actions should be provided for each</span>
<span class="c1"># agent that returned an observation before:</span>
<span class="n">new_obs</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span>
    <span class="n">actions</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;car_1&quot;</span><span class="p">:</span> <span class="o">...</span><span class="p">,</span> <span class="s2">&quot;car_2&quot;</span><span class="p">:</span> <span class="o">...</span><span class="p">,</span> <span class="s2">&quot;traffic_light_1&quot;</span><span class="p">:</span> <span class="o">...</span><span class="p">})</span>

<span class="c1"># Similarly, new_obs, rewards, dones, etc. also become dicts.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
<span class="c1"># ... {&quot;car_1&quot;: 3, &quot;car_2&quot;: -1, &quot;traffic_light_1&quot;: 0}</span>

<span class="c1"># Individual agents can early exit; The entire episode is done when</span>
<span class="c1"># dones[&quot;__all__&quot;] = True.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dones</span><span class="p">)</span>
<span class="c1"># ... {&quot;car_2&quot;: True, &quot;__all__&quot;: False}</span>
</pre></div>
</div>
<p>And another example, where agents step one after the other (turn-based game):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Env, in which two agents step in sequence (tuen-based game).</span>
<span class="c1"># The env is in charge of the produced agent ID. Our env here produces</span>
<span class="c1"># agent IDs: &quot;player1&quot; and &quot;player2&quot;.</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">TicTacToe</span><span class="p">()</span>

<span class="c1"># Observations are a dict mapping agent names to their obs. Only those</span>
<span class="c1"># agents&#39; names that require actions in the next call to `step()` should</span>
<span class="c1"># be present in the returned observation dict (here: one agent at a time).</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">())</span>
<span class="c1"># ... {</span>
<span class="c1"># ...   &quot;player1&quot;: [[...]],</span>
<span class="c1"># ... }</span>

<span class="c1"># In the following call to `step`, only those agents&#39; actions should be</span>
<span class="c1"># provided that were present in the returned obs dict:</span>
<span class="n">new_obs</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">actions</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;player1&quot;</span><span class="p">:</span> <span class="o">...</span><span class="p">})</span>

<span class="c1"># Similarly, new_obs, rewards, dones, etc. also become dicts.</span>
<span class="c1"># Note that only in the `rewards` dict, any agent may be listed (even those that have</span>
<span class="c1"># not(!) acted in the `step()` call). Rewards for individual agents will be added</span>
<span class="c1"># up to the point where a new action for that agent is needed. This way, you may</span>
<span class="c1"># implement a turn-based 2-player game, in which player-2&#39;s reward is published</span>
<span class="c1"># in the `rewards` dict immediately after player-1 has acted.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
<span class="c1"># ... {&quot;player1&quot;: 0, &quot;player2&quot;: 0}</span>

<span class="c1"># Individual agents can early exit; The entire episode is done when</span>
<span class="c1"># dones[&quot;__all__&quot;] = True.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dones</span><span class="p">)</span>
<span class="c1"># ... {&quot;player1&quot;: False, &quot;__all__&quot;: False}</span>

<span class="c1"># In the next step, it&#39;s player2&#39;s turn. Therefore, `new_obs` only container</span>
<span class="c1"># this agent&#39;s ID:</span>
<span class="nb">print</span><span class="p">(</span><span class="n">new_obs</span><span class="p">)</span>
<span class="c1"># ... {</span>
<span class="c1"># ...   &quot;player2&quot;: [[...]]</span>
<span class="c1"># ... }</span>
</pre></div>
</div>
<p>If all the agents will be using the same algorithm class to train, then you can setup multi-agent training as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">algo</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">PGAgent</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;my_multiagent_env&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span>
    <span class="s2">&quot;multiagent&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;policies&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="c1"># Use the PolicySpec namedtuple to specify an individual policy:</span>
            <span class="s2">&quot;car1&quot;</span><span class="p">:</span> <span class="n">PolicySpec</span><span class="p">(</span>
                <span class="n">policy_class</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># infer automatically from Algorithm</span>
                <span class="n">observation_space</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># infer automatically from env</span>
                <span class="n">action_space</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># infer automatically from env</span>
                <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.85</span><span class="p">},</span>  <span class="c1"># use main config plus &lt;- this override here</span>
                <span class="p">),</span>  <span class="c1"># alternatively, simply do: `PolicySpec(config={&quot;gamma&quot;: 0.85})`</span>

            <span class="c1"># Deprecated way: Tuple specifying class, obs-/action-spaces,</span>
            <span class="c1"># config-overrides for each policy as a tuple.</span>
            <span class="c1"># If class is None -&gt; Uses Algorithm&#39;s default policy class.</span>
            <span class="s2">&quot;car2&quot;</span><span class="p">:</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">car_obs_space</span><span class="p">,</span> <span class="n">car_act_space</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">}),</span>

            <span class="c1"># New way: Use PolicySpec() with keywords: `policy_class`,</span>
            <span class="c1"># `observation_space`, `action_space`, `config`.</span>
            <span class="s2">&quot;traffic_light&quot;</span><span class="p">:</span> <span class="n">PolicySpec</span><span class="p">(</span>
                <span class="n">observation_space</span><span class="o">=</span><span class="n">tl_obs_space</span><span class="p">,</span>  <span class="c1"># special obs space for lights?</span>
                <span class="n">action_space</span><span class="o">=</span><span class="n">tl_act_space</span><span class="p">,</span>  <span class="c1"># special action space for lights?</span>
                <span class="p">),</span>
        <span class="p">},</span>
        <span class="s2">&quot;policy_mapping_fn&quot;</span><span class="p">:</span>
            <span class="k">lambda</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">worker</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span>
                <span class="s2">&quot;traffic_light&quot;</span>  <span class="c1"># Traffic lights are always controlled by this policy</span>
                <span class="k">if</span> <span class="n">agent_id</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;traffic_light_&quot;</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s2">&quot;car1&quot;</span><span class="p">,</span> <span class="s2">&quot;car2&quot;</span><span class="p">])</span>  <span class="c1"># Randomly choose from car policies</span>
    <span class="p">},</span>
<span class="p">})</span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">())</span>
</pre></div>
</div>
<p>To exclude some policies in your <code class="docutils literal notranslate"><span class="pre">multiagent.policies</span></code> dictionary, you can use the <code class="docutils literal notranslate"><span class="pre">multiagent.policies_to_train</span></code> setting.
For example, you may want to have one or more random (non learning) policies interact with your learning ones:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example for a mapping function that maps agent IDs &quot;player1&quot; and &quot;player2&quot; to either</span>
<span class="c1"># &quot;random_policy&quot; or &quot;learning_policy&quot;, making sure that in each episode, both policies</span>
<span class="c1"># are always playing each other.</span>
<span class="k">def</span> <span class="nf">policy_mapping_fn</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">worker</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">agent_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">agent_id</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># 0 (player1) or 1 (player2)</span>
    <span class="c1"># agent_id = &quot;player[1|2]&quot; -&gt; policy depends on episode ID</span>
    <span class="c1"># This way, we make sure that both policies sometimes play player1</span>
    <span class="c1"># (start player) and sometimes player2 (player to move 2nd).</span>
    <span class="k">return</span> <span class="s2">&quot;learning_policy&quot;</span> <span class="k">if</span> <span class="n">episode</span><span class="o">.</span><span class="n">episode_id</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="n">agent_idx</span> <span class="k">else</span> <span class="s2">&quot;random_policy&quot;</span>

<span class="n">algo</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">PGAgent</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;two_player_game&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span>
    <span class="s2">&quot;multiagent&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;policies&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;learning_policy&quot;</span><span class="p">:</span> <span class="n">PolicySpec</span><span class="p">(),</span>  <span class="c1"># &lt;- use default class &amp; infer obs-/act-spaces from env.</span>
            <span class="s2">&quot;random_policy&quot;</span><span class="p">:</span> <span class="n">PolicySpec</span><span class="p">(</span><span class="n">policy_class</span><span class="o">=</span><span class="n">RandomPolicy</span><span class="p">),</span>  <span class="c1"># infer obs-/act-spaces from env.</span>
        <span class="p">},</span>
        <span class="c1"># Example for a mapping function that maps agent IDs &quot;player1&quot; and &quot;player2&quot; to either</span>
        <span class="c1"># &quot;random_policy&quot; or &quot;learning_policy&quot;, making sure that in each episode, both policies</span>
        <span class="c1"># are always playing each other.</span>
        <span class="s2">&quot;policy_mapping_fn&quot;</span><span class="p">:</span> <span class="n">policy_mapping_fn</span><span class="p">,</span>
        <span class="c1"># Specify a (fixed) list (or set) of policy IDs that should be updated.</span>
        <span class="s2">&quot;policies_to_train&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;learning_policy&quot;</span><span class="p">],</span>

        <span class="c1"># Alternatively, you can provide a callable that returns True or False, when provided</span>
        <span class="c1"># with a policy ID and an (optional) SampleBatch:</span>

        <span class="c1"># &quot;policies_to_train&quot;: lambda pid, batch: ... (&lt;- return True or False)</span>

        <span class="c1"># This allows you to more flexibly update (or not) policies, based on</span>
        <span class="c1"># who they played with in the episode (or other information that can be</span>
        <span class="c1"># found in the given batch, e.g. rewards).</span>
    <span class="p">},</span>
<span class="p">})</span>
</pre></div>
</div>
<p>RLlib will create three distinct policies and route agent decisions to its bound policy using the given <code class="docutils literal notranslate"><span class="pre">policy_mapping_fn</span></code>.
When an agent first appears in the env, <code class="docutils literal notranslate"><span class="pre">policy_mapping_fn</span></code> will be called to determine which policy it is bound to.
RLlib reports separate training statistics for each policy in the return from <code class="docutils literal notranslate"><span class="pre">train()</span></code>, along with the combined reward.</p>
<p>Here is a simple <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_cartpole.py">example training script</a>
in which you can vary the number of agents and policies in the environment.
For how to use multiple training methods at once (here DQN and PPO),
see the <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_two_trainers.py">two-trainer example</a>.
Metrics are reported for each policy separately, for example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span> Result <span class="k">for</span> PPO_multi_cartpole_0:
   episode_len_mean: <span class="m">34</span>.025862068965516
   episode_reward_max: <span class="m">159</span>.0
   episode_reward_mean: <span class="m">86</span>.06896551724138
   info:
<span class="hll">     policy_0:
</span>       cur_lr: <span class="m">4</span>.999999873689376e-05
       entropy: <span class="m">0</span>.6833480000495911
       kl: <span class="m">0</span>.010264254175126553
       policy_loss: -11.95590591430664
       total_loss: <span class="m">197</span>.7039794921875
       vf_explained_var: <span class="m">0</span>.0010995268821716309
       vf_loss: <span class="m">209</span>.6578826904297
<span class="hll">     policy_1:
</span>       cur_lr: <span class="m">4</span>.999999873689376e-05
       entropy: <span class="m">0</span>.6827034950256348
       kl: <span class="m">0</span>.01119876280426979
       policy_loss: -8.787769317626953
       total_loss: <span class="m">88</span>.26161193847656
       vf_explained_var: <span class="m">0</span>.0005457401275634766
       vf_loss: <span class="m">97</span>.0471420288086
<span class="hll">   policy_reward_mean:
</span>     policy_0: <span class="m">21</span>.194444444444443
     policy_1: <span class="m">21</span>.798387096774192
</pre></div>
</div>
<p>To scale to hundreds of agents (if these agents are using the same policy), MultiAgentEnv batches policy evaluations across multiple agents internally.
Your <code class="docutils literal notranslate"><span class="pre">MultiAgentEnvs</span></code> are also auto-vectorized (as can be normal, single-agent envs, e.g. gym.Env) by setting <code class="docutils literal notranslate"><span class="pre">num_envs_per_worker</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>.</p>
<section id="pettingzoo-multi-agent-environments">
<h3>PettingZoo Multi-Agent Environments<a class="headerlink" href="../rllib-env.html#pettingzoo-multi-agent-environments" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="https://github.com/Farama-Foundation/PettingZoo">PettingZoo</a> is a repository of over 50 diverse multi-agent environments. However, the API is not directly compatible with rllib, but it can be converted into an rllib MultiAgentEnv like in this example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.tune.registry</span> <span class="kn">import</span> <span class="n">register_env</span>
<span class="c1"># import the pettingzoo environment</span>
<span class="kn">from</span> <span class="nn">pettingzoo.butterfly</span> <span class="kn">import</span> <span class="n">prison_v3</span>
<span class="c1"># import rllib pettingzoo interface</span>
<span class="kn">from</span> <span class="nn">ray.rllib.env</span> <span class="kn">import</span> <span class="n">PettingZooEnv</span>
<span class="c1"># define how to make the environment. This way takes an optional environment config, num_floors</span>
<span class="n">env_creator</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">config</span><span class="p">:</span> <span class="n">prison_v3</span><span class="o">.</span><span class="n">env</span><span class="p">(</span><span class="n">num_floors</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;num_floors&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="c1"># register that way to make the environment under an rllib name</span>
<span class="n">register_env</span><span class="p">(</span><span class="s1">&#39;prison&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">config</span><span class="p">:</span> <span class="n">PettingZooEnv</span><span class="p">(</span><span class="n">env_creator</span><span class="p">(</span><span class="n">config</span><span class="p">)))</span>
<span class="c1"># now you can use `prison` as an environment</span>
<span class="c1"># you can pass arguments to the environment creator with the env_config option in the config</span>
<span class="n">config</span><span class="p">[</span><span class="s1">&#39;env_config&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;num_floors&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">}</span>
</pre></div>
</div>
<p>A more complete example is here: <a class="reference external" href="https://github.com/Farama-Foundation/PettingZoo/blob/master/tutorials/Ray/rllib_pistonball.py">rllib_pistonball.py</a></p>
</section>
<section id="rock-paper-scissors-example">
<h3>Rock Paper Scissors Example<a class="headerlink" href="../rllib-env.html#rock-paper-scissors-example" title="Permalink to this headline">#</a></h3>
<p>The <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/rock_paper_scissors_multiagent.py">rock_paper_scissors_multiagent.py</a> example demonstrates several types of policies competing against each other: heuristic policies of repeating the same move, beating the last opponent move, and learned LSTM and feedforward policies.</p>
<figure class="align-default" id="id2">
<img alt="../_images/rock-paper-scissors.png" src="../_images/rock-paper-scissors.png" />
<figcaption>
<p><span class="caption-text">TensorBoard output of running the rock-paper-scissors example, where a learned policy faces off between a random selection of the same-move and beat-last-move heuristics. Here the performance of heuristic policies vs the learned policy is compared with LSTM enabled (blue) and a plain feed-forward policy (red). While the feedforward policy can easily beat the same-move heuristic by simply avoiding the last move taken, it takes a LSTM policy to distinguish between and consistently beat both policies.</span><a class="headerlink" href="../rllib-env.html#id2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="variable-sharing-between-policies">
<h3>Variable-Sharing Between Policies<a class="headerlink" href="../rllib-env.html#variable-sharing-between-policies" title="Permalink to this headline">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>With <a class="reference external" href="rllib-models.html#tensorflow-models">ModelV2</a>, you can put layers in global variables and straightforwardly share those layer objects between models instead of using variable scopes.</p>
</div>
<p>RLlib will create each policy’s model in a separate <code class="docutils literal notranslate"><span class="pre">tf.variable_scope</span></code>. However, variables can still be shared between policies by explicitly entering a globally shared variable scope with <code class="docutils literal notranslate"><span class="pre">tf.VariableScope(reuse=tf.AUTO_REUSE)</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">VariableScope</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">AUTO_REUSE</span><span class="p">,</span> <span class="s2">&quot;name_of_global_shared_scope&quot;</span><span class="p">),</span>
        <span class="n">reuse</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">AUTO_REUSE</span><span class="p">,</span>
        <span class="n">auxiliary_name_scope</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="o">&lt;</span><span class="n">create</span> <span class="n">the</span> <span class="n">shared</span> <span class="n">layers</span> <span class="n">here</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>There is a full example of this in the <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_cartpole.py">example training script</a>.</p>
</section>
<section id="implementing-a-centralized-critic">
<h3>Implementing a Centralized Critic<a class="headerlink" href="../rllib-env.html#implementing-a-centralized-critic" title="Permalink to this headline">#</a></h3>
<p>Here are two ways to implement a centralized critic compatible with the multi-agent API:</p>
<p><strong>Strategy 1: Sharing experiences in the trajectory preprocessor</strong>:</p>
<p>The most general way of implementing a centralized critic involves defining the <code class="docutils literal notranslate"><span class="pre">postprocess_fn</span></code> method of a custom policy. <code class="docutils literal notranslate"><span class="pre">postprocess_fn</span></code> is called by <code class="docutils literal notranslate"><span class="pre">Policy.postprocess_trajectory</span></code>, which has full access to the policies and observations of concurrent agents via the <code class="docutils literal notranslate"><span class="pre">other_agent_batches</span></code> and <code class="docutils literal notranslate"><span class="pre">episode</span></code> arguments. The batch of critic predictions can then be added to the postprocessed trajectory. Here’s an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">postprocess_fn</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">sample_batch</span><span class="p">,</span> <span class="n">other_agent_batches</span><span class="p">,</span> <span class="n">episode</span><span class="p">):</span>
    <span class="n">agents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;agent_1&quot;</span><span class="p">,</span> <span class="s2">&quot;agent_2&quot;</span><span class="p">,</span> <span class="s2">&quot;agent_3&quot;</span><span class="p">]</span>  <span class="c1"># simple example of 3 agents</span>
    <span class="n">global_obs_batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span><span class="n">other_agent_batches</span><span class="p">[</span><span class="n">agent_id</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;obs&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="n">agents</span><span class="p">],</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># add the global obs and global critic value</span>
    <span class="n">sample_batch</span><span class="p">[</span><span class="s2">&quot;global_obs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">global_obs_batch</span>
    <span class="n">sample_batch</span><span class="p">[</span><span class="s2">&quot;central_vf&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">critic_network</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;obs&quot;</span><span class="p">:</span> <span class="n">global_obs_batch</span><span class="p">})</span>
    <span class="k">return</span> <span class="n">sample_batch</span>
</pre></div>
</div>
<p>To update the critic, you’ll also have to modify the loss of the policy. For an end-to-end runnable example, see <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/centralized_critic.py">examples/centralized_critic.py</a>.</p>
<p><strong>Strategy 2: Sharing observations through an observation function</strong>:</p>
<p>Alternatively, you can use an observation function to share observations between agents. In this strategy, each observation includes all global state, and policies use a custom model to ignore state they aren’t supposed to “see” when computing actions. The advantage of this approach is that it’s very simple and you don’t have to change the algorithm at all – just use the observation func (i.e., like an env wrapper) and custom model. However, it is a bit less principled in that you have to change the agent observation spaces to include training-time only information. You can find a runnable example of this strategy at <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/centralized_critic_2.py">examples/centralized_critic_2.py</a>.</p>
</section>
<section id="grouping-agents">
<h3>Grouping Agents<a class="headerlink" href="../rllib-env.html#grouping-agents" title="Permalink to this headline">#</a></h3>
<p>It is common to have groups of agents in multi-agent RL. RLlib treats agent groups like a single agent with a Tuple action and observation space. The group agent can then be assigned to a single policy for centralized execution, or to specialized multi-agent policies such as <a class="reference internal" href="rllib-algorithms.html#qmix"><span class="std std-ref">Q-Mix</span></a> that implement centralized training but decentralized execution. You can use the <code class="docutils literal notranslate"><span class="pre">MultiAgentEnv.with_agent_groups()</span></code> method to define these groups:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">with_agent_groups</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">AgentID</span><span class="p">]],</span>
        <span class="n">obs_space</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">act_space</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiAgentEnv&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Convenience method for grouping together agents in this env.</span>

<span class="sd">        An agent group is a list of agent IDs that are mapped to a single</span>
<span class="sd">        logical agent. All agents of the group must act at the same time in the</span>
<span class="sd">        environment. The grouped agent exposes Tuple action and observation</span>
<span class="sd">        spaces that are the concatenated action and obs spaces of the</span>
<span class="sd">        individual agents.</span>

<span class="sd">        The rewards of all the agents in a group are summed. The individual</span>
<span class="sd">        agent rewards are available under the &quot;individual_rewards&quot; key of the</span>
<span class="sd">        group info return.</span>

<span class="sd">        Agent grouping is required to leverage algorithms such as Q-Mix.</span>

<span class="sd">        Args:</span>
<span class="sd">            groups: Mapping from group id to a list of the agent ids</span>
<span class="sd">                of group members. If an agent id is not present in any group</span>
<span class="sd">                value, it will be left ungrouped. The group id becomes a new agent ID</span>
<span class="sd">                in the final environment.</span>
<span class="sd">            obs_space: Optional observation space for the grouped</span>
<span class="sd">                env. Must be a tuple space. If not provided, will infer this to be a</span>
<span class="sd">                Tuple of n individual agents spaces (n=num agents in a group).</span>
<span class="sd">            act_space: Optional action space for the grouped env.</span>
<span class="sd">                Must be a tuple space. If not provided, will infer this to be a Tuple</span>
<span class="sd">                of n individual agents spaces (n=num agents in a group).</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from ray.rllib.env.multi_agent_env import MultiAgentEnv</span>
<span class="sd">            &gt;&gt;&gt; class MyMultiAgentEnv(MultiAgentEnv): # doctest: +SKIP</span>
<span class="sd">            ...     # define your env here</span>
<span class="sd">            ...     ... # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; env = MyMultiAgentEnv(...) # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; grouped_env = env.with_agent_groups(env, { # doctest: +SKIP</span>
<span class="sd">            ...   &quot;group1&quot;: [&quot;agent1&quot;, &quot;agent2&quot;, &quot;agent3&quot;], # doctest: +SKIP</span>
<span class="sd">            ...   &quot;group2&quot;: [&quot;agent4&quot;, &quot;agent5&quot;], # doctest: +SKIP</span>
<span class="sd">            ... }) # doctest: +SKIP</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="kn">from</span> <span class="nn">ray.rllib.env.wrappers.group_agents_wrapper</span> <span class="kn">import</span> \
            <span class="n">GroupAgentsWrapper</span>
        <span class="k">return</span> <span class="n">GroupAgentsWrapper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">obs_space</span><span class="p">,</span> <span class="n">act_space</span><span class="p">)</span>

</pre></div>
</div>
<p>For environments with multiple groups, or mixtures of agent groups and individual agents, you can use grouping in conjunction with the policy mapping API described in prior sections.</p>
</section>
<section id="hierarchical-environments">
<h3>Hierarchical Environments<a class="headerlink" href="../rllib-env.html#hierarchical-environments" title="Permalink to this headline">#</a></h3>
<p>Hierarchical training can sometimes be implemented as a special case of multi-agent RL. For example, consider a three-level hierarchy of policies, where a top-level policy issues high level actions that are executed at finer timescales by a mid-level and low-level policy. The following timeline shows one step of the top-level policy, which corresponds to two mid-level actions and five low-level actions:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>top_level ---------------------------------------------------------------&gt; top_level ---&gt;
mid_level_0 -------------------------------&gt; mid_level_0 ----------------&gt; mid_level_1 -&gt;
low_level_0 -&gt; low_level_0 -&gt; low_level_0 -&gt; low_level_1 -&gt; low_level_1 -&gt; low_level_2 -&gt;
</pre></div>
</div>
<p>This can be implemented as a multi-agent environment with three types of agents. Each higher-level action creates a new lower-level agent instance with a new id (e.g., <code class="docutils literal notranslate"><span class="pre">low_level_0</span></code>, <code class="docutils literal notranslate"><span class="pre">low_level_1</span></code>, <code class="docutils literal notranslate"><span class="pre">low_level_2</span></code> in the above example). These lower-level agents pop in existence at the start of higher-level steps, and terminate when their higher-level action ends. Their experiences are aggregated by policy, so from RLlib’s perspective it’s just optimizing three different types of policies. The configuration might look something like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;multiagent&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="s2">&quot;policies&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;top_level&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">custom_policy</span> <span class="ow">or</span> <span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">),</span>
        <span class="s2">&quot;mid_level&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">custom_policy</span> <span class="ow">or</span> <span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">),</span>
        <span class="s2">&quot;low_level&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">custom_policy</span> <span class="ow">or</span> <span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">),</span>
    <span class="p">},</span>
    <span class="s2">&quot;policy_mapping_fn&quot;</span><span class="p">:</span>
        <span class="k">lambda</span> <span class="n">agent_id</span><span class="p">:</span>
            <span class="s2">&quot;low_level&quot;</span> <span class="k">if</span> <span class="n">agent_id</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;low_level_&quot;</span><span class="p">)</span> <span class="k">else</span>
            <span class="s2">&quot;mid_level&quot;</span> <span class="k">if</span> <span class="n">agent_id</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;mid_level_&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;top_level&quot;</span>
    <span class="s2">&quot;policies_to_train&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;top_level&quot;</span><span class="p">],</span>
<span class="p">},</span>
</pre></div>
</div>
<p>In this setup, the appropriate rewards for training lower-level agents must be provided by the multi-agent env implementation.
The environment class is also responsible for routing between the agents, e.g., conveying <a class="reference external" href="https://arxiv.org/pdf/1703.01161.pdf">goals</a> from higher-level
agents to lower-level agents as part of the lower-level agent observation.</p>
<p>See this file for a runnable example: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/hierarchical_training.py">hierarchical_training.py</a>.</p>
</section>
</section>
<section id="external-agents-and-applications">
<h2>External Agents and Applications<a class="headerlink" href="../rllib-env.html#external-agents-and-applications" title="Permalink to this headline">#</a></h2>
<p>In many situations, it does not make sense for an environment to be “stepped” by RLlib. For example, if a policy is to be used in a web serving system, then it is more natural for an agent to query a service that serves policy decisions, and for that service to learn from experience over time. This case also naturally arises with <strong>external simulators</strong> (e.g. Unity3D, other game engines, or the Gazebo robotics simulator) that run independently outside the control of RLlib, but may still want to leverage RLlib for training.</p>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="../_images/rllib-training-inside-a-unity3d-env.png"><img alt="../_images/rllib-training-inside-a-unity3d-env.png" src="../_images/rllib-training-inside-a-unity3d-env.png" style="width: 697.5px; height: 412.5px;" /></a>
<figcaption>
<p><span class="caption-text">A Unity3D soccer game being learnt by RLlib via the ExternalEnv API.</span><a class="headerlink" href="../rllib-env.html#id3" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>RLlib provides the <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/env/external_env.py">ExternalEnv</a> class for this purpose.
Unlike other envs, ExternalEnv has its own thread of control. At any point, agents on that thread can query the current policy for decisions via <code class="docutils literal notranslate"><span class="pre">self.get_action()</span></code> and reports rewards, done-dicts, and infos via <code class="docutils literal notranslate"><span class="pre">self.log_returns()</span></code>.
This can be done for multiple concurrent episodes as well.</p>
<p>Take a look at the examples here for a <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/serving/cartpole_server.py">simple “CartPole-v1” server</a>
and <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/serving/cartpole_client.py">n client(s)</a>
scripts, in which we setup an RLlib policy server that listens on one or more ports for client connections
and connect several clients to this server to learn the env.</p>
<p>Another <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/serving/unity3d_server.py">example</a> shows,
how to run a similar setup against a Unity3D external game engine.</p>
<section id="logging-off-policy-actions">
<h3>Logging off-policy actions<a class="headerlink" href="../rllib-env.html#logging-off-policy-actions" title="Permalink to this headline">#</a></h3>
<p>ExternalEnv provides a <code class="docutils literal notranslate"><span class="pre">self.log_action()</span></code> call to support off-policy actions. This allows the client to make independent decisions, e.g., to compare two different policies, and for RLlib to still learn from those off-policy actions. Note that this requires the algorithm used to support learning from off-policy decisions (e.g., DQN).</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference external" href="rllib-offline.html">Offline Datasets</a> provide higher-level interfaces for working with off-policy experience datasets.</p>
</div>
</section>
<section id="external-application-clients">
<h3>External Application Clients<a class="headerlink" href="../rllib-env.html#external-application-clients" title="Permalink to this headline">#</a></h3>
<p>For applications that are running entirely outside the Ray cluster (i.e., cannot be packaged into a Python environment of any form), RLlib provides the <code class="docutils literal notranslate"><span class="pre">PolicyServerInput</span></code> application connector, which can be connected to over the network using <code class="docutils literal notranslate"><span class="pre">PolicyClient</span></code> instances.</p>
<p>You can configure any Algorithm to launch a policy server with the following config:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># An environment class is still required, but it doesn&#39;t need to be runnable.</span>
    <span class="c1"># You only need to define its action and observation space attributes.</span>
    <span class="c1"># See examples/serving/unity3d_server.py for an example using a RandomMultiAgentEnv stub.</span>
    <span class="s2">&quot;env&quot;</span><span class="p">:</span> <span class="n">YOUR_ENV_STUB</span><span class="p">,</span>
    <span class="c1"># Use the policy server to generate experiences.</span>
    <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="p">(</span>
        <span class="k">lambda</span> <span class="n">ioctx</span><span class="p">:</span> <span class="n">PolicyServerInput</span><span class="p">(</span><span class="n">ioctx</span><span class="p">,</span> <span class="n">SERVER_ADDRESS</span><span class="p">,</span> <span class="n">SERVER_PORT</span><span class="p">)</span>
    <span class="p">),</span>
    <span class="c1"># Use the existing algorithm process to run the server.</span>
    <span class="s2">&quot;num_workers&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Clients can then connect in either <em>local</em> or <em>remote</em> inference mode.
In local inference mode, copies of the policy are downloaded from the server and cached on the client for a configurable period of time.
This allows actions to be computed by the client without requiring a network round trip each time.
In remote inference mode, each computed action requires a network call to the server.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">client</span> <span class="o">=</span> <span class="n">PolicyClient</span><span class="p">(</span><span class="s2">&quot;http://localhost:9900&quot;</span><span class="p">,</span> <span class="n">inference_mode</span><span class="o">=</span><span class="s2">&quot;local&quot;</span><span class="p">)</span>
<span class="n">episode_id</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">start_episode</span><span class="p">()</span>
<span class="o">...</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">episode_id</span><span class="p">,</span> <span class="n">cur_obs</span><span class="p">)</span>
<span class="o">...</span>
<span class="n">client</span><span class="o">.</span><span class="n">end_episode</span><span class="p">(</span><span class="n">episode_id</span><span class="p">,</span> <span class="n">last_obs</span><span class="p">)</span>
</pre></div>
</div>
<p>To understand the difference between standard envs, external envs, and connecting with a <code class="docutils literal notranslate"><span class="pre">PolicyClient</span></code>, refer to the following figure:</p>
<img alt="../_images/rllib-external.svg" src="../_images/rllib-external.svg" /><p>Try it yourself by launching either a
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/serving/cartpole_server.py">simple CartPole server</a> (see below), and connecting it to any number of clients
(<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/serving/cartpole_client.py">cartpole_client.py</a>) or
run a <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/serving/unity3d_server.py">Unity3D learning sever</a>
against distributed Unity game engines in the cloud.</p>
<p>CartPole Example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start the server by running:</span>
&gt;&gt;&gt; python rllib/examples/serving/cartpole_server.py --run<span class="o">=</span>PPO
--
-- Starting policy server at localhost:9900
--

<span class="c1"># To connect from a client with inference_mode=&quot;remote&quot;.</span>
&gt;&gt;&gt; python rllib/examples/serving/cartpole_client.py --inference-mode<span class="o">=</span>remote
Total reward: <span class="m">10</span>.0
Total reward: <span class="m">58</span>.0
...
Total reward: <span class="m">200</span>.0
...

<span class="c1"># To connect from a client with inference_mode=&quot;local&quot; (faster).</span>
&gt;&gt;&gt; python rllib/examples/serving/cartpole_client.py --inference-mode<span class="o">=</span><span class="nb">local</span>
Querying server <span class="k">for</span> new policy weights.
Generating new batch of experiences.
Total reward: <span class="m">13</span>.0
Total reward: <span class="m">11</span>.0
...
Sending batch of <span class="m">1000</span> steps back to server.
Querying server <span class="k">for</span> new policy weights.
...
Total reward: <span class="m">200</span>.0
...
</pre></div>
</div>
<p>For the best performance, we recommend using <code class="docutils literal notranslate"><span class="pre">inference_mode=&quot;local&quot;</span></code> when possible.</p>
</section>
</section>
<section id="advanced-integrations">
<h2>Advanced Integrations<a class="headerlink" href="../rllib-env.html#advanced-integrations" title="Permalink to this headline">#</a></h2>
<p>For more complex / high-performance environment integrations, you can instead extend the low-level <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/env/base_env.py">BaseEnv</a> class. This low-level API models multiple agents executing asynchronously in multiple environments. A call to <code class="docutils literal notranslate"><span class="pre">BaseEnv:poll()</span></code> returns observations from ready agents keyed by 1) their environment, then 2) agent ids. Actions for those agents are sent back via <code class="docutils literal notranslate"><span class="pre">BaseEnv:send_actions()</span></code>. BaseEnv is used to implement all the other env types in RLlib, so it offers a superset of their functionality. For example, <code class="docutils literal notranslate"><span class="pre">BaseEnv</span></code> is used to implement dynamic batching of observations for inference over <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/env/remote_vector_env.py">multiple simulator actors</a>.</p>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="key-concepts.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Key Concepts</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="rllib-algorithms.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Algorithms</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>