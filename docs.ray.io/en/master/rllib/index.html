
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>RLlib: Industry-Grade Reinforcement Learning &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/versionwarning.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../_static/js/docsearch.js"></script>
    <script src="../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../_static/js/termynal.js"></script>
    <script defer="defer" src="../_static/js/custom.js"></script>
    <script defer="defer" src="../_static/js/top-navigation.js"></script>
    <script src="../_static/js/tags.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/rllib/index.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Getting Started with RLlib" href="rllib-training.html" />
    <link rel="prev" title="Ray Serve API" href="../serve/api/index.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "rllib/index", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="current reference internal" href="index.html#">
   Ray RLlib
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-training.html">
     Getting Started with RLlib
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="key-concepts.html">
     Key Concepts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../rllib-env.html">
     Environments
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-algorithms.html">
     Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="user-guides.html">
     User Guides
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-examples.html">
     Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="package_ref/index.html">
     Ray RLlib API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Frllib/index.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/rllib/index.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/rllib/index.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="index.html#rllib-in-60-seconds">
   RLlib in 60 seconds
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="index.html#feature-overview">
   Feature Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="index.html#customizing-rllib">
   Customizing RLlib
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>RLlib: Industry-Grade Reinforcement Learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="index.html#rllib-in-60-seconds">
   RLlib in 60 seconds
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="index.html#feature-overview">
   Feature Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="index.html#customizing-rllib">
   Customizing RLlib
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <a class="reference external image-reference" href="https://ray-docs-promo.netlify.app/rllib"><img alt="" src="https://ray-docs-promo.netlify.app/assets/img/rllib/top.png" /></a>
<div class="bottom-right-promo-banner docutils">
<a class="reference external image-reference" href="https://ray-docs-promo.netlify.app/rllib"><img alt="" src="https://ray-docs-promo.netlify.app/assets/img/rllib/square.png" /></a>
</div>
<section id="rllib-industry-grade-reinforcement-learning">
<span id="rllib-index"></span><h1>RLlib: Industry-Grade Reinforcement Learning<a class="headerlink" href="index.html#rllib-industry-grade-reinforcement-learning" title="Permalink to this headline">#</a></h1>
<img alt="../_images/rllib-logo.png" class="align-center" src="../_images/rllib-logo.png" />
<p><strong>RLlib</strong> is an open-source library for reinforcement learning (RL),
offering support for
production-level, highly distributed RL workloads while maintaining
unified and simple APIs for a large variety of industry applications.
Whether you would like to train your agents in a <strong>multi-agent</strong> setup,
purely from <strong>offline</strong> (historic) datasets, or using <strong>externally
connected simulators</strong>, RLlib offers a simple solution for each of your decision
making needs.</p>
<p>If you either have your problem coded (in python) as an
<a class="reference external" href="../rllib-env.html#configuring-environments">RL environment</a>
or own lots of pre-recorded, historic behavioral data to learn from, you will be
up and running in only a few days.</p>
<p>RLlib is already used in production by industry leaders in many different verticals,
such as
<a class="reference external" href="https://www.anyscale.com/events/2021/06/23/applying-ray-and-rllib-to-real-life-industrial-use-cases">climate control</a>,
<a class="reference external" href="https://www.anyscale.com/events/2021/06/23/applying-ray-and-rllib-to-real-life-industrial-use-cases">industrial control</a>,
<a class="reference external" href="https://www.anyscale.com/events/2022/03/29/alphadow-leveraging-rays-ecosystem-to-train-and-deploy-an-rl-industrial">manufacturing and logistics</a>,
<a class="reference external" href="https://www.anyscale.com/events/2021/06/22/a-24x-speedup-for-reinforcement-learning-with-rllib-+-ray">finance</a>,
<a class="reference external" href="https://www.anyscale.com/events/2021/06/22/using-reinforcement-learning-to-optimize-iap-offer-recommendations-in-mobile-games">gaming</a>,
<a class="reference external" href="https://www.anyscale.com/events/2021/06/23/using-rllib-in-an-enterprise-scale-reinforcement-learning-solution">automobile</a>,
<a class="reference external" href="https://www.anyscale.com/events/2021/06/23/introducing-amazon-sagemaker-kubeflow-reinforcement-learning-pipelines-for">robotics</a>,
<a class="reference external" href="https://www.youtube.com/watch?v=cLCK13ryTpw">boat design</a>,
and many others.</p>
<section id="rllib-in-60-seconds">
<h2>RLlib in 60 seconds<a class="headerlink" href="index.html#rllib-in-60-seconds" title="Permalink to this headline">#</a></h2>
<figure class="align-default">
<img alt="../_images/rllib-index-header.svg" src="../_images/rllib-index-header.svg" /></figure>
<p>It only takes a few steps to get your first RLlib workload
up and running on your laptop.</p>
<p>RLlib does not automatically install a deep-learning framework, but supports
<strong>TensorFlow</strong> (both 1.x with static-graph and 2.x with eager mode) as well as
<strong>PyTorch</strong>.
Depending on your needs, make sure to install either TensorFlow or
PyTorch (or both, as shown below):</p>
<div class="termynal" data-termynal>
    <span data-ty="input">pip install "ray[rllib]" tensorflow torch</span>
</div><aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>For installation on computers running Apple Silicon (such as M1), please follow instructions
<code class="xref py py-obj docutils literal notranslate"><span class="pre">here</span></code>._
To be able to run our Atari examples, you should also install
<code class="xref py py-obj docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">&quot;gym[atari]&quot;</span> <span class="pre">&quot;gym[accept-rom-license]&quot;</span> <span class="pre">atari_py</span></code>.</p>
</aside>
<p>This is all you need to start coding against RLlib.
Here is an example of running a PPO Algorithm on the
<a class="reference external" href="https://www.gymlibrary.dev/environments/toy_text/taxi/">Taxi domain</a>.
We first create a <code class="xref py py-obj docutils literal notranslate"><span class="pre">config</span></code> for the algorithm, which sets the right environment, and
defines all training parameters we want.
Next, we <code class="xref py py-obj docutils literal notranslate"><span class="pre">build</span></code> the algorithm and <code class="xref py py-obj docutils literal notranslate"><span class="pre">train</span></code> it for a total of <code class="xref py py-obj docutils literal notranslate"><span class="pre">5</span></code> iterations.
A training iteration includes parallel sample collection by the environment workers, as well as loss calculation on the collected batch and a model update.
As a last step, we <code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code> the trained Algorithm:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPOConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># 1. Configure the algorithm,</span>
    <span class="n">PPOConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="s2">&quot;Taxi-v3&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="o">.</span><span class="n">framework</span><span class="p">(</span><span class="s2">&quot;torch&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;fcnet_hiddens&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">]})</span>
    <span class="o">.</span><span class="n">evaluation</span><span class="p">(</span><span class="n">evaluation_num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>  <span class="c1"># 2. build the algorithm,</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">())</span>  <span class="c1"># 3. train it,</span>

<span class="n">algo</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>  <span class="c1"># 4. and evaluate it.</span>
</pre></div>
</div>
<p>Note that you can use any Farama-Foundation Gymnasium environment as <code class="xref py py-obj docutils literal notranslate"><span class="pre">env</span></code>.
In <code class="xref py py-obj docutils literal notranslate"><span class="pre">rollouts</span></code> you can for instance specify the number of parallel workers to collect samples from the environment.
The <code class="xref py py-obj docutils literal notranslate"><span class="pre">framework</span></code> config lets you choose between “tf2”, “tf” and “torch” for execution.
You can also tweak RLlib’s default <code class="xref py py-obj docutils literal notranslate"><span class="pre">model</span></code> config,and set up a separate config for <code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluation</span></code>.</p>
<p>If you want to learn more about the RLlib training API,
<a class="reference external" href="rllib-training.html#using-the-python-api">you can learn more about it here</a>.
Also, see <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/inference_and_serving/policy_inference_after_training.py">here for a simple example on how to write an action inference loop after training.</a></p>
<p>If you want to get a quick preview of which <strong>algorithms</strong> and <strong>environments</strong> RLlib supports,
click on the dropdowns below:</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 sd-fade-in-slide-down">
<summary class="sd-summary-title sd-card-header">
<strong>RLlib Algorithms</strong><div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<ul class="simple">
<li><p class="sd-card-text">High-throughput architectures</p>
<ul>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#apex"><span class="std std-ref">Distributed Prioritized Experience Replay (Ape-X)</span></a></p></li>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#impala"><span class="std std-ref">Importance Weighted Actor-Learner Architecture (IMPALA)</span></a></p></li>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#appo"><span class="std std-ref">Asynchronous Proximal Policy Optimization (APPO)</span></a></p></li>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#ddppo"><span class="std std-ref">Decentralized Distributed Proximal Policy Optimization (DD-PPO)</span></a></p></li>
</ul>
</li>
<li><p class="sd-card-text">Gradient-based</p>
<ul>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#a3c"><span class="std std-ref">Advantage Actor-Critic (A2C, A3C)</span></a></p></li>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#ddpg"><span class="std std-ref">Deep Deterministic Policy Gradients (DDPG, TD3)</span></a></p></li>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#dqn"><span class="std std-ref">Deep Q Networks (DQN, Rainbow, Parametric DQN)</span></a></p></li>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#pg"><span class="std std-ref">Policy Gradients</span></a></p></li>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#ppo"><span class="std std-ref">Proximal Policy Optimization (PPO)</span></a></p></li>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#sac"><span class="std std-ref">Soft Actor Critic (SAC)</span></a></p></li>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#slateq"><span class="std std-ref">Slate Q-Learning (SlateQ)</span></a></p></li>
</ul>
</li>
<li><p class="sd-card-text">Derivative-free</p>
<ul>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#ars"><span class="std std-ref">Augmented Random Search (ARS)</span></a></p></li>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#es"><span class="std std-ref">Evolution Strategies</span></a></p></li>
</ul>
</li>
<li><p class="sd-card-text">Model-based / Meta-learning / Offline</p>
<ul>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#alphazero"><span class="std std-ref">Single-Player AlphaZero (AlphaZero)</span></a></p></li>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#maml"><span class="std std-ref">Model-Agnostic Meta-Learning (MAML)</span></a></p></li>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#mbmpo"><span class="std std-ref">Model-Based Meta-Policy-Optimization (MBMPO)</span></a></p></li>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#dreamer"><span class="std std-ref">Dreamer (DREAMER)</span></a></p></li>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#cql"><span class="std std-ref">Conservative Q-Learning (CQL)</span></a></p></li>
</ul>
</li>
<li><p class="sd-card-text">Multi-agent</p>
<ul>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#qmix"><span class="std std-ref">QMIX Monotonic Value Factorisation (QMIX, VDN, IQN)</span></a></p></li>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#maddpg"><span class="std std-ref">Multi-Agent Deep Deterministic Policy Gradient (MADDPG)</span></a></p></li>
</ul>
</li>
<li><p class="sd-card-text">Offline</p>
<ul>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#marwil"><span class="std std-ref">Advantage Re-Weighted Imitation Learning (MARWIL)</span></a></p></li>
</ul>
</li>
<li><p class="sd-card-text">Contextual bandits</p>
<ul>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#lin-ucb"><span class="std std-ref">Linear Upper Confidence Bound (LinUCB)</span></a></p></li>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#lints"><span class="std std-ref">Linear Thompson Sampling (LinTS)</span></a></p></li>
</ul>
</li>
<li><p class="sd-card-text">Exploration-based plug-ins (can be combined with any algo)</p>
<ul>
<li><p class="sd-card-text"><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 16px;" /></a> <a class="reference internal" href="rllib-algorithms.html#curiosity"><span class="std std-ref">Curiosity (ICM: Intrinsic Curiosity Module)</span></a></p></li>
</ul>
</li>
</ul>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 sd-fade-in-slide-down">
<summary class="sd-summary-title sd-card-header">
<strong>RLlib Environments</strong><div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<ul class="simple">
<li><p class="sd-card-text"><a class="reference external" href="../rllib-env.html">RLlib Environments Overview</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="../rllib-env.html#gymnasium">Farama-Foundation gymnasium</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="../rllib-env.html#vectorized">Vectorized</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="../rllib-env.html#multi-agent-and-hierarchical">Multi-Agent and Hierarchical</a></p></li>
<li><p class="sd-card-text"><a class="reference external" href="../rllib-env.html#external-agents-and-applications">External Agents and Applications</a></p>
<ul>
<li><p class="sd-card-text"><a class="reference external" href="../rllib-env.html#external-application-clients">External Application Clients</a></p></li>
</ul>
</li>
<li><p class="sd-card-text"><a class="reference external" href="../rllib-env.html#advanced-integrations">Advanced Integrations</a></p></li>
</ul>
</div>
</details></section>
<section id="feature-overview">
<h2>Feature Overview<a class="headerlink" href="index.html#feature-overview" title="Permalink to this headline">#</a></h2>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 container pb-4 docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-2 sd-row-cols-md-3 sd-row-cols-lg-3 sd-g-1 sd-g-xs-1 sd-g-sm-1 sd-g-md-1 sd-g-lg-1 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-header docutils">
<p class="sd-card-text"><strong>RLlib Key Concepts</strong></p>
</div>
<div class="sd-card-body docutils">
<p class="sd-card-text">Learn more about the core concepts of RLlib, such as environments, algorithms and
policies.</p>
</div>
<div class="sd-card-footer docutils">
<p class="sd-card-text"><span class="sd-d-grid"><a class="sd-sphinx-override sd-btn sd-text-wrap sd-btn-outline-primary reference internal" href="key-concepts.html#rllib-core-concepts"><span class="std std-ref">Key Concepts</span></a></span></p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-header docutils">
<p class="sd-card-text"><strong>RLlib Algorithms</strong></p>
</div>
<div class="sd-card-body docutils">
<p class="sd-card-text">Check out the many available RL algorithms of RLlib for model-free and model-based
RL, on-policy and off-policy training, multi-agent RL, and more.</p>
</div>
<div class="sd-card-footer docutils">
<p class="sd-card-text"><span class="sd-d-grid"><a class="sd-sphinx-override sd-btn sd-text-wrap sd-btn-outline-primary reference internal" href="rllib-algorithms.html#rllib-algorithms-doc"><span class="std std-ref">Algorithms</span></a></span></p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-header docutils">
<p class="sd-card-text"><strong>RLlib Environments</strong></p>
</div>
<div class="sd-card-body docutils">
<p class="sd-card-text">Get started with environments supported by RLlib, such as Farama foundation’s Gymnasium, Petting Zoo,
and many custom formats for vectorized and multi-agent environments.</p>
</div>
<div class="sd-card-footer docutils">
<p class="sd-card-text"><span class="sd-d-grid"><a class="sd-sphinx-override sd-btn sd-text-wrap sd-btn-outline-primary reference internal" href="../rllib-env.html#rllib-environments-doc"><span class="std std-ref">Environments</span></a></span></p>
</div>
</div>
</div>
</div>
</div>
<p>The following is a summary of RLlib’s most striking features.
Click on the images below to see an example script for each of the listed features:</p>
<div class="clear-both docutils container">
<div class="buttons-float-left docutils container">
<a class="reference external image-reference" href="https://github.com/ray-project/ray/blob/master/rllib/examples/custom_tf_policy.py"><img alt="../_images/rllib-sigil-tf-and-torch.svg" src="../_images/rllib-sigil-tf-and-torch.svg" width="100" /></a>
</div>
<div class="docutils container">
<p>The most <strong>popular deep-learning frameworks</strong>: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/custom_torch_policy.py">PyTorch</a> and <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/custom_tf_policy.py">TensorFlow
(tf1.x/2.x static-graph/eager/traced)</a>.</p>
</div>
</div>
<div class="clear-both docutils container">
<div class="buttons-float-left docutils container">
<a class="reference external image-reference" href="https://github.com/ray-project/ray/blob/master/rllib/examples/tune/framework.py"><img alt="../_images/rllib-sigil-distributed-learning.svg" src="../_images/rllib-sigil-distributed-learning.svg" width="100" /></a>
</div>
<div class="docutils container">
<p><strong>Highly distributed learning</strong>: Our RLlib algorithms (such as our “PPO” or “IMPALA”)
allow you to set the <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> config parameter, such that your workloads can run
on 100s of CPUs/nodes thus parallelizing and speeding up learning.</p>
</div>
</div>
<div class="clear-both docutils container">
<div class="buttons-float-left docutils container">
<a class="reference external image-reference" href="https://github.com/ray-project/ray/blob/master/rllib/examples/env_rendering_and_recording.py"><img alt="../_images/rllib-sigil-vector-envs.svg" src="../_images/rllib-sigil-vector-envs.svg" width="100" /></a>
</div>
<div class="docutils container">
<p><strong>Vectorized (batched) and remote (parallel) environments</strong>: RLlib auto-vectorizes
your <code class="docutils literal notranslate"><span class="pre">gym.Envs</span></code> via the <code class="docutils literal notranslate"><span class="pre">num_envs_per_worker</span></code> config. Environment workers can
then batch and thus significantly speedup the action computing forward pass.
On top of that, RLlib offers the <code class="docutils literal notranslate"><span class="pre">remote_worker_envs</span></code> config to create
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/remote_envs_with_inference_done_on_main_node.py">single environments (within a vectorized one) as ray Actors</a>,
thus parallelizing even the env stepping process.</p>
</div>
</div>
<div class="clear-both docutils container">
<div class="buttons-float-left docutils container">
<a class="reference external image-reference" href="https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_independent_learning.py"><img alt="../_images/rllib-sigil-multi-agent.svg" src="../_images/rllib-sigil-multi-agent.svg" width="100" /></a>
</div>
<div class="docutils container">
<div class="line-block">
<div class="line"><strong>Multi-agent RL</strong> (MARL): Convert your (custom) <code class="docutils literal notranslate"><span class="pre">gym.Envs</span></code> into a multi-agent one
via a few simple steps and start training your agents in any of the following fashions:</div>
<div class="line">1) Cooperative with <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/centralized_critic.py">shared</a> or
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/two_step_game.py">separate</a>
policies and/or value functions.</div>
<div class="line">2) Adversarial scenarios using <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/self_play_with_open_spiel.py">self-play</a>
and <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/self_play_league_based_with_open_spiel.py">league-based training</a>.</div>
<div class="line">3) <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_independent_learning.py">Independent learning</a>
of neutral/co-existing agents.</div>
</div>
</div>
</div>
<div class="clear-both docutils container">
<div class="buttons-float-left docutils container">
<a class="reference external image-reference" href="https://github.com/ray-project/ray/tree/master/rllib/examples/serving"><img alt="../_images/rllib-sigil-external-simulators.svg" src="../_images/rllib-sigil-external-simulators.svg" width="100" /></a>
</div>
<div class="docutils container">
<p><strong>External simulators</strong>: Don’t have your simulation running as a gym.Env in python?
No problem! RLlib supports an external environment API and comes with a pluggable,
off-the-shelve
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/serving/cartpole_client.py">client</a>/
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/serving/cartpole_server.py">server</a>
setup that allows you to run 100s of independent simulators on the “outside”
(e.g. a Windows cloud) connecting to a central RLlib Policy-Server that learns
and serves actions. Alternatively, actions can be computed on the client side
to save on network traffic.</p>
</div>
</div>
<div class="clear-both docutils container">
<div class="buttons-float-left docutils container">
<a class="reference external image-reference" href="https://github.com/ray-project/ray/blob/master/rllib/examples/offline_rl.py"><img alt="../_images/rllib-sigil-offline-rl.svg" src="../_images/rllib-sigil-offline-rl.svg" width="100" /></a>
</div>
<div class="docutils container">
<p><strong>Offline RL and imitation learning/behavior cloning</strong>: You don’t have a simulator
for your particular problem, but tons of historic data recorded by a legacy (maybe
non-RL/ML) system? This branch of reinforcement learning is for you!
RLlib’s comes with several <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/offline_rl.py">offline RL</a>
algorithms (<em>CQL</em>, <em>MARWIL</em>, and <em>DQfD</em>), allowing you to either purely
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/bc/tests/test_bc.py">behavior-clone</a>
your existing system or learn how to further improve over it.</p>
</div>
</div>
</section>
<section id="customizing-rllib">
<h2>Customizing RLlib<a class="headerlink" href="index.html#customizing-rllib" title="Permalink to this headline">#</a></h2>
<p>RLlib provides simple APIs to customize all aspects of your training- and experimental workflows.
For example, you may code your own <a class="reference external" href="../rllib-env.html#configuring-environments">environments</a>
in python using Farama-Foundation’s gymnasium or DeepMind’s OpenSpiel, provide custom
<a class="reference external" href="rllib-models.html#tensorflow-models">TensorFlow/Keras-</a> or ,
<a class="reference external" href="rllib-models.html#torch-models">Torch models</a>, write your own
<a class="reference external" href="rllib-concepts.html#policies">policy- and loss definitions</a>, or define
custom <a class="reference external" href="rllib-training.html#exploration-api">exploratory behavior</a>.</p>
<p>Via mapping one or more agents in your environments to (one or more) policies, multi-agent
RL (MARL) becomes an easy-to-use low-level primitive for our users.</p>
<figure class="align-left" id="id1">
<a class="reference internal image-reference" href="../_images/rllib-stack.svg"><img alt="../_images/rllib-stack.svg" src="../_images/rllib-stack.svg" width="650" /></a>
<figcaption>
<p><span class="caption-text"><strong>RLlib’s API stack:</strong> Built on top of Ray, RLlib offers off-the-shelf, highly distributed
algorithms, policies, loss functions, and default models (including the option to
auto-wrap a neural network with an LSTM or an attention net). Furthermore, our library
comes with a built-in Server/Client setup, allowing you to connect
hundreds of external simulators (clients) via the network to an RLlib server process,
which provides learning functionality and serves action queries. User customizations
are realized via sub-classing the existing abstractions and - by overriding certain
methods in those sub-classes - define custom behavior.</span><a class="headerlink" href="index.html#id1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<div class="toctree-wrapper compound">
</div>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../serve/api/index.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Ray Serve API</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="rllib-training.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Getting Started with RLlib</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>