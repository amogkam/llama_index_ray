
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Key Concepts &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/versionwarning.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../_static/js/docsearch.js"></script>
    <script src="../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../_static/js/termynal.js"></script>
    <script defer="defer" src="../_static/js/custom.js"></script>
    <script defer="defer" src="../_static/js/top-navigation.js"></script>
    <script src="../_static/js/tags.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/rllib/key-concepts.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Environments" href="../rllib-env.html" />
    <link rel="prev" title="Getting Started with RLlib" href="rllib-training.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "rllib/key-concepts", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Ray RLlib
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-training.html">
     Getting Started with RLlib
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="key-concepts.html#">
     Key Concepts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../rllib-env.html">
     Environments
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-algorithms.html">
     Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="user-guides.html">
     User Guides
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-examples.html">
     Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="package_ref/index.html">
     Ray RLlib API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Frllib/key-concepts.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/rllib/key-concepts.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/rllib/key-concepts.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="key-concepts.html#environments">
   Environments
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="key-concepts.html#algorithms">
   Algorithms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="key-concepts.html#policies">
   Policies
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="key-concepts.html#policy-evaluation">
   Policy Evaluation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="key-concepts.html#sample-batches">
   Sample Batches
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="key-concepts.html#training-step-method-algorithm-training-step">
   Training Step Method (
   <code class="docutils literal notranslate">
    <span class="pre">
     Algorithm.training_step()
    </span>
   </code>
   )
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="key-concepts.html#what-is-it">
     What is it?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="key-concepts.html#when-is-training-step-invoked">
     When is
     <code class="docutils literal notranslate">
      <span class="pre">
       training_step()
      </span>
     </code>
     invoked?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="key-concepts.html#key-subconcepts">
     Key Subconcepts
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="key-concepts.html#training-step-method-utilities">
     Training Step Method Utilities
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Key Concepts</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="key-concepts.html#environments">
   Environments
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="key-concepts.html#algorithms">
   Algorithms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="key-concepts.html#policies">
   Policies
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="key-concepts.html#policy-evaluation">
   Policy Evaluation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="key-concepts.html#sample-batches">
   Sample Batches
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="key-concepts.html#training-step-method-algorithm-training-step">
   Training Step Method (
   <code class="docutils literal notranslate">
    <span class="pre">
     Algorithm.training_step()
    </span>
   </code>
   )
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="key-concepts.html#what-is-it">
     What is it?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="key-concepts.html#when-is-training-step-invoked">
     When is
     <code class="docutils literal notranslate">
      <span class="pre">
       training_step()
      </span>
     </code>
     invoked?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="key-concepts.html#key-subconcepts">
     Key Subconcepts
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="key-concepts.html#training-step-method-utilities">
     Training Step Method Utilities
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <a class="reference external image-reference" href="https://ray-docs-promo.netlify.app/rllib"><img alt="" src="https://ray-docs-promo.netlify.app/assets/img/rllib/top.png" /></a>
<div class="bottom-right-promo-banner docutils">
<a class="reference external image-reference" href="https://ray-docs-promo.netlify.app/rllib"><img alt="" src="https://ray-docs-promo.netlify.app/assets/img/rllib/square.png" /></a>
</div>
<section id="key-concepts">
<span id="rllib-core-concepts"></span><h1>Key Concepts<a class="headerlink" href="key-concepts.html#key-concepts" title="Permalink to this headline">#</a></h1>
<p>On this page, we’ll cover the key concepts to help you understand how RLlib works and
how to use it. In RLlib, you use <code class="docutils literal notranslate"><span class="pre">Algorithm</span></code>’s to learn how to solve problem <code class="docutils literal notranslate"><span class="pre">environments</span></code>.
The algorithms use <code class="docutils literal notranslate"><span class="pre">policies</span></code> to select actions. Given a policy,
<code class="docutils literal notranslate"><span class="pre">rollouts</span></code> throughout an <code class="docutils literal notranslate"><span class="pre">environment</span></code> produce
<code class="docutils literal notranslate"><span class="pre">sample</span> <span class="pre">batches</span></code> (or <code class="docutils literal notranslate"><span class="pre">trajectories</span></code>) of experiences.
You can also customize the <code class="docutils literal notranslate"><span class="pre">training_step</span></code>s of your RL experiments.</p>
<section id="environments">
<span id="id1"></span><h2>Environments<a class="headerlink" href="key-concepts.html#environments" title="Permalink to this headline">#</a></h2>
<p>Solving a problem in RL begins with an <strong>environment</strong>. In the simplest definition of RL:</p>
<blockquote>
<div><p>An <strong>agent</strong> interacts with an <strong>environment</strong> and receives a reward.</p>
</div></blockquote>
<p>An environment in RL is the agent’s world, it is a simulation of the problem to be solved.</p>
<img alt="../_images/env_key_concept1.png" src="../_images/env_key_concept1.png" />
<p>An RLlib environment consists of:</p>
<ol class="arabic simple">
<li><p>all possible actions (<strong>action space</strong>)</p></li>
<li><p>a complete description of the environment, nothing hidden (<strong>state space</strong>)</p></li>
<li><p>an observation by the agent of certain parts of the state (<strong>observation space</strong>)</p></li>
<li><p><strong>reward</strong>, which is the only feedback the agent receives per action.</p></li>
</ol>
<p>The model that tries to maximize the expected sum over all future rewards is called a <strong>policy</strong>. The policy is a function mapping the environment’s observations to an action to take, usually written <strong>π</strong> (s(t)) -&gt; a(t). Below is a diagram of the RL iterative learning process.</p>
<img alt="../_images/env_key_concept2.png" src="../_images/env_key_concept2.png" />
<p>The RL simulation feedback loop repeatedly collects data, for one (single-agent case) or multiple (multi-agent case) policies, trains the policies on these collected data, and makes sure the policies’ weights are kept in sync. Thereby, the collected environment data contains observations, taken actions, received rewards and so-called <strong>done</strong> flags, indicating the boundaries of different episodes the agents play through in the simulation.</p>
<p>The simulation iterations of action -&gt; reward -&gt; next state -&gt; train -&gt; repeat, until the end state, is called an <strong>episode</strong>, or in RLlib, a <strong>rollout</strong>.</p>
</section>
<section id="algorithms">
<span id="id2"></span><h2>Algorithms<a class="headerlink" href="key-concepts.html#algorithms" title="Permalink to this headline">#</a></h2>
<p>Algorithms bring all RLlib components together, making learning of different tasks
accessible via RLlib’s Python API and its command line interface (CLI).
Each <code class="docutils literal notranslate"><span class="pre">Algorithm</span></code> class is managed by its respective <code class="docutils literal notranslate"><span class="pre">AlgorithmConfig</span></code>, for example to
configure a <code class="docutils literal notranslate"><span class="pre">PPO</span></code> instance, you should use the <code class="docutils literal notranslate"><span class="pre">PPOConfig</span></code> class.
An <code class="docutils literal notranslate"><span class="pre">Algorithm</span></code> sets up its rollout workers and optimizers, and collects training metrics.
<code class="docutils literal notranslate"><span class="pre">Algorithms</span></code> also implement the <a class="reference internal" href="../tune/key-concepts.html#tune-60-seconds"><span class="std std-ref">Tune Trainable API</span></a> for
easy experiment management.</p>
<p>You have three ways to interact with an algorithm. You can use the basic Python API or the command line to train it, or you
can use Ray Tune to tune hyperparameters of your reinforcement learning algorithm.
The following example shows three equivalent ways of interacting with <code class="docutils literal notranslate"><span class="pre">PPO</span></code>,
which implements the proximal policy optimization algorithm in RLlib.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-0">
Basic RLlib Algorithm</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Configure.</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPOConfig</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">PPOConfig</span><span class="p">()</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">train_batch_size</span><span class="o">=</span><span class="mi">4000</span><span class="p">)</span>

<span class="c1"># Build.</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>

<span class="c1"># Train.</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">())</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-1">
RLlib Algorithms and Tune</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>

<span class="c1"># Configure.</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPOConfig</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">PPOConfig</span><span class="p">()</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">train_batch_size</span><span class="o">=</span><span class="mi">4000</span><span class="p">)</span>

<span class="c1"># Train via Ray Tune.</span>
<span class="n">tune</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;PPO&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-2" name="sd-tab-set-0" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-2">
RLlib Command Line</label><div class="sd-tab-content docutils">
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>rllib train --run<span class="o">=</span>PPO --env<span class="o">=</span>CartPole-v1 --config<span class="o">=</span><span class="s1">&#39;{&quot;train_batch_size&quot;: 4000}&#39;</span>
</pre></div>
</div>
</div>
</div>
<p>RLlib <a class="reference external" href="rllib-concepts.html#algorithms">Algorithm classes</a> coordinate the distributed workflow of running rollouts and optimizing policies.
Algorithm classes leverage parallel iterators to implement the desired computation pattern.
The following figure shows <em>synchronous sampling</em>, the simplest of <a class="reference external" href="rllib-algorithms.html">these patterns</a>:</p>
<figure class="align-default" id="id3">
<img alt="../_images/a2c-arch.svg" src="../_images/a2c-arch.svg" /><figcaption>
<p><span class="caption-text">Synchronous Sampling (e.g., A2C, PG, PPO)</span><a class="headerlink" href="key-concepts.html#id3" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>RLlib uses <a class="reference external" href="actors.html">Ray actors</a> to scale training from a single core to many thousands of cores in a cluster.
You can <a class="reference external" href="rllib-training.html#specifying-resources">configure the parallelism</a> used for training by changing the <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> parameter.
Check out our <a class="reference external" href="rllib-training.html#scaling-guide">scaling guide</a> for more details here.</p>
</section>
<section id="policies">
<h2>Policies<a class="headerlink" href="key-concepts.html#policies" title="Permalink to this headline">#</a></h2>
<p><a class="reference external" href="rllib-concepts.html#policies">Policies</a> are a core concept in RLlib. In a nutshell, policies are
Python classes that define how an agent acts in an environment.
<a class="reference external" href="rllib-concepts.html#policy-evaluation">Rollout workers</a> query the policy to determine agent actions.
In a <a class="reference external" href="../rllib-env.html#gymnasium">Farama-Foundation Gymnasium</a> environment, there is a single agent and policy.
In <a class="reference external" href="../rllib-env.html#vectorized">vector envs</a>, policy inference is for multiple agents at once,
and in <a class="reference external" href="../rllib-env.html#multi-agent-and-hierarchical">multi-agent</a>, there may be multiple policies,
each controlling one or more agents:</p>
<img alt="../_images/multi-flat.svg" src="../_images/multi-flat.svg" /><p>Policies can be implemented using <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/policy/policy.py">any framework</a>.
However, for TensorFlow and PyTorch, RLlib has
<a class="reference external" href="rllib-concepts.html#building-policies-in-tensorflow">build_tf_policy</a> and
<a class="reference external" href="rllib-concepts.html#building-policies-in-pytorch">build_torch_policy</a> helper functions that let you
define a trainable policy with a functional-style API, for example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policy_gradient_loss</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">dist_class</span><span class="p">,</span> <span class="n">train_batch</span><span class="p">):</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">from_batch</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
    <span class="n">action_dist</span> <span class="o">=</span> <span class="n">dist_class</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
        <span class="n">action_dist</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="n">train_batch</span><span class="p">[</span><span class="s2">&quot;actions&quot;</span><span class="p">])</span> <span class="o">*</span> <span class="n">train_batch</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">])</span>

<span class="c1"># &lt;class &#39;ray.rllib.policy.tf_policy_template.MyTFPolicy&#39;&gt;</span>
<span class="n">MyTFPolicy</span> <span class="o">=</span> <span class="n">build_tf_policy</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;MyTFPolicy&quot;</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">policy_gradient_loss</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="policy-evaluation">
<h2>Policy Evaluation<a class="headerlink" href="key-concepts.html#policy-evaluation" title="Permalink to this headline">#</a></h2>
<p>Given an environment and policy, policy evaluation produces <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py">batches</a> of experiences. This is your classic “environment interaction loop”. Efficient policy evaluation can be burdensome to get right, especially when leveraging vectorization, RNNs, or when operating in a multi-agent environment. RLlib provides a <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/evaluation/rollout_worker.py">RolloutWorker</a> class that manages all of this, and this class is used in most RLlib algorithms.</p>
<p>You can use rollout workers standalone to produce batches of experiences. This can be done by calling <code class="docutils literal notranslate"><span class="pre">worker.sample()</span></code> on a worker instance, or <code class="docutils literal notranslate"><span class="pre">worker.sample.remote()</span></code> in parallel on worker instances created as Ray actors (see <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/evaluation/worker_set.py">WorkerSet</a>).</p>
<p>Here is an example of creating a set of rollout workers and using them gather experiences in parallel. The trajectories are concatenated, the policy learns on the trajectory batch, and then we broadcast the policy weights to the workers for the next round of rollouts:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup policy and rollout workers.</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">CustomPolicy</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span> <span class="p">{})</span>
<span class="n">workers</span> <span class="o">=</span> <span class="n">WorkerSet</span><span class="p">(</span>
    <span class="n">policy_class</span><span class="o">=</span><span class="n">CustomPolicy</span><span class="p">,</span>
    <span class="n">env_creator</span><span class="o">=</span><span class="k">lambda</span> <span class="n">c</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">),</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="c1"># Gather a batch of samples.</span>
    <span class="n">T1</span> <span class="o">=</span> <span class="n">SampleBatch</span><span class="o">.</span><span class="n">concat_samples</span><span class="p">(</span>
        <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">([</span><span class="n">w</span><span class="o">.</span><span class="n">sample</span><span class="o">.</span><span class="n">remote</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">workers</span><span class="o">.</span><span class="n">remote_workers</span><span class="p">()]))</span>

    <span class="c1"># Improve the policy using the T1 batch.</span>
    <span class="n">policy</span><span class="o">.</span><span class="n">learn_on_batch</span><span class="p">(</span><span class="n">T1</span><span class="p">)</span>

    <span class="c1"># The local worker acts as a &quot;parameter server&quot; here.</span>
    <span class="c1"># We put the weights of its `policy` into the Ray object store once (`ray.put`)...</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">put</span><span class="p">({</span><span class="s2">&quot;default_policy&quot;</span><span class="p">:</span> <span class="n">policy</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()})</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">workers</span><span class="o">.</span><span class="n">remote_workers</span><span class="p">():</span>
        <span class="c1"># ... so that we can broacast these weights to all rollout-workers once.</span>
        <span class="n">w</span><span class="o">.</span><span class="n">set_weights</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="sample-batches">
<h2>Sample Batches<a class="headerlink" href="key-concepts.html#sample-batches" title="Permalink to this headline">#</a></h2>
<p>Whether running in a single process or a <a class="reference external" href="rllib-training.html#specifying-resources">large cluster</a>,
all data in RLlib is interchanged in the form of <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py">sample batches</a>.
Sample batches encode one or more fragments of a trajectory.
Typically, RLlib collects batches of size <code class="docutils literal notranslate"><span class="pre">rollout_fragment_length</span></code> from rollout workers, and concatenates one or
more of these batches into a batch of size <code class="docutils literal notranslate"><span class="pre">train_batch_size</span></code> that is the input to SGD.</p>
<p>A typical sample batch looks something like the following when summarized.
Since all values are kept in arrays, this allows for efficient encoding and transmission across the network:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sample_batch</span> <span class="o">=</span> <span class="p">{</span> <span class="s1">&#39;action_logp&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">((</span><span class="mi">200</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">0.701</span><span class="p">,</span> <span class="nb">max</span><span class="o">=-</span><span class="mf">0.685</span><span class="p">,</span> <span class="n">mean</span><span class="o">=-</span><span class="mf">0.694</span><span class="p">),</span>
    <span class="s1">&#39;actions&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">((</span><span class="mi">200</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int64</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.495</span><span class="p">),</span>
    <span class="s1">&#39;dones&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">((</span><span class="mi">200</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.055</span><span class="p">),</span>
    <span class="s1">&#39;infos&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">((</span><span class="mi">200</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="p">{}),</span>
    <span class="s1">&#39;new_obs&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">((</span><span class="mi">200</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">2.46</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">2.259</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.018</span><span class="p">),</span>
    <span class="s1">&#39;obs&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">((</span><span class="mi">200</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">2.46</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">2.259</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.016</span><span class="p">),</span>
    <span class="s1">&#39;rewards&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">((</span><span class="mi">200</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span>
    <span class="s1">&#39;t&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">((</span><span class="mi">200</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int64</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">34.0</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">9.14</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In <a class="reference external" href="rllib-concepts.html#policies-in-multi-agent">multi-agent mode</a>,
sample batches are collected separately for each individual policy.
These batches are wrapped up together in a <code class="docutils literal notranslate"><span class="pre">MultiAgentBatch</span></code>,
serving as a container for the individual agents’ sample batches.</p>
</section>
<section id="training-step-method-algorithm-training-step">
<h2>Training Step Method (<code class="docutils literal notranslate"><span class="pre">Algorithm.training_step()</span></code>)<a class="headerlink" href="key-concepts.html#training-step-method-algorithm-training-step" title="Permalink to this headline">#</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It’s important to have a good understanding of the basic <a class="reference internal" href="../ray-core/walkthrough.html#core-walkthrough"><span class="std std-ref">ray core methods</span></a> before reading this section.
Furthermore, we utilize concepts such as the <code class="docutils literal notranslate"><span class="pre">SampleBatch</span></code> (and its more advanced sibling: the <code class="docutils literal notranslate"><span class="pre">MultiAgentBatch</span></code>),
<code class="docutils literal notranslate"><span class="pre">RolloutWorker</span></code>, and <code class="docutils literal notranslate"><span class="pre">Algorithm</span></code>, which can be read about on this page
and the <a class="reference internal" href="package_ref/evaluation.html#rolloutworker-reference-docs"><span class="std std-ref">rollout worker reference docs</span></a>.</p>
<p>Finally, developers who are looking to implement custom algorithms should familiarize themselves with the <a class="reference internal" href="rllib-concepts.html#rllib-policy-walkthrough"><span class="std std-ref">Policy</span></a> and
<a class="reference internal" href="rllib-models.html#rllib-models-walkthrough"><span class="std std-ref">Model</span></a> classes.</p>
</div>
<section id="what-is-it">
<h3>What is it?<a class="headerlink" href="key-concepts.html#what-is-it" title="Permalink to this headline">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> method of the <code class="docutils literal notranslate"><span class="pre">Algorithm</span></code> class defines the repeatable
execution logic that sits at the core of any algorithm. Think of it as the python implementation
of an algorithm’s pseudocode you can find in research papers.
You can use <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> to express how you want to
coordinate the collection of samples from the environment(s), the movement of this data to other
parts of the algorithm, and the updates and management of your policy’s weights
across the different distributed components.</p>
<p><strong>In short, a developer will need to override/modify the ``training_step`` method if they want to
make custom changes to an existing algorithm, write their own algo from scratch, or implement some algorithm from a paper.</strong></p>
</section>
<section id="when-is-training-step-invoked">
<h3>When is <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> invoked?<a class="headerlink" href="key-concepts.html#when-is-training-step-invoked" title="Permalink to this headline">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">Algorithm</span></code>’s <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> method is called:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>when the <code class="docutils literal notranslate"><span class="pre">train()</span></code> method of <code class="docutils literal notranslate"><span class="pre">Algorithm</span></code> is called (e.g. “manually” by a user that has constructed an <code class="docutils literal notranslate"><span class="pre">Algorithm</span></code> instance).</p></li>
<li><p>when an RLlib Algorithm is being run by Ray Tune. <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> will be continuously called till the
<a class="reference internal" href="../tune/api/execution.html#tune-run-ref"><span class="std std-ref">ray tune stop criteria</span></a> is met.</p></li>
</ol>
</div></blockquote>
</section>
<section id="key-subconcepts">
<h3>Key Subconcepts<a class="headerlink" href="key-concepts.html#key-subconcepts" title="Permalink to this headline">#</a></h3>
<p>In the following, using the example of VPG (“vanilla policy gradient”), we will try to illustrate
how to use the <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> method to implement this algorithm in RLlib.
The “vanilla policy gradient” algo can be thought of as a sequence of repeating steps, or <em>dataflow</em>, of:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Sampling (to collect data from an env)</p></li>
<li><p>Updating the Policy (to learn a behavior)</p></li>
<li><p>Broadcasting the updated Policy’s weights (to make sure all distributed units have the same weights again)</p></li>
<li><p>Metrics reporting (returning relevant stats from all the above operations with regards to performance and runtime)</p></li>
</ol>
</div></blockquote>
<p>An example implementation of VPG could look like the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ResultDict</span><span class="p">:</span>
    <span class="c1"># 1. Sampling.</span>
    <span class="n">train_batch</span> <span class="o">=</span> <span class="n">synchronous_parallel_sample</span><span class="p">(</span>
                    <span class="n">worker_set</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">,</span>
                    <span class="n">max_env_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;train_batch_size&quot;</span><span class="p">]</span>
                <span class="p">)</span>

    <span class="c1"># 2. Updating the Policy.</span>
    <span class="n">train_results</span> <span class="o">=</span> <span class="n">train_one_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_batch</span><span class="p">)</span>

    <span class="c1"># 3. Synchronize worker weights.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">()</span>

    <span class="c1"># 4. Return results.</span>
    <span class="k">return</span> <span class="n">train_results</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">training_step</span></code> method is deep learning framework agnostic.
This means that you should not write PyTorch- or TensorFlow specific code inside this module,
allowing for a strict separation of concerns and enabling us to use the same <code class="docutils literal notranslate"><span class="pre">training_step()</span></code>
method for both TF- and PyTorch versions of your algorithms.
DL framework specific code should only be added to the
<a class="reference internal" href="rllib-concepts.html#rllib-policy-walkthrough"><span class="std std-ref">Policy</span></a> (e.g. in its loss function(s)) and
<a class="reference internal" href="rllib-models.html#rllib-models-walkthrough"><span class="std std-ref">Model</span></a> (e.g. tf.keras or torch.nn neural network code) classes.</p>
</div>
<p>Let’s further break down our above <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> code.
In the first step, we collect trajectory data from the environment(s):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_batch</span> <span class="o">=</span> <span class="n">synchronous_parallel_sample</span><span class="p">(</span>
                    <span class="n">worker_set</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">,</span>
                    <span class="n">max_env_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;train_batch_size&quot;</span><span class="p">]</span>
                <span class="p">)</span>
</pre></div>
</div>
<p>Here, <code class="docutils literal notranslate"><span class="pre">self.workers</span></code> is a set of <code class="docutils literal notranslate"><span class="pre">RolloutWorkers</span></code> that are created in the <code class="docutils literal notranslate"><span class="pre">Algorithm</span></code>’s <code class="docutils literal notranslate"><span class="pre">setup()</span></code> method
(prior to calling <code class="docutils literal notranslate"><span class="pre">training_step()</span></code>).
This <code class="docutils literal notranslate"><span class="pre">WorkerSet</span></code> is covered in greater depth on the <a class="reference internal" href="package_ref/evaluation.html#workerset-reference-docs"><span class="std std-ref">WorkerSet documentation page</span></a>.
The utility function <code class="docutils literal notranslate"><span class="pre">synchronous_parallel_sample</span></code> can be used for parallel sampling in a blocking
fashion across multiple rollout workers (returns once all rollout workers are done sampling).
It returns one final MultiAgentBatch resulting from concatenating n smaller MultiAgentBatches
(exactly one from each remote rollout worker).</p>
<p>The <code class="docutils literal notranslate"><span class="pre">train_batch</span></code> is then passed to another utility function: <code class="docutils literal notranslate"><span class="pre">train_one_step</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_results</span> <span class="o">=</span> <span class="n">train_one_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_batch</span><span class="p">)</span>
</pre></div>
</div>
<p>Methods like <code class="docutils literal notranslate"><span class="pre">train_one_step</span></code> and <code class="docutils literal notranslate"><span class="pre">multi_gpu_train_one_step</span></code> are used for training our Policy.
Further documentation with examples can be found on the <a class="reference internal" href="package_ref/utils.html#train-ops-docs"><span class="std std-ref">train ops documentation page</span></a>.</p>
<p>The training updates on the policy are only applied to its version inside <code class="docutils literal notranslate"><span class="pre">self.workers.local_worker</span></code>.
Note that each WorkerSet has n remote workers and exactly one “local worker” and that each worker (remote and local ones)
holds a copy of the policy.</p>
<p>Now that we updated the local policy (the copy in <code class="docutils literal notranslate"><span class="pre">self.workers.local_worker</span></code>), we need to make sure
that the copies in all remote workers (<code class="docutils literal notranslate"><span class="pre">self.workers.remote_workers</span></code>) have their weights synchronized
(from the local one):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">()</span>
</pre></div>
</div>
<p>By calling <code class="docutils literal notranslate"><span class="pre">self.workers.sync_weights()</span></code>,
weights are broadcasted from the local worker to the remote workers. See <a class="reference internal" href="package_ref/evaluation.html#rolloutworker-reference-docs"><span class="std std-ref">rollout worker
reference docs</span></a> for further details.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">return</span> <span class="n">train_results</span>
</pre></div>
</div>
<p>A dictionary is expected to be returned that contains the results of the training update.
It maps keys of type <code class="docutils literal notranslate"><span class="pre">str</span></code> to values that are of type <code class="docutils literal notranslate"><span class="pre">float</span></code> or to dictionaries of
the same form, allowing for a nested structure.</p>
<p>For example, a results dictionary could map policy_ids to learning and sampling statistics for that policy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
   <span class="s1">&#39;policy_1&#39;</span><span class="p">:</span> <span class="p">{</span>
                 <span class="s1">&#39;learner_stats&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;policy_loss&#39;</span><span class="p">:</span> <span class="mf">6.7291455</span><span class="p">},</span>
                 <span class="s1">&#39;num_agent_steps_trained&#39;</span><span class="p">:</span> <span class="mi">32</span>
              <span class="p">},</span>
   <span class="s1">&#39;policy_2&#39;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s1">&#39;learner_stats&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;policy_loss&#39;</span><span class="p">:</span> <span class="mf">3.554927</span><span class="p">},</span>
                <span class="s1">&#39;num_agent_steps_trained&#39;</span><span class="p">:</span> <span class="mi">32</span>
              <span class="p">},</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="training-step-method-utilities">
<h3>Training Step Method Utilities<a class="headerlink" href="key-concepts.html#training-step-method-utilities" title="Permalink to this headline">#</a></h3>
<p>RLlib provides a collection of utilities that abstract away common tasks in RL training.
In particular, if you would like to work with the various <code class="docutils literal notranslate"><span class="pre">training_step</span></code> methods or implement your
own, it’s recommended to familiarize yourself first with these following concepts here:</p>
<p><a class="reference external" href="https://docs.ray.io/en/master/rllib/core-concepts.html#sample-batches">Sample Batch</a>:
<code class="docutils literal notranslate"><span class="pre">SampleBatch</span></code> and <code class="docutils literal notranslate"><span class="pre">MultiAgentBatch</span></code> are the two types that we use for storing trajectory data in RLlib. All of our
RLlib abstractions (policies, replay buffers, etc.) operate on these two types.</p>
<p><a class="reference internal" href="package_ref/evaluation.html#rolloutworker-reference-docs"><span class="std std-ref">Rollout Workers</span></a>:
Rollout workers are an abstraction that wraps a policy (or policies in the case of multi-agent) and an environment.
From a high level, we can use rollout workers to collect experiences from the environment by calling
their <code class="docutils literal notranslate"><span class="pre">sample()</span></code> method and we can train their policies by calling their <code class="docutils literal notranslate"><span class="pre">learn_on_batch()</span></code> method.
By default, in RLlib, we create a set of workers that can be used for sampling and training.
We create a <code class="docutils literal notranslate"><span class="pre">WorkerSet</span></code> object inside of <code class="docutils literal notranslate"><span class="pre">setup</span></code> which is called when an RLlib algorithm is created. The <code class="docutils literal notranslate"><span class="pre">WorkerSet</span></code> has a <code class="docutils literal notranslate"><span class="pre">local_worker</span></code>
and <code class="docutils literal notranslate"><span class="pre">remote_workers</span></code> if <code class="docutils literal notranslate"><span class="pre">num_workers</span> <span class="pre">&gt;</span> <span class="pre">0</span></code> in the experiment config. In RLlib we typically use <code class="docutils literal notranslate"><span class="pre">local_worker</span></code>
for training and <code class="docutils literal notranslate"><span class="pre">remote_workers</span></code> for sampling.</p>
<p><a class="reference internal" href="package_ref/utils.html#train-ops-docs"><span class="std std-ref">Train Ops</span></a>:
These are methods that improve the policy and update workers. The most basic operator, <code class="docutils literal notranslate"><span class="pre">train_one_step</span></code>, takes in as
input a batch of experiences and emits a <code class="docutils literal notranslate"><span class="pre">ResultDict</span></code> with metrics as output. For training with GPUs, use
<code class="docutils literal notranslate"><span class="pre">multi_gpu_train_one_step</span></code>. These methods use the <code class="docutils literal notranslate"><span class="pre">learn_on_batch</span></code> method of rollout workers to complete the
training update.</p>
<p><a class="reference internal" href="rllib-replay-buffers.html#replay-buffer-reference-docs"><span class="std std-ref">Replay Buffers</span></a>:
RLlib provides <a class="reference external" href="https://github.com/ray-project/ray/tree/master/rllib/utils/replay_buffers">a collection</a> of replay
buffers that can be used for storing and sampling experiences.</p>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="rllib-training.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Getting Started with RLlib</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../rllib-env.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Environments</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>