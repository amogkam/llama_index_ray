
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Getting Started with RLlib &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/versionwarning.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../_static/js/docsearch.js"></script>
    <script src="../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../_static/js/termynal.js"></script>
    <script defer="defer" src="../_static/js/custom.js"></script>
    <script defer="defer" src="../_static/js/top-navigation.js"></script>
    <script src="../_static/js/tags.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/rllib/rllib-training.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Key Concepts" href="key-concepts.html" />
    <link rel="prev" title="RLlib: Industry-Grade Reinforcement Learning" href="index.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "rllib/rllib-training", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Ray RLlib
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="rllib-training.html#">
     Getting Started with RLlib
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="key-concepts.html">
     Key Concepts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../rllib-env.html">
     Environments
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-algorithms.html">
     Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="user-guides.html">
     User Guides
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-examples.html">
     Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="package_ref/index.html">
     Ray RLlib API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Frllib/rllib-training.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/rllib/rllib-training.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/rllib/rllib-training.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-training.html#using-the-rllib-cli">
   Using the RLlib CLI
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-training.html#running-tuned-examples">
   Running Tuned Examples
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-training.html#using-the-python-api">
   Using the Python API
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#computing-actions">
     Computing Actions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#accessing-policy-state">
     Accessing Policy State
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#accessing-model-state">
     Accessing Model State
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-training.html#configuring-rllib-algorithms">
   Configuring RLlib Algorithms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-training-options">
     Specifying Training Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-environments">
     Specifying Environments
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-framework-options">
     Specifying Framework Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-rollout-workers">
     Specifying Rollout Workers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-evaluation-options">
     Specifying Evaluation Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-exploration-options">
     Specifying Exploration Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-offline-data-options">
     Specifying Offline Data Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-multi-agent-options">
     Specifying Multi-Agent Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-reporting-options">
     Specifying Reporting Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-checkpointing-options">
     Specifying Checkpointing Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-debugging-options">
     Specifying Debugging Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-callback-options">
     Specifying Callback Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-resources">
     Specifying Resources
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-experimental-features">
     Specifying Experimental Features
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-training.html#rllib-scaling-guide">
   RLlib Scaling Guide
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-training.html#debugging-rllib-experiments">
   Debugging RLlib Experiments
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#gym-monitor">
     Gym Monitor
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#eager-mode">
     Eager Mode
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#using-pytorch">
     Using PyTorch
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#episode-traces">
     Episode Traces
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#log-verbosity">
     Log Verbosity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#stack-traces">
     Stack Traces
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-training.html#next-steps">
   Next Steps
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Getting Started with RLlib</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-training.html#using-the-rllib-cli">
   Using the RLlib CLI
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-training.html#running-tuned-examples">
   Running Tuned Examples
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-training.html#using-the-python-api">
   Using the Python API
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#computing-actions">
     Computing Actions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#accessing-policy-state">
     Accessing Policy State
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#accessing-model-state">
     Accessing Model State
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-training.html#configuring-rllib-algorithms">
   Configuring RLlib Algorithms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-training-options">
     Specifying Training Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-environments">
     Specifying Environments
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-framework-options">
     Specifying Framework Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-rollout-workers">
     Specifying Rollout Workers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-evaluation-options">
     Specifying Evaluation Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-exploration-options">
     Specifying Exploration Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-offline-data-options">
     Specifying Offline Data Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-multi-agent-options">
     Specifying Multi-Agent Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-reporting-options">
     Specifying Reporting Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-checkpointing-options">
     Specifying Checkpointing Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-debugging-options">
     Specifying Debugging Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-callback-options">
     Specifying Callback Options
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-resources">
     Specifying Resources
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#specifying-experimental-features">
     Specifying Experimental Features
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-training.html#rllib-scaling-guide">
   RLlib Scaling Guide
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-training.html#debugging-rllib-experiments">
   Debugging RLlib Experiments
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#gym-monitor">
     Gym Monitor
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#eager-mode">
     Eager Mode
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#using-pytorch">
     Using PyTorch
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#episode-traces">
     Episode Traces
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#log-verbosity">
     Log Verbosity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-training.html#stack-traces">
     Stack Traces
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-training.html#next-steps">
   Next Steps
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <a class="reference external image-reference" href="https://ray-docs-promo.netlify.app/rllib"><img alt="" src="https://ray-docs-promo.netlify.app/assets/img/rllib/top.png" /></a>
<div class="bottom-right-promo-banner docutils">
<a class="reference external image-reference" href="https://ray-docs-promo.netlify.app/rllib"><img alt="" src="https://ray-docs-promo.netlify.app/assets/img/rllib/square.png" /></a>
</div>
<section id="getting-started-with-rllib">
<span id="rllib-getting-started"></span><h1>Getting Started with RLlib<a class="headerlink" href="rllib-training.html#getting-started-with-rllib" title="Permalink to this headline">#</a></h1>
<p>At a high level, RLlib provides you with an <code class="docutils literal notranslate"><span class="pre">Algorithm</span></code> class which
holds a policy for environment interaction.
Through the algorithm’s interface, you can train the policy compute actions, or store
your algorithms.
In multi-agent training, the algorithm manages the querying
and optimization of multiple policies at once.</p>
<img alt="../_images/rllib-api.svg" src="../_images/rllib-api.svg" /><p>In this guide, we will first walk you through running your first experiments with
the RLlib CLI, and then discuss our Python API in more detail.</p>
<section id="using-the-rllib-cli">
<h2>Using the RLlib CLI<a class="headerlink" href="rllib-training.html#using-the-rllib-cli" title="Permalink to this headline">#</a></h2>
<p>The quickest way to run your first RLlib algorithm is to use the command line interface.
You can train DQN with the following commands:</p>
<div class="termynal" data-termynal>
    <span data-ty="input">pip install "ray[rllib]" tensorflow</span>
    <span data-ty="input">rllib train --algo DQN --env CartPole-v1 --stop  '{"training_iteration": 30}'</span>
</div><aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The <code class="docutils literal notranslate"><span class="pre">rllib</span> <span class="pre">train</span></code> command (same as the <code class="docutils literal notranslate"><span class="pre">train.py</span></code> script in the repo)
has a number of options you can show by running <code class="xref py py-obj docutils literal notranslate"><span class="pre">rllib</span> <span class="pre">train</span> <span class="pre">--help</span></code>.</p>
</aside>
<p>Note that you choose any supported RLlib algorithm (<code class="docutils literal notranslate"><span class="pre">--algo</span></code>) and environment (<code class="docutils literal notranslate"><span class="pre">--env</span></code>).
RLlib supports any Farama-Foundation Gymnasium environment, as well as a number of other environments
(see <a class="reference internal" href="../rllib-env.html#rllib-environments-doc"><span class="std std-ref">Environments</span></a>).
It also supports a large number of algorithms (see <a class="reference internal" href="rllib-algorithms.html#rllib-algorithms-doc"><span class="std std-ref">Algorithms</span></a>) to
choose from.</p>
<p>Running the above will return one of the <code class="xref py py-obj docutils literal notranslate"><span class="pre">checkpoints</span></code> that get generated during training after 30 training iterations,
as well as a command that you can use to evaluate the trained algorithm.
You can evaluate the trained algorithm with the following command (assuming the checkpoint path is called <code class="docutils literal notranslate"><span class="pre">checkpoint</span></code>):</p>
<div class="termynal" data-termynal>
    <span data-ty="input">rllib evaluate checkpoint --algo DQN --env CartPole-v1</span>
</div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>By default, the results will be logged to a subdirectory of <code class="docutils literal notranslate"><span class="pre">~/ray_results</span></code>.
This subdirectory will contain a file <code class="docutils literal notranslate"><span class="pre">params.json</span></code> which contains the
hyper-parameters, a file <code class="docutils literal notranslate"><span class="pre">result.json</span></code> which contains a training summary
for each episode and a TensorBoard file that can be used to visualize
training process with TensorBoard by running</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tensorboard --logdir<span class="o">=</span>~/ray_results
</pre></div>
</div>
</div>
<p>For more advanced evaluation functionality, refer to <a class="reference external" href="rllib-training.html#customized-evaluation-during-training">Customized Evaluation During Training</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Each algorithm has specific hyperparameters that can be set with <code class="docutils literal notranslate"><span class="pre">--config</span></code>,
see the <a class="reference external" href="rllib-algorithms.html">algorithms documentation</a> for more information.
For instance, you can train the A2C algorithm on 8 workers by specifying
<code class="xref py py-obj docutils literal notranslate"><span class="pre">num_workers:</span> <span class="pre">8</span></code> in a JSON string passed to <code class="docutils literal notranslate"><span class="pre">--config</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>rllib train --env<span class="o">=</span>PongDeterministic-v4 --run<span class="o">=</span>A2C --config <span class="s1">&#39;{&quot;num_workers&quot;: 8}&#39;</span>
</pre></div>
</div>
</div>
</section>
<section id="running-tuned-examples">
<h2>Running Tuned Examples<a class="headerlink" href="rllib-training.html#running-tuned-examples" title="Permalink to this headline">#</a></h2>
<p>Some good hyperparameters and settings are available in
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples">the RLlib repository</a>
(some of them are tuned to run on GPUs).</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>If you find better settings or tune an algorithm on a different domain,
consider submitting a Pull Request!</p>
</aside>
<p>You can run these with the <code class="docutils literal notranslate"><span class="pre">rllib</span> <span class="pre">train</span> <span class="pre">file</span></code> command as follows:</p>
<div class="termynal" data-termynal>
    <span data-ty="input">rllib train file /path/to/tuned/example.yaml</span>
</div><p>Note that this works with any local YAML file in the correct format, or with remote URLs
pointing to such files.
If you want to learn more about the RLlib CLI, please check out
the <a class="reference internal" href="rllib-cli.html#rllib-cli-doc"><span class="std std-ref">RLlib CLI user guide</span></a>.</p>
</section>
<section id="using-the-python-api">
<span id="rllib-training-api"></span><h2>Using the Python API<a class="headerlink" href="rllib-training.html#using-the-python-api" title="Permalink to this headline">#</a></h2>
<p>The Python API provides the needed flexibility for applying RLlib to new problems.
For instance, you will need to use this API if you wish to use
<a class="reference external" href="rllib-models.html">custom environments, preprocessors, or models</a> with RLlib.</p>
<p>Here is an example of the basic usage.
We first create a <code class="xref py py-obj docutils literal notranslate"><span class="pre">PPOConfig</span></code> and add properties to it, like the <code class="xref py py-obj docutils literal notranslate"><span class="pre">environment</span></code> we want
to use, or the <code class="xref py py-obj docutils literal notranslate"><span class="pre">resources</span></code> we want to leverage for training.
After we <code class="xref py py-obj docutils literal notranslate"><span class="pre">build</span></code> the <code class="xref py py-obj docutils literal notranslate"><span class="pre">algo</span></code> from its configuration, we can <code class="xref py py-obj docutils literal notranslate"><span class="pre">train</span></code> it for a number of
episodes (here <code class="xref py py-obj docutils literal notranslate"><span class="pre">10</span></code>) and <code class="xref py py-obj docutils literal notranslate"><span class="pre">save</span></code> the resulting policy periodically
(here every <code class="xref py py-obj docutils literal notranslate"><span class="pre">5</span></code> episodes).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPOConfig</span>
<span class="kn">from</span> <span class="nn">ray.tune.logger</span> <span class="kn">import</span> <span class="n">pretty_print</span>


<span class="n">algo</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">PPOConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">build</span><span class="p">()</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">pretty_print</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Checkpoint saved in directory </span><span class="si">{</span><span class="n">checkpoint_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>All RLlib algorithms are compatible with the <a class="reference internal" href="../tune/api/api.html#tune-api-ref"><span class="std std-ref">Tune API</span></a>.
This enables them to be easily used in experiments with <a class="reference internal" href="../tune.html#tune-main"><span class="std std-ref">Ray Tune</span></a>.
For example, the following code performs a simple hyper-parameter sweep of PPO.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span><span class="p">,</span> <span class="n">tune</span>

<span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">PPOConfig</span><span class="p">()</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]))</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="s2">&quot;PPO&quot;</span><span class="p">,</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span>
        <span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">150</span><span class="p">},</span>
    <span class="p">),</span>
    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>Tune will schedule the trials to run in parallel on your Ray cluster:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">==</span> <span class="n">Status</span> <span class="o">==</span>
<span class="n">Using</span> <span class="n">FIFO</span> <span class="n">scheduling</span> <span class="n">algorithm</span><span class="o">.</span>
<span class="n">Resources</span> <span class="n">requested</span><span class="p">:</span> <span class="mi">4</span><span class="o">/</span><span class="mi">4</span> <span class="n">CPUs</span><span class="p">,</span> <span class="mi">0</span><span class="o">/</span><span class="mi">0</span> <span class="n">GPUs</span>
<span class="n">Result</span> <span class="n">logdir</span><span class="p">:</span> <span class="o">~/</span><span class="n">ray_results</span><span class="o">/</span><span class="n">my_experiment</span>
<span class="n">PENDING</span> <span class="n">trials</span><span class="p">:</span>
 <span class="o">-</span> <span class="n">PPO_CartPole</span><span class="o">-</span><span class="n">v1_2_lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">:</span>     <span class="n">PENDING</span>
<span class="n">RUNNING</span> <span class="n">trials</span><span class="p">:</span>
 <span class="o">-</span> <span class="n">PPO_CartPole</span><span class="o">-</span><span class="n">v1_0_lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">:</span>       <span class="n">RUNNING</span> <span class="p">[</span><span class="n">pid</span><span class="o">=</span><span class="mi">21940</span><span class="p">],</span> <span class="mi">16</span> <span class="n">s</span><span class="p">,</span> <span class="mi">4013</span> <span class="n">ts</span><span class="p">,</span> <span class="mi">22</span> <span class="n">rew</span>
 <span class="o">-</span> <span class="n">PPO_CartPole</span><span class="o">-</span><span class="n">v1_1_lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">:</span>      <span class="n">RUNNING</span> <span class="p">[</span><span class="n">pid</span><span class="o">=</span><span class="mi">21942</span><span class="p">],</span> <span class="mi">27</span> <span class="n">s</span><span class="p">,</span> <span class="mi">8111</span> <span class="n">ts</span><span class="p">,</span> <span class="mf">54.7</span> <span class="n">rew</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Tuner.fit()</span></code> returns an <code class="docutils literal notranslate"><span class="pre">ResultGrid</span></code> object that allows further analysis
of the training results and retrieving the checkpoint(s) of the trained agent.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># ``Tuner.fit()`` allows setting a custom log directory (other than ``~/ray-results``)</span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="s2">&quot;PPO&quot;</span><span class="p">,</span>
    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span>
        <span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">150</span><span class="p">},</span>
        <span class="n">checkpoint_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">checkpoint_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Get the best result based on a particular metric.</span>
<span class="n">best_result</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">get_best_result</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span>

<span class="c1"># Get the best checkpoint corresponding to the best result.</span>
<span class="n">best_checkpoint</span> <span class="o">=</span> <span class="n">best_result</span><span class="o">.</span><span class="n">checkpoint</span>
</pre></div>
</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>You can find your checkpoint’s version by
looking into the <code class="docutils literal notranslate"><span class="pre">rllib_checkpoint.json</span></code> file inside your checkpoint directory.</p>
</aside>
<p>Loading and restoring a trained algorithm from a checkpoint is simple.
Let’s assume you have a local checkpoint directory called <code class="docutils literal notranslate"><span class="pre">checkpoint_path</span></code>.
To load newer RLlib checkpoints (version &gt;= 1.0), use the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm</span> <span class="kn">import</span> <span class="n">Algorithm</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">Algorithm</span><span class="o">.</span><span class="n">from_checkpoint</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>
</pre></div>
</div>
<p>For older RLlib checkpoint versions (version &lt; 1.0), you can
restore an algorithm via:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPO</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">PPO</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">env</span><span class="o">=</span><span class="n">env_class</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>
</pre></div>
</div>
<section id="computing-actions">
<h3>Computing Actions<a class="headerlink" href="rllib-training.html#computing-actions" title="Permalink to this headline">#</a></h3>
<p>The simplest way to programmatically compute actions from a trained agent is to
use <code class="docutils literal notranslate"><span class="pre">Algorithm.compute_single_action()</span></code>.
This method preprocesses and filters the observation before passing it to the agent
policy.
Here is a simple example of testing a trained agent for one episode:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note: `gymnasium` (not `gym`) will be **the** API supported by RLlib from Ray 2.3 on.</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>

    <span class="n">gymnasium</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">gym</span>

    <span class="n">gymnasium</span> <span class="o">=</span> <span class="kc">False</span>

<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPOConfig</span>

<span class="n">env_name</span> <span class="o">=</span> <span class="s2">&quot;CartPole-v1&quot;</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_name</span><span class="p">)</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">PPOConfig</span><span class="p">()</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env_name</span><span class="p">)</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>

<span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="n">truncated</span> <span class="o">=</span> <span class="kc">False</span>

<span class="k">if</span> <span class="n">gymnasium</span><span class="p">:</span>
    <span class="n">obs</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">truncated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">compute_single_action</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">gymnasium</span><span class="p">:</span>
        <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">episode_reward</span> <span class="o">+=</span> <span class="n">reward</span>
</pre></div>
</div>
<p>For more advanced usage on computing actions and other functionality,
you can consult the <a class="reference internal" href="package_ref/algorithm.html#rllib-algorithm-api"><span class="std std-ref">RLlib Algorithm API documentation</span></a>.</p>
</section>
<section id="accessing-policy-state">
<h3>Accessing Policy State<a class="headerlink" href="rllib-training.html#accessing-policy-state" title="Permalink to this headline">#</a></h3>
<p>It is common to need to access a algorithm’s internal state, for instance to set
or get model weights.</p>
<p>In RLlib algorithm state is replicated across multiple <em>rollout workers</em> (Ray actors)
in the cluster.
However, you can easily get and update this state between calls to <code class="docutils literal notranslate"><span class="pre">train()</span></code>
via <code class="docutils literal notranslate"><span class="pre">Algorithm.workers.foreach_worker()</span></code>
or <code class="docutils literal notranslate"><span class="pre">Algorithm.workers.foreach_worker_with_index()</span></code>.
These functions take a lambda function that is applied with the worker as an argument.
These functions return values for each worker as a list.</p>
<p>You can also access just the “master” copy of the algorithm state through
<code class="docutils literal notranslate"><span class="pre">Algorithm.get_policy()</span></code> or <code class="docutils literal notranslate"><span class="pre">Algorithm.workers.local_worker()</span></code>,
but note that updates here may not be immediately reflected in
your rollout workers (if you have configured <code class="docutils literal notranslate"><span class="pre">num_rollout_workers</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>).
Here’s a quick example of how to access state of a model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.dqn</span> <span class="kn">import</span> <span class="n">DQNConfig</span>

<span class="n">algo</span> <span class="o">=</span> <span class="n">DQNConfig</span><span class="p">()</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>

<span class="c1"># Get weights of the default local policy</span>
<span class="n">algo</span><span class="o">.</span><span class="n">get_policy</span><span class="p">()</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>

<span class="c1"># Same as above</span>
<span class="n">algo</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">local_worker</span><span class="p">()</span><span class="o">.</span><span class="n">policy_map</span><span class="p">[</span><span class="s2">&quot;default_policy&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>

<span class="c1"># Get list of weights of each worker, including remote replicas</span>
<span class="n">algo</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span><span class="k">lambda</span> <span class="n">worker</span><span class="p">:</span> <span class="n">worker</span><span class="o">.</span><span class="n">get_policy</span><span class="p">()</span><span class="o">.</span><span class="n">get_weights</span><span class="p">())</span>

<span class="c1"># Same as above, but with index.</span>
<span class="n">algo</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">foreach_worker_with_id</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">_id</span><span class="p">,</span> <span class="n">worker</span><span class="p">:</span> <span class="n">worker</span><span class="o">.</span><span class="n">get_policy</span><span class="p">()</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="accessing-model-state">
<h3>Accessing Model State<a class="headerlink" href="rllib-training.html#accessing-model-state" title="Permalink to this headline">#</a></h3>
<p>Similar to accessing policy state, you may want to get a reference to the
underlying neural network model being trained. For example, you may want to
pre-train it separately, or otherwise update its weights outside of RLlib.
This can be done by accessing the <code class="docutils literal notranslate"><span class="pre">model</span></code> of the policy.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>To run these examples, you need to install a few extra dependencies, namely
<code class="xref py py-obj docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">&quot;gym[atari]&quot;</span> <span class="pre">&quot;gym[accept-rom-license]&quot;</span> <span class="pre">atari_py</span></code>.</p>
</aside>
<p>Below you find three explicit examples showing how to access the model state of
an algorithm.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<strong>Example: Preprocessing observations for feeding into a model</strong><div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Then for the code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>

    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;GymV26Environment-v0&quot;</span><span class="p">,</span> <span class="n">env_id</span><span class="o">=</span><span class="s2">&quot;ALE/Pong-v5&quot;</span><span class="p">)</span>
    <span class="n">obs</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">gym</span>

    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;PongNoFrameskip-v4&quot;</span><span class="p">)</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="c1"># RLlib uses preprocessors to implement transforms such as one-hot encoding</span>
<span class="c1"># and flattening of tuple and dict observations.</span>
<span class="kn">from</span> <span class="nn">ray.rllib.models.preprocessors</span> <span class="kn">import</span> <span class="n">get_preprocessor</span>

<span class="n">prep</span> <span class="o">=</span> <span class="n">get_preprocessor</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">)(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">)</span>
<span class="c1"># &lt;ray.rllib.models.preprocessors.GenericPixelPreprocessor object at 0x7fc4d049de80&gt;</span>

<span class="c1"># Observations should be preprocessed prior to feeding into a model</span>
<span class="n">obs</span><span class="o">.</span><span class="n">shape</span>
<span class="c1"># (210, 160, 3)</span>
<span class="n">prep</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="c1"># (84, 84, 3)</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<strong>Example: Querying a policy’s action distribution</strong><div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get a reference to the policy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.dqn</span> <span class="kn">import</span> <span class="n">DQNConfig</span>

<span class="n">algo</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">DQNConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">framework</span><span class="p">(</span><span class="s2">&quot;tf2&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="o">.</span><span class="n">build</span><span class="p">()</span>
<span class="p">)</span>
<span class="c1"># &lt;ray.rllib.algorithms.ppo.PPO object at 0x7fd020186384&gt;</span>

<span class="n">policy</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">get_policy</span><span class="p">()</span>
<span class="c1"># &lt;ray.rllib.policy.eager_tf_policy.PPOTFPolicy_eager object at 0x7fd020165470&gt;</span>

<span class="c1"># Run a forward pass to get model output logits. Note that complex observations</span>
<span class="c1"># must be preprocessed as in the above code block.</span>
<span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">model</span><span class="p">({</span><span class="s2">&quot;obs&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]])})</span>
<span class="c1"># (&lt;tf.Tensor: id=1274, shape=(1, 2), dtype=float32, numpy=...&gt;, [])</span>

<span class="c1"># Compute action distribution given logits</span>
<span class="n">policy</span><span class="o">.</span><span class="n">dist_class</span>
<span class="c1"># &lt;class_object &#39;ray.rllib.models.tf.tf_action_dist.Categorical&#39;&gt;</span>
<span class="n">dist</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">dist_class</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="c1"># &lt;ray.rllib.models.tf.tf_action_dist.Categorical object at 0x7fd02301d710&gt;</span>

<span class="c1"># Query the distribution for samples, sample logps</span>
<span class="n">dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="c1"># &lt;tf.Tensor: id=661, shape=(1,), dtype=int64, numpy=..&gt;</span>
<span class="n">dist</span><span class="o">.</span><span class="n">logp</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="c1"># &lt;tf.Tensor: id=1298, shape=(1,), dtype=float32, numpy=...&gt;</span>

<span class="c1"># Get the estimated values for the most recent forward pass</span>
<span class="n">policy</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">value_function</span><span class="p">()</span>
<span class="c1"># &lt;tf.Tensor: id=670, shape=(1,), dtype=float32, numpy=...&gt;</span>

<span class="n">policy</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Model: &quot;model&quot;</span>
<span class="sd">_____________________________________________________________________</span>
<span class="sd">Layer (type)               Output Shape  Param #  Connected to</span>
<span class="sd">=====================================================================</span>
<span class="sd">observations (InputLayer)  [(None, 4)]   0</span>
<span class="sd">_____________________________________________________________________</span>
<span class="sd">fc_1 (Dense)               (None, 256)   1280     observations[0][0]</span>
<span class="sd">_____________________________________________________________________</span>
<span class="sd">fc_value_1 (Dense)         (None, 256)   1280     observations[0][0]</span>
<span class="sd">_____________________________________________________________________</span>
<span class="sd">fc_2 (Dense)               (None, 256)   65792    fc_1[0][0]</span>
<span class="sd">_____________________________________________________________________</span>
<span class="sd">fc_value_2 (Dense)         (None, 256)   65792    fc_value_1[0][0]</span>
<span class="sd">_____________________________________________________________________</span>
<span class="sd">fc_out (Dense)             (None, 2)     514      fc_2[0][0]</span>
<span class="sd">_____________________________________________________________________</span>
<span class="sd">value_out (Dense)          (None, 1)     257      fc_value_2[0][0]</span>
<span class="sd">=====================================================================</span>
<span class="sd">Total params: 134,915</span>
<span class="sd">Trainable params: 134,915</span>
<span class="sd">Non-trainable params: 0</span>
<span class="sd">_____________________________________________________________________</span>
<span class="sd">&quot;&quot;&quot;</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<strong>Example: Getting Q values from a DQN model</strong><div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get a reference to the model through the policy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.dqn</span> <span class="kn">import</span> <span class="n">DQNConfig</span>

<span class="n">algo</span> <span class="o">=</span> <span class="n">DQNConfig</span><span class="p">()</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">framework</span><span class="p">(</span><span class="s2">&quot;tf2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">get_policy</span><span class="p">()</span><span class="o">.</span><span class="n">model</span>
<span class="c1"># &lt;ray.rllib.models.catalog.FullyConnectedNetwork_as_DistributionalQModel ...&gt;</span>

<span class="c1"># List of all model variables</span>
<span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">()</span>

<span class="c1"># Run a forward pass to get base model output. Note that complex observations</span>
<span class="c1"># must be preprocessed. An example of preprocessing is examples/saving_experiences.py</span>
<span class="n">model_out</span> <span class="o">=</span> <span class="n">model</span><span class="p">({</span><span class="s2">&quot;obs&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]])})</span>
<span class="c1"># (&lt;tf.Tensor: id=832, shape=(1, 256), dtype=float32, numpy=...)</span>

<span class="c1"># Access the base Keras models (all default models have a base)</span>
<span class="n">model</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Model: &quot;model&quot;</span>
<span class="sd">_______________________________________________________________________</span>
<span class="sd">Layer (type)                Output Shape    Param #  Connected to</span>
<span class="sd">=======================================================================</span>
<span class="sd">observations (InputLayer)   [(None, 4)]     0</span>
<span class="sd">_______________________________________________________________________</span>
<span class="sd">fc_1 (Dense)                (None, 256)     1280     observations[0][0]</span>
<span class="sd">_______________________________________________________________________</span>
<span class="sd">fc_out (Dense)              (None, 256)     65792    fc_1[0][0]</span>
<span class="sd">_______________________________________________________________________</span>
<span class="sd">value_out (Dense)           (None, 1)       257      fc_1[0][0]</span>
<span class="sd">=======================================================================</span>
<span class="sd">Total params: 67,329</span>
<span class="sd">Trainable params: 67,329</span>
<span class="sd">Non-trainable params: 0</span>
<span class="sd">______________________________________________________________________________</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Access the Q value model (specific to DQN)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">get_q_value_distributions</span><span class="p">(</span><span class="n">model_out</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># tf.Tensor([[ 0.13023682 -0.36805138]], shape=(1, 2), dtype=float32)</span>
<span class="c1"># ^ exact numbers may differ due to randomness</span>

<span class="n">model</span><span class="o">.</span><span class="n">q_value_head</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="c1"># Access the state value model (specific to DQN)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">get_state_value</span><span class="p">(</span><span class="n">model_out</span><span class="p">))</span>
<span class="c1"># tf.Tensor([[0.09381643]], shape=(1, 1), dtype=float32)</span>
<span class="c1"># ^ exact number may differ due to randomness</span>

<span class="n">model</span><span class="o">.</span><span class="n">state_value_head</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p class="sd-card-text">This is especially useful when used with
<a class="reference external" href="rllib-models.html">custom model classes</a>.</p>
</div>
</details></section>
</section>
<section id="configuring-rllib-algorithms">
<span id="rllib-algo-configuration"></span><h2>Configuring RLlib Algorithms<a class="headerlink" href="rllib-training.html#configuring-rllib-algorithms" title="Permalink to this headline">#</a></h2>
<p>You can configure RLlib algorithms in a modular fashion by working with so-called
<code class="xref py py-obj docutils literal notranslate"><span class="pre">AlgorithmConfig</span></code> objects.
In essence, you first create a <code class="xref py py-obj docutils literal notranslate"><span class="pre">config</span> <span class="pre">=</span> <span class="pre">AlgorithmConfig()</span></code> object and then call methods
on it to set the desired configuration options.
Each RLlib algorithm has its own config class that inherits from <code class="xref py py-obj docutils literal notranslate"><span class="pre">AlgorithmConfig</span></code>.
For instance, to create a <code class="xref py py-obj docutils literal notranslate"><span class="pre">PPO</span></code> algorithm, you start with a <code class="xref py py-obj docutils literal notranslate"><span class="pre">PPOConfig</span></code> object, to work
with a <code class="xref py py-obj docutils literal notranslate"><span class="pre">DQN</span></code> algorithm, you start with a <code class="xref py py-obj docutils literal notranslate"><span class="pre">DQNConfig</span></code> object, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Each algorithm has its specific settings, but most configuration options are shared.
We discuss the common options below, and refer to
<a class="reference internal" href="rllib-algorithms.html#rllib-algorithms-doc"><span class="std std-ref">the RLlib algorithms guide</span></a> for algorithm-specific
properties.
Algorithms differ mostly in their <code class="xref py py-obj docutils literal notranslate"><span class="pre">training</span></code> settings.</p>
</div>
<p>Below you find the basic signature of the <code class="xref py py-obj docutils literal notranslate"><span class="pre">AlgorithmConfig</span></code> class, as well as some
advanced usage examples:</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.algorithm_config.</span></span><span class="sig-name descname"><span class="pre">AlgorithmConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/algorithm_config.html#AlgorithmConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>A RLlib AlgorithmConfig builds an RLlib Algorithm from a given configuration.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.callbacks</span> <span class="kn">import</span> <span class="n">MemoryTrackingCallbacks</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Construct a generic config object, specifying values within different</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># sub-categories, e.g. &quot;training&quot;.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AlgorithmConfig</span><span class="p">()</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>  
<span class="gp">... </span>    <span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">... </span>    <span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">... </span>    <span class="o">.</span><span class="n">callbacks</span><span class="p">(</span><span class="n">MemoryTrackingCallbacks</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># A config object can be used to construct the respective Trainer.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rllib_algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># In combination with a tune.grid_search:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AlgorithmConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">]))</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use `to_dict()` method to get the legacy plain python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># for usage with `tune.Tuner().fit()`.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;[registered trainer class]&quot;</span><span class="p">,</span> <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
<span class="gp">... </span>    <span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<p>As RLlib algorithms are fairly complex, they come with many configuration options.
To make things easier, the common properties of algorithms are naturally grouped into
the following categories:</p>
<ul class="simple">
<li><p><a class="reference internal" href="rllib-training.html#rllib-config-train"><span class="std std-ref">training options</span></a>,</p></li>
<li><p><a class="reference internal" href="rllib-training.html#rllib-config-env"><span class="std std-ref">environment options</span></a>,</p></li>
<li><p><a class="reference internal" href="rllib-training.html#rllib-config-framework"><span class="std std-ref">deep learning framework options</span></a>,</p></li>
<li><p><a class="reference internal" href="rllib-training.html#rllib-config-rollouts"><span class="std std-ref">rollout worker options</span></a>,</p></li>
<li><p><a class="reference internal" href="rllib-training.html#rllib-config-evaluation"><span class="std std-ref">evaluation options</span></a>,</p></li>
<li><p><a class="reference internal" href="rllib-training.html#rllib-config-exploration"><span class="std std-ref">exploration options</span></a>,</p></li>
<li><p><a class="reference internal" href="rllib-training.html#rllib-config-offline-data"><span class="std std-ref">options for training with offline data</span></a>,</p></li>
<li><p><a class="reference internal" href="rllib-training.html#rllib-config-multi-agent"><span class="std std-ref">options for training multiple agents</span></a>,</p></li>
<li><p><a class="reference internal" href="rllib-training.html#rllib-config-reporting"><span class="std std-ref">reporting options</span></a>,</p></li>
<li><p><a class="reference internal" href="rllib-training.html#rllib-config-checkpointing"><span class="std std-ref">options for saving and restoring checkpoints</span></a>,</p></li>
<li><p><a class="reference internal" href="rllib-training.html#rllib-config-debugging"><span class="std std-ref">debugging options</span></a>,</p></li>
<li><p><a class="reference internal" href="rllib-training.html#rllib-config-callbacks"><span class="std std-ref">options for adding callbacks to algorithms</span></a>,</p></li>
<li><p><a class="reference internal" href="rllib-training.html#rllib-config-resources"><span class="std std-ref">Resource options</span></a></p></li>
<li><p><a class="reference internal" href="rllib-training.html#rllib-config-experimental"><span class="std std-ref">and options for experimental features</span></a></p></li>
</ul>
<p>Let’s discuss each category one by one, starting with training options.</p>
<section id="specifying-training-options">
<span id="rllib-config-train"></span><h3>Specifying Training Options<a class="headerlink" href="rllib-training.html#specifying-training-options" title="Permalink to this headline">#</a></h3>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>For instance, a <code class="xref py py-obj docutils literal notranslate"><span class="pre">DQNConfig</span></code> takes a <code class="xref py py-obj docutils literal notranslate"><span class="pre">double_q</span></code> <code class="xref py py-obj docutils literal notranslate"><span class="pre">training</span></code> argument to specify whether
to use a double-Q DQN, whereas in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">PPOConfig</span></code> this does not make sense.</p>
</aside>
<p>For individual algorithms, this is probably the most relevant configuration group,
as this is where all the algorithm-specific options go.
But the base configuration for <code class="xref py py-obj docutils literal notranslate"><span class="pre">training</span></code> of an <code class="xref py py-obj docutils literal notranslate"><span class="pre">AlgorithmConfig</span></code> is actually quite small:</p>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">AlgorithmConfig.</span></span><span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr:</span> <span class="pre">Optional[Union[float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">List[List[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]]]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip_by:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_batch_size:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_requests_in_flight_per_sampler_worker:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_enable_learner_api:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learner_class:</span> <span class="pre">Optional[Type[Learner]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/algorithm_config.html#AlgorithmConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gamma</strong> – Float specifying the discount factor of the Markov Decision process.</p></li>
<li><p><strong>lr</strong> – The learning rate (float) or learning rate schedule in the format of
[[timestep, lr-value], [timestep, lr-value], …]
In case of a schedule, intermediary timesteps will be assigned to
linearly interpolated learning rate values. A schedule config’s first
entry must start with timestep 0, i.e.: [[0, initial_value], […]].
Note: If you require a) more than one optimizer (per RLModule),
b) optimizer types that are not Adam, c) a learning rate schedule that
is not a linearly interpolated, piecewise schedule as described above,
or d) specifying c’tor arguments of the optimizer that are not the
learning rate (e.g. Adam’s epsilon), then you must override your
Learner’s <code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_optimizer_for_module()</span></code> method and handle
lr-scheduling yourself.</p></li>
<li><p><strong>grad_clip</strong> – If None, no gradient clipping will be applied. Otherwise,
depending on the setting of <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip_by</span></code>, the (float) value of
<code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip</span></code> will have the following effect:
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip_by=value</span></code>: Will clip all computed gradients individually
inside the interval [-<code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip</span></code>, +`grad_clip`].
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip_by=norm</span></code>, will compute the L2-norm of each weight/bias
gradient tensor individually and then clip all gradients such that these
L2-norms do not exceed <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip</span></code>. The L2-norm of a tensor is computed
via: <code class="xref py py-obj docutils literal notranslate"><span class="pre">sqrt(SUM(w0^2,</span> <span class="pre">w1^2,</span> <span class="pre">...,</span> <span class="pre">wn^2))</span></code> where w[i] are the elements of
the tensor (no matter what the shape of this tensor is).
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip_by=global_norm</span></code>, will compute the square of the L2-norm of
each weight/bias gradient tensor individually, sum up all these squared
L2-norms across all given gradient tensors (e.g. the entire module to
be updated), square root that overall sum, and then clip all gradients
such that this global L2-norm does not exceed the given value.
The global L2-norm over a list of tensors (e.g. W and V) is computed
via:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">sqrt[SUM(w0^2,</span> <span class="pre">w1^2,</span> <span class="pre">...,</span> <span class="pre">wn^2)</span> <span class="pre">+</span> <span class="pre">SUM(v0^2,</span> <span class="pre">v1^2,</span> <span class="pre">...,</span> <span class="pre">vm^2)]</span></code>, where
w[i] and v[j] are the elements of the tensors W and V (no matter what
the shapes of these tensors are).</p></li>
<li><p><strong>grad_clip_by</strong> – See <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip</span></code> for the effect of this setting on gradient
clipping. Allowed values are <code class="xref py py-obj docutils literal notranslate"><span class="pre">value</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">norm</span></code>, and <code class="xref py py-obj docutils literal notranslate"><span class="pre">global_norm</span></code>.</p></li>
<li><p><strong>train_batch_size</strong> – Training batch size, if applicable.</p></li>
<li><p><strong>model</strong> – Arguments passed into the policy model. See models/catalog.py for a
full list of the available model options.
TODO: Provide ModelConfig objects instead of dicts.</p></li>
<li><p><strong>optimizer</strong> – Arguments to pass to the policy optimizer. This setting is not
used when <code class="xref py py-obj docutils literal notranslate"><span class="pre">_enable_learner_api=True</span></code>.</p></li>
<li><p><strong>max_requests_in_flight_per_sampler_worker</strong> – Max number of inflight requests
to each sampling worker. See the FaultTolerantActorManager class for
more details.
Tuning these values is important when running experimens with
large sample batches, where there is the risk that the object store may
fill up, causing spilling of objects to disk. This can cause any
asynchronous requests to become very slow, making your experiment run
slow as well. You can inspect the object store during your experiment
via a call to ray memory on your headnode, and by using the ray
dashboard. If you’re seeing that the object store is filling up,
turn down the number of remote requests in flight, or enable compression
in your experiment of timesteps.</p></li>
<li><p><strong>_enable_learner_api</strong> – Whether to enable the LearnerGroup and Learner
for training. This API uses ray.train to run the training loop which
allows for a more flexible distributed training.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="specifying-environments">
<span id="rllib-config-env"></span><h3>Specifying Environments<a class="headerlink" href="rllib-training.html#specifying-environments" title="Permalink to this headline">#</a></h3>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">AlgorithmConfig.</span></span><span class="sig-name descname"><span class="pre">environment</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">env:</span> <span class="pre">Optional[Union[str,</span> <span class="pre">Any]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">*,</span> <span class="pre">env_config:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">observation_space:</span> <span class="pre">Optional[&lt;MagicMock</span> <span class="pre">name='mock.spaces.Space'</span> <span class="pre">id='140714602219600'&gt;]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">action_space:</span> <span class="pre">Optional[&lt;MagicMock</span> <span class="pre">name='mock.spaces.Space'</span> <span class="pre">id='140714602219600'&gt;]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">env_task_fn:</span> <span class="pre">Optional[Callable[[dict,</span> <span class="pre">Any,</span> <span class="pre">ray.rllib.env.env_context.EnvContext],</span> <span class="pre">Any]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">render_env:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">clip_rewards:</span> <span class="pre">Optional[Union[bool,</span> <span class="pre">float]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">normalize_actions:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">clip_actions:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">disable_env_checking:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">is_atari:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">auto_wrap_old_gym_envs:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">action_mask_key:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig"><span class="pre">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/algorithm_config.html#AlgorithmConfig.environment"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Sets the config’s RL-environment settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>env</strong> – The environment specifier. This can either be a tune-registered env,
via <code class="xref py py-obj docutils literal notranslate"><span class="pre">tune.register_env([name],</span> <span class="pre">lambda</span> <span class="pre">env_ctx:</span> <span class="pre">[env</span> <span class="pre">object])</span></code>,
or a string specifier of an RLlib supported type. In the latter case,
RLlib will try to interpret the specifier as either an Farama-Foundation
gymnasium env, a PyBullet env, a ViZDoomGym env, or a fully qualified
classpath to an Env class, e.g.
“ray.rllib.examples.env.random_env.RandomEnv”.</p></li>
<li><p><strong>env_config</strong> – Arguments dict passed to the env creator as an EnvContext
object (which is a dict plus the properties: num_rollout_workers,
worker_index, vector_index, and remote).</p></li>
<li><p><strong>observation_space</strong> – The observation space for the Policies of this Algorithm.</p></li>
<li><p><strong>action_space</strong> – The action space for the Policies of this Algorithm.</p></li>
<li><p><strong>env_task_fn</strong> – A callable taking the last train results, the base env and the
env context as args and returning a new task to set the env to.
The env must be a <code class="xref py py-obj docutils literal notranslate"><span class="pre">TaskSettableEnv</span></code> sub-class for this to work.
See <code class="xref py py-obj docutils literal notranslate"><span class="pre">examples/curriculum_learning.py</span></code> for an example.</p></li>
<li><p><strong>render_env</strong> – If True, try to render the environment on the local worker or on
worker 1 (if num_rollout_workers &gt; 0). For vectorized envs, this usually
means that only the first sub-environment will be rendered.
In order for this to work, your env will have to implement the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">render()</span></code> method which either:
a) handles window generation and rendering itself (returning True) or
b) returns a numpy uint8 image of shape [height x width x 3 (RGB)].</p></li>
<li><p><strong>clip_rewards</strong> – Whether to clip rewards during Policy’s postprocessing.
None (default): Clip for Atari only (r=sign(r)).
True: r=sign(r): Fixed rewards -1.0, 1.0, or 0.0.
False: Never clip.
[float value]: Clip at -value and + value.
Tuple[value1, value2]: Clip at value1 and value2.</p></li>
<li><p><strong>normalize_actions</strong> – If True, RLlib will learn entirely inside a normalized
action space (0.0 centered with small stddev; only affecting Box
components). We will unsquash actions (and clip, just in case) to the
bounds of the env’s action space before sending actions back to the env.</p></li>
<li><p><strong>clip_actions</strong> – If True, RLlib will clip actions according to the env’s bounds
before sending them back to the env.
TODO: (sven) This option should be deprecated and always be False.</p></li>
<li><p><strong>disable_env_checking</strong> – If True, disable the environment pre-checking module.</p></li>
<li><p><strong>is_atari</strong> – This config can be used to explicitly specify whether the env is
an Atari env or not. If not specified, RLlib will try to auto-detect
this during config validation.</p></li>
<li><p><strong>auto_wrap_old_gym_envs</strong> – <dl class="simple">
<dt>Whether to auto-wrap old gym environments (using</dt><dd><p>the pre 0.24 gym APIs, e.g. reset() returning single obs and no info
dict). If True, RLlib will automatically wrap the given gym env class
with the gym-provided compatibility wrapper
(gym.wrappers.EnvCompatibility). If False, RLlib will produce a
descriptive error on which steps to perform to upgrade to gymnasium
(or to switch this flag to True).</p>
</dd>
<dt>action_mask_key: If observation is a dictionary, expect the value by</dt><dd><p>the key <code class="xref py py-obj docutils literal notranslate"><span class="pre">action_mask_key</span></code> to contain a valid actions mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.int8</span></code>
array of zeros and ones). Defaults to “action_mask”.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="specifying-framework-options">
<span id="rllib-config-framework"></span><h3>Specifying Framework Options<a class="headerlink" href="rllib-training.html#specifying-framework-options" title="Permalink to this headline">#</a></h3>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">AlgorithmConfig.</span></span><span class="sig-name descname"><span class="pre">framework</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">framework:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eager_tracing:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eager_max_retraces:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tf_session_args:</span> <span class="pre">Optional[Dict[str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Any]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_tf_session_args:</span> <span class="pre">Optional[Dict[str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Any]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch_compile_learner:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch_compile_learner_dynamo_mode:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch_compile_learner_dynamo_backend:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch_compile_worker:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch_compile_worker_dynamo_backend:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch_compile_worker_dynamo_mode:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig"><span class="pre">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/algorithm_config.html#AlgorithmConfig.framework"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Sets the config’s DL framework settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>framework</strong> – torch: PyTorch; tf2: TensorFlow 2.x (eager execution or traced
if eager_tracing=True); tf: TensorFlow (static-graph);</p></li>
<li><p><strong>eager_tracing</strong> – Enable tracing in eager mode. This greatly improves
performance (speedup ~2x), but makes it slightly harder to debug
since Python code won’t be evaluated after the initial eager pass.
Only possible if framework=tf2.</p></li>
<li><p><strong>eager_max_retraces</strong> – Maximum number of tf.function re-traces before a
runtime error is raised. This is to prevent unnoticed retraces of
methods inside the <code class="xref py py-obj docutils literal notranslate"><span class="pre">_eager_traced</span></code> Policy, which could slow down
execution by a factor of 4, without the user noticing what the root
cause for this slowdown could be.
Only necessary for framework=tf2.
Set to None to ignore the re-trace count and never throw an error.</p></li>
<li><p><strong>tf_session_args</strong> – Configures TF for single-process operation by default.</p></li>
<li><p><strong>local_tf_session_args</strong> – Override the following tf session args on the local
worker</p></li>
<li><p><strong>torch_compile_learner</strong> – If True, forward_train methods on TorchRLModules</p></li>
<li><p><strong>specified</strong> (<em>on the learner are compiled. If not</em>) – </p></li>
<li><p><strong>compile</strong> (<em>the default is to</em>) – </p></li>
<li><p><strong>learner.</strong> (<em>forward train on the</em>) – </p></li>
<li><p><strong>torch_compile_learner_dynamo_backend</strong> – The torch dynamo backend to use on
the learner.</p></li>
<li><p><strong>torch_compile_learner_dynamo_mode</strong> – The torch dynamo mode to use on the
learner.</p></li>
<li><p><strong>torch_compile_worker</strong> – If True, forward exploration and inference methods on
TorchRLModules on the workers are compiled. If not specified,
the default is to not compile forward methods on the workers because
retracing can be expensive.</p></li>
<li><p><strong>torch_compile_worker_dynamo_backend</strong> – The torch dynamo backend to use on
the workers.</p></li>
<li><p><strong>torch_compile_worker_dynamo_mode</strong> – The torch dynamo mode to use on the
workers.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="specifying-rollout-workers">
<span id="rllib-config-rollouts"></span><h3>Specifying Rollout Workers<a class="headerlink" href="rllib-training.html#specifying-rollout-workers" title="Permalink to this headline">#</a></h3>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">AlgorithmConfig.</span></span><span class="sig-name descname"><span class="pre">rollouts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env_runner_cls:</span> <span class="pre">Optional[type]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_rollout_workers:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_envs_per_worker:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">create_env_on_local_worker:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_collector:</span> <span class="pre">Optional[Type[ray.rllib.evaluation.collectors.sample_collector.SampleCollector]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_async:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_connectors:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_worker_filter_stats:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_worker_filter_stats:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rollout_fragment_length:</span> <span class="pre">Optional[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">str]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_mode:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remote_worker_envs:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remote_env_batch_wait_ms:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_workers_after_construction:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocessor_pref:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">observation_filter:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compress_observations:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_tf1_exec_eagerly:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler_perf_stats_ema_coef:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_worker_failures=-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recreate_failed_workers=-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">restart_failed_sub_environments=-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_consecutive_worker_failures_tolerance=-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker_health_probe_timeout_s=-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker_restore_timeout_s=-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synchronize_filter=-1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig"><span class="pre">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/algorithm_config.html#AlgorithmConfig.rollouts"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Sets the rollout worker configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>env_runner_cls</strong> – The EnvRunner class to use for environment rollouts (data
collection).</p></li>
<li><p><strong>num_rollout_workers</strong> – Number of rollout worker actors to create for
parallel sampling. Setting this to 0 will force rollouts to be done in
the local worker (driver process or the Algorithm’s actor when using
Tune).</p></li>
<li><p><strong>num_envs_per_worker</strong> – Number of environments to evaluate vector-wise per
worker. This enables model inference batching, which can improve
performance for inference bottlenecked workloads.</p></li>
<li><p><strong>sample_collector</strong> – The SampleCollector class to be used to collect and
retrieve environment-, model-, and sampler data. Override the
SampleCollector base class to implement your own
collection/buffering/retrieval logic.</p></li>
<li><p><strong>create_env_on_local_worker</strong> – When <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_rollout_workers</span></code> &gt; 0, the driver
(local_worker; worker-idx=0) does not need an environment. This is
because it doesn’t have to sample (done by remote_workers;
worker_indices &gt; 0) nor evaluate (done by evaluation workers;
see below).</p></li>
<li><p><strong>sample_async</strong> – Use a background thread for sampling (slightly off-policy,
usually not advisable to turn on unless your env specifically requires
it).</p></li>
<li><p><strong>enable_connectors</strong> – Use connector based environment runner, so that all
preprocessing of obs and postprocessing of actions are done in agent
and action connectors.</p></li>
<li><p><strong>use_worker_filter_stats</strong> – Whether to use the workers in the WorkerSet to
update the central filters (held by the local worker). If False, stats
from the workers will not be used and discarded.</p></li>
<li><p><strong>update_worker_filter_stats</strong> – Whether to push filter updates from the central
filters (held by the local worker) to the remote workers’ filters.
Setting this to True might be useful within the evaluation config in
order to disable the usage of evaluation trajectories for synching
the central filter (used for training).</p></li>
<li><p><strong>rollout_fragment_length</strong> – Divide episodes into fragments of this many steps
each during rollouts. Trajectories of this size are collected from
rollout workers and combined into a larger batch of <code class="xref py py-obj docutils literal notranslate"><span class="pre">train_batch_size</span></code>
for learning.
For example, given rollout_fragment_length=100 and
train_batch_size=1000:
1. RLlib collects 10 fragments of 100 steps each from rollout workers.
2. These fragments are concatenated and we perform an epoch of SGD.
When using multiple envs per worker, the fragment size is multiplied by
<code class="xref py py-obj docutils literal notranslate"><span class="pre">num_envs_per_worker</span></code>. This is since we are collecting steps from
multiple envs in parallel. For example, if num_envs_per_worker=5, then
rollout workers will return experiences in chunks of 5*100 = 500 steps.
The dataflow here can vary per algorithm. For example, PPO further
divides the train batch into minibatches for multi-epoch SGD.
Set to “auto” to have RLlib compute an exact <code class="xref py py-obj docutils literal notranslate"><span class="pre">rollout_fragment_length</span></code>
to match the given batch size.</p></li>
<li><p><strong>batch_mode</strong> – How to build individual batches with the EnvRunner(s). Batches
coming from distributed EnvRunners are usually concat’d to form the
train batch. Note that “steps” below can mean different things (either
env- or agent-steps) and depends on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">count_steps_by</span></code> setting,
adjustable via <code class="xref py py-obj docutils literal notranslate"><span class="pre">AlgorithmConfig.multi_agent(count_steps_by=..)</span></code>:
1) “truncate_episodes”: Each call to <code class="xref py py-obj docutils literal notranslate"><span class="pre">EnvRunner.sample()</span></code> will return a
batch of at most <code class="xref py py-obj docutils literal notranslate"><span class="pre">rollout_fragment_length</span> <span class="pre">*</span> <span class="pre">num_envs_per_worker</span></code> in
size. The batch will be exactly <code class="xref py py-obj docutils literal notranslate"><span class="pre">rollout_fragment_length</span> <span class="pre">*</span> <span class="pre">num_envs</span></code>
in size if postprocessing does not change batch sizes. Episodes
may be truncated in order to meet this size requirement.
This mode guarantees evenly sized batches, but increases
variance as the future return must now be estimated at truncation
boundaries.
2) “complete_episodes”: Each call to <code class="xref py py-obj docutils literal notranslate"><span class="pre">EnvRunner.sample()</span></code> will return a
batch of at least <code class="xref py py-obj docutils literal notranslate"><span class="pre">rollout_fragment_length</span> <span class="pre">*</span> <span class="pre">num_envs_per_worker</span></code> in
size. Episodes will not be truncated, but multiple episodes
may be packed within one batch to meet the (minimum) batch size.
Note that when <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_envs_per_worker</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, episode steps will be buffered
until the episode completes, and hence batches may contain
significant amounts of off-policy data.</p></li>
<li><p><strong>remote_worker_envs</strong> – If using num_envs_per_worker &gt; 1, whether to create
those new envs in remote processes instead of in the same worker.
This adds overheads, but can make sense if your envs can take much
time to step / reset (e.g., for StarCraft). Use this cautiously;
overheads are significant.</p></li>
<li><p><strong>remote_env_batch_wait_ms</strong> – Timeout that remote workers are waiting when
polling environments. 0 (continue when at least one env is ready) is
a reasonable default, but optimal value could be obtained by measuring
your environment step / reset and model inference perf.</p></li>
<li><p><strong>validate_workers_after_construction</strong> – Whether to validate that each created
remote worker is healthy after its construction process.</p></li>
<li><p><strong>preprocessor_pref</strong> – Whether to use “rllib” or “deepmind” preprocessors by
default. Set to None for using no preprocessor. In this case, the
model will have to handle possibly complex observations from the
environment.</p></li>
<li><p><strong>observation_filter</strong> – Element-wise observation filter, either “NoFilter”
or “MeanStdFilter”.</p></li>
<li><p><strong>compress_observations</strong> – Whether to LZ4 compress individual observations
in the SampleBatches collected during rollouts.</p></li>
<li><p><strong>enable_tf1_exec_eagerly</strong> – Explicitly tells the rollout worker to enable
TF eager execution. This is useful for example when framework is
“torch”, but a TF2 policy needs to be restored for evaluation or
league-based purposes.</p></li>
<li><p><strong>sampler_perf_stats_ema_coef</strong> – If specified, perf stats are in EMAs. This
is the coeff of how much new data points contribute to the averages.
Default is None, which uses simple global average instead.
The EMA update rule is: updated = (1 - ema_coef) * old + ema_coef * new</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="specifying-evaluation-options">
<span id="rllib-config-evaluation"></span><h3>Specifying Evaluation Options<a class="headerlink" href="rllib-training.html#specifying-evaluation-options" title="Permalink to this headline">#</a></h3>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">AlgorithmConfig.</span></span><span class="sig-name descname"><span class="pre">evaluation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_interval:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_duration:</span> <span class="pre">Optional[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">str]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_duration_unit:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_sample_timeout_s:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_parallel_to_training:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_config:</span> <span class="pre">Optional[Union[ray.rllib.algorithms.algorithm_config.AlgorithmConfig</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dict]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">off_policy_estimation_methods:</span> <span class="pre">Optional[Dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ope_split_batch_by_episode:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_num_workers:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_evaluation_function:</span> <span class="pre">Optional[Callable]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">always_attach_evaluation_results:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_async_evaluation:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_num_episodes=-1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig"><span class="pre">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/algorithm_config.html#AlgorithmConfig.evaluation"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Sets the config’s evaluation settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>evaluation_interval</strong> – Evaluate with every <code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluation_interval</span></code> training
iterations. The evaluation stats will be reported under the “evaluation”
metric key. Note that for Ape-X metrics are already only reported for
the lowest epsilon workers (least random workers).
Set to None (or 0) for no evaluation.</p></li>
<li><p><strong>evaluation_duration</strong> – Duration for which to run evaluation each
<code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluation_interval</span></code>. The unit for the duration can be set via
<code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluation_duration_unit</span></code> to either “episodes” (default) or
“timesteps”. If using multiple evaluation workers
(evaluation_num_workers &gt; 1), the load to run will be split amongst
these.
If the value is “auto”:
- For <code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluation_parallel_to_training=True</span></code>: Will run as many
episodes/timesteps that fit into the (parallel) training step.
- For <code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluation_parallel_to_training=False</span></code>: Error.</p></li>
<li><p><strong>evaluation_duration_unit</strong> – The unit, with which to count the evaluation
duration. Either “episodes” (default) or “timesteps”.</p></li>
<li><p><strong>evaluation_sample_timeout_s</strong> – The timeout (in seconds) for the ray.get call
to the remote evaluation worker(s) <code class="xref py py-obj docutils literal notranslate"><span class="pre">sample()</span></code> method. After this time,
the user will receive a warning and instructions on how to fix the
issue. This could be either to make sure the episode ends, increasing
the timeout, or switching to <code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluation_duration_unit=timesteps</span></code>.</p></li>
<li><p><strong>evaluation_parallel_to_training</strong> – Whether to run evaluation in parallel to
a Algorithm.train() call using threading. Default=False.
E.g. evaluation_interval=2 -&gt; For every other training iteration,
the Algorithm.train() and Algorithm.evaluate() calls run in parallel.
Note: This is experimental. Possible pitfalls could be race conditions
for weight synching at the beginning of the evaluation loop.</p></li>
<li><p><strong>evaluation_config</strong> – Typical usage is to pass extra args to evaluation env
creator and to disable exploration by computing deterministic actions.
IMPORTANT NOTE: Policy gradient algorithms are able to find the optimal
policy, even if this is a stochastic one. Setting “explore=False” here
will result in the evaluation workers not using this optimal policy!</p></li>
<li><p><strong>off_policy_estimation_methods</strong> – Specify how to evaluate the current policy,
along with any optional config parameters. This only has an effect when
reading offline experiences (“input” is not “sampler”).
Available keys:
{ope_method_name: {“type”: ope_type, …}} where <code class="xref py py-obj docutils literal notranslate"><span class="pre">ope_method_name</span></code>
is a user-defined string to save the OPE results under, and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">ope_type</span></code> can be any subclass of OffPolicyEstimator, e.g.
ray.rllib.offline.estimators.is::ImportanceSampling
or your own custom subclass, or the full class path to the subclass.
You can also add additional config arguments to be passed to the
OffPolicyEstimator in the dict, e.g.
{“qreg_dr”: {“type”: DoublyRobust, “q_model_type”: “qreg”, “k”: 5}}</p></li>
<li><p><strong>ope_split_batch_by_episode</strong> – Whether to use SampleBatch.split_by_episode() to
split the input batch to episodes before estimating the ope metrics. In
case of bandits you should make this False to see improvements in ope
evaluation speed. In case of bandits, it is ok to not split by episode,
since each record is one timestep already. The default is True.</p></li>
<li><p><strong>evaluation_num_workers</strong> – Number of parallel workers to use for evaluation.
Note that this is set to zero by default, which means evaluation will
be run in the algorithm process (only if evaluation_interval is not
None). If you increase this, it will increase the Ray resource usage of
the algorithm since evaluation workers are created separately from
rollout workers (used to sample data for training).</p></li>
<li><p><strong>custom_evaluation_function</strong> – Customize the evaluation method. This must be a
function of signature (algo: Algorithm, eval_workers: WorkerSet) -&gt;
metrics: dict. See the Algorithm.evaluate() method to see the default
implementation. The Algorithm guarantees all eval workers have the
latest policy state before this function is called.</p></li>
<li><p><strong>always_attach_evaluation_results</strong> – Make sure the latest available evaluation
results are always attached to a step result dict. This may be useful
if Tune or some other meta controller needs access to evaluation metrics
all the time.</p></li>
<li><p><strong>enable_async_evaluation</strong> – If True, use an AsyncRequestsManager for
the evaluation workers and use this manager to send <code class="xref py py-obj docutils literal notranslate"><span class="pre">sample()</span></code> requests
to the evaluation workers. This way, the Algorithm becomes more robust
against long running episodes and/or failing (and restarting) workers.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="specifying-exploration-options">
<span id="rllib-config-exploration"></span><h3>Specifying Exploration Options<a class="headerlink" href="rllib-training.html#specifying-exploration-options" title="Permalink to this headline">#</a></h3>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">AlgorithmConfig.</span></span><span class="sig-name descname"><span class="pre">exploration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">explore:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exploration_config:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig"><span class="pre">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/algorithm_config.html#AlgorithmConfig.exploration"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Sets the config’s exploration settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>explore</strong> – Default exploration behavior, iff <code class="xref py py-obj docutils literal notranslate"><span class="pre">explore=None</span></code> is passed into
compute_action(s). Set to False for no exploration behavior (e.g.,
for evaluation).</p></li>
<li><p><strong>exploration_config</strong> – A dict specifying the Exploration object’s config.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="specifying-offline-data-options">
<span id="rllib-config-offline-data"></span><h3>Specifying Offline Data Options<a class="headerlink" href="rllib-training.html#specifying-offline-data-options" title="Permalink to this headline">#</a></h3>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">AlgorithmConfig.</span></span><span class="sig-name descname"><span class="pre">offline_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_config=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actions_in_input_normalized=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_evaluation=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">postprocess_inputs=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_buffer_size=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_config=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_compress_columns=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_max_file_size=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offline_sampling=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig"><span class="pre">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/algorithm_config.html#AlgorithmConfig.offline_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Sets the config’s offline data settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – Specify how to generate experiences:
- “sampler”: Generate experiences via online (env) simulation (default).
- A local directory or file glob expression (e.g., “/tmp/<em>.json”).
- A list of individual file paths/URIs (e.g., [“/tmp/1.json”,
“s3://bucket/2.json”]).
- A dict with string keys and sampling probabilities as values (e.g.,
{“sampler”: 0.4, “/tmp/</em>.json”: 0.4, “s3://bucket/expert.json”: 0.2}).
- A callable that takes an <code class="xref py py-obj docutils literal notranslate"><span class="pre">IOContext</span></code> object as only arg and returns a
ray.rllib.offline.InputReader.
- A string key that indexes a callable with tune.registry.register_input</p></li>
<li><p><strong>input_config</strong> – Arguments that describe the settings for reading the input.
If input is <code class="xref py py-obj docutils literal notranslate"><span class="pre">sample</span></code>, this will be environment configuation, e.g.
<code class="xref py py-obj docutils literal notranslate"><span class="pre">env_name</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">env_config</span></code>, etc. See <code class="xref py py-obj docutils literal notranslate"><span class="pre">EnvContext</span></code> for more info.
If the input is <code class="xref py py-obj docutils literal notranslate"><span class="pre">dataset</span></code>, this will be e.g. <code class="xref py py-obj docutils literal notranslate"><span class="pre">format</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">path</span></code>.</p></li>
<li><p><strong>actions_in_input_normalized</strong> – True, if the actions in a given offline “input”
are already normalized (between -1.0 and 1.0). This is usually the case
when the offline file has been generated by another RLlib algorithm
(e.g. PPO or SAC), while “normalize_actions” was set to True.</p></li>
<li><p><strong>postprocess_inputs</strong> – Whether to run postprocess_trajectory() on the
trajectory fragments from offline inputs. Note that postprocessing will
be done using the <em>current</em> policy, not the <em>behavior</em> policy, which
is typically undesirable for on-policy algorithms.</p></li>
<li><p><strong>shuffle_buffer_size</strong> – If positive, input batches will be shuffled via a
sliding window buffer of this number of batches. Use this if the input
data is not in random enough order. Input is delayed until the shuffle
buffer is filled.</p></li>
<li><p><strong>output</strong> – Specify where experiences should be saved:
- None: don’t save any experiences
- “logdir” to save to the agent log dir
- a path/URI to save to a custom output directory (e.g., “s3://bckt/”)
- a function that returns a rllib.offline.OutputWriter</p></li>
<li><p><strong>output_config</strong> – Arguments accessible from the IOContext for configuring
custom output.</p></li>
<li><p><strong>output_compress_columns</strong> – What sample batch columns to LZ4 compress in the
output data.</p></li>
<li><p><strong>output_max_file_size</strong> – Max output file size (in bytes) before rolling over
to a new file.</p></li>
<li><p><strong>offline_sampling</strong> – Whether sampling for the Algorithm happens via
reading from offline data. If True, RolloutWorkers will NOT limit the
number of collected batches within the same <code class="xref py py-obj docutils literal notranslate"><span class="pre">sample()</span></code> call based on
the number of sub-environments within the worker (no sub-environments
present).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="specifying-multi-agent-options">
<span id="rllib-config-multi-agent"></span><h3>Specifying Multi-Agent Options<a class="headerlink" href="rllib-training.html#specifying-multi-agent-options" title="Permalink to this headline">#</a></h3>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">AlgorithmConfig.</span></span><span class="sig-name descname"><span class="pre">multi_agent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">*,</span> <span class="pre">policies=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">algorithm_config_overrides_per_module:</span> <span class="pre">Optional[Dict[str,</span> <span class="pre">dict]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">policy_map_capacity:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">policy_mapping_fn:</span> <span class="pre">Optional[Callable[[Any,</span> <span class="pre">Episode],</span> <span class="pre">str]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">policies_to_train:</span> <span class="pre">Optional[Union[Container[str],</span> <span class="pre">Callable[[str,</span> <span class="pre">Union[SampleBatch,</span> <span class="pre">MultiAgentBatch]],</span> <span class="pre">bool]]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">policy_states_are_swappable:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">observation_fn:</span> <span class="pre">Optional[Callable]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">count_steps_by:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">replay_mode=-1,</span> <span class="pre">policy_map_cache=-1</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/algorithm_config.html#AlgorithmConfig.multi_agent"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Sets the config’s multi-agent settings.</p>
<p>Validates the new multi-agent settings and translates everything into
a unified multi-agent setup format. For example a <code class="xref py py-obj docutils literal notranslate"><span class="pre">policies</span></code> list or set
of IDs is properly converted into a dict mapping these IDs to PolicySpecs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>policies</strong> – Map of type MultiAgentPolicyConfigDict from policy ids to either
4-tuples of (policy_cls, obs_space, act_space, config) or PolicySpecs.
These tuples or PolicySpecs define the class of the policy, the
observation- and action spaces of the policies, and any extra config.</p></li>
<li><p><strong>algorithm_config_overrides_per_module</strong> – Only used if both
<code class="xref py py-obj docutils literal notranslate"><span class="pre">_enable_learner_api</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">_enable_rl_module_api</span></code> are True.
A mapping from ModuleIDs to
per-module AlgorithmConfig override dicts, which apply certain settings,
e.g. the learning rate, from the main AlgorithmConfig only to this
particular module (within a MultiAgentRLModule).
You can create override dicts by using the <a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.overrides.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.overrides" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig.overrides"><code class="xref py py-obj docutils literal notranslate"><span class="pre">AlgorithmConfig.overrides</span></code></a>
utility. For example, to override your learning rate and (PPO) lambda
setting just for a single RLModule with your MultiAgentRLModule, do:
config.multi_agent(algorithm_config_overrides_per_module={
“module_1”: PPOConfig.overrides(lr=0.0002, lambda_=0.75),
})</p></li>
<li><p><strong>policy_map_capacity</strong> – Keep this many policies in the “policy_map” (before
writing least-recently used ones to disk/S3).</p></li>
<li><p><strong>policy_mapping_fn</strong> – Function mapping agent ids to policy ids. The signature
is: <code class="xref py py-obj docutils literal notranslate"><span class="pre">(agent_id,</span> <span class="pre">episode,</span> <span class="pre">worker,</span> <span class="pre">**kwargs)</span> <span class="pre">-&gt;</span> <span class="pre">PolicyID</span></code>.</p></li>
<li><p><strong>policies_to_train</strong> – Determines those policies that should be updated.
Options are:
- None, for training all policies.
- An iterable of PolicyIDs that should be trained.
- A callable, taking a PolicyID and a SampleBatch or MultiAgentBatch
and returning a bool (indicating whether the given policy is trainable
or not, given the particular batch). This allows you to have a policy
trained only on certain data (e.g. when playing against a certain
opponent).</p></li>
<li><p><strong>policy_states_are_swappable</strong> – Whether all Policy objects in this map can be
“swapped out” via a simple <code class="xref py py-obj docutils literal notranslate"><span class="pre">state</span> <span class="pre">=</span> <span class="pre">A.get_state();</span> <span class="pre">B.set_state(state)</span></code>,
where <code class="xref py py-obj docutils literal notranslate"><span class="pre">A</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">B</span></code> are policy instances in this map. You should set
this to True for significantly speeding up the PolicyMap’s cache lookup
times, iff your policies all share the same neural network
architecture and optimizer types. If True, the PolicyMap will not
have to garbage collect old, least recently used policies, but instead
keep them in memory and simply override their state with the state of
the most recently accessed one.
For example, in a league-based training setup, you might have 100s of
the same policies in your map (playing against each other in various
combinations), but all of them share the same state structure
(are “swappable”).</p></li>
<li><p><strong>observation_fn</strong> – Optional function that can be used to enhance the local
agent observations to include more state. See
rllib/evaluation/observation_function.py for more info.</p></li>
<li><p><strong>count_steps_by</strong> – Which metric to use as the “batch size” when building a
MultiAgentBatch. The two supported values are:
“env_steps”: Count each time the env is “stepped” (no matter how many
multi-agent actions are passed/how many multi-agent observations
have been returned in the previous step).
“agent_steps”: Count each individual agent step as one step.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="specifying-reporting-options">
<span id="rllib-config-reporting"></span><h3>Specifying Reporting Options<a class="headerlink" href="rllib-training.html#specifying-reporting-options" title="Permalink to this headline">#</a></h3>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">AlgorithmConfig.</span></span><span class="sig-name descname"><span class="pre">reporting</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_per_episode_custom_metrics:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics_episode_collection_timeout_s:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics_num_episodes_for_smoothing:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_time_s_per_iteration:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_train_timesteps_per_iteration:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_sample_timesteps_per_iteration:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig"><span class="pre">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/algorithm_config.html#AlgorithmConfig.reporting"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Sets the config’s reporting settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>keep_per_episode_custom_metrics</strong> – Store raw custom metrics without
calculating max, min, mean</p></li>
<li><p><strong>metrics_episode_collection_timeout_s</strong> – Wait for metric batches for at most
this many seconds. Those that have not returned in time will be
collected in the next train iteration.</p></li>
<li><p><strong>metrics_num_episodes_for_smoothing</strong> – Smooth rollout metrics over this many
episodes, if possible.
In case rollouts (sample collection) just started, there may be fewer
than this many episodes in the buffer and we’ll compute metrics
over this smaller number of available episodes.
In case there are more than this many episodes collected in a single
training iteration, use all of these episodes for metrics computation,
meaning don’t ever cut any “excess” episodes.</p></li>
<li><p><strong>min_time_s_per_iteration</strong> – Minimum time to accumulate within a single
<code class="xref py py-obj docutils literal notranslate"><span class="pre">train()</span></code> call. This value does not affect learning,
only the number of times <code class="xref py py-obj docutils literal notranslate"><span class="pre">Algorithm.training_step()</span></code> is called by
<code class="xref py py-obj docutils literal notranslate"><span class="pre">Algorithm.train()</span></code>. If - after one such step attempt, the time taken
has not reached <code class="xref py py-obj docutils literal notranslate"><span class="pre">min_time_s_per_iteration</span></code>, will perform n more
<code class="xref py py-obj docutils literal notranslate"><span class="pre">training_step()</span></code> calls until the minimum time has been
consumed. Set to 0 or None for no minimum time.</p></li>
<li><p><strong>min_train_timesteps_per_iteration</strong> – Minimum training timesteps to accumulate
within a single <code class="xref py py-obj docutils literal notranslate"><span class="pre">train()</span></code> call. This value does not affect learning,
only the number of times <code class="xref py py-obj docutils literal notranslate"><span class="pre">Algorithm.training_step()</span></code> is called by
<code class="xref py py-obj docutils literal notranslate"><span class="pre">Algorithm.train()</span></code>. If - after one such step attempt, the training
timestep count has not been reached, will perform n more
<code class="xref py py-obj docutils literal notranslate"><span class="pre">training_step()</span></code> calls until the minimum timesteps have been
executed. Set to 0 or None for no minimum timesteps.</p></li>
<li><p><strong>min_sample_timesteps_per_iteration</strong> – Minimum env sampling timesteps to
accumulate within a single <code class="xref py py-obj docutils literal notranslate"><span class="pre">train()</span></code> call. This value does not affect
learning, only the number of times <code class="xref py py-obj docutils literal notranslate"><span class="pre">Algorithm.training_step()</span></code> is
called by <code class="xref py py-obj docutils literal notranslate"><span class="pre">Algorithm.train()</span></code>. If - after one such step attempt, the env
sampling timestep count has not been reached, will perform n more
<code class="xref py py-obj docutils literal notranslate"><span class="pre">training_step()</span></code> calls until the minimum timesteps have been
executed. Set to 0 or None for no minimum timesteps.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="specifying-checkpointing-options">
<span id="rllib-config-checkpointing"></span><h3>Specifying Checkpointing Options<a class="headerlink" href="rllib-training.html#specifying-checkpointing-options" title="Permalink to this headline">#</a></h3>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">AlgorithmConfig.</span></span><span class="sig-name descname"><span class="pre">checkpointing</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_native_model_files:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_trainable_policies_only:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig"><span class="pre">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/algorithm_config.html#AlgorithmConfig.checkpointing"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Sets the config’s checkpointing settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>export_native_model_files</strong> – Whether an individual Policy-
or the Algorithm’s checkpoints also contain (tf or torch) native
model files. These could be used to restore just the NN models
from these files w/o requiring RLlib. These files are generated
by calling the tf- or torch- built-in saving utility methods on
the actual models.</p></li>
<li><p><strong>checkpoint_trainable_policies_only</strong> – Whether to only add Policies to the
Algorithm checkpoint (in sub-directory “policies/”) that are trainable
according to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">is_trainable_policy</span></code> callable of the local worker.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="specifying-debugging-options">
<span id="rllib-config-debugging"></span><h3>Specifying Debugging Options<a class="headerlink" href="rllib-training.html#specifying-debugging-options" title="Permalink to this headline">#</a></h3>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">AlgorithmConfig.</span></span><span class="sig-name descname"><span class="pre">debugging</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger_creator:</span> <span class="pre">Optional[Callable[[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ray.tune.logger.logger.Logger]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger_config:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_level:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_sys_usage:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fake_sampler:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker_cls=None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig"><span class="pre">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/algorithm_config.html#AlgorithmConfig.debugging"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Sets the config’s debugging settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>logger_creator</strong> – Callable that creates a ray.tune.Logger
object. If unspecified, a default logger is created.</p></li>
<li><p><strong>logger_config</strong> – Define logger-specific configuration to be used inside Logger
Default value None allows overwriting with nested dicts.</p></li>
<li><p><strong>log_level</strong> – Set the ray.rllib.* log level for the agent process and its
workers. Should be one of DEBUG, INFO, WARN, or ERROR. The DEBUG level
will also periodically print out summaries of relevant internal dataflow
(this is also printed out once at startup at the INFO level). When using
the <code class="xref py py-obj docutils literal notranslate"><span class="pre">rllib</span> <span class="pre">train</span></code> command, you can also use the <code class="xref py py-obj docutils literal notranslate"><span class="pre">-v</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">-vv</span></code> flags as
shorthand for INFO and DEBUG.</p></li>
<li><p><strong>log_sys_usage</strong> – Log system resource metrics to results. This requires
<code class="xref py py-obj docutils literal notranslate"><span class="pre">psutil</span></code> to be installed for sys stats, and <code class="xref py py-obj docutils literal notranslate"><span class="pre">gputil</span></code> for GPU metrics.</p></li>
<li><p><strong>fake_sampler</strong> – Use fake (infinite speed) sampler. For testing only.</p></li>
<li><p><strong>seed</strong> – This argument, in conjunction with worker_index, sets the random
seed of each worker, so that identically configured trials will have
identical results. This makes experiments reproducible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="specifying-callback-options">
<span id="rllib-config-callbacks"></span><h3>Specifying Callback Options<a class="headerlink" href="rllib-training.html#specifying-callback-options" title="Permalink to this headline">#</a></h3>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">AlgorithmConfig.</span></span><span class="sig-name descname"><span class="pre">callbacks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">callbacks_class</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig"><span class="pre">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/algorithm_config.html#AlgorithmConfig.callbacks"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Sets the callbacks configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>callbacks_class</strong> – Callbacks class, whose methods will be run during
various phases of training and environment sample collection.
See the <code class="xref py py-obj docutils literal notranslate"><span class="pre">DefaultCallbacks</span></code> class and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">examples/custom_metrics_and_callbacks.py</span></code> for more usage information.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="specifying-resources">
<span id="rllib-config-resources"></span><h3>Specifying Resources<a class="headerlink" href="rllib-training.html#specifying-resources" title="Permalink to this headline">#</a></h3>
<p>You can control the degree of parallelism used by setting the <code class="docutils literal notranslate"><span class="pre">num_workers</span></code>
hyperparameter for most algorithms. The Algorithm will construct that many
“remote worker” instances (<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/evaluation/rollout_worker.py">see RolloutWorker class</a>)
that are constructed as ray.remote actors, plus exactly one “local worker”, a <code class="docutils literal notranslate"><span class="pre">RolloutWorker</span></code> object that is not a
ray actor, but lives directly inside the Algorithm.
For most algorithms, learning updates are performed on the local worker and sample collection from
one or more environments is performed by the remote workers (in parallel).
For example, setting <code class="docutils literal notranslate"><span class="pre">num_workers=0</span></code> will only create the local worker, in which case both
sample collection and training will be done by the local worker.
On the other hand, setting <code class="docutils literal notranslate"><span class="pre">num_workers=5</span></code> will create the local worker (responsible for training updates)
and 5 remote workers (responsible for sample collection).</p>
<p>Since learning is most of the time done on the local worker, it may help to provide one or more GPUs
to that worker via the <code class="docutils literal notranslate"><span class="pre">num_gpus</span></code> setting.
Similarly, the resource allocation to remote workers can be controlled via <code class="docutils literal notranslate"><span class="pre">num_cpus_per_worker</span></code>, <code class="docutils literal notranslate"><span class="pre">num_gpus_per_worker</span></code>, and <code class="docutils literal notranslate"><span class="pre">custom_resources_per_worker</span></code>.</p>
<p>The number of GPUs can be fractional quantities (e.g. 0.5) to allocate only a fraction
of a GPU. For example, with DQN you can pack five algorithms onto one GPU by setting
<code class="docutils literal notranslate"><span class="pre">num_gpus:</span> <span class="pre">0.2</span></code>. Check out <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/fractional_gpus.py">this fractional GPU example here</a>
as well that also demonstrates how environments (running on the remote workers) that
require a GPU can benefit from the <code class="docutils literal notranslate"><span class="pre">num_gpus_per_worker</span></code> setting.</p>
<p>For synchronous algorithms like PPO and A2C, the driver and workers can make use of
the same GPU. To do this for an amount of <code class="docutils literal notranslate"><span class="pre">n</span></code> GPUS:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gpu_count</span> <span class="o">=</span> <span class="n">n</span>
<span class="n">num_gpus</span> <span class="o">=</span> <span class="mf">0.0001</span> <span class="c1"># Driver GPU</span>
<span class="n">num_gpus_per_worker</span> <span class="o">=</span> <span class="p">(</span><span class="n">gpu_count</span> <span class="o">-</span> <span class="n">num_gpus</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_workers</span>
</pre></div>
</div>
<img alt="../_images/rllib-config.svg" src="../_images/rllib-config.svg" /><p>If you specify <code class="docutils literal notranslate"><span class="pre">num_gpus</span></code> and your machine does not have the required number of GPUs
available, a RuntimeError will be thrown by the respective worker. On the other hand,
if you set <code class="docutils literal notranslate"><span class="pre">num_gpus=0</span></code>, your policies will be built solely on the CPU, even if
GPUs are available on the machine.</p>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">AlgorithmConfig.</span></span><span class="sig-name descname"><span class="pre">resources</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_gpus:</span> <span class="pre">Optional[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_fake_gpus:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cpus_per_worker:</span> <span class="pre">Optional[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_gpus_per_worker:</span> <span class="pre">Optional[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cpus_for_local_worker:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_learner_workers:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cpus_per_learner_worker:</span> <span class="pre">Optional[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_gpus_per_learner_worker:</span> <span class="pre">Optional[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_gpu_idx:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_resources_per_worker:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placement_strategy:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig"><span class="pre">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/algorithm_config.html#AlgorithmConfig.resources"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Specifies resources allocated for an Algorithm and its ray actors/workers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_gpus</strong> – Number of GPUs to allocate to the algorithm process.
Note that not all algorithms can take advantage of GPUs.
Support for multi-GPU is currently only available for
tf-[PPO/IMPALA/DQN/PG]. This can be fractional (e.g., 0.3 GPUs).</p></li>
<li><p><strong>_fake_gpus</strong> – Set to True for debugging (multi-)?GPU funcitonality on a
CPU machine. GPU towers will be simulated by graphs located on
CPUs in this case. Use <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_gpus</span></code> to test for different numbers of
fake GPUs.</p></li>
<li><p><strong>num_cpus_per_worker</strong> – Number of CPUs to allocate per worker.</p></li>
<li><p><strong>num_gpus_per_worker</strong> – Number of GPUs to allocate per worker. This can be
fractional. This is usually needed only if your env itself requires a
GPU (i.e., it is a GPU-intensive video game), or model inference is
unusually expensive.</p></li>
<li><p><strong>num_learner_workers</strong> – Number of workers used for training. A value of 0
means training will take place on a local worker on head node CPUs or 1
GPU (determined by <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_gpus_per_learner_worker</span></code>). For multi-gpu
training, set number of workers greater than 1 and set
<code class="xref py py-obj docutils literal notranslate"><span class="pre">num_gpus_per_learner_worker</span></code> accordingly (e.g. 4 GPUs total, and model
needs 2 GPUs: <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_learner_workers</span> <span class="pre">=</span> <span class="pre">2</span></code> and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">num_gpus_per_learner_worker</span> <span class="pre">=</span> <span class="pre">2</span></code>)</p></li>
<li><p><strong>num_cpus_per_learner_worker</strong> – Number of CPUs allocated per trainer worker.
Only necessary for custom processing pipeline inside each Learner
requiring multiple CPU cores. Ignored if <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_learner_workers</span> <span class="pre">=</span> <span class="pre">0</span></code>.</p></li>
<li><p><strong>num_gpus_per_learner_worker</strong> – Number of GPUs allocated per worker. If
<code class="xref py py-obj docutils literal notranslate"><span class="pre">num_learner_workers</span> <span class="pre">=</span> <span class="pre">0</span></code>, any value greater than 0 will run the
training on a single GPU on the head node, while a value of 0 will run
the training on head node CPU cores. If num_gpus_per_learner_worker is
set, then num_cpus_per_learner_worker cannot be set.</p></li>
<li><p><strong>local_gpu_idx</strong> – if num_gpus_per_worker &gt; 0, and num_workers&lt;2, then this gpu
index will be used for training. This is an index into the available
cuda devices. For example if os.environ[“CUDA_VISIBLE_DEVICES”] = “1”
then a local_gpu_idx of 0 will use the gpu with id 1 on the node.</p></li>
<li><p><strong>custom_resources_per_worker</strong> – Any custom Ray resources to allocate per
worker.</p></li>
<li><p><strong>num_cpus_for_local_worker</strong> – Number of CPUs to allocate for the algorithm.
Note: this only takes effect when running in Tune. Otherwise,
the algorithm runs in the main program (driver).</p></li>
<li><p><strong>custom_resources_per_worker</strong> – Any custom Ray resources to allocate per
worker.</p></li>
<li><p><strong>placement_strategy</strong> – The strategy for the placement group factory returned by
<code class="xref py py-obj docutils literal notranslate"><span class="pre">Algorithm.default_resource_request()</span></code>. A PlacementGroup defines, which
devices (resources) should always be co-located on the same node.
For example, an Algorithm with 2 rollout workers, running with
num_gpus=1 will request a placement group with the bundles:
[{“gpu”: 1, “cpu”: 1}, {“cpu”: 1}, {“cpu”: 1}], where the first bundle
is for the driver and the other 2 bundles are for the two workers.
These bundles can now be “placed” on the same or different
nodes depending on the value of <code class="xref py py-obj docutils literal notranslate"><span class="pre">placement_strategy</span></code>:
“PACK”: Packs bundles into as few nodes as possible.
“SPREAD”: Places bundles across distinct nodes as even as possible.
“STRICT_PACK”: Packs bundles into one node. The group is not allowed
to span multiple nodes.
“STRICT_SPREAD”: Packs bundles across distinct nodes.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="specifying-experimental-features">
<span id="rllib-config-experimental"></span><h3>Specifying Experimental Features<a class="headerlink" href="rllib-training.html#specifying-experimental-features" title="Permalink to this headline">#</a></h3>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">AlgorithmConfig.</span></span><span class="sig-name descname"><span class="pre">experimental</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_tf_policy_handles_more_than_one_loss:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_disable_preprocessor_api:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_disable_action_flattening:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_disable_execution_plan_api:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_disable_initialize_loss_from_dummy_batch:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig"><span class="pre">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/algorithm_config.html#AlgorithmConfig.experimental"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Sets the config’s experimental settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>_tf_policy_handles_more_than_one_loss</strong> – Experimental flag.
If True, TFPolicy will handle more than one loss/optimizer.
Set this to True, if you would like to return more than
one loss term from your <code class="xref py py-obj docutils literal notranslate"><span class="pre">loss_fn</span></code> and an equal number of optimizers
from your <code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizer_fn</span></code>. In the future, the default for this will be
True.</p></li>
<li><p><strong>_disable_preprocessor_api</strong> – Experimental flag.
If True, no (observation) preprocessor will be created and
observations will arrive in model as they are returned by the env.
In the future, the default for this will be True.</p></li>
<li><p><strong>_disable_action_flattening</strong> – Experimental flag.
If True, RLlib will no longer flatten the policy-computed actions into
a single tensor (for storage in SampleCollectors/output files/etc..),
but leave (possibly nested) actions as-is. Disabling flattening affects:
- SampleCollectors: Have to store possibly nested action structs.
- Models that have the previous action(s) as part of their input.
- Algorithms reading from offline files (incl. action information).</p></li>
<li><p><strong>_disable_execution_plan_api</strong> – Experimental flag.
If True, the execution plan API will not be used. Instead,
a Algorithm’s <code class="xref py py-obj docutils literal notranslate"><span class="pre">training_iteration</span></code> method will be called as-is each
training iteration.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="rllib-scaling-guide">
<span id="id1"></span><h2>RLlib Scaling Guide<a class="headerlink" href="rllib-training.html#rllib-scaling-guide" title="Permalink to this headline">#</a></h2>
<p>Here are some rules of thumb for scaling training with RLlib.</p>
<ol class="arabic simple">
<li><p>If the environment is slow and cannot be replicated (e.g., since it requires interaction with physical systems), then you should use a sample-efficient off-policy algorithm such as <a class="reference internal" href="rllib-algorithms.html#dqn"><span class="std std-ref">DQN</span></a> or <a class="reference internal" href="rllib-algorithms.html#sac"><span class="std std-ref">SAC</span></a>. These algorithms default to <code class="docutils literal notranslate"><span class="pre">num_workers:</span> <span class="pre">0</span></code> for single-process operation. Make sure to set <code class="docutils literal notranslate"><span class="pre">num_gpus:</span> <span class="pre">1</span></code> if you want to use a GPU. Consider also batch RL training with the <a class="reference external" href="rllib-offline.html">offline data</a> API.</p></li>
<li><p>If the environment is fast and the model is small (most models for RL are), use time-efficient algorithms such as <a class="reference internal" href="rllib-algorithms.html#ppo"><span class="std std-ref">PPO</span></a>, <a class="reference internal" href="rllib-algorithms.html#impala"><span class="std std-ref">IMPALA</span></a>, or <a class="reference internal" href="rllib-algorithms.html#apex"><span class="std std-ref">APEX</span></a>. These can be scaled by increasing <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> to add rollout workers. It may also make sense to enable <a class="reference external" href="../rllib-env.html#vectorized">vectorization</a> for inference. Make sure to set <code class="docutils literal notranslate"><span class="pre">num_gpus:</span> <span class="pre">1</span></code> if you want to use a GPU. If the learner becomes a bottleneck, multiple GPUs can be used for learning by setting <code class="docutils literal notranslate"><span class="pre">num_gpus</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>.</p></li>
<li><p>If the model is compute intensive (e.g., a large deep residual network) and inference is the bottleneck, consider allocating GPUs to workers by setting <code class="docutils literal notranslate"><span class="pre">num_gpus_per_worker:</span> <span class="pre">1</span></code>. If you only have a single GPU, consider <code class="docutils literal notranslate"><span class="pre">num_workers:</span> <span class="pre">0</span></code> to use the learner GPU for inference. For efficient use of GPU time, use a small number of GPU workers and a large number of <a class="reference external" href="../rllib-env.html#vectorized">envs per worker</a>.</p></li>
<li><p>Finally, if both model and environment are compute intensive, then enable <a class="reference external" href="../rllib-env.html#vectorized">remote worker envs</a> with <a class="reference external" href="../rllib-env.html#vectorized">async batching</a> by setting <code class="docutils literal notranslate"><span class="pre">remote_worker_envs:</span> <span class="pre">True</span></code> and optionally <code class="docutils literal notranslate"><span class="pre">remote_env_batch_wait_ms</span></code>. This batches inference on GPUs in the rollout workers while letting envs run asynchronously in separate actors, similar to the <a class="reference external" href="https://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html">SEED</a> architecture. The number of workers and number of envs per worker should be tuned to maximize GPU utilization. If your env requires GPUs to function, or if multi-node SGD is needed, then also consider <a class="reference internal" href="rllib-algorithms.html#ddppo"><span class="std std-ref">DD-PPO</span></a>.</p></li>
</ol>
<p>In case you are using lots of workers (<code class="docutils literal notranslate"><span class="pre">num_workers</span> <span class="pre">&gt;&gt;</span> <span class="pre">10</span></code>) and you observe worker failures for whatever reasons, which normally interrupt your RLlib training runs, consider using
the config settings <code class="docutils literal notranslate"><span class="pre">ignore_worker_failures=True</span></code>, <code class="docutils literal notranslate"><span class="pre">recreate_failed_workers=True</span></code>, or <code class="docutils literal notranslate"><span class="pre">restart_failed_sub_environments=True</span></code>:</p>
<p><code class="docutils literal notranslate"><span class="pre">ignore_worker_failures</span></code>: When set to True, your Algorithm will not crash due to a single worker error but continue for as long as there is at least one functional worker remaining.
<code class="docutils literal notranslate"><span class="pre">recreate_failed_workers</span></code>: When set to True, your Algorithm will attempt to replace/recreate any failed worker(s) with newly created one(s). This way, your number of workers will never decrease, even if some of them fail from time to time.
<code class="docutils literal notranslate"><span class="pre">restart_failed_sub_environments</span></code>: When set to True and there is a failure in one of the vectorized sub-environments in one of your workers, the worker will try to recreate only the failed sub-environment and re-integrate the newly created one into your vectorized env stack on that worker.</p>
<p>Note that only one of <code class="docutils literal notranslate"><span class="pre">ignore_worker_failures</span></code> or <code class="docutils literal notranslate"><span class="pre">recreate_failed_workers</span></code> may be set to True (they are mutually exclusive settings). However,
you can combine each of these with the <code class="docutils literal notranslate"><span class="pre">restart_failed_sub_environments=True</span></code> setting.
Using these options will make your training runs much more stable and more robust against occasional OOM or other similar “once in a while” errors on your workers
themselves or inside your environments.</p>
</section>
<section id="debugging-rllib-experiments">
<h2>Debugging RLlib Experiments<a class="headerlink" href="rllib-training.html#debugging-rllib-experiments" title="Permalink to this headline">#</a></h2>
<section id="gym-monitor">
<h3>Gym Monitor<a class="headerlink" href="rllib-training.html#gym-monitor" title="Permalink to this headline">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">&quot;monitor&quot;:</span> <span class="pre">true</span></code> config can be used to save Gym episode videos to the result dir. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>rllib train --env<span class="o">=</span>PongDeterministic-v4 <span class="se">\</span>
    --run<span class="o">=</span>A2C --config <span class="s1">&#39;{&quot;num_workers&quot;: 2, &quot;monitor&quot;: true}&#39;</span>

<span class="c1"># videos will be saved in the ~/ray_results/&lt;experiment&gt; dir, for example</span>
openaigym.video.0.31401.video000000.meta.json
openaigym.video.0.31401.video000000.mp4
openaigym.video.0.31403.video000000.meta.json
openaigym.video.0.31403.video000000.mp4
</pre></div>
</div>
</section>
<section id="eager-mode">
<h3>Eager Mode<a class="headerlink" href="rllib-training.html#eager-mode" title="Permalink to this headline">#</a></h3>
<p>Policies built with <code class="docutils literal notranslate"><span class="pre">build_tf_policy</span></code> (most of the reference algorithms are)
can be run in eager mode by setting the
<code class="docutils literal notranslate"><span class="pre">&quot;framework&quot;:</span> <span class="pre">&quot;tf2&quot;</span></code> / <code class="docutils literal notranslate"><span class="pre">&quot;eager_tracing&quot;:</span> <span class="pre">true</span></code> config options or using
<code class="docutils literal notranslate"><span class="pre">rllib</span> <span class="pre">train</span> <span class="pre">--config</span> <span class="pre">'{&quot;framework&quot;:</span> <span class="pre">&quot;tf2&quot;}'</span> <span class="pre">[--trace]</span></code>.
This will tell RLlib to execute the model forward pass, action distribution,
loss, and stats functions in eager mode.</p>
<p>Eager mode makes debugging much easier, since you can now use line-by-line
debugging with breakpoints or Python <code class="docutils literal notranslate"><span class="pre">print()</span></code> to inspect
intermediate tensor values.
However, eager can be slower than graph mode unless tracing is enabled.</p>
</section>
<section id="using-pytorch">
<h3>Using PyTorch<a class="headerlink" href="rllib-training.html#using-pytorch" title="Permalink to this headline">#</a></h3>
<p>Algorithms that have an implemented TorchPolicy, will allow you to run
<code class="xref py py-obj docutils literal notranslate"><span class="pre">rllib</span> <span class="pre">train</span></code> using the command line <code class="docutils literal notranslate"><span class="pre">--framework=torch</span></code> flag.
Algorithms that do not have a torch version yet will complain with an error in
this case.</p>
</section>
<section id="episode-traces">
<h3>Episode Traces<a class="headerlink" href="rllib-training.html#episode-traces" title="Permalink to this headline">#</a></h3>
<p>You can use the <a class="reference external" href="rllib-offline.html">data output API</a> to save episode traces
for debugging. For example, the following command will run PPO while saving episode
traces to <code class="docutils literal notranslate"><span class="pre">/tmp/debug</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>rllib train --run<span class="o">=</span>PPO --env<span class="o">=</span>CartPole-v1 <span class="se">\</span>
    --config<span class="o">=</span><span class="s1">&#39;{&quot;output&quot;: &quot;/tmp/debug&quot;, &quot;output_compress_columns&quot;: []}&#39;</span>

<span class="c1"># episode traces will be saved in /tmp/debug, for example</span>
output-2019-02-23_12-02-03_worker-2_0.json
output-2019-02-23_12-02-04_worker-1_0.json
</pre></div>
</div>
</section>
<section id="log-verbosity">
<h3>Log Verbosity<a class="headerlink" href="rllib-training.html#log-verbosity" title="Permalink to this headline">#</a></h3>
<p>You can control the log level via the <code class="docutils literal notranslate"><span class="pre">&quot;log_level&quot;</span></code> flag. Valid values are “DEBUG”,
“INFO”, “WARN” (default), and “ERROR”. This can be used to increase or decrease the
verbosity of internal logging. You can also use the <code class="docutils literal notranslate"><span class="pre">-v</span></code> and <code class="docutils literal notranslate"><span class="pre">-vv</span></code> flags.
For example, the following two commands are about equivalent:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>rllib train --env<span class="o">=</span>PongDeterministic-v4 <span class="se">\</span>
    --run<span class="o">=</span>A2C --config <span class="s1">&#39;{&quot;num_workers&quot;: 2, &quot;log_level&quot;: &quot;DEBUG&quot;}&#39;</span>

rllib train --env<span class="o">=</span>PongDeterministic-v4 <span class="se">\</span>
    --run<span class="o">=</span>A2C --config <span class="s1">&#39;{&quot;num_workers&quot;: 2}&#39;</span> -vv
</pre></div>
</div>
<p>The default log level is <code class="docutils literal notranslate"><span class="pre">WARN</span></code>. We strongly recommend using at least <code class="docutils literal notranslate"><span class="pre">INFO</span></code>
level logging for development.</p>
</section>
<section id="stack-traces">
<h3>Stack Traces<a class="headerlink" href="rllib-training.html#stack-traces" title="Permalink to this headline">#</a></h3>
<p>You can use the <code class="docutils literal notranslate"><span class="pre">ray</span> <span class="pre">stack</span></code> command to dump the stack traces of all the
Python workers on a single node. This can be useful for debugging unexpected
hangs or performance issues.</p>
</section>
</section>
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="rllib-training.html#next-steps" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>To check how your application is doing, you can use the <a class="reference internal" href="../ray-observability/getting-started.html#observability-getting-started"><span class="std std-ref">Ray dashboard</span></a>.</p></li>
</ul>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="index.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">RLlib: Industry-Grade Reinforcement Learning</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="key-concepts.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Key Concepts</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>