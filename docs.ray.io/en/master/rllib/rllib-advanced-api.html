
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Advanced Python APIs &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/versionwarning.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../_static/js/docsearch.js"></script>
    <script src="../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../_static/js/termynal.js"></script>
    <script defer="defer" src="../_static/js/custom.js"></script>
    <script defer="defer" src="../_static/js/top-navigation.js"></script>
    <script src="../_static/js/tags.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/rllib/rllib-advanced-api.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Models, Preprocessors, and Action Distributions" href="rllib-models.html" />
    <link rel="prev" title="User Guides" href="user-guides.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "rllib/rllib-advanced-api", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Ray RLlib
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-training.html">
     Getting Started with RLlib
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="key-concepts.html">
     Key Concepts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../rllib-env.html">
     Environments
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-algorithms.html">
     Algorithms
    </a>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="user-guides.html">
     User Guides
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="rllib-advanced-api.html#">
       Advanced Python APIs
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-models.html">
       Models, Preprocessors, and Action Distributions
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-saving-and-loading-algos-and-policies.html">
       Saving and Loading your RL Algorithms and Policies
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-concepts.html">
       How To Customize Policies
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-sample-collection.html">
       Sample Collections and Trajectory Views
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-replay-buffers.html">
       Replay Buffers
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-offline.html">
       Working With Offline Data
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-catalogs.html">
       Catalog (Alpha)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-connector.html">
       Connectors (Alpha)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-rlmodule.html">
       RL Modules (Alpha)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-fault-tolerance.html">
       Fault Tolerance And Elastic Training
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-dev.html">
       How To Contribute to RLlib
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="rllib-cli.html">
       Working with the RLlib CLI
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-examples.html">
     Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="package_ref/index.html">
     Ray RLlib API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Frllib/rllib-advanced-api.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/rllib/rllib-advanced-api.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/rllib/rllib-advanced-api.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#custom-training-workflows">
   Custom Training Workflows
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#curriculum-learning">
   Curriculum Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#global-coordination">
   Global Coordination
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#callbacks-and-custom-metrics">
   Callbacks and Custom Metrics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#chaining-callbacks">
   Chaining Callbacks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#visualizing-custom-metrics">
   Visualizing Custom Metrics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#customizing-exploration-behavior">
   Customizing Exploration Behavior
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#customized-evaluation-during-training">
   Customized Evaluation During Training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#rewriting-trajectories">
   Rewriting Trajectories
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Advanced Python APIs</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#custom-training-workflows">
   Custom Training Workflows
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#curriculum-learning">
   Curriculum Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#global-coordination">
   Global Coordination
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#callbacks-and-custom-metrics">
   Callbacks and Custom Metrics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#chaining-callbacks">
   Chaining Callbacks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#visualizing-custom-metrics">
   Visualizing Custom Metrics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#customizing-exploration-behavior">
   Customizing Exploration Behavior
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#customized-evaluation-during-training">
   Customized Evaluation During Training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-advanced-api.html#rewriting-trajectories">
   Rewriting Trajectories
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="advanced-python-apis">
<span id="rllib-advanced-api-doc"></span><h1>Advanced Python APIs<a class="headerlink" href="rllib-advanced-api.html#advanced-python-apis" title="Permalink to this headline">#</a></h1>
<section id="custom-training-workflows">
<h2>Custom Training Workflows<a class="headerlink" href="rllib-advanced-api.html#custom-training-workflows" title="Permalink to this headline">#</a></h2>
<p>In the <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/custom_env.py">basic training example</a>,
Tune will call <code class="docutils literal notranslate"><span class="pre">train()</span></code> on your algorithm once per training iteration and report
the new training results.
Sometimes, it is desirable to have full control over training, but still run inside Tune.
Tune supports <a class="reference internal" href="../tune/api/trainable.html#trainable-docs"><span class="std std-ref">custom trainable functions</span></a> that can be used to
implement <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/custom_train_fn.py">custom training workflows (example)</a>.</p>
<p>For even finer-grained control over training, you can use RLlib’s lower-level
<a class="reference external" href="rllib-concepts.html">building blocks</a> directly to implement
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/rollout_worker_custom_workflow.py">fully customized training workflows</a>.</p>
</section>
<section id="curriculum-learning">
<h2>Curriculum Learning<a class="headerlink" href="rllib-advanced-api.html#curriculum-learning" title="Permalink to this headline">#</a></h2>
<p>In Curriculum learning, the environment can be set to different difficulties
(or “tasks”) to allow for learning to progress through controlled phases (from easy to
more difficult). RLlib comes with a basic curriculum learning API utilizing the
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/env/apis/task_settable_env.py">TaskSettableEnv</a> environment API.
Your environment only needs to implement the <code class="xref py py-obj docutils literal notranslate"><span class="pre">set_task</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">get_task</span></code> methods
for this to work. You can then define an <code class="xref py py-obj docutils literal notranslate"><span class="pre">env_task_fn</span></code> in your config,
which receives the last training results and returns a new task for the env to be set to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.env.apis.task_settable_env</span> <span class="kn">import</span> <span class="n">TaskSettableEnv</span>

<span class="k">class</span> <span class="nc">MyEnv</span><span class="p">(</span><span class="n">TaskSettableEnv</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">get_task</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_difficulty</span>

    <span class="k">def</span> <span class="nf">set_task</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_difficulty</span> <span class="o">=</span> <span class="n">task</span>

<span class="k">def</span> <span class="nf">curriculum_fn</span><span class="p">(</span><span class="n">train_results</span><span class="p">,</span> <span class="n">task_settable_env</span><span class="p">,</span> <span class="n">env_ctx</span><span class="p">):</span>
    <span class="c1"># Very simple curriculum function.</span>
    <span class="n">current_task</span> <span class="o">=</span> <span class="n">task_settable_env</span><span class="o">.</span><span class="n">get_task</span><span class="p">()</span>
    <span class="n">new_task</span> <span class="o">=</span> <span class="n">current_task</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">new_task</span>

<span class="c1"># Setup your Algorithm&#39;s config like so:</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;env&quot;</span><span class="p">:</span> <span class="n">MyEnv</span><span class="p">,</span>
    <span class="s2">&quot;env_task_fn&quot;</span><span class="p">:</span> <span class="n">curriculum_fn</span><span class="p">,</span>
<span class="p">}</span>
<span class="c1"># Train using `Tuner.fit()` or `Algorithm.train()` and the above config stub.</span>
<span class="c1"># ...</span>
</pre></div>
</div>
<p>There are two more ways to use the RLlib’s other APIs to implement
<a class="reference external" href="https://bair.berkeley.edu/blog/2017/12/20/reverse-curriculum/">curriculum learning</a>.</p>
<p>Use the Algorithm API and update the environment between calls to <code class="docutils literal notranslate"><span class="pre">train()</span></code>.
This example shows the algorithm being run inside a Tune function.
This is basically the same as what the built-in <code class="xref py py-obj docutils literal notranslate"><span class="pre">env_task_fn</span></code> API described above
already does under the hood, but allows you to do even more customizations to your
training loop.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPO</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">reporter</span><span class="p">):</span>
    <span class="n">algo</span> <span class="o">=</span> <span class="n">PPO</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">env</span><span class="o">=</span><span class="n">YourEnv</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">reporter</span><span class="p">(</span><span class="o">**</span><span class="n">result</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">200</span><span class="p">:</span>
            <span class="n">task</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="k">elif</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">100</span><span class="p">:</span>
            <span class="n">task</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">task</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">algo</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">ev</span><span class="p">:</span> <span class="n">ev</span><span class="o">.</span><span class="n">foreach_env</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">env</span><span class="p">:</span> <span class="n">env</span><span class="o">.</span><span class="n">set_task</span><span class="p">(</span><span class="n">task</span><span class="p">)))</span>

<span class="n">num_gpus</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">num_workers</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="n">tune</span><span class="o">.</span><span class="n">with_resources</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">resources</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">PlacementGroupFactory</span><span class="p">(</span>
        <span class="p">[{</span><span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="p">{</span><span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="n">num_gpus</span><span class="p">}]</span> <span class="o">+</span> <span class="p">[{</span><span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}]</span> <span class="o">*</span> <span class="n">num_workers</span>
    <span class="p">),)</span>
    <span class="n">param_space</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;num_gpus&quot;</span><span class="p">:</span> <span class="n">num_gpus</span><span class="p">,</span>
        <span class="s2">&quot;num_workers&quot;</span><span class="p">:</span> <span class="n">num_workers</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>You could also use RLlib’s callbacks API to update the environment on new training
results:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="kn">from</span> <span class="nn">ray.rllib.agents.callbacks</span> <span class="kn">import</span> <span class="n">DefaultCallbacks</span>

<span class="k">class</span> <span class="nc">MyCallbacks</span><span class="p">(</span><span class="n">DefaultCallbacks</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">on_train_result</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">algorithm</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">200</span><span class="p">:</span>
            <span class="n">task</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="k">elif</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">100</span><span class="p">:</span>
            <span class="n">task</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">task</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">algorithm</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">ev</span><span class="p">:</span> <span class="n">ev</span><span class="o">.</span><span class="n">foreach_env</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">env</span><span class="p">:</span> <span class="n">env</span><span class="o">.</span><span class="n">set_task</span><span class="p">(</span><span class="n">task</span><span class="p">)))</span>

<span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="s2">&quot;PPO&quot;</span><span class="p">,</span>
    <span class="n">param_space</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;env&quot;</span><span class="p">:</span> <span class="n">YourEnv</span><span class="p">,</span>
        <span class="s2">&quot;callbacks&quot;</span><span class="p">:</span> <span class="n">MyCallbacks</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="global-coordination">
<h2>Global Coordination<a class="headerlink" href="rllib-advanced-api.html#global-coordination" title="Permalink to this headline">#</a></h2>
<p>Sometimes, it is necessary to coordinate between pieces of code that live in different
processes managed by RLlib.
For example, it can be useful to maintain a global average of a certain variable,
or centrally control a hyperparameter used by policies.
Ray provides a general way to achieve this through <em>named actors</em>
(learn more about <a class="reference internal" href="../ray-more-libs/actors.html#actor-guide"><span class="std std-ref">Ray actors here</span></a>).
These actors are assigned a global name and handles to them can be retrieved using
these names. As an example, consider maintaining a shared global counter that is
incremented by environments and read periodically from your driver program:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>


<span class="nd">@ray</span><span class="o">.</span><span class="n">remote</span>
<span class="k">class</span> <span class="nc">Counter</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">inc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">+=</span> <span class="n">n</span>

    <span class="k">def</span> <span class="nf">get</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">count</span>


<span class="c1"># on the driver</span>
<span class="n">counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="o">.</span><span class="n">options</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;global_counter&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">remote</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">get</span><span class="o">.</span><span class="n">remote</span><span class="p">()))</span>  <span class="c1"># get the latest count</span>

<span class="c1"># in your envs</span>
<span class="n">counter</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get_actor</span><span class="p">(</span><span class="s2">&quot;global_counter&quot;</span><span class="p">)</span>
<span class="n">counter</span><span class="o">.</span><span class="n">inc</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># async call to increment the global count</span>
</pre></div>
</div>
<p>Ray actors provide high levels of performance, so in more complex cases they can be
used implement communication patterns such as parameter servers and allreduce.</p>
</section>
<section id="callbacks-and-custom-metrics">
<h2>Callbacks and Custom Metrics<a class="headerlink" href="rllib-advanced-api.html#callbacks-and-custom-metrics" title="Permalink to this headline">#</a></h2>
<p>You can provide callbacks to be called at points during policy evaluation.
These callbacks have access to state for the current
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/evaluation/episode.py">episode</a>.
Certain callbacks such as <code class="docutils literal notranslate"><span class="pre">on_postprocess_trajectory</span></code>, <code class="docutils literal notranslate"><span class="pre">on_sample_end</span></code>,
and <code class="docutils literal notranslate"><span class="pre">on_train_result</span></code> are also places where custom postprocessing can be applied to
intermediate data or results.</p>
<p>User-defined state can be stored for the
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/evaluation/episode.py">episode</a>
in the <code class="docutils literal notranslate"><span class="pre">episode.user_data</span></code> dict, and custom scalar metrics reported by saving values
to the <code class="docutils literal notranslate"><span class="pre">episode.custom_metrics</span></code> dict. These custom metrics will be aggregated and
reported as part of training results. For a full example, take a look at
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/custom_metrics_and_callbacks.py">this example script here</a>
and
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/tests/test_callbacks.py">these unit test cases here</a>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can create custom logic that can run on each evaluation episode by checking
if the <a class="reference internal" href="package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.html#ray.rllib.evaluation.rollout_worker.RolloutWorker" title="ray.rllib.evaluation.rollout_worker.RolloutWorker"><code class="xref py py-class docutils literal notranslate"><span class="pre">RolloutWorker</span></code></a> is in
evaluation mode, through accessing <code class="docutils literal notranslate"><span class="pre">worker.policy_config[&quot;in_evaluation&quot;]</span></code>.
You can then implement this check in <code class="docutils literal notranslate"><span class="pre">on_episode_start()</span></code> or <code class="docutils literal notranslate"><span class="pre">on_episode_end()</span></code>
in your subclass of <a class="reference internal" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks" title="ray.rllib.algorithms.callbacks.DefaultCallbacks"><code class="xref py py-class docutils literal notranslate"><span class="pre">DefaultCallbacks</span></code></a>.
For running callbacks before and after the evaluation
runs in whole we provide <code class="docutils literal notranslate"><span class="pre">on_evaluate_start()</span></code> and <code class="docutils literal notranslate"><span class="pre">on_evaluate_end</span></code>.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Click here to see the full API of the <code class="docutils literal notranslate"><span class="pre">DefaultCallbacks</span></code> class<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.callbacks.DefaultCallbacks">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.callbacks.</span></span><span class="sig-name descname"><span class="pre">DefaultCallbacks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">legacy_callbacks_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">callable</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/callbacks.html#DefaultCallbacks"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Abstract base class for RLlib callbacks (similar to Keras callbacks).</p>
<p class="sd-card-text">These callbacks can be used for custom metrics and custom postprocessing.</p>
<p class="sd-card-text">By default, all of these callbacks are no-ops. To configure custom training
callbacks, subclass DefaultCallbacks and then set
{“callbacks”: YourCallbacksClass} in the algo config.</p>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.callbacks.DefaultCallbacks.on_algorithm_init">
<span class="sig-name descname"><span class="pre">on_algorithm_init</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Algorithm</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/callbacks.html#DefaultCallbacks.on_algorithm_init"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks.on_algorithm_init" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Callback run when a new algorithm instance has finished setup.</p>
<p class="sd-card-text">This method gets called at the end of Algorithm.setup() after all
the initialization is done, and before actually training starts.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>algorithm</strong> – Reference to the trainer instance.</p></li>
<li><p class="sd-card-text"><strong>kwargs</strong> – Forward compatibility placeholder.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.callbacks.DefaultCallbacks.on_create_policy">
<span class="sig-name descname"><span class="pre">on_create_policy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="package_ref/doc/ray.rllib.policy.policy.Policy.html#ray.rllib.policy.policy.Policy" title="ray.rllib.policy.policy.Policy"><span class="pre">ray.rllib.policy.policy.Policy</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/callbacks.html#DefaultCallbacks.on_create_policy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks.on_create_policy" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Callback run whenever a new policy is added to an algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>policy_id</strong> – ID of the newly created policy.</p></li>
<li><p class="sd-card-text"><strong>policy</strong> – the policy just created.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.callbacks.DefaultCallbacks.on_sub_environment_created">
<span class="sig-name descname"><span class="pre">on_sub_environment_created</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutWorker</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sub_environment</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env_context</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ray.rllib.env.env_context.EnvContext</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/callbacks.html#DefaultCallbacks.on_sub_environment_created"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks.on_sub_environment_created" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Callback run when a new sub-environment has been created.</p>
<p class="sd-card-text">This method gets called after each sub-environment (usually a
gym.Env) has been created, validated (RLlib built-in validation
+ possible custom validation function implemented by overriding
<code class="xref py py-obj docutils literal notranslate"><span class="pre">Algorithm.validate_env()</span></code>), wrapped (e.g. video-wrapper), and seeded.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>worker</strong> – Reference to the current rollout worker.</p></li>
<li><p class="sd-card-text"><strong>sub_environment</strong> – The sub-environment instance that has been
created. This is usually a gym.Env object.</p></li>
<li><p class="sd-card-text"><strong>env_context</strong> – The <code class="xref py py-obj docutils literal notranslate"><span class="pre">EnvContext</span></code> object that has been passed to
the env’s constructor.</p></li>
<li><p class="sd-card-text"><strong>kwargs</strong> – Forward compatibility placeholder.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.callbacks.DefaultCallbacks.on_episode_created">
<span class="sig-name descname"><span class="pre">on_episode_created</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutWorker</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="package_ref/env/base_env.html#ray.rllib.env.base_env.BaseEnv" title="ray.rllib.env.base_env.BaseEnv"><span class="pre">ray.rllib.env.base_env.BaseEnv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">policies</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="package_ref/doc/ray.rllib.policy.policy.Policy.html#ray.rllib.policy.policy.Policy" title="ray.rllib.policy.policy.Policy"><span class="pre">ray.rllib.policy.policy.Policy</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">episode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">ray.rllib.evaluation.episode.Episode</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ray.rllib.evaluation.episode_v2.EpisodeV2</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/callbacks.html#DefaultCallbacks.on_episode_created"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks.on_episode_created" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Callback run when a new episode is created (but has not started yet!).</p>
<p class="sd-card-text">This method gets called after a new Episode(V2) instance is created to
start a new episode. This happens before the respective sub-environment’s
(usually a gym.Env) <code class="xref py py-obj docutils literal notranslate"><span class="pre">reset()</span></code> is called by RLlib.</p>
<ol class="arabic simple">
<li><p class="sd-card-text">Episode(V2) created: This callback fires.</p></li>
<li><p class="sd-card-text">Respective sub-environment (gym.Env) is <code class="xref py py-obj docutils literal notranslate"><span class="pre">reset()</span></code>.</p></li>
<li><p class="sd-card-text">Callback <a class="reference internal" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks.on_episode_start" title="ray.rllib.algorithms.callbacks.DefaultCallbacks.on_episode_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_episode_start</span></code></a> is fired.</p></li>
<li><p class="sd-card-text">Stepping through sub-environment/episode commences.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>worker</strong> – Reference to the current rollout worker.</p></li>
<li><p class="sd-card-text"><strong>base_env</strong> – BaseEnv running the episode. The underlying
sub environment objects can be retrieved by calling
<code class="xref py py-obj docutils literal notranslate"><span class="pre">base_env.get_sub_environments()</span></code>.</p></li>
<li><p class="sd-card-text"><strong>policies</strong> – Mapping of policy id to policy objects. In single
agent mode there will only be a single “default” policy.</p></li>
<li><p class="sd-card-text"><strong>env_index</strong> – The index of the sub-environment that is about to be reset
(within the vector of sub-environments of the BaseEnv).</p></li>
<li><p class="sd-card-text"><strong>episode</strong> – The newly created episode. This is the one that will be started
with the upcoming reset. Only after the reset call, the
<a class="reference internal" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks.on_episode_start" title="ray.rllib.algorithms.callbacks.DefaultCallbacks.on_episode_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_episode_start</span></code></a> event will be triggered.</p></li>
<li><p class="sd-card-text"><strong>kwargs</strong> – Forward compatibility placeholder.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.callbacks.DefaultCallbacks.on_episode_start">
<span class="sig-name descname"><span class="pre">on_episode_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutWorker</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="package_ref/env/base_env.html#ray.rllib.env.base_env.BaseEnv" title="ray.rllib.env.base_env.BaseEnv"><span class="pre">ray.rllib.env.base_env.BaseEnv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">policies</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="package_ref/doc/ray.rllib.policy.policy.Policy.html#ray.rllib.policy.policy.Policy" title="ray.rllib.policy.policy.Policy"><span class="pre">ray.rllib.policy.policy.Policy</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">episode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">ray.rllib.evaluation.episode.Episode</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ray.rllib.evaluation.episode_v2.EpisodeV2</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/callbacks.html#DefaultCallbacks.on_episode_start"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks.on_episode_start" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Callback run right after an Episode has started.</p>
<p class="sd-card-text">This method gets called after the Episode(V2)’s respective sub-environment’s
(usually a gym.Env) <code class="xref py py-obj docutils literal notranslate"><span class="pre">reset()</span></code> is called by RLlib.</p>
<ol class="arabic simple">
<li><p class="sd-card-text">Episode(V2) created: Triggers callback <a class="reference internal" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks.on_episode_created" title="ray.rllib.algorithms.callbacks.DefaultCallbacks.on_episode_created"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_episode_created</span></code></a>.</p></li>
<li><p class="sd-card-text">Respective sub-environment (gym.Env) is <code class="xref py py-obj docutils literal notranslate"><span class="pre">reset()</span></code>.</p></li>
<li><p class="sd-card-text">Episode(V2) starts: This callback fires.</p></li>
<li><p class="sd-card-text">Stepping through sub-environment/episode commences.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>worker</strong> – Reference to the current rollout worker.</p></li>
<li><p class="sd-card-text"><strong>base_env</strong> – BaseEnv running the episode. The underlying
sub environment objects can be retrieved by calling
<code class="xref py py-obj docutils literal notranslate"><span class="pre">base_env.get_sub_environments()</span></code>.</p></li>
<li><p class="sd-card-text"><strong>policies</strong> – Mapping of policy id to policy objects. In single
agent mode there will only be a single “default” policy.</p></li>
<li><p class="sd-card-text"><strong>episode</strong> – Episode object which contains the episode’s
state. You can use the <code class="xref py py-obj docutils literal notranslate"><span class="pre">episode.user_data</span></code> dict to store
temporary data, and <code class="xref py py-obj docutils literal notranslate"><span class="pre">episode.custom_metrics</span></code> to store custom
metrics for the episode.</p></li>
<li><p class="sd-card-text"><strong>env_index</strong> – The index of the sub-environment that started the episode
(within the vector of sub-environments of the BaseEnv).</p></li>
<li><p class="sd-card-text"><strong>kwargs</strong> – Forward compatibility placeholder.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.callbacks.DefaultCallbacks.on_episode_step">
<span class="sig-name descname"><span class="pre">on_episode_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutWorker</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="package_ref/env/base_env.html#ray.rllib.env.base_env.BaseEnv" title="ray.rllib.env.base_env.BaseEnv"><span class="pre">ray.rllib.env.base_env.BaseEnv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">policies</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="package_ref/doc/ray.rllib.policy.policy.Policy.html#ray.rllib.policy.policy.Policy" title="ray.rllib.policy.policy.Policy"><span class="pre">ray.rllib.policy.policy.Policy</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">episode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">ray.rllib.evaluation.episode.Episode</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ray.rllib.evaluation.episode_v2.EpisodeV2</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/callbacks.html#DefaultCallbacks.on_episode_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks.on_episode_step" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Runs on each episode step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>worker</strong> – Reference to the current rollout worker.</p></li>
<li><p class="sd-card-text"><strong>base_env</strong> – BaseEnv running the episode. The underlying
sub environment objects can be retrieved by calling
<code class="xref py py-obj docutils literal notranslate"><span class="pre">base_env.get_sub_environments()</span></code>.</p></li>
<li><p class="sd-card-text"><strong>policies</strong> – Mapping of policy id to policy objects.
In single agent mode there will only be a single
“default_policy”.</p></li>
<li><p class="sd-card-text"><strong>episode</strong> – Episode object which contains episode
state. You can use the <code class="xref py py-obj docutils literal notranslate"><span class="pre">episode.user_data</span></code> dict to store
temporary data, and <code class="xref py py-obj docutils literal notranslate"><span class="pre">episode.custom_metrics</span></code> to store custom
metrics for the episode.</p></li>
<li><p class="sd-card-text"><strong>env_index</strong> – The index of the sub-environment that stepped the episode
(within the vector of sub-environments of the BaseEnv).</p></li>
<li><p class="sd-card-text"><strong>kwargs</strong> – Forward compatibility placeholder.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.callbacks.DefaultCallbacks.on_episode_end">
<span class="sig-name descname"><span class="pre">on_episode_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutWorker</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="package_ref/env/base_env.html#ray.rllib.env.base_env.BaseEnv" title="ray.rllib.env.base_env.BaseEnv"><span class="pre">ray.rllib.env.base_env.BaseEnv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">policies</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="package_ref/doc/ray.rllib.policy.policy.Policy.html#ray.rllib.policy.policy.Policy" title="ray.rllib.policy.policy.Policy"><span class="pre">ray.rllib.policy.policy.Policy</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">episode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">ray.rllib.evaluation.episode.Episode</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ray.rllib.evaluation.episode_v2.EpisodeV2</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Exception</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/callbacks.html#DefaultCallbacks.on_episode_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks.on_episode_end" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Runs when an episode is done.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>worker</strong> – Reference to the current rollout worker.</p></li>
<li><p class="sd-card-text"><strong>base_env</strong> – BaseEnv running the episode. The underlying
sub environment objects can be retrieved by calling
<code class="xref py py-obj docutils literal notranslate"><span class="pre">base_env.get_sub_environments()</span></code>.</p></li>
<li><p class="sd-card-text"><strong>policies</strong> – Mapping of policy id to policy
objects. In single agent mode there will only be a single
“default_policy”.</p></li>
<li><p class="sd-card-text"><strong>episode</strong> – Episode object which contains episode
state. You can use the <code class="xref py py-obj docutils literal notranslate"><span class="pre">episode.user_data</span></code> dict to store
temporary data, and <code class="xref py py-obj docutils literal notranslate"><span class="pre">episode.custom_metrics</span></code> to store custom
metrics for the episode.
In case of environment failures, episode may also be an Exception
that gets thrown from the environment before the episode finishes.
Users of this callback may then handle these error cases properly
with their custom logics.</p></li>
<li><p class="sd-card-text"><strong>env_index</strong> – The index of the sub-environment that ended the episode
(within the vector of sub-environments of the BaseEnv).</p></li>
<li><p class="sd-card-text"><strong>kwargs</strong> – Forward compatibility placeholder.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.callbacks.DefaultCallbacks.on_evaluate_start">
<span class="sig-name descname"><span class="pre">on_evaluate_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Algorithm</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/callbacks.html#DefaultCallbacks.on_evaluate_start"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks.on_evaluate_start" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Callback before evaluation starts.</p>
<p class="sd-card-text">This method gets called at the beginning of Algorithm.evaluate().</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>algorithm</strong> – Reference to the algorithm instance.</p></li>
<li><p class="sd-card-text"><strong>kwargs</strong> – Forward compatibility placeholder.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.callbacks.DefaultCallbacks.on_evaluate_end">
<span class="sig-name descname"><span class="pre">on_evaluate_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Algorithm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/callbacks.html#DefaultCallbacks.on_evaluate_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks.on_evaluate_end" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Runs when the evaluation is done.</p>
<p class="sd-card-text">Runs at the end of Algorithm.evaluate().</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>algorithm</strong> – Reference to the algorithm instance.</p></li>
<li><p class="sd-card-text"><strong>evaluation_metrics</strong> – Results dict to be returned from algorithm.evaluate().
You can mutate this object to add additional metrics.</p></li>
<li><p class="sd-card-text"><strong>kwargs</strong> – Forward compatibility placeholder.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.callbacks.DefaultCallbacks.on_postprocess_trajectory">
<span class="sig-name descname"><span class="pre">on_postprocess_trajectory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutWorker</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">episode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ray.rllib.evaluation.episode.Episode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">agent_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policies</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="package_ref/doc/ray.rllib.policy.policy.Policy.html#ray.rllib.policy.policy.Policy" title="ray.rllib.policy.policy.Policy"><span class="pre">ray.rllib.policy.policy.Policy</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">postprocessed_batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.html#ray.rllib.policy.sample_batch.SampleBatch" title="ray.rllib.policy.sample_batch.SampleBatch"><span class="pre">ray.rllib.policy.sample_batch.SampleBatch</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">original_batches</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="package_ref/doc/ray.rllib.policy.policy.Policy.html#ray.rllib.policy.policy.Policy" title="ray.rllib.policy.policy.Policy"><span class="pre">ray.rllib.policy.policy.Policy</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.html#ray.rllib.policy.sample_batch.SampleBatch" title="ray.rllib.policy.sample_batch.SampleBatch"><span class="pre">ray.rllib.policy.sample_batch.SampleBatch</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/callbacks.html#DefaultCallbacks.on_postprocess_trajectory"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks.on_postprocess_trajectory" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Called immediately after a policy’s postprocess_fn is called.</p>
<p class="sd-card-text">You can use this callback to do additional postprocessing for a policy,
including looking at the trajectory data of other agents in multi-agent
settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>worker</strong> – Reference to the current rollout worker.</p></li>
<li><p class="sd-card-text"><strong>episode</strong> – Episode object.</p></li>
<li><p class="sd-card-text"><strong>agent_id</strong> – Id of the current agent.</p></li>
<li><p class="sd-card-text"><strong>policy_id</strong> – Id of the current policy for the agent.</p></li>
<li><p class="sd-card-text"><strong>policies</strong> – Mapping of policy id to policy objects. In single
agent mode there will only be a single “default_policy”.</p></li>
<li><p class="sd-card-text"><strong>postprocessed_batch</strong> – The postprocessed sample batch
for this agent. You can mutate this object to apply your own
trajectory postprocessing.</p></li>
<li><p class="sd-card-text"><strong>original_batches</strong> – Mapping of agents to their unpostprocessed
trajectory data. You should not mutate this object.</p></li>
<li><p class="sd-card-text"><strong>kwargs</strong> – Forward compatibility placeholder.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.callbacks.DefaultCallbacks.on_sample_end">
<span class="sig-name descname"><span class="pre">on_sample_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutWorker</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">samples</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.html#ray.rllib.policy.sample_batch.SampleBatch" title="ray.rllib.policy.sample_batch.SampleBatch"><span class="pre">ray.rllib.policy.sample_batch.SampleBatch</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/callbacks.html#DefaultCallbacks.on_sample_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks.on_sample_end" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Called at the end of RolloutWorker.sample().</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>worker</strong> – Reference to the current rollout worker.</p></li>
<li><p class="sd-card-text"><strong>samples</strong> – Batch to be returned. You can mutate this
object to modify the samples generated.</p></li>
<li><p class="sd-card-text"><strong>kwargs</strong> – Forward compatibility placeholder.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.callbacks.DefaultCallbacks.on_learn_on_batch">
<span class="sig-name descname"><span class="pre">on_learn_on_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="package_ref/doc/ray.rllib.policy.policy.Policy.html#ray.rllib.policy.policy.Policy" title="ray.rllib.policy.policy.Policy"><span class="pre">ray.rllib.policy.policy.Policy</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.html#ray.rllib.policy.sample_batch.SampleBatch" title="ray.rllib.policy.sample_batch.SampleBatch"><span class="pre">ray.rllib.policy.sample_batch.SampleBatch</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">result</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/callbacks.html#DefaultCallbacks.on_learn_on_batch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks.on_learn_on_batch" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Called at the beginning of Policy.learn_on_batch().</p>
<p class="sd-card-text">Note: This is called before 0-padding via
<code class="xref py py-obj docutils literal notranslate"><span class="pre">pad_batch_to_sequences_of_same_size</span></code>.</p>
<p class="sd-card-text">Also note, SampleBatch.INFOS column will not be available on
train_batch within this callback if framework is tf1, due to
the fact that tf1 static graph would mistake it as part of the
input dict if present.
It is available though, for tf2 and torch frameworks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>policy</strong> – Reference to the current Policy object.</p></li>
<li><p class="sd-card-text"><strong>train_batch</strong> – SampleBatch to be trained on. You can
mutate this object to modify the samples generated.</p></li>
<li><p class="sd-card-text"><strong>result</strong> – A results dict to add custom metrics to.</p></li>
<li><p class="sd-card-text"><strong>kwargs</strong> – Forward compatibility placeholder.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.callbacks.DefaultCallbacks.on_train_result">
<span class="sig-name descname"><span class="pre">on_train_result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Algorithm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">result</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/callbacks.html#DefaultCallbacks.on_train_result"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.DefaultCallbacks.on_train_result" title="Permalink to this definition">#</a></dt>
<dd><p class="sd-card-text">Called at the end of Algorithm.train().</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p class="sd-card-text"><strong>algorithm</strong> – Current Algorithm instance.</p></li>
<li><p class="sd-card-text"><strong>result</strong> – Dict of results returned from Algorithm.train() call.
You can mutate this object to add additional metrics.</p></li>
<li><p class="sd-card-text"><strong>kwargs</strong> – Forward compatibility placeholder.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</details></section>
<section id="chaining-callbacks">
<h2>Chaining Callbacks<a class="headerlink" href="rllib-advanced-api.html#chaining-callbacks" title="Permalink to this headline">#</a></h2>
<p>Use the <code class="docutils literal notranslate"><span class="pre">MultiCallbacks</span></code> class to chaim multiple callbacks together.</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.callbacks.MultiCallbacks">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.callbacks.</span></span><span class="sig-name descname"><span class="pre">MultiCallbacks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/callbacks.html#MultiCallbacks"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-advanced-api.html#ray.rllib.algorithms.callbacks.MultiCallbacks" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</section>
<section id="visualizing-custom-metrics">
<h2>Visualizing Custom Metrics<a class="headerlink" href="rllib-advanced-api.html#visualizing-custom-metrics" title="Permalink to this headline">#</a></h2>
<p>Custom metrics can be accessed and visualized like any other training result:</p>
<img alt="../_images/custom_metric.png" src="../_images/custom_metric.png" />
</section>
<section id="customizing-exploration-behavior">
<span id="exploration-api"></span><h2>Customizing Exploration Behavior<a class="headerlink" href="rllib-advanced-api.html#customizing-exploration-behavior" title="Permalink to this headline">#</a></h2>
<p>RLlib offers a unified top-level API to configure and customize an agent’s
exploration behavior, including the decisions (how and whether) to sample
actions from distributions (stochastically or deterministically).
The setup can be done via using built-in Exploration classes
(see <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/utils/exploration/">this package</a>),
which are specified (and further configured) inside
<code class="docutils literal notranslate"><span class="pre">AlgorithmConfig().exploration(..)</span></code>.
Besides using one of the available classes, one can sub-class any of
these built-ins, add custom behavior to it, and use that new class in
the config instead.</p>
<p>Every policy has-an Exploration object, which is created from the AlgorithmConfig’s
<code class="docutils literal notranslate"><span class="pre">.exploration(exploration_config=...)</span></code> method, which specifies the class to use via the
special “type” key, as well as constructor arguments via all other keys,
e.g.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">AlgorithmConfig</span><span class="p">()</span><span class="o">.</span><span class="n">exploration</span><span class="p">(</span>
    <span class="n">exploration_config</span><span class="o">=</span><span class="p">{</span>
        <span class="c1"># Special `type` key provides class information</span>
        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;StochasticSampling&quot;</span><span class="p">,</span>
        <span class="c1"># Add any needed constructor args here.</span>
        <span class="s2">&quot;constructor_arg&quot;</span><span class="p">:</span> <span class="s2">&quot;value&quot;</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The following table lists all built-in Exploration sub-classes and the agents
that currently use these by default:</p>
<img alt="../_images/rllib-exploration-api-table.svg" src="../_images/rllib-exploration-api-table.svg" /><p>An Exploration class implements the <code class="docutils literal notranslate"><span class="pre">get_exploration_action</span></code> method,
in which the exact exploratory behavior is defined.
It takes the model’s output, the action distribution class, the model itself,
a timestep (the global env-sampling steps already taken),
and an <code class="docutils literal notranslate"><span class="pre">explore</span></code> switch and outputs a tuple of a) action and
b) log-likelihood:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">get_exploration_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                               <span class="o">*</span><span class="p">,</span>
                               <span class="n">action_distribution</span><span class="p">:</span> <span class="n">ActionDistribution</span><span class="p">,</span>
                               <span class="n">timestep</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorType</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
                               <span class="n">explore</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns a (possibly) exploratory action and its log-likelihood.</span>

<span class="sd">        Given the Model&#39;s logits outputs and action distribution, returns an</span>
<span class="sd">        exploratory action.</span>

<span class="sd">        Args:</span>
<span class="sd">            action_distribution: The instantiated</span>
<span class="sd">                ActionDistribution object to work with when creating</span>
<span class="sd">                exploration actions.</span>
<span class="sd">            timestep: The current sampling time step. It can be a tensor</span>
<span class="sd">                for TF graph mode, otherwise an integer.</span>
<span class="sd">            explore: True: &quot;Normal&quot; exploration behavior.</span>
<span class="sd">                False: Suppress all exploratory behavior and return</span>
<span class="sd">                a deterministic action.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple consisting of 1) the chosen exploration action or a</span>
<span class="sd">            tf-op to fetch the exploration action from the graph and</span>
<span class="sd">            2) the log-likelihood of the exploration action.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>

</pre></div>
</div>
<p>On the highest level, the <code class="docutils literal notranslate"><span class="pre">Algorithm.compute_actions</span></code> and <code class="docutils literal notranslate"><span class="pre">Policy.compute_actions</span></code>
methods have a boolean <code class="docutils literal notranslate"><span class="pre">explore</span></code> switch, which is passed into
<code class="docutils literal notranslate"><span class="pre">Exploration.get_exploration_action</span></code>. If <code class="docutils literal notranslate"><span class="pre">explore=None</span></code>, the value of
<code class="docutils literal notranslate"><span class="pre">Algorithm.config[“explore”]</span></code> is used, which thus serves as a main switch for
exploratory behavior, allowing e.g. turning off any exploration easily for
evaluation purposes (see <a class="reference internal" href="rllib-advanced-api.html#customevaluation"><span class="std std-ref">Customized Evaluation During Training</span></a>).</p>
<p>The following are example excerpts from different Algorithms’ configs
(see <code class="docutils literal notranslate"><span class="pre">rllib/algorithms/algorithm.py</span></code>) to setup different exploration behaviors:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># All of the following configs go into Algorithm.config.</span>

<span class="c1"># 1) Switching *off* exploration by default.</span>
<span class="c1"># Behavior: Calling `compute_action(s)` without explicitly setting its `explore`</span>
<span class="c1"># param will result in no exploration.</span>
<span class="c1"># However, explicitly calling `compute_action(s)` with `explore=True` will</span>
<span class="c1"># still(!) result in exploration (per-call overrides default).</span>
<span class="s2">&quot;explore&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>

<span class="c1"># 2) Switching *on* exploration by default.</span>
<span class="c1"># Behavior: Calling `compute_action(s)` without explicitly setting its</span>
<span class="c1"># explore param will result in exploration.</span>
<span class="c1"># However, explicitly calling `compute_action(s)` with `explore=False`</span>
<span class="c1"># will result in no(!) exploration (per-call overrides default).</span>
<span class="s2">&quot;explore&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>

<span class="c1"># 3) Example exploration_config usages:</span>
<span class="c1"># a) DQN: see rllib/algorithms/dqn/dqn.py</span>
<span class="s2">&quot;explore&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="s2">&quot;exploration_config&quot;</span><span class="p">:</span> <span class="p">{</span>
   <span class="c1"># Exploration sub-class by name or full path to module+class</span>
   <span class="c1"># (e.g. “ray.rllib.utils.exploration.epsilon_greedy.EpsilonGreedy”)</span>
   <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;EpsilonGreedy&quot;</span><span class="p">,</span>
   <span class="c1"># Parameters for the Exploration class&#39; constructor:</span>
   <span class="s2">&quot;initial_epsilon&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
   <span class="s2">&quot;final_epsilon&quot;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>
   <span class="s2">&quot;epsilon_timesteps&quot;</span><span class="p">:</span> <span class="mi">10000</span><span class="p">,</span>  <span class="c1"># Timesteps over which to anneal epsilon.</span>
<span class="p">},</span>

<span class="c1"># b) DQN Soft-Q: In order to switch to Soft-Q exploration, do instead:</span>
<span class="s2">&quot;explore&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="s2">&quot;exploration_config&quot;</span><span class="p">:</span> <span class="p">{</span>
   <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;SoftQ&quot;</span><span class="p">,</span>
   <span class="c1"># Parameters for the Exploration class&#39; constructor:</span>
   <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
<span class="p">},</span>

<span class="c1"># c) All policy-gradient algos and SAC: see rllib/algorithms/algorithm.py</span>
<span class="c1"># Behavior: The algo samples stochastically from the</span>
<span class="c1"># model-parameterized distribution. This is the global Algorithm default</span>
<span class="c1"># setting defined in algorithm.py and used by all PG-type algos (plus SAC).</span>
<span class="s2">&quot;explore&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="s2">&quot;exploration_config&quot;</span><span class="p">:</span> <span class="p">{</span>
   <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;StochasticSampling&quot;</span><span class="p">,</span>
   <span class="s2">&quot;random_timesteps&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>  <span class="c1"># timesteps at beginning, over which to act uniformly randomly</span>
<span class="p">},</span>
</pre></div>
</div>
</section>
<section id="customized-evaluation-during-training">
<span id="customevaluation"></span><h2>Customized Evaluation During Training<a class="headerlink" href="rllib-advanced-api.html#customized-evaluation-during-training" title="Permalink to this headline">#</a></h2>
<p>RLlib will report online training rewards, however in some cases you may want to compute
rewards with different settings (e.g., with exploration turned off, or on a specific set
of environment configurations). You can activate evaluating policies during training
(<code class="docutils literal notranslate"><span class="pre">Algorithm.train()</span></code>) by setting the <code class="docutils literal notranslate"><span class="pre">evaluation_interval</span></code> to an int value (&gt; 0)
indicating every how many <code class="docutils literal notranslate"><span class="pre">Algorithm.train()</span></code> calls an “evaluation step” is run:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run one evaluation step on every 3rd `Algorithm.train()` call.</span>
<span class="p">{</span>
    <span class="s2">&quot;evaluation_interval&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>An evaluation step runs - using its own <code class="docutils literal notranslate"><span class="pre">RolloutWorker``s</span> <span class="pre">-</span> <span class="pre">for</span> <span class="pre">``evaluation_duration</span></code>
episodes or time-steps, depending on the <code class="docutils literal notranslate"><span class="pre">evaluation_duration_unit</span></code> setting, which can
take values of either <code class="docutils literal notranslate"><span class="pre">&quot;episodes&quot;</span></code> (default) or <code class="docutils literal notranslate"><span class="pre">&quot;timesteps&quot;</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Every time we run an evaluation step, run it for exactly 10 episodes.</span>
<span class="p">{</span>
    <span class="s2">&quot;evaluation_duration&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s2">&quot;evaluation_duration_unit&quot;</span><span class="p">:</span> <span class="s2">&quot;episodes&quot;</span><span class="p">,</span>
<span class="p">}</span>
<span class="c1"># Every time we run an evaluation step, run it for (close to) 200 timesteps.</span>
<span class="p">{</span>
    <span class="s2">&quot;evaluation_duration&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
    <span class="s2">&quot;evaluation_duration_unit&quot;</span><span class="p">:</span> <span class="s2">&quot;timesteps&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Note: When using <code class="docutils literal notranslate"><span class="pre">evaluation_duration_unit=timesteps</span></code> and your <code class="docutils literal notranslate"><span class="pre">evaluation_duration</span></code>
setting is not divisible by the number of evaluation workers (configurable via
<code class="docutils literal notranslate"><span class="pre">evaluation_num_workers</span></code>), RLlib will round up the number of time-steps specified to
the nearest whole number of time-steps that is divisible by the number of evaluation
workers.
Also, when using <code class="docutils literal notranslate"><span class="pre">evaluation_duration_unit=episodes</span></code> and your
<code class="docutils literal notranslate"><span class="pre">evaluation_duration</span></code> setting is not divisible by the number of evaluation workers
(configurable via <code class="docutils literal notranslate"><span class="pre">evaluation_num_workers</span></code>), RLlib will run the remainder of episodes
on the first n eval RolloutWorkers and leave the remaining workers idle for that time.</p>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Every time we run an evaluation step, run it for exactly 10 episodes, no matter, how many eval workers we have.</span>
<span class="p">{</span>
    <span class="s2">&quot;evaluation_duration&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s2">&quot;evaluation_duration_unit&quot;</span><span class="p">:</span> <span class="s2">&quot;episodes&quot;</span><span class="p">,</span>

    <span class="c1"># What if number of eval workers is non-dividable by 10?</span>
    <span class="c1"># -&gt; Run 7 episodes (1 per eval worker), then run 3 more episodes only using</span>
    <span class="c1">#    evaluation workers 1-3 (evaluation workers 4-7 remain idle during that time).</span>
    <span class="s2">&quot;evaluation_num_workers&quot;</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Before each evaluation step, weights from the main model are synchronized
to all evaluation workers.</p>
<p>By default, the evaluation step (if there is one in the current iteration) is run
right <strong>after</strong> the respective training step.
For example, for <code class="docutils literal notranslate"><span class="pre">evaluation_interval=1</span></code>, the sequence of events is:
<code class="docutils literal notranslate"><span class="pre">train(0-&gt;1),</span> <span class="pre">eval(1),</span> <span class="pre">train(1-&gt;2),</span> <span class="pre">eval(2),</span> <span class="pre">train(2-&gt;3),</span> <span class="pre">...</span></code>.
Here, the indices show the version of neural network weights used.
<code class="docutils literal notranslate"><span class="pre">train(0-&gt;1)</span></code> is an update step that changes the weights from version 0 to
version 1 and <code class="docutils literal notranslate"><span class="pre">eval(1)</span></code> then uses weights version 1.
Weights index 0 represents the randomly initialized weights of our neural network(s).</p>
<p>Another example: For <code class="docutils literal notranslate"><span class="pre">evaluation_interval=2</span></code>, the sequence is:
<code class="docutils literal notranslate"><span class="pre">train(0-&gt;1),</span> <span class="pre">train(1-&gt;2),</span> <span class="pre">eval(2),</span> <span class="pre">train(2-&gt;3),</span> <span class="pre">train(3-&gt;4),</span> <span class="pre">eval(4),</span> <span class="pre">...</span></code>.</p>
<p>Instead of running <code class="docutils literal notranslate"><span class="pre">train</span></code>- and <code class="docutils literal notranslate"><span class="pre">eval</span></code>-steps in sequence, it is also possible to
run them in parallel via the <code class="docutils literal notranslate"><span class="pre">evaluation_parallel_to_training=True</span></code> config setting.
In this case, both training- and evaluation steps are run at the same time via
multi-threading.
This can speed up the evaluation process significantly, but leads to a 1-iteration
delay between reported training- and evaluation results.
The evaluation results are behind in this case b/c they use slightly outdated
model weights (synchronized after the previous training step).</p>
<p>For example, for <code class="docutils literal notranslate"><span class="pre">evaluation_parallel_to_training=True</span></code> and <code class="docutils literal notranslate"><span class="pre">evaluation_interval=1</span></code>,
the sequence is now:
<code class="docutils literal notranslate"><span class="pre">train(0-&gt;1)</span> <span class="pre">+</span> <span class="pre">eval(0),</span> <span class="pre">train(1-&gt;2)</span> <span class="pre">+</span> <span class="pre">eval(1),</span> <span class="pre">train(2-&gt;3)</span> <span class="pre">+</span> <span class="pre">eval(2)</span></code>,
where <code class="docutils literal notranslate"><span class="pre">+</span></code> means: “at the same time”.
Note that the change in the weights indices with respect to the non-parallel examples above.
The evaluation weights indices are now “one behind”
the resulting train weights indices (<code class="docutils literal notranslate"><span class="pre">train(1-&gt;**2**)</span> <span class="pre">+</span> <span class="pre">eval(**1**)</span></code>).</p>
<p>When running with the <code class="docutils literal notranslate"><span class="pre">evaluation_parallel_to_training=True</span></code> setting, a special “auto” value
is supported for <code class="docutils literal notranslate"><span class="pre">evaluation_duration</span></code>. This can be used to make the evaluation step take
roughly as long as the concurrently ongoing training step:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run evaluation and training at the same time via threading and make sure they roughly</span>
<span class="c1"># take the same time, such that the next `Algorithm.train()` call can execute</span>
<span class="c1"># immediately and not have to wait for a still ongoing (e.g. b/c of very long episodes)</span>
<span class="c1"># evaluation step:</span>
<span class="p">{</span>
    <span class="s2">&quot;evaluation_interval&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;evaluation_parallel_to_training&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;evaluation_duration&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>  <span class="c1"># automatically end evaluation when train step has finished</span>
    <span class="s2">&quot;evaluation_duration_unit&quot;</span><span class="p">:</span> <span class="s2">&quot;timesteps&quot;</span><span class="p">,</span>  <span class="c1"># &lt;- more fine grained than &quot;episodes&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">evaluation_config</span></code> key allows you to override any config settings for
the evaluation workers. For example, to switch off exploration in the evaluation steps,
do:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Switching off exploration behavior for evaluation workers</span>
<span class="c1"># (see rllib/algorithms/algorithm.py). Use any keys in this sub-dict that are</span>
<span class="c1"># also supported in the main Algorithm config.</span>
<span class="s2">&quot;evaluation_config&quot;</span><span class="p">:</span> <span class="p">{</span>
   <span class="s2">&quot;explore&quot;</span><span class="p">:</span> <span class="kc">False</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Policy gradient algorithms are able to find the optimal
policy, even if this is a stochastic one. Setting “explore=False” above
will result in the evaluation workers not using this stochastic policy.</p>
</div>
<p>The level of parallelism within the evaluation step is determined via the
<code class="docutils literal notranslate"><span class="pre">evaluation_num_workers</span></code> setting. Set this to larger values if you want the desired
evaluation episodes or time-steps to run as much in parallel as possible.
For example, if your <code class="docutils literal notranslate"><span class="pre">evaluation_duration=10</span></code>, <code class="docutils literal notranslate"><span class="pre">evaluation_duration_unit=episodes</span></code>,
and <code class="docutils literal notranslate"><span class="pre">evaluation_num_workers=10</span></code>, each evaluation <code class="docutils literal notranslate"><span class="pre">RolloutWorker</span></code>
only has to run one episode in each evaluation step.</p>
<p>In case you observe occasional failures in your (evaluation) RolloutWorkers during
evaluation (e.g. you have an environment that sometimes crashes),
you can use an (experimental) new setting: <code class="docutils literal notranslate"><span class="pre">enable_async_evaluation=True</span></code>.
This will run the parallel sampling of all evaluation RolloutWorkers via a fault
tolerant, asynchronous manager, such that if one of the workers takes too long to run
through an episode and return data or fails entirely, the other evaluation
RolloutWorkers will pick up its task and complete the job.</p>
<p>Note that with or without async evaluation, all
<a class="reference internal" href="rllib-training.html#rllib-scaling-guide"><span class="std std-ref">fault tolerance settings</span></a>, such as
<code class="docutils literal notranslate"><span class="pre">ignore_worker_failures</span></code> or <code class="docutils literal notranslate"><span class="pre">recreate_failed_workers</span></code> will be respected and applied
to the failed evaluation workers.</p>
<p>Here’s an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Having an environment that occasionally blocks completely for e.g. 10min would</span>
<span class="c1"># also affect (and block) training:</span>
<span class="p">{</span>
    <span class="s2">&quot;evaluation_interval&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;evaluation_parallel_to_training&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;evaluation_num_workers&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>  <span class="c1"># each worker runs two episodes</span>
    <span class="s2">&quot;evaluation_duration&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s2">&quot;evaluation_duration_unit&quot;</span><span class="p">:</span> <span class="s2">&quot;episodes&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Problem with the above example:</strong></p>
<p>In case the environment used by worker 3 blocks for 10min, the entire training
and evaluation pipeline will come to a (10min) halt b/c of this.
The next <code class="docutils literal notranslate"><span class="pre">train</span></code> step cannot start before all evaluation has been finished.</p>
<p><strong>Solution:</strong></p>
<p>Switch on asynchronous evaluation, meaning, we don’t wait for individual
evaluation RolloutWorkers to complete their n episode(s) (or <code class="docutils literal notranslate"><span class="pre">n</span></code> time-steps).
Instead, any evaluation RolloutWorker can cover the load of another one that failed
or is stuck in a very long lasting environment step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="c1"># ...</span>
    <span class="c1"># same settings as above, plus:</span>
    <span class="s2">&quot;enable_async_evaluation&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># evaluate asynchronously</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In case you would like to entirely customize the evaluation step,
set <code class="docutils literal notranslate"><span class="pre">custom_eval_function</span></code> in your config to a callable, which takes the Algorithm
object and a WorkerSet object (the Algorithm’s <code class="docutils literal notranslate"><span class="pre">self.evaluation_workers</span></code>
WorkerSet instance) and returns a metrics dictionary.
See <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/algorithm.py">algorithm.py</a>
for further documentation.</p>
<p>There is also an end-to-end example of how to set up a custom online evaluation in
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/custom_eval.py">custom_eval.py</a>.
Note that if you only want to evaluate your policy at the end of training,
you can set <code class="docutils literal notranslate"><span class="pre">evaluation_interval:</span> <span class="pre">[int]</span></code>, where <code class="docutils literal notranslate"><span class="pre">[int]</span></code> should be the number
of training iterations before stopping.</p>
<p>Below are some examples of how the custom evaluation metrics are reported nested under
the <code class="docutils literal notranslate"><span class="pre">evaluation</span></code> key of normal training results:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>------------------------------------------------------------------------
Sample output <span class="k">for</span> <span class="sb">`</span>python custom_eval.py<span class="sb">`</span>
------------------------------------------------------------------------

INFO algorithm.py:623 -- Evaluating current policy <span class="k">for</span> <span class="m">10</span> episodes.
INFO algorithm.py:650 -- Running round <span class="m">0</span> of parallel evaluation <span class="o">(</span><span class="m">2</span>/10 episodes<span class="o">)</span>
INFO algorithm.py:650 -- Running round <span class="m">1</span> of parallel evaluation <span class="o">(</span><span class="m">4</span>/10 episodes<span class="o">)</span>
INFO algorithm.py:650 -- Running round <span class="m">2</span> of parallel evaluation <span class="o">(</span><span class="m">6</span>/10 episodes<span class="o">)</span>
INFO algorithm.py:650 -- Running round <span class="m">3</span> of parallel evaluation <span class="o">(</span><span class="m">8</span>/10 episodes<span class="o">)</span>
INFO algorithm.py:650 -- Running round <span class="m">4</span> of parallel evaluation <span class="o">(</span><span class="m">10</span>/10 episodes<span class="o">)</span>

Result <span class="k">for</span> PG_SimpleCorridor_2c6b27dc:
  ...
  evaluation:
    custom_metrics: <span class="o">{}</span>
    episode_len_mean: <span class="m">15</span>.864661654135338
    episode_reward_max: <span class="m">1</span>.0
    episode_reward_mean: <span class="m">0</span>.49624060150375937
    episode_reward_min: <span class="m">0</span>.0
    episodes_this_iter: <span class="m">133</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>------------------------------------------------------------------------
Sample output <span class="k">for</span> <span class="sb">`</span>python custom_eval.py --custom-eval<span class="sb">`</span>
------------------------------------------------------------------------

INFO algorithm.py:631 -- Running custom <span class="nb">eval</span> <span class="k">function</span> &lt;<span class="k">function</span> ...&gt;
Update corridor length to <span class="m">4</span>
Update corridor length to <span class="m">7</span>
Custom evaluation round <span class="m">1</span>
Custom evaluation round <span class="m">2</span>
Custom evaluation round <span class="m">3</span>
Custom evaluation round <span class="m">4</span>

Result <span class="k">for</span> PG_SimpleCorridor_0de4e686:
  ...
  evaluation:
    custom_metrics: <span class="o">{}</span>
    episode_len_mean: <span class="m">9</span>.15695067264574
    episode_reward_max: <span class="m">1</span>.0
    episode_reward_mean: <span class="m">0</span>.9596412556053812
    episode_reward_min: <span class="m">0</span>.0
    episodes_this_iter: <span class="m">223</span>
    foo: <span class="m">1</span>
</pre></div>
</div>
</section>
<section id="rewriting-trajectories">
<h2>Rewriting Trajectories<a class="headerlink" href="rllib-advanced-api.html#rewriting-trajectories" title="Permalink to this headline">#</a></h2>
<p>Note that in the <code class="docutils literal notranslate"><span class="pre">on_postprocess_traj</span></code> callback you have full access to the
trajectory batch (<code class="docutils literal notranslate"><span class="pre">post_batch</span></code>) and other training state. This can be used to
rewrite the trajectory, which has a number of uses including:</p>
<blockquote>
<div><ul class="simple">
<li><p>Backdating rewards to previous time steps (e.g., based on values in <code class="docutils literal notranslate"><span class="pre">info</span></code>).</p></li>
<li><p>Adding model-based curiosity bonuses to rewards (you can train the model with a
<a class="reference external" href="rllib-models.html#supervised-model-losses">custom model supervised loss</a>).</p></li>
</ul>
</div></blockquote>
<p>To access the policy / model (<code class="docutils literal notranslate"><span class="pre">policy.model</span></code>) in the callbacks, note that
<code class="docutils literal notranslate"><span class="pre">info['pre_batch']</span></code> returns a tuple where the first element is a policy and the
second one is the batch itself. You can also access all the rollout worker state
using the following call:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.evaluation.rollout_worker</span> <span class="kn">import</span> <span class="n">get_global_worker</span>

<span class="c1"># You can use this from any callback to get a reference to the</span>
<span class="c1"># RolloutWorker running in the process, which in turn has references to</span>
<span class="c1"># all the policies, etc: see rollout_worker.py for more info.</span>
<span class="n">rollout_worker</span> <span class="o">=</span> <span class="n">get_global_worker</span><span class="p">()</span>
</pre></div>
</div>
<p>Policy losses are defined over the <code class="docutils literal notranslate"><span class="pre">post_batch</span></code> data, so you can mutate that in
the callbacks to change what data the policy loss function sees.</p>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="user-guides.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">User Guides</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="rllib-models.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Models, Preprocessors, and Action Distributions</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>