
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Algorithms &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/versionwarning.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../_static/js/docsearch.js"></script>
    <script src="../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../_static/js/termynal.js"></script>
    <script defer="defer" src="../_static/js/custom.js"></script>
    <script defer="defer" src="../_static/js/top-navigation.js"></script>
    <script src="../_static/js/tags.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/rllib/rllib-algorithms.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="User Guides" href="user-guides.html" />
    <link rel="prev" title="Environments" href="../rllib-env.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "rllib/rllib-algorithms", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Ray RLlib
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-training.html">
     Getting Started with RLlib
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="key-concepts.html">
     Key Concepts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../rllib-env.html">
     Environments
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="rllib-algorithms.html#">
     Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="user-guides.html">
     User Guides
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-examples.html">
     Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="package_ref/index.html">
     Ray RLlib API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Frllib/rllib-algorithms.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/rllib/rllib-algorithms.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/rllib/rllib-algorithms.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#available-algorithms-overview">
   Available Algorithms - Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#offline">
   Offline
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#behavior-cloning-bc-derived-from-marwil-implementation">
     Behavior Cloning (BC; derived from MARWIL implementation)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#critic-regularized-regression-crr">
     Critic Regularized Regression (CRR)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#conservative-q-learning-cql">
     Conservative Q-Learning (CQL)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#monotonic-advantage-re-weighted-imitation-learning-marwil">
     Monotonic Advantage Re-Weighted Imitation Learning (MARWIL)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#model-free-on-policy-rl">
   Model-free On-policy RL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#asynchronous-proximal-policy-optimization-appo">
     Asynchronous Proximal Policy Optimization (APPO)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#decentralized-distributed-proximal-policy-optimization-dd-ppo">
     Decentralized Distributed Proximal Policy Optimization (DD-PPO)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#proximal-policy-optimization-ppo">
     Proximal Policy Optimization (PPO)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#importance-weighted-actor-learner-architecture-impala">
     Importance Weighted Actor-Learner Architecture (IMPALA)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#advantage-actor-critic-a2c">
     Advantage Actor-Critic (A2C)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#asynchronous-advantage-actor-critic-a3c">
     Asynchronous Advantage Actor-Critic (A3C)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#policy-gradients-pg">
     Policy Gradients (PG)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#model-agnostic-meta-learning-maml">
     Model-Agnostic Meta-Learning (MAML)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#model-free-off-policy-rl">
   Model-free Off-policy RL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#distributed-prioritized-experience-replay-ape-x">
     Distributed Prioritized Experience Replay (Ape-X)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#recurrent-replay-distributed-dqn-r2d2">
     Recurrent Replay Distributed DQN (R2D2)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn">
     Deep Q Networks (DQN, Rainbow, Parametric DQN)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#deep-deterministic-policy-gradients-ddpg">
     Deep Deterministic Policy Gradients (DDPG)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#twin-delayed-ddpg-td3">
     Twin Delayed DDPG (TD3)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#soft-actor-critic-sac">
     Soft Actor Critic (SAC)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#model-based-rl">
   Model-based RL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#dreamer">
     Dreamer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#model-based-meta-policy-optimization-mb-mpo">
     Model-Based Meta-Policy-Optimization (MB-MPO)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#derivative-free">
   Derivative-free
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#augmented-random-search-ars">
     Augmented Random Search (ARS)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#evolution-strategies-es">
     Evolution Strategies (ES)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#rl-for-recommender-systems">
   RL for recommender systems
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#slateq">
     SlateQ
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#contextual-bandits">
   Contextual Bandits
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#linear-upper-confidence-bound-banditlinucb">
     Linear Upper Confidence Bound (BanditLinUCB)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#linear-thompson-sampling-banditlints">
     Linear Thompson Sampling (BanditLinTS)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#multi-agent">
   Multi-agent
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#parameter-sharing">
     Parameter Sharing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#qmix-monotonic-value-factorisation-qmix-vdn-iqn">
     QMIX Monotonic Value Factorisation (QMIX, VDN, IQN)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#multi-agent-deep-deterministic-policy-gradient-maddpg">
     Multi-Agent Deep Deterministic Policy Gradient (MADDPG)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#shared-critic-methods">
     Shared Critic Methods
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#others">
   Others
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#single-player-alpha-zero-alphazero">
     Single-Player Alpha Zero (AlphaZero)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#multiagent-leelachesszero-leelachesszero">
     MultiAgent LeelaChessZero (LeelaChessZero)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#curiosity-icm-intrinsic-curiosity-module">
     Curiosity (ICM: Intrinsic Curiosity Module)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#re3-random-encoders-for-efficient-exploration">
     RE3 (Random Encoders for Efficient Exploration)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#fully-independent-learning">
     Fully Independent Learning
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Algorithms</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#available-algorithms-overview">
   Available Algorithms - Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#offline">
   Offline
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#behavior-cloning-bc-derived-from-marwil-implementation">
     Behavior Cloning (BC; derived from MARWIL implementation)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#critic-regularized-regression-crr">
     Critic Regularized Regression (CRR)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#conservative-q-learning-cql">
     Conservative Q-Learning (CQL)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#monotonic-advantage-re-weighted-imitation-learning-marwil">
     Monotonic Advantage Re-Weighted Imitation Learning (MARWIL)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#model-free-on-policy-rl">
   Model-free On-policy RL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#asynchronous-proximal-policy-optimization-appo">
     Asynchronous Proximal Policy Optimization (APPO)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#decentralized-distributed-proximal-policy-optimization-dd-ppo">
     Decentralized Distributed Proximal Policy Optimization (DD-PPO)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#proximal-policy-optimization-ppo">
     Proximal Policy Optimization (PPO)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#importance-weighted-actor-learner-architecture-impala">
     Importance Weighted Actor-Learner Architecture (IMPALA)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#advantage-actor-critic-a2c">
     Advantage Actor-Critic (A2C)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#asynchronous-advantage-actor-critic-a3c">
     Asynchronous Advantage Actor-Critic (A3C)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#policy-gradients-pg">
     Policy Gradients (PG)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#model-agnostic-meta-learning-maml">
     Model-Agnostic Meta-Learning (MAML)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#model-free-off-policy-rl">
   Model-free Off-policy RL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#distributed-prioritized-experience-replay-ape-x">
     Distributed Prioritized Experience Replay (Ape-X)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#recurrent-replay-distributed-dqn-r2d2">
     Recurrent Replay Distributed DQN (R2D2)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn">
     Deep Q Networks (DQN, Rainbow, Parametric DQN)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#deep-deterministic-policy-gradients-ddpg">
     Deep Deterministic Policy Gradients (DDPG)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#twin-delayed-ddpg-td3">
     Twin Delayed DDPG (TD3)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#soft-actor-critic-sac">
     Soft Actor Critic (SAC)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#model-based-rl">
   Model-based RL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#dreamer">
     Dreamer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#model-based-meta-policy-optimization-mb-mpo">
     Model-Based Meta-Policy-Optimization (MB-MPO)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#derivative-free">
   Derivative-free
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#augmented-random-search-ars">
     Augmented Random Search (ARS)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#evolution-strategies-es">
     Evolution Strategies (ES)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#rl-for-recommender-systems">
   RL for recommender systems
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#slateq">
     SlateQ
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#contextual-bandits">
   Contextual Bandits
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#linear-upper-confidence-bound-banditlinucb">
     Linear Upper Confidence Bound (BanditLinUCB)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#linear-thompson-sampling-banditlints">
     Linear Thompson Sampling (BanditLinTS)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#multi-agent">
   Multi-agent
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#parameter-sharing">
     Parameter Sharing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#qmix-monotonic-value-factorisation-qmix-vdn-iqn">
     QMIX Monotonic Value Factorisation (QMIX, VDN, IQN)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#multi-agent-deep-deterministic-policy-gradient-maddpg">
     Multi-Agent Deep Deterministic Policy Gradient (MADDPG)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#shared-critic-methods">
     Shared Critic Methods
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="rllib-algorithms.html#others">
   Others
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#single-player-alpha-zero-alphazero">
     Single-Player Alpha Zero (AlphaZero)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#multiagent-leelachesszero-leelachesszero">
     MultiAgent LeelaChessZero (LeelaChessZero)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#curiosity-icm-intrinsic-curiosity-module">
     Curiosity (ICM: Intrinsic Curiosity Module)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#re3-random-encoders-for-efficient-exploration">
     RE3 (Random Encoders for Efficient Exploration)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="rllib-algorithms.html#fully-independent-learning">
     Fully Independent Learning
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <a class="reference external image-reference" href="https://ray-docs-promo.netlify.app/rllib"><img alt="" src="https://ray-docs-promo.netlify.app/assets/img/rllib/top.png" /></a>
<div class="bottom-right-promo-banner docutils">
<a class="reference external image-reference" href="https://ray-docs-promo.netlify.app/rllib"><img alt="" src="https://ray-docs-promo.netlify.app/assets/img/rllib/square.png" /></a>
</div>
<section id="algorithms">
<span id="rllib-algorithms-doc"></span><h1>Algorithms<a class="headerlink" href="rllib-algorithms.html#algorithms" title="Permalink to this headline">#</a></h1>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Check out the <a class="reference external" href="../rllib-env.html">environments</a> page to learn more about different environment types.</p>
</div>
<section id="available-algorithms-overview">
<h2>Available Algorithms - Overview<a class="headerlink" href="rllib-algorithms.html#available-algorithms-overview" title="Permalink to this headline">#</a></h2>
<table class="table">
<colgroup>
<col style="width: 17%" />
<col style="width: 6%" />
<col style="width: 17%" />
<col style="width: 10%" />
<col style="width: 6%" />
<col style="width: 35%" />
<col style="width: 9%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Algorithm</p></th>
<th class="head"><p>Frameworks</p></th>
<th class="head"><p>Discrete Actions</p></th>
<th class="head"><p>Continuous Actions</p></th>
<th class="head"><p>Multi-Agent</p></th>
<th class="head"><p>Model Support</p></th>
<th class="head"><p>Multi-GPU</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="rllib-algorithms.html#a2c">A2C</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a>, <a class="reference external" href="rllib-models.html#built-in-models">+LSTM auto-wrapping</a>, <a class="reference external" href="rllib-models.html#attention">+Attention</a>, <a class="reference external" href="rllib-models.html#autoregressive-action-distributions">+autoreg</a></p></td>
<td><p>A2C: tf + torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="rllib-algorithms.html#a3c">A3C</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a>, <a class="reference external" href="rllib-models.html#built-in-models">+LSTM auto-wrapping</a>, <a class="reference external" href="rllib-models.html#attention">+Attention</a>, <a class="reference external" href="rllib-models.html#autoregressive-action-distributions">+autoreg</a></p></td>
<td><p>No</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="rllib-algorithms.html#alphazero">AlphaZero</a></p></td>
<td><p>torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td></td>
<td><p>No</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="rllib-algorithms.html#appo">APPO</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a>, <a class="reference external" href="rllib-models.html#built-in-models">+LSTM auto-wrapping</a>, <a class="reference external" href="rllib-models.html#attention">+Attention</a>, <a class="reference external" href="rllib-models.html#autoregressive-action-distributions">+autoreg</a></p></td>
<td><p>tf + torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="rllib-algorithms.html#ars">ARS</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p>No</p></td>
<td></td>
<td><p>No</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="rllib-algorithms.html#bandits">Bandits</a> (<a class="reference external" href="rllib-algorithms.html#lints">TS</a> &amp; <a class="reference external" href="rllib-algorithms.html#lin-ucb">LinUCB</a>)</p></td>
<td><p>torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>No</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="rllib-algorithms.html#bc">BC</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a></p></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="rllib-algorithms.html#cql">CQL</a></p></td>
<td><p>tf + torch</p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p>No</p></td>
<td></td>
<td><p>tf + torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="rllib-algorithms.html#crr">CRR</a></p></td>
<td><p>torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="rllib-algorithms.html#ddpg">DDPG</a></p></td>
<td><p>tf + torch</p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="rllib-algorithms.html#apex">APEX-DDPG</a></p></td>
<td><p>tf + torch</p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="rllib-algorithms.html#es">ES</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p>No</p></td>
<td></td>
<td><p>No</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="rllib-algorithms.html#dreamer">Dreamer</a></p></td>
<td><p>torch</p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p>No</p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a></p></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="rllib-algorithms.html#dqn">DQN</a>, <a class="reference external" href="rllib-algorithms.html#dqn">Rainbow</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>tf + torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="rllib-algorithms.html#apex">APEX-DQN</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="rllib-algorithms.html#impala">IMPALA</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a>, <a class="reference external" href="rllib-models.html#built-in-models">+LSTM auto-wrapping</a>, <a class="reference external" href="rllib-models.html#attention">+Attention</a>, <a class="reference external" href="rllib-models.html#autoregressive-action-distributions">+autoreg</a></p></td>
<td><p>tf + torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="rllib-algorithms.html#leelachesszero">LeelaChessZero</a></p></td>
<td><p>torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="rllib-algorithms.html#maml">MAML</a></p></td>
<td><p>tf + torch</p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p>No</p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="rllib-algorithms.html#marwil">MARWIL</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a></p></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="rllib-algorithms.html#mbmpo">MBMPO</a></p></td>
<td><p>torch</p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p>No</p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="rllib-algorithms.html#pg">PG</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a>, <a class="reference external" href="rllib-models.html#built-in-models">+LSTM auto-wrapping</a>, <a class="reference external" href="rllib-models.html#attention">+Attention</a>, <a class="reference external" href="rllib-models.html#autoregressive-action-distributions">+autoreg</a></p></td>
<td><p>tf + torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="rllib-algorithms.html#ppo">PPO</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a>, <a class="reference external" href="rllib-models.html#built-in-models">+LSTM auto-wrapping</a>, <a class="reference external" href="rllib-models.html#attention">+Attention</a>, <a class="reference external" href="rllib-models.html#autoregressive-action-distributions">+autoreg</a></p></td>
<td><p>tf + torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="rllib-algorithms.html#r2d2">R2D2</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a>, <a class="reference external" href="rllib-models.html#built-in-models">+LSTM auto-wrapping</a>, <a class="reference external" href="rllib-models.html#autoregressive-action-distributions">+autoreg</a></p></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="rllib-algorithms.html#sac">SAC</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="rllib-algorithms.html#slateq">SlateQ</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> (multi-discr. slates)</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="rllib-algorithms.html#td3">TD3</a></p></td>
<td><p>tf + torch</p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>torch</p></td>
</tr>
</tbody>
</table>
<p>Multi-Agent only Methods</p>
<table class="table">
<colgroup>
<col style="width: 28%" />
<col style="width: 9%" />
<col style="width: 20%" />
<col style="width: 16%" />
<col style="width: 10%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Algorithm</p></th>
<th class="head"><p>Frameworks</p></th>
<th class="head"><p>Discrete Actions</p></th>
<th class="head"><p>Continuous Actions</p></th>
<th class="head"><p>Multi-Agent</p></th>
<th class="head"><p>Model Support</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="rllib-algorithms.html#qmix">QMIX</a></p></td>
<td><p>torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="rllib-algorithms.html#maddpg">MADDPG</a></p></td>
<td><p>tf</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p>Partial</p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="rllib-algorithms.html#parameter-sharing">Parameter Sharing</a></p></td>
<td colspan="5"><p>Depends on bootstrapped algorithm</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="rllib-algorithms.html#fully-independent-learning">Fully Independent Learning</a></p></td>
<td colspan="5"><p>Depends on bootstrapped algorithm</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="rllib-algorithms.html#shared-critic-methods">Shared Critic Methods</a></p></td>
<td colspan="5"><p>Depends on bootstrapped algorithm</p></td>
</tr>
</tbody>
</table>
<p>Exploration-based plug-ins (can be combined with any algo)</p>
<table class="table">
<colgroup>
<col style="width: 28%" />
<col style="width: 9%" />
<col style="width: 20%" />
<col style="width: 16%" />
<col style="width: 10%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Algorithm</p></th>
<th class="head"><p>Frameworks</p></th>
<th class="head"><p>Discrete Actions</p></th>
<th class="head"><p>Continuous Actions</p></th>
<th class="head"><p>Multi-Agent</p></th>
<th class="head"><p>Model Support</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="rllib-algorithms.html#curiosity">Curiosity</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="offline">
<h2>Offline<a class="headerlink" href="rllib-algorithms.html#offline" title="Permalink to this headline">#</a></h2>
<section id="behavior-cloning-bc-derived-from-marwil-implementation">
<span id="bc"></span><h3>Behavior Cloning (BC; derived from MARWIL implementation)<a class="headerlink" href="rllib-algorithms.html#behavior-cloning-bc-derived-from-marwil-implementation" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="http://papers.nips.cc/paper/7866-exponentially-weighted-imitation-learning-for-batched-historical-data">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/bc/bc.py">[implementation]</a></p>
<p>Our behavioral cloning implementation is directly derived from our <a class="reference internal" href="rllib-algorithms.html#marwil">MARWIL</a> implementation,
with the only difference being the <code class="docutils literal notranslate"><span class="pre">beta</span></code> parameter force-set to 0.0. This makes
BC try to match the behavior policy, which generated the offline data, disregarding any resulting rewards.
BC requires the <a class="reference external" href="rllib-offline.html">offline datasets API</a> to be used.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/bc/cartpole-bc.yaml">CartPole-v1</a></p>
<p><strong>BC-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.bc.bc.BCConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.bc.bc.</span></span><span class="sig-name descname"><span class="pre">BCConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/bc/bc.html#BCConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.bc.bc.BCConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a new BC Trainer can be built</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.bc</span> <span class="kn">import</span> <span class="n">BCConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Run this from the ray directory root.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">BCConfig</span><span class="p">()</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="n">input_</span><span class="o">=</span><span class="s2">&quot;./rllib/tests/data/cartpole/large.json&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Trainer object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.bc</span> <span class="kn">import</span> <span class="n">BCConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">BCConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]),</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.75</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s data path.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Run this from the ray directory root.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="n">input_</span><span class="o">=</span><span class="s2">&quot;./rllib/tests/data/cartpole/large.json&quot;</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env, used for evaluation.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>   
<span class="gp">... </span>    <span class="s2">&quot;BC&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.bc.bc.BCConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bc_logstd_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moving_average_sqd_adv_norm_update_rate:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moving_average_sqd_adv_norm_start:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_gae:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.marwil.marwil.MARWILConfig" title="ray.rllib.algorithms.marwil.marwil.MARWILConfig"><span class="pre">ray.rllib.algorithms.marwil.marwil.MARWILConfig</span></a></span></span><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.bc.bc.BCConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>beta</strong>  Scaling  of advantages in exponential terms. When beta is 0.0,
MARWIL is reduced to behavior cloning (imitation learning);
see bc.py algorithm in this same directory.</p></li>
<li><p><strong>bc_logstd_coeff</strong>  A coefficient to encourage higher action distribution
entropy for exploration.</p></li>
<li><p><strong>moving_average_sqd_adv_norm_start</strong>  Starting value for the
squared moving average advantage norm (c^2).</p></li>
<li><p><strong>use_gae</strong>  If True, use the Generalized Advantage Estimator (GAE)
with a value function, see <a class="reference external" href="https://arxiv.org/pdf/1506.02438.pdf">https://arxiv.org/pdf/1506.02438.pdf</a> in
case an input line ends with a non-terminal timestep.</p></li>
<li><p><strong>vf_coeff</strong>  Balancing value estimation loss and policy optimization loss.
moving_average_sqd_adv_norm_update_rate: Update rate for the
squared moving average advantage norm (c^2).</p></li>
<li><p><strong>grad_clip</strong>  If specified, clip the global norm of gradients by this amount.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="critic-regularized-regression-crr">
<span id="crr"></span><h3>Critic Regularized Regression (CRR)<a class="headerlink" href="rllib-algorithms.html#critic-regularized-regression-crr" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/2006.15134">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/crr/crr.py">[implementation]</a></p>
<p>CRR is another offline RL algorithm based on Q-learning that can learn from an offline experience replay.
The challenge in applying existing Q-learning algorithms to offline RL lies in the overestimation of the Q-function, as well as, the lack of exploration beyond the observed data.
The latter becomes increasingly important during bootstrapping in the bellman equation, where the Q-function queried for the next states Q-value(s) does not have support in the observed data.
To mitigate these issues, CRR implements a simple and yet powerful idea of value-filtered regression.
The key idea is to use a learned critic to filter-out the non-promising transitions from the replay dataset. For more details, please refer to the paper (see link above).</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/crr/cartpole-v1-crr.yaml">CartPole-v1</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/crr/pendulum-v1-crr.yaml">Pendulum-v1</a></p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.crr.crr.CRRConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.crr.crr.</span></span><span class="sig-name descname"><span class="pre">CRRConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/crr/crr.html#CRRConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.crr.crr.CRRConfig" title="Permalink to this definition">#</a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.crr.crr.CRRConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_type:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_weight:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">advantage_type:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_action_sample:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">twin_q:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_network_update_freq:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_hiddens:</span> <span class="pre">Optional[List[int]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_hidden_activation:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic_hiddens:</span> <span class="pre">Optional[List[int]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic_hidden_activation:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">td_error_loss_fn:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">categorical_distribution_temperature:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.crr.crr.CRRConfig" title="ray.rllib.algorithms.crr.crr.CRRConfig"><span class="pre">ray.rllib.algorithms.crr.crr.CRRConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/crr/crr.html#CRRConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.crr.crr.CRRConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>CRR training configuration</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight_type</strong>  weight type to use <code class="xref py py-obj docutils literal notranslate"><span class="pre">bin</span></code> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">exp</span></code>.</p></li>
<li><p><strong>temperature</strong>  the exponent temperature used in exp weight type.</p></li>
<li><p><strong>max_weight</strong>  the max weight limit for exp weight type.</p></li>
<li><p><strong>advantage_type</strong>  <p>The way we reduce q values to v_t values
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max</span></code> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">mean</span></code> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">expectation</span></code>. <code class="xref py py-obj docutils literal notranslate"><span class="pre">max</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">mean</span></code> work for both
discrete and continuous action spaces while <code class="xref py py-obj docutils literal notranslate"><span class="pre">expectation</span></code> only
works for discrete action spaces.
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max</span></code>: Uses max over sampled actions to estimate the value.</p>
<div class="math notranslate nohighlight">
\[A(s_t, a_t) = Q(s_t, a_t) - \max_{a^j} Q(s_t, a^j)\]</div>
<p>where <span class="math notranslate nohighlight">\(a^j\)</span> is <code class="xref py py-obj docutils literal notranslate"><span class="pre">n_action_sample</span></code> times sampled from the
policy <span class="math notranslate nohighlight">\(\pi(a | s_t)\)</span>
<code class="xref py py-obj docutils literal notranslate"><span class="pre">mean</span></code>: Uses mean over sampled actions to estimate the value.</p>
<div class="math notranslate nohighlight">
\[A(s_t, a_t) = Q(s_t, a_t) - \frac{1}{m}\sum_{j=1}^{m}
[Q(s_t, a^j)]\]</div>
<p>where <span class="math notranslate nohighlight">\(a^j\)</span> is <code class="xref py py-obj docutils literal notranslate"><span class="pre">n_action_sample</span></code> times sampled from the
policy <span class="math notranslate nohighlight">\(\pi(a | s_t)\)</span>
<code class="xref py py-obj docutils literal notranslate"><span class="pre">expectation</span></code>: This uses categorical distribution to evaluate
the expectation of the q values directly to estimate the value.</p>
<div class="math notranslate nohighlight">
\[A(s_t, a_t) = Q(s_t, a_t) - E_{a^j\sim \pi(a|s_t)}[Q(s_t,a^j)]\]</div>
</p></li>
<li><p><strong>n_action_sample</strong>  the number of actions to sample for v_t estimation.</p></li>
<li><p><strong>twin_q</strong>  if True, uses pessimistic q estimation.</p></li>
<li><p><strong>target_network_update_freq</strong>  The frequency at which we update the
target copy of the model in terms of the number of gradient updates
applied to the main model.</p></li>
<li><p><strong>actor_hiddens</strong>  The number of hidden units in the actors fc network.</p></li>
<li><p><strong>actor_hidden_activation</strong>  The activation used in the actors fc network.</p></li>
<li><p><strong>critic_hiddens</strong>  The number of hidden units in the critics fc network.</p></li>
<li><p><strong>critic_hidden_activation</strong>  The activation used in the critics fc network.</p></li>
<li><p><strong>tau</strong>  Polyak averaging coefficient
(making it 1 is reduces it to a hard update).</p></li>
<li><p><strong>td_error_loss_fn</strong>  huber or mse.
Loss function for calculating critic error.</p></li>
<li><p><strong>categorical_distribution_temperature</strong>  Set the temperature parameter used
by Categorical action distribution. A valid temperature is in the range
of [0, 1]. Note that this mostly affects evaluation since critic error
uses selected action for return calculation.</p></li>
<li><p><strong>**kwargs</strong>  forward compatibility kwargs</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated CRRConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="conservative-q-learning-cql">
<span id="cql"></span><h3>Conservative Q-Learning (CQL)<a class="headerlink" href="rllib-algorithms.html#conservative-q-learning-cql" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/2006.04779">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/cql/cql.py">[implementation]</a></p>
<p>In offline RL, the algorithm has no access to an environment, but can only sample from a fixed dataset of pre-collected state-action-reward tuples.
In particular, CQL (Conservative Q-Learning) is an offline RL algorithm that mitigates the overestimation of Q-values outside the dataset distribution via
conservative critic estimates. It does so by adding a simple Q regularizer loss to the standard Bellman update loss.
This ensures that the critic does not output overly-optimistic Q-values. This conservative
correction term can be added on top of any off-policy Q-learning algorithm (here, we provide this for SAC).</p>
<p>RLlibs CQL is evaluated against the Behavior Cloning (BC) benchmark at 500K gradient steps over the dataset. The only difference between the BC- and CQL configs is the <code class="docutils literal notranslate"><span class="pre">bc_iters</span></code> parameter in CQL, indicating how many gradient steps we perform over the BC loss. CQL is evaluated on the <a class="reference external" href="https://github.com/rail-berkeley/d4rl">D4RL</a> benchmark, which has pre-collected offline datasets for many types of environments.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/cql/halfcheetah-cql.yaml">HalfCheetah Random</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/cql/hopper-cql.yaml">Hopper Random</a></p>
<p><strong>CQL-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.cql.cql.CQLConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.cql.cql.</span></span><span class="sig-name descname"><span class="pre">CQLConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/cql/cql.html#CQLConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.cql.cql.CQLConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a CQL Trainer can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.cql</span> <span class="kn">import</span> <span class="n">CQLConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">CQLConfig</span><span class="p">()</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Trainer object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.cql.cql.CQLConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bc_iters:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_actions:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lagrangian:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lagrangian_thresh:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_q_weight:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.cql.cql.CQLConfig" title="ray.rllib.algorithms.cql.cql.CQLConfig"><span class="pre">ray.rllib.algorithms.cql.cql.CQLConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/cql/cql.html#CQLConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.cql.cql.CQLConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training-related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>bc_iters</strong>  Number of iterations with Behavior Cloning pretraining.</p></li>
<li><p><strong>temperature</strong>  CQL loss temperature.</p></li>
<li><p><strong>num_actions</strong>  Number of actions to sample for CQL loss</p></li>
<li><p><strong>lagrangian</strong>  Whether to use the Lagrangian for Alpha Prime (in CQL loss).</p></li>
<li><p><strong>lagrangian_thresh</strong>  Lagrangian threshold.</p></li>
<li><p><strong>min_q_weight</strong>  in Q weight multiplier.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="monotonic-advantage-re-weighted-imitation-learning-marwil">
<span id="marwil"></span><h3>Monotonic Advantage Re-Weighted Imitation Learning (MARWIL)<a class="headerlink" href="rllib-algorithms.html#monotonic-advantage-re-weighted-imitation-learning-marwil" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="http://papers.nips.cc/paper/7866-exponentially-weighted-imitation-learning-for-batched-historical-data">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/marwil/marwil.py">[implementation]</a></p>
<p>MARWIL is a hybrid imitation learning and policy gradient algorithm suitable for training on batched historical data.
When the <code class="docutils literal notranslate"><span class="pre">beta</span></code> hyperparameter is set to zero, the MARWIL objective reduces to vanilla imitation learning (see <a class="reference internal" href="rllib-algorithms.html#bc">BC</a>).
MARWIL requires the <a class="reference external" href="rllib-offline.html">offline datasets API</a> to be used.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/marwil/cartpole-marwil.yaml">CartPole-v1</a></p>
<p><strong>MARWIL-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.marwil.marwil.MARWILConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.marwil.marwil.</span></span><span class="sig-name descname"><span class="pre">MARWILConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/marwil/marwil.html#MARWILConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.marwil.marwil.MARWILConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a MARWIL Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.marwil</span> <span class="kn">import</span> <span class="n">MARWILConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Run this from the ray directory root.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">MARWILConfig</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="n">input_</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;./rllib/tests/data/cartpole/large.json&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span> 
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build an Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> 
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.marwil</span> <span class="kn">import</span> <span class="n">MARWILConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">MARWILConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]),</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s data path.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Run this from the ray directory root.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">offline_data</span><span class="p">(</span> 
<span class="gp">... </span>    <span class="n">input_</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;./rllib/tests/data/cartpole/large.json&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env, used for evaluation.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;MARWIL&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.marwil.marwil.MARWILConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bc_logstd_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moving_average_sqd_adv_norm_update_rate:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moving_average_sqd_adv_norm_start:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_gae:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.marwil.marwil.MARWILConfig" title="ray.rllib.algorithms.marwil.marwil.MARWILConfig"><span class="pre">ray.rllib.algorithms.marwil.marwil.MARWILConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/marwil/marwil.html#MARWILConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.marwil.marwil.MARWILConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>beta</strong>  Scaling  of advantages in exponential terms. When beta is 0.0,
MARWIL is reduced to behavior cloning (imitation learning);
see bc.py algorithm in this same directory.</p></li>
<li><p><strong>bc_logstd_coeff</strong>  A coefficient to encourage higher action distribution
entropy for exploration.</p></li>
<li><p><strong>moving_average_sqd_adv_norm_start</strong>  Starting value for the
squared moving average advantage norm (c^2).</p></li>
<li><p><strong>use_gae</strong>  If True, use the Generalized Advantage Estimator (GAE)
with a value function, see <a class="reference external" href="https://arxiv.org/pdf/1506.02438.pdf">https://arxiv.org/pdf/1506.02438.pdf</a> in
case an input line ends with a non-terminal timestep.</p></li>
<li><p><strong>vf_coeff</strong>  Balancing value estimation loss and policy optimization loss.
moving_average_sqd_adv_norm_update_rate: Update rate for the
squared moving average advantage norm (c^2).</p></li>
<li><p><strong>grad_clip</strong>  If specified, clip the global norm of gradients by this amount.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="model-free-on-policy-rl">
<h2>Model-free On-policy RL<a class="headerlink" href="rllib-algorithms.html#model-free-on-policy-rl" title="Permalink to this headline">#</a></h2>
<section id="asynchronous-proximal-policy-optimization-appo">
<span id="appo"></span><h3>Asynchronous Proximal Policy Optimization (APPO)<a class="headerlink" href="rllib-algorithms.html#asynchronous-proximal-policy-optimization-appo" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1707.06347">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/appo/appo.py">[implementation]</a>
We include an asynchronous variant of Proximal Policy Optimization (PPO) based on the IMPALA architecture. This is similar to IMPALA but using a surrogate policy loss with clipping. Compared to synchronous PPO, APPO is more efficient in wall-clock time due to its use of asynchronous sampling. Using a clipped loss also allows for multiple SGD passes, and therefore the potential for better sample efficiency compared to IMPALA. V-trace can also be enabled to correct for off-policy samples.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>APPO is not always more efficient; it is often better to use <a class="reference internal" href="rllib-algorithms.html#ppo"><span class="std std-ref">standard PPO</span></a> or <a class="reference internal" href="rllib-algorithms.html#impala"><span class="std std-ref">IMPALA</span></a>.</p>
</div>
<figure class="align-default" id="id3">
<img alt="../_images/impala-arch.svg" src="../_images/impala-arch.svg" /><figcaption>
<p><span class="caption-text">APPO architecture (same as IMPALA)</span><a class="headerlink" href="rllib-algorithms.html#id3" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/appo/pong-appo.yaml">PongNoFrameskip-v4</a></p>
<p><strong>APPO-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.appo.appo.APPOConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.appo.appo.</span></span><span class="sig-name descname"><span class="pre">APPOConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/appo/appo.html#APPOConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.appo.appo.APPOConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which an APPO Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.appo</span> <span class="kn">import</span> <span class="n">APPOConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">APPOConfig</span><span class="p">()</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">grad_clip</span><span class="o">=</span><span class="mf">30.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build an Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.appo</span> <span class="kn">import</span> <span class="n">APPOConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">APPOConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sample_async</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;APPO&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.appo.appo.APPOConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vtrace:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_critic:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_gae:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_param:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_kl_loss:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kl_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kl_target:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_update_frequency:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.appo.appo.APPOConfig" title="ray.rllib.algorithms.appo.appo.APPOConfig"><span class="pre">ray.rllib.algorithms.appo.appo.APPOConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/appo/appo.html#APPOConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.appo.appo.APPOConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vtrace</strong>  Whether to use V-trace weighted advantages. If false, PPO GAE
advantages will be used instead.</p></li>
<li><p><strong>use_critic</strong>  Should use a critic as a baseline (otherwise dont use value
baseline; required for using GAE). Only applies if vtrace=False.</p></li>
<li><p><strong>use_gae</strong>  If true, use the Generalized Advantage Estimator (GAE)
with a value function, see <a class="reference external" href="https://arxiv.org/pdf/1506.02438.pdf">https://arxiv.org/pdf/1506.02438.pdf</a>.
Only applies if vtrace=False.</p></li>
<li><p><strong>lambda</strong>  GAE (lambda) parameter.</p></li>
<li><p><strong>clip_param</strong>  PPO surrogate slipping parameter.</p></li>
<li><p><strong>use_kl_loss</strong>  Whether to use the KL-term in the loss function.</p></li>
<li><p><strong>kl_coeff</strong>  Coefficient for weighting the KL-loss term.</p></li>
<li><p><strong>kl_target</strong>  Target term for the KL-term to reach (via adjusting the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">kl_coeff</span></code> automatically).</p></li>
<li><p><strong>tau</strong>  The factor by which to update the target policy network towards
the current policy network. Can range between 0 and 1.
e.g. updated_param = tau * current_param + (1 - tau) * target_param</p></li>
<li><p><strong>target_update_frequency</strong>  The frequency to update the target policy and
tune the kl loss coefficients that are used during training. After
setting this parameter, the algorithm waits for at least
<code class="xref py py-obj docutils literal notranslate"><span class="pre">target_update_frequency</span> <span class="pre">*</span> <span class="pre">minibatch_size</span> <span class="pre">*</span> <span class="pre">num_sgd_iter</span></code> number of
samples to be trained on by the learner group before updating the target
networks and tuned the kl loss coefficients that are used during
training.
NOTE: this parameter is only applicable when using the learner api
(_enable_learner_api=True and _enable_rl_module_api=True).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="decentralized-distributed-proximal-policy-optimization-dd-ppo">
<span id="ddppo"></span><h3>Decentralized Distributed Proximal Policy Optimization (DD-PPO)<a class="headerlink" href="rllib-algorithms.html#decentralized-distributed-proximal-policy-optimization-dd-ppo" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1911.00357">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/ddppo/ddppo.py">[implementation]</a>
Unlike APPO or PPO, with DD-PPO policy improvement is no longer done centralized in the algorithm process. Instead, gradients are computed remotely on each rollout worker and all-reduced at each mini-batch using <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html">torch distributed</a>. This allows each workers GPU to be used both for sampling and for training.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>DD-PPO is best for envs that require GPUs to function, or if you need to scale out SGD to multiple nodes. If you dont meet these requirements, <a class="reference external" href="rllib-algorithms.html#proximal-policy-optimization-ppo">standard PPO</a> will be more efficient.</p>
</div>
<figure class="align-default" id="id4">
<img alt="../_images/ddppo-arch.svg" src="../_images/ddppo-arch.svg" /><figcaption>
<p><span class="caption-text">DD-PPO architecture (both sampling and learning are done on worker GPUs)</span><a class="headerlink" href="rllib-algorithms.html#id4" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ddppo/cartpole-ddppo.yaml">CartPole-v1</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ddppo/atari-ddppo.yaml">BreakoutNoFrameskip-v4</a></p>
<p><strong>DDPPO-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.ddppo.ddppo.DDPPOConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.ddppo.ddppo.</span></span><span class="sig-name descname"><span class="pre">DDPPOConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/ddppo/ddppo.html#DDPPOConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.ddppo.ddppo.DDPPOConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a DDPPO Algorithm can be built.</p>
<p>Note(jungong) : despite best efforts, DDPPO does not use fault tolerant and
elastic features of WorkerSet, because of the way Torch DDP is set up.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ddppo</span> <span class="kn">import</span> <span class="n">DDPPOConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">DDPPOConfig</span><span class="p">()</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.003</span><span class="p">,</span> <span class="n">keep_local_weights_in_sync</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ddppo</span> <span class="kn">import</span> <span class="n">DDPPOConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">DDPPOConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">kl_coeff</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span> 
<span class="gp">... </span>    <span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]),</span> <span class="n">num_sgd_iter</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span> 
<span class="gp">... </span>    <span class="s2">&quot;DDPPO&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.ddppo.ddppo.DDPPOConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_local_weights_in_sync:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch_distributed_backend:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.ddppo.ddppo.DDPPOConfig" title="ray.rllib.algorithms.ddppo.ddppo.DDPPOConfig"><span class="pre">ray.rllib.algorithms.ddppo.ddppo.DDPPOConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/ddppo/ddppo.html#DDPPOConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.ddppo.ddppo.DDPPOConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>keep_local_weights_in_sync</strong>  Download weights between each training step.
This adds a bit of overhead but allows the user to access the weights
from the trainer.</p></li>
<li><p><strong>torch_distributed_backend</strong>  The communication backend for PyTorch
distributed.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="proximal-policy-optimization-ppo">
<span id="ppo"></span><h3>Proximal Policy Optimization (PPO)<a class="headerlink" href="rllib-algorithms.html#proximal-policy-optimization-ppo" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1707.06347">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py">[implementation]</a>
PPOs clipped objective supports multiple SGD passes over the same batch of experiences. RLlibs multi-GPU optimizer pins that data in GPU memory to avoid unnecessary transfers from host memory, substantially improving performance over a naive implementation. PPO scales out using multiple workers for experience collection, and also to multiple GPUs for SGD.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you need to scale out with GPUs on multiple nodes, consider using <a class="reference external" href="rllib-algorithms.html#decentralized-distributed-proximal-policy-optimization-dd-ppo">decentralized PPO</a>.</p>
</div>
<figure class="align-default" id="id5">
<img alt="../_images/ppo-arch.svg" src="../_images/ppo-arch.svg" /><figcaption>
<p><span class="caption-text">PPO architecture</span><a class="headerlink" href="rllib-algorithms.html#id5" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples:
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/unity3d-soccer-strikers-vs-goalie-ppo.yaml">Unity3D Soccer (multi-agent: Strikers vs Goalie)</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/humanoid-ppo-gae.yaml">Humanoid-v1</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/hopper-ppo.yaml">Hopper-v1</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/pendulum-ppo.yaml">Pendulum-v1</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/pong-ppo.yaml">PongDeterministic-v4</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/walker2d-ppo.yaml">Walker2d-v1</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/halfcheetah-ppo.yaml">HalfCheetah-v2</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/atari-ppo.yaml">{BeamRider,Breakout,Qbert,SpaceInvaders}NoFrameskip-v4</a></p>
<p><strong>Atari results</strong>: <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 22%" />
<col style="width: 24%" />
<col style="width: 24%" />
<col style="width: 31%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Atari env</p></th>
<th class="head"><p>RLlib PPO &#64;10M</p></th>
<th class="head"><p>RLlib PPO &#64;25M</p></th>
<th class="head"><p>Baselines PPO &#64;10M</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BeamRider</p></td>
<td><p>2807</p></td>
<td><p>4480</p></td>
<td><p>~1800</p></td>
</tr>
<tr class="row-odd"><td><p>Breakout</p></td>
<td><p>104</p></td>
<td><p>201</p></td>
<td><p>~250</p></td>
</tr>
<tr class="row-even"><td><p>Qbert</p></td>
<td><p>11085</p></td>
<td><p>14247</p></td>
<td><p>~14000</p></td>
</tr>
<tr class="row-odd"><td><p>SpaceInvaders</p></td>
<td><p>671</p></td>
<td><p>944</p></td>
<td><p>~800</p></td>
</tr>
</tbody>
</table>
<p><strong>Scalability:</strong> <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 19%" />
<col style="width: 37%" />
<col style="width: 43%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>MuJoCo env</p></th>
<th class="head"><p>RLlib PPO 16-workers &#64; 1h</p></th>
<th class="head"><p>Fan et al PPO 16-workers &#64; 1h</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>HalfCheetah</p></td>
<td><p>9664</p></td>
<td><p>~7700</p></td>
</tr>
</tbody>
</table>
<figure class="align-default" id="id6">
<a class="reference internal image-reference" href="../_images/ppo.png"><img alt="../_images/ppo.png" src="../_images/ppo.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-text">RLlibs multi-GPU PPO scales to multiple GPUs and hundreds of CPUs on solving the Humanoid-v1 task. Here we compare against a reference MPI-based implementation.</span><a class="headerlink" href="rllib-algorithms.html#id6" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong>PPO-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.ppo.ppo.PPOConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.ppo.ppo.</span></span><span class="sig-name descname"><span class="pre">PPOConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/ppo/ppo.html#PPOConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.ppo.ppo.PPOConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a PPO Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPOConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">PPOConfig</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">kl_coeff</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPOConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">PPOConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">clip_param</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span>  
<span class="gp">... </span><span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]),</span> <span class="n">clip_param</span><span class="o">=</span><span class="mf">0.2</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;PPO&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.ppo.ppo.PPOConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_schedule:</span> <span class="pre">Optional[List[List[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_critic:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_gae:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_kl_loss:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kl_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kl_target:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sgd_minibatch_size:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_sgd_iter:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_sequences:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_loss_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entropy_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entropy_coeff_schedule:</span> <span class="pre">Optional[List[List[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_param:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_clip_param:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_share_layers=-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.ppo.ppo.PPOConfig" title="ray.rllib.algorithms.ppo.ppo.PPOConfig"><span class="pre">ray.rllib.algorithms.ppo.ppo.PPOConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/ppo/ppo.html#PPOConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.ppo.ppo.PPOConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr_schedule</strong>  Learning rate schedule. In the format of
[[timestep, lr-value], [timestep, lr-value], ]
Intermediary timesteps will be assigned to interpolated learning rate
values. A schedule should normally start from timestep 0.</p></li>
<li><p><strong>use_critic</strong>  Should use a critic as a baseline (otherwise dont use value
baseline; required for using GAE).</p></li>
<li><p><strong>use_gae</strong>  If true, use the Generalized Advantage Estimator (GAE)
with a value function, see <a class="reference external" href="https://arxiv.org/pdf/1506.02438.pdf">https://arxiv.org/pdf/1506.02438.pdf</a>.</p></li>
<li><p><strong>lambda</strong>  The GAE (lambda) parameter.</p></li>
<li><p><strong>use_kl_loss</strong>  Whether to use the KL-term in the loss function.</p></li>
<li><p><strong>kl_coeff</strong>  Initial coefficient for KL divergence.</p></li>
<li><p><strong>kl_target</strong>  Target value for KL divergence.</p></li>
<li><p><strong>sgd_minibatch_size</strong>  Total SGD batch size across all devices for SGD.
This defines the minibatch size within each epoch.</p></li>
<li><p><strong>num_sgd_iter</strong>  Number of SGD iterations in each outer loop (i.e., number of
epochs to execute per train batch).</p></li>
<li><p><strong>shuffle_sequences</strong>  Whether to shuffle sequences in the batch when training
(recommended).</p></li>
<li><p><strong>vf_loss_coeff</strong>  Coefficient of the value function loss. IMPORTANT: you must
tune this if you set vf_share_layers=True inside your models config.</p></li>
<li><p><strong>entropy_coeff</strong>  Coefficient of the entropy regularizer.</p></li>
<li><p><strong>entropy_coeff_schedule</strong>  Decay schedule for the entropy regularizer.</p></li>
<li><p><strong>clip_param</strong>  The PPO clip parameter.</p></li>
<li><p><strong>vf_clip_param</strong>  Clip param for the value function. Note that this is
sensitive to the scale of the rewards. If your expected V is large,
increase this.</p></li>
<li><p><strong>grad_clip</strong>  If specified, clip the global norm of gradients by this amount.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="importance-weighted-actor-learner-architecture-impala">
<span id="impala"></span><h3>Importance Weighted Actor-Learner Architecture (IMPALA)<a class="headerlink" href="rllib-algorithms.html#importance-weighted-actor-learner-architecture-impala" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1802.01561">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/impala/impala.py">[implementation]</a>
In IMPALA, a central learner runs SGD in a tight loop while asynchronously pulling sample batches from many actor processes. RLlibs IMPALA implementation uses DeepMinds reference <a class="reference external" href="https://github.com/deepmind/scalable_agent/blob/master/vtrace.py">V-trace code</a>. Note that we do not provide a deep residual network out of the box, but one can be plugged in as a <a class="reference external" href="rllib-models.html#custom-models-tensorflow">custom model</a>. Multiple learner GPUs and experience replay are also supported.</p>
<figure class="align-default" id="id7">
<img alt="../_images/impala-arch.svg" src="../_images/impala-arch.svg" /><figcaption>
<p><span class="caption-text">IMPALA architecture</span><a class="headerlink" href="rllib-algorithms.html#id7" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/impala/pong-impala.yaml">PongNoFrameskip-v4</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/impala/pong-impala-vectorized.yaml">vectorized configuration</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/impala/pong-impala-fast.yaml">multi-gpu configuration</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/impala/atari-impala.yaml">{BeamRider,Breakout,Qbert,SpaceInvaders}NoFrameskip-v4</a></p>
<p><strong>Atari results &#64;10M steps</strong>: <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 16%" />
<col style="width: 41%" />
<col style="width: 43%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Atari env</p></th>
<th class="head"><p>RLlib IMPALA 32-workers</p></th>
<th class="head"><p>Mnih et al A3C 16-workers</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BeamRider</p></td>
<td><p>2071</p></td>
<td><p>~3000</p></td>
</tr>
<tr class="row-odd"><td><p>Breakout</p></td>
<td><p>385</p></td>
<td><p>~150</p></td>
</tr>
<tr class="row-even"><td><p>Qbert</p></td>
<td><p>4068</p></td>
<td><p>~1000</p></td>
</tr>
<tr class="row-odd"><td><p>SpaceInvaders</p></td>
<td><p>719</p></td>
<td><p>~600</p></td>
</tr>
</tbody>
</table>
<p><strong>Scalability:</strong></p>
<table class="table">
<colgroup>
<col style="width: 17%" />
<col style="width: 40%" />
<col style="width: 43%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Atari env</p></th>
<th class="head"><p>RLlib IMPALA 32-workers &#64;1 hour</p></th>
<th class="head"><p>Mnih et al A3C 16-workers &#64;1 hour</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BeamRider</p></td>
<td><p>3181</p></td>
<td><p>~1000</p></td>
</tr>
<tr class="row-odd"><td><p>Breakout</p></td>
<td><p>538</p></td>
<td><p>~10</p></td>
</tr>
<tr class="row-even"><td><p>Qbert</p></td>
<td><p>10850</p></td>
<td><p>~500</p></td>
</tr>
<tr class="row-odd"><td><p>SpaceInvaders</p></td>
<td><p>843</p></td>
<td><p>~300</p></td>
</tr>
</tbody>
</table>
<figure class="align-default" id="id8">
<img alt="../_images/impala.png" src="../_images/impala.png" />
<figcaption>
<p><span class="caption-text">Multi-GPU IMPALA scales up to solve PongNoFrameskip-v4 in ~3 minutes using a pair of V100 GPUs and 128 CPU workers.
The maximum training throughput reached is ~30k transitions per second (~120k environment frames per second).</span><a class="headerlink" href="rllib-algorithms.html#id8" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong>IMPALA-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.impala.impala.ImpalaConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.impala.impala.</span></span><span class="sig-name descname"><span class="pre">ImpalaConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/impala/impala.html#ImpalaConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.impala.impala.ImpalaConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which an Impala can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.impala</span> <span class="kn">import</span> <span class="n">ImpalaConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">ImpalaConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0003</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.impala</span> <span class="kn">import</span> <span class="n">ImpalaConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">ImpalaConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vtrace</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span>   
<span class="gp">... </span>    <span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.0003</span><span class="p">]),</span> <span class="n">grad_clip</span><span class="o">=</span><span class="mf">20.0</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;IMPALA&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.impala.impala.ImpalaConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">*,</span> <span class="pre">vtrace:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">vtrace_clip_rho_threshold:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">vtrace_clip_pg_rho_threshold:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">vtrace_drop_last_ts:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">gamma:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">num_multi_gpu_tower_stacks:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">minibatch_buffer_size:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">minibatch_size:</span> <span class="pre">Optional[Union[int,</span> <span class="pre">str]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">num_sgd_iter:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">replay_proportion:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">replay_buffer_num_slots:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">learner_queue_size:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">learner_queue_timeout:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">max_requests_in_flight_per_aggregator_worker:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">timeout_s_sampler_manager:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">timeout_s_aggregator_manager:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">broadcast_interval:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">num_aggregation_workers:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">grad_clip:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">opt_type:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">lr_schedule:</span> <span class="pre">Optional[List[List[Union[int,</span> <span class="pre">float]]]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">decay:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">momentum:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">epsilon:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">vf_loss_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">entropy_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">entropy_coeff_schedule:</span> <span class="pre">Optional[List[List[Union[int,</span> <span class="pre">float]]]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">_separate_vf_optimizer:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">_lr_vf:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">after_train_step:</span> <span class="pre">Optional[Callable[[dict],</span> <span class="pre">None]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">**kwargs</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.impala.impala.ImpalaConfig" title="ray.rllib.algorithms.impala.impala.ImpalaConfig"><span class="pre">ray.rllib.algorithms.impala.impala.ImpalaConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/impala/impala.html#ImpalaConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.impala.impala.ImpalaConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vtrace</strong>  V-trace params (see vtrace_tf/torch.py).</p></li>
<li><p><strong>vtrace_clip_rho_threshold</strong>  </p></li>
<li><p><strong>vtrace_clip_pg_rho_threshold</strong>  </p></li>
<li><p><strong>vtrace_drop_last_ts</strong>  If True, drop the last timestep for the vtrace
calculations, such that all data goes into the calculations as [B x T-1]
(+ the bootstrap value). This is the default and legacy RLlib behavior,
however, could potentially have a destabilizing effect on learning,
especially in sparse reward or reward-at-goal environments.
False for not dropping the last timestep.
System params.</p></li>
<li><p><strong>gamma</strong>  Float specifying the discount factor of the Markov Decision process.</p></li>
<li><p><strong>num_multi_gpu_tower_stacks</strong>  For each stack of multi-GPU towers, how many
slots should we reserve for parallel data loading? Set this to &gt;1 to
load data into GPUs in parallel. This will increase GPU memory usage
proportionally with the number of stacks.
Example:
2 GPUs and <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_multi_gpu_tower_stacks=3</span></code>:
- One tower stack consists of 2 GPUs, each with a copy of the
model/graph.
- Each of the stacks will create 3 slots for batch data on each of its
GPUs, increasing memory requirements on each GPU by 3x.
- This enables us to preload data into these stacks while another stack
is performing gradient calculations.</p></li>
<li><p><strong>minibatch_buffer_size</strong>  How many train batches should be retained for
minibatching. This conf only has an effect if <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_sgd_iter</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>.</p></li>
<li><p><strong>minibatch_size</strong>  The size of minibatches that are trained over during
each SGD iteration. If auto, will use the same value as
<code class="xref py py-obj docutils literal notranslate"><span class="pre">train_batch_size</span></code>.
Note that this setting only has an effect if <code class="xref py py-obj docutils literal notranslate"><span class="pre">_enable_learner_api=True</span></code>
and it must be a multiple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">rollout_fragment_length</span></code> or
<code class="xref py py-obj docutils literal notranslate"><span class="pre">sequence_length</span></code> and smaller than or equal to <code class="xref py py-obj docutils literal notranslate"><span class="pre">train_batch_size</span></code>.</p></li>
<li><p><strong>num_sgd_iter</strong>  Number of passes to make over each train batch.</p></li>
<li><p><strong>replay_proportion</strong>  Set &gt;0 to enable experience replay. Saved samples will
be replayed with a p:1 proportion to new data samples.</p></li>
<li><p><strong>replay_buffer_num_slots</strong>  Number of sample batches to store for replay.
The number of transitions saved total will be
(replay_buffer_num_slots * rollout_fragment_length).</p></li>
<li><p><strong>learner_queue_size</strong>  Max queue size for train batches feeding into the
learner.</p></li>
<li><p><strong>learner_queue_timeout</strong>  Wait for train batches to be available in minibatch
buffer queue this many seconds. This may need to be increased e.g. when
training with a slow environment.</p></li>
<li><p><strong>max_requests_in_flight_per_aggregator_worker</strong>  Level of queuing for replay
aggregator operations (if using aggregator workers).</p></li>
<li><p><strong>timeout_s_sampler_manager</strong>  The timeout for waiting for sampling results
for workers  typically if this is too low, the manager wont be able
to retrieve ready sampling results.</p></li>
<li><p><strong>timeout_s_aggregator_manager</strong>  The timeout for waiting for replay worker
results  typically if this is too low, the manager wont be able to
retrieve ready replay requests.</p></li>
<li><p><strong>broadcast_interval</strong>  Number of training step calls before weights are
broadcasted to rollout workers that are sampled during any iteration.</p></li>
<li><p><strong>num_aggregation_workers</strong>  Use n (<code class="xref py py-obj docutils literal notranslate"><span class="pre">num_aggregation_workers</span></code>) extra Actors for
multi-level aggregation of the data produced by the m RolloutWorkers
(<code class="xref py py-obj docutils literal notranslate"><span class="pre">num_workers</span></code>). Note that n should be much smaller than m.
This can make sense if ingesting &gt;2GB/s of samples, or if
the data requires decompression.</p></li>
<li><p><strong>grad_clip</strong>  If specified, clip the global norm of gradients by this amount.</p></li>
<li><p><strong>opt_type</strong>  Either adam or rmsprop.</p></li>
<li><p><strong>lr_schedule</strong>  Learning rate schedule. In the format of
[[timestep, lr-value], [timestep, lr-value], ]
Intermediary timesteps will be assigned to interpolated learning rate
values. A schedule should normally start from timestep 0.</p></li>
<li><p><strong>decay</strong>  Decay setting for the RMSProp optimizer, in case <code class="xref py py-obj docutils literal notranslate"><span class="pre">opt_type=rmsprop</span></code>.</p></li>
<li><p><strong>momentum</strong>  Momentum setting for the RMSProp optimizer, in case
<code class="xref py py-obj docutils literal notranslate"><span class="pre">opt_type=rmsprop</span></code>.</p></li>
<li><p><strong>epsilon</strong>  Epsilon setting for the RMSProp optimizer, in case
<code class="xref py py-obj docutils literal notranslate"><span class="pre">opt_type=rmsprop</span></code>.</p></li>
<li><p><strong>vf_loss_coeff</strong>  Coefficient for the value function term in the loss function.</p></li>
<li><p><strong>entropy_coeff</strong>  Coefficient for the entropy regularizer term in the loss
function.</p></li>
<li><p><strong>entropy_coeff_schedule</strong>  Decay schedule for the entropy regularizer.</p></li>
<li><p><strong>_separate_vf_optimizer</strong>  Set this to true to have two separate optimizers
optimize the policy-and value networks.</p></li>
<li><p><strong>_lr_vf</strong>  If _separate_vf_optimizer is True, define separate learning rate
for the value network.</p></li>
<li><p><strong>after_train_step</strong>  Callback for APPO to use to update KL, target network
periodically. The input to the callback is the learner fetches dict.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="advantage-actor-critic-a2c">
<span id="a2c"></span><h3>Advantage Actor-Critic (A2C)<a class="headerlink" href="rllib-algorithms.html#advantage-actor-critic-a2c" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1602.01783">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/a2c/a2c.py">[implementation]</a>
A2C scales to 16-32+ worker processes depending on the environment and supports microbatching
(i.e., gradient accumulation), which can be enabled by setting the <code class="docutils literal notranslate"><span class="pre">microbatch_size</span></code> config.
Microbatching allows for training with a <code class="docutils literal notranslate"><span class="pre">train_batch_size</span></code> much larger than GPU memory.</p>
<figure class="align-default" id="id9">
<img alt="../_images/a2c-arch.svg" src="../_images/a2c-arch.svg" /><figcaption>
<p><span class="caption-text">A2C architecture</span><a class="headerlink" href="rllib-algorithms.html#id9" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/a2c/atari-a2c.yaml">Atari environments</a></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Consider using <a class="reference external" href="rllib-algorithms.html#importance-weighted-actor-learner-architecture-impala">IMPALA</a> for faster training with similar timestep efficiency.</p>
</div>
<p><strong>Atari results &#64;10M steps</strong>: <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 19%" />
<col style="width: 36%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Atari env</p></th>
<th class="head"><p>RLlib A2C 5-workers</p></th>
<th class="head"><p>Mnih et al A3C 16-workers</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BeamRider</p></td>
<td><p>1401</p></td>
<td><p>~3000</p></td>
</tr>
<tr class="row-odd"><td><p>Breakout</p></td>
<td><p>374</p></td>
<td><p>~150</p></td>
</tr>
<tr class="row-even"><td><p>Qbert</p></td>
<td><p>3620</p></td>
<td><p>~1000</p></td>
</tr>
<tr class="row-odd"><td><p>SpaceInvaders</p></td>
<td><p>692</p></td>
<td><p>~600</p></td>
</tr>
</tbody>
</table>
<p><strong>A2C-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.a2c.a2c.A2CConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.a2c.a2c.</span></span><span class="sig-name descname"><span class="pre">A2CConfig</span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/a2c/a2c.html#A2CConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.a2c.a2c.A2CConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a new A2C can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.a2c</span> <span class="kn">import</span> <span class="n">A2CConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">A2CConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">grad_clip</span><span class="o">=</span><span class="mf">30.0</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ray.air</span> <span class="k">as</span> <span class="nn">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.a2c</span> <span class="kn">import</span> <span class="n">A2CConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">A2CConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sample_async</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]),</span> <span class="n">use_critic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;A2C&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.a2c.a2c.A2CConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">microbatch_size:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.a2c.a2c.A2CConfig" title="ray.rllib.algorithms.a2c.a2c.A2CConfig"><span class="pre">ray.rllib.algorithms.a2c.a2c.A2CConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/a2c/a2c.html#A2CConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.a2c.a2c.A2CConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>microbatch_size</strong>  A2C supports microbatching, in which we accumulate
gradients over batch of this size until the train batch size is reached.
This allows training with batch sizes much larger than can fit in GPU
memory. To enable, set this to a value less than the train batch size.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="asynchronous-advantage-actor-critic-a3c">
<span id="a3c"></span><h3>Asynchronous Advantage Actor-Critic (A3C)<a class="headerlink" href="rllib-algorithms.html#asynchronous-advantage-actor-critic-a3c" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1602.01783">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/a3c/a3c.py">[implementation]</a>
A3C is the asynchronous version of A2C, where gradients are computed on the workers directly after trajectory rollouts,
and only then shipped to a central learner to accumulate these gradients on the central model. After the central model update, parameters are broadcast back to
all workers.
Similar to A2C, A3C scales to 16-32+ worker processes depending on the environment.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/a3c/pong-a3c.yaml">PongDeterministic-v4</a></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Consider using <a class="reference external" href="rllib-algorithms.html#importance-weighted-actor-learner-architecture-impala">IMPALA</a> for faster training with similar timestep efficiency.</p>
</div>
<p><strong>A3C-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.a3c.a3c.A3CConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.a3c.a3c.</span></span><span class="sig-name descname"><span class="pre">A3CConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/a3c/a3c.html#A3CConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.a3c.a3c.A3CConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a A3C Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.a3c</span> <span class="kn">import</span> <span class="n">A3CConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">A3CConfig</span><span class="p">()</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">grad_clip</span><span class="o">=</span><span class="mf">30.0</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.a3c</span> <span class="kn">import</span> <span class="n">A3CConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">A3CConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">sample_async</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span> 
<span class="gp">... </span>    <span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]),</span> <span class="n">use_critic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;A3C&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">},</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.a3c.a3c.A3CConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_schedule:</span> <span class="pre">Optional[List[List[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_critic:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_gae:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_loss_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entropy_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entropy_coeff_schedule:</span> <span class="pre">Optional[List[List[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_async:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.a3c.a3c.A3CConfig" title="ray.rllib.algorithms.a3c.a3c.A3CConfig"><span class="pre">ray.rllib.algorithms.a3c.a3c.A3CConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/a3c/a3c.html#A3CConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.a3c.a3c.A3CConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr_schedule</strong>  Learning rate schedule. In the format of
[[timestep, lr-value], [timestep, lr-value], ]
Intermediary timesteps will be assigned to interpolated learning rate
values. A schedule should normally start from timestep 0.</p></li>
<li><p><strong>use_critic</strong>  Should use a critic as a baseline (otherwise dont use value
baseline; required for using GAE).</p></li>
<li><p><strong>use_gae</strong>  If true, use the Generalized Advantage Estimator (GAE)
with a value function, see <a class="reference external" href="https://arxiv.org/pdf/1506.02438.pdf">https://arxiv.org/pdf/1506.02438.pdf</a>.</p></li>
<li><p><strong>lambda</strong>  GAE(gamma) parameter.</p></li>
<li><p><strong>grad_clip</strong>  Max global norm for each gradient calculated by worker.</p></li>
<li><p><strong>vf_loss_coeff</strong>  Value Function Loss coefficient.</p></li>
<li><p><strong>entropy_coeff</strong>  Coefficient of the entropy regularizer.</p></li>
<li><p><strong>entropy_coeff_schedule</strong>  Decay schedule for the entropy regularizer.</p></li>
<li><p><strong>sample_async</strong>  Whether workers should sample async. Note that this
increases the effective rollout_fragment_length by up to 5x due
to async buffering of batches.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="policy-gradients-pg">
<span id="pg"></span><h3>Policy Gradients (PG)<a class="headerlink" href="rllib-algorithms.html#policy-gradients-pg" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/pg/pg.py">[implementation]</a>
We include a vanilla policy gradients implementation as an example algorithm.</p>
<figure class="align-default" id="id10">
<img alt="../_images/a2c-arch.svg" src="../_images/a2c-arch.svg" /><figcaption>
<p><span class="caption-text">Policy gradients architecture (same as A2C)</span><a class="headerlink" href="rllib-algorithms.html#id10" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/pg/cartpole-pg.yaml">CartPole-v1</a></p>
<p><strong>PG-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.pg.pg.PGConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.pg.pg.</span></span><span class="sig-name descname"><span class="pre">PGConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/pg/pg.html#PGConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.pg.pg.PGConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a PG Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.pg</span> <span class="kn">import</span> <span class="n">PGConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">PGConfig</span><span class="p">()</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.pg</span> <span class="kn">import</span> <span class="n">PGConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">PGConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span> 
<span class="go">0.0004</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;PG&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.pg.pg.PGConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_schedule:</span> <span class="pre">Optional[List[List[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.pg.pg.PGConfig" title="ray.rllib.algorithms.pg.pg.PGConfig"><span class="pre">ray.rllib.algorithms.pg.pg.PGConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/pg/pg.html#PGConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.pg.pg.PGConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gamma</strong>  Float specifying the discount factor of the Markov Decision process.</p></li>
<li><p><strong>lr</strong>  The default learning rate.</p></li>
<li><p><strong>train_batch_size</strong>  Training batch size, if applicable.</p></li>
<li><p><strong>model</strong>  Arguments passed into the policy model. See models/catalog.py for a
full list of the available model options.</p></li>
<li><p><strong>optimizer</strong>  Arguments to pass to the policy optimizer.</p></li>
<li><p><strong>lr_schedule</strong>  Learning rate schedule. In the format of
[[timestep, lr-value], [timestep, lr-value], ]
Intermediary timesteps will be assigned to interpolated learning rate
values. A schedule should normally start from timestep 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="model-agnostic-meta-learning-maml">
<span id="maml"></span><h3>Model-Agnostic Meta-Learning (MAML)<a class="headerlink" href="rllib-algorithms.html#model-agnostic-meta-learning-maml" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1703.03400">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/maml/maml.py">[implementation]</a></p>
<p>RLlibs MAML implementation is a meta-learning method for learning and quick adaptation across different tasks for continuous control. Code here is adapted from <a class="reference external" href="https://github.com/jonasrothfuss">https://github.com/jonasrothfuss</a>, which outperforms vanilla MAML and avoids computation of the higher order gradients during the meta-update step. MAML is evaluated on custom environments that are described in greater detail <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/env/apis/task_settable_env.py">here</a>.</p>
<p>MAML uses additional metrics to measure performance; <code class="docutils literal notranslate"><span class="pre">episode_reward_mean</span></code> measures the agents returns before adaptation, <code class="docutils literal notranslate"><span class="pre">episode_reward_mean_adapt_N</span></code> measures the agents returns after N gradient steps of inner adaptation, and <code class="docutils literal notranslate"><span class="pre">adaptation_delta</span></code> measures the difference in performance before and after adaptation. Examples can be seen <a class="reference external" href="https://github.com/ray-project/rl-experiments/tree/master/maml">here</a>.</p>
<p>Tuned examples: HalfCheetahRandDirecEnv (<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/env/halfcheetah_rand_direc.py">Env</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/maml/halfcheetah-rand-direc-maml.yaml">Config</a>), AntRandGoalEnv (<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/env/ant_rand_goal.py">Env</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/maml/ant-rand-goal-maml.yaml">Config</a>), PendulumMassEnv (<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/env/pendulum_mass.py">Env</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/maml/pendulum-mass-maml.yaml">Config</a>)</p>
<p><strong>MAML-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.maml.maml.MAMLConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.maml.maml.</span></span><span class="sig-name descname"><span class="pre">MAMLConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/maml/maml.html#MAMLConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.maml.maml.MAMLConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a MAML Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.maml</span> <span class="kn">import</span> <span class="n">MAMLConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">MAMLConfig</span><span class="p">()</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">use_gae</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.maml</span> <span class="kn">import</span> <span class="n">MAMLConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">MAMLConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="n">grad_clip</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;MAML&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.maml.maml.MAMLConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_gae:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kl_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_loss_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entropy_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_param:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_clip_param:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kl_target:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inner_adaptation_steps:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maml_optimizer_steps:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inner_lr:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_meta_env:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.maml.maml.MAMLConfig" title="ray.rllib.algorithms.maml.maml.MAMLConfig"><span class="pre">ray.rllib.algorithms.maml.maml.MAMLConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/maml/maml.html#MAMLConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.maml.maml.MAMLConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_gae</strong>  If true, use the Generalized Advantage Estimator (GAE)
with a value function, see <a class="reference external" href="https://arxiv.org/pdf/1506.02438.pdf">https://arxiv.org/pdf/1506.02438.pdf</a>.</p></li>
<li><p><strong>lambda</strong>  The GAE (lambda) parameter.</p></li>
<li><p><strong>kl_coeff</strong>  Initial coefficient for KL divergence.</p></li>
<li><p><strong>vf_loss_coeff</strong>  Coefficient of the value function loss.</p></li>
<li><p><strong>entropy_coeff</strong>  Coefficient of the entropy regularizer.</p></li>
<li><p><strong>clip_param</strong>  PPO clip parameter.</p></li>
<li><p><strong>vf_clip_param</strong>  Clip param for the value function. Note that this is
sensitive to the scale of the rewards. If your expected V is large,
increase this.</p></li>
<li><p><strong>grad_clip</strong>  If specified, clip the global norm of gradients by this amount.</p></li>
<li><p><strong>kl_target</strong>  Target value for KL divergence.</p></li>
<li><p><strong>inner_adaptation_steps</strong>  Number of Inner adaptation steps for the MAML
algorithm.</p></li>
<li><p><strong>maml_optimizer_steps</strong>  Number of MAML steps per meta-update iteration
(PPO steps).</p></li>
<li><p><strong>inner_lr</strong>  Inner Adaptation Step size.</p></li>
<li><p><strong>use_meta_env</strong>  Use Meta Env Template.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="model-free-off-policy-rl">
<h2>Model-free Off-policy RL<a class="headerlink" href="rllib-algorithms.html#model-free-off-policy-rl" title="Permalink to this headline">#</a></h2>
<section id="distributed-prioritized-experience-replay-ape-x">
<span id="apex"></span><h3>Distributed Prioritized Experience Replay (Ape-X)<a class="headerlink" href="rllib-algorithms.html#distributed-prioritized-experience-replay-ape-x" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1803.00933">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/apex_dqn/apex_dqn.py">[implementation]</a>
Ape-X variations of DQN and DDPG (<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/apex_dqn/apex_dqn.py">APEX_DQN</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/apex_ddpg/apex_ddpg.py">APEX_DDPG</a>) use a single GPU learner and many CPU workers for experience collection. Experience collection can scale to hundreds of CPU workers due to the distributed prioritization of experience prior to storage in replay buffers.</p>
<figure class="align-default" id="id11">
<img alt="../_images/apex-arch.svg" src="../_images/apex-arch.svg" /><figcaption>
<p><span class="caption-text">Ape-X architecture</span><a class="headerlink" href="rllib-algorithms.html#id11" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/apex_dqn/pong-apex-dqn.yaml">PongNoFrameskip-v4</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/apex_ddpg/pendulum-apex-ddpg.yaml">Pendulum-v1</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/apex_ddpg/mountaincarcontinuous-apex-ddpg.yaml">MountainCarContinuous-v0</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/apex_dqn/atari-apex-dqn.yaml">{BeamRider,Breakout,Qbert,SpaceInvaders}NoFrameskip-v4</a>.</p>
<p><strong>Atari results &#64;10M steps</strong>: <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 15%" />
<col style="width: 38%" />
<col style="width: 47%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Atari env</p></th>
<th class="head"><p>RLlib Ape-X 8-workers</p></th>
<th class="head"><p>Mnih et al Async DQN 16-workers</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BeamRider</p></td>
<td><p>6134</p></td>
<td><p>~6000</p></td>
</tr>
<tr class="row-odd"><td><p>Breakout</p></td>
<td><p>123</p></td>
<td><p>~50</p></td>
</tr>
<tr class="row-even"><td><p>Qbert</p></td>
<td><p>15302</p></td>
<td><p>~1200</p></td>
</tr>
<tr class="row-odd"><td><p>SpaceInvaders</p></td>
<td><p>686</p></td>
<td><p>~600</p></td>
</tr>
</tbody>
</table>
<p><strong>Scalability</strong>:</p>
<table class="table">
<colgroup>
<col style="width: 15%" />
<col style="width: 38%" />
<col style="width: 47%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Atari env</p></th>
<th class="head"><p>RLlib Ape-X 8-workers &#64;1 hour</p></th>
<th class="head"><p>Mnih et al Async DQN 16-workers &#64;1 hour</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BeamRider</p></td>
<td><p>4873</p></td>
<td><p>~1000</p></td>
</tr>
<tr class="row-odd"><td><p>Breakout</p></td>
<td><p>77</p></td>
<td><p>~10</p></td>
</tr>
<tr class="row-even"><td><p>Qbert</p></td>
<td><p>4083</p></td>
<td><p>~500</p></td>
</tr>
<tr class="row-odd"><td><p>SpaceInvaders</p></td>
<td><p>646</p></td>
<td><p>~300</p></td>
</tr>
</tbody>
</table>
<figure class="align-default" id="id12">
<img alt="../_images/apex.png" src="../_images/apex.png" />
<figcaption>
<p><span class="caption-text">Ape-X using 32 workers in RLlib vs vanilla DQN (orange) and A3C (blue) on PongNoFrameskip-v4.</span><a class="headerlink" href="rllib-algorithms.html#id12" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Ape-X specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.apex_dqn.apex_dqn.ApexDQNConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.apex_dqn.apex_dqn.</span></span><span class="sig-name descname"><span class="pre">ApexDQNConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/apex_dqn/apex_dqn.html#ApexDQNConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.apex_dqn.apex_dqn.ApexDQNConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which an ApexDQN Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.apex_dqn.apex_dqn</span> <span class="kn">import</span> <span class="n">ApexDQNConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">ApexDQNConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">replay_buffer_config</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">replay_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">replay_buffer_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span> 
<span class="gp">... </span>    <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;capacity&quot;</span><span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;prioritized_replay_alpha&quot;</span><span class="p">:</span> <span class="mf">0.45</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;prioritized_replay_beta&quot;</span><span class="p">:</span> <span class="mf">0.55</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;prioritized_replay_eps&quot;</span><span class="p">:</span> <span class="mf">3e-6</span><span class="p">,</span>
<span class="gp">... </span>    <span class="p">}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">replay_buffer_config</span><span class="o">=</span><span class="n">replay_config</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">()</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.apex_dqn.apex_dqn</span> <span class="kn">import</span> <span class="n">ApexDQNConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">ApexDQNConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="n">num_atoms</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span> 
<span class="gp">... </span>    <span class="s2">&quot;APEX&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span><span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.apex_dqn.apex_dqn</span> <span class="kn">import</span> <span class="n">ApexDQNConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">ApexDQNConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">exploration_config</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">explore_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">exploration_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;EpsilonGreedy&quot;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;initial_epsilon&quot;</span><span class="p">:</span> <span class="mf">0.96</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;final_epsilon&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;epsilone_timesteps&quot;</span><span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>
<span class="gp">... </span>    <span class="p">}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="n">lr_schedule</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="p">[</span><span class="mi">500</span><span class="p">,</span> <span class="mf">5e-3</span><span class="p">]]</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">exploration</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="n">exploration_config</span><span class="o">=</span><span class="n">explore_config</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.apex_dqn.apex_dqn</span> <span class="kn">import</span> <span class="n">ApexDQNConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">ApexDQNConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">exploration_config</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">explore_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">exploration_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;SoftQ&quot;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">],</span>
<span class="gp">... </span>    <span class="p">}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="n">lr_schedule</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="p">[</span><span class="mi">500</span><span class="p">,</span> <span class="mf">5e-3</span><span class="p">]]</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">exploration</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="n">exploration_config</span><span class="o">=</span><span class="n">explore_config</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.apex_dqn.apex_dqn.ApexDQNConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_requests_in_flight_per_replay_worker:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout_s_sampler_manager:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout_s_replay_manager:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.apex_dqn.apex_dqn.ApexDQNConfig" title="ray.rllib.algorithms.apex_dqn.apex_dqn.ApexDQNConfig"><span class="pre">ray.rllib.algorithms.apex_dqn.apex_dqn.ApexDQNConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/apex_dqn/apex_dqn.html#ApexDQNConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.apex_dqn.apex_dqn.ApexDQNConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_atoms</strong>  Number of atoms for representing the distribution of return.
When this is greater than 1, distributional Q-learning is used.</p></li>
<li><p><strong>v_min</strong>  Minimum value estimation</p></li>
<li><p><strong>v_max</strong>  Maximum value estimation</p></li>
<li><p><strong>noisy</strong>  Whether to use noisy network to aid exploration. This adds parametric
noise to the model weights.</p></li>
<li><p><strong>sigma0</strong>  Control the initial parameter noise for noisy nets.</p></li>
<li><p><strong>dueling</strong>  Whether to use dueling DQN.</p></li>
<li><p><strong>hiddens</strong>  Dense-layer setup for each the advantage branch and the value
branch</p></li>
<li><p><strong>double_q</strong>  Whether to use double DQN.</p></li>
<li><p><strong>n_step</strong>  N-step for Q-learning.</p></li>
<li><p><strong>before_learn_on_batch</strong>  Callback to run before learning on a multi-agent
batch of experiences.</p></li>
<li><p><strong>training_intensity</strong>  The intensity with which to update the model (vs
collecting samples from the env).
If None, uses natural values of:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">train_batch_size</span></code> / (<code class="xref py py-obj docutils literal notranslate"><span class="pre">rollout_fragment_length</span></code> x <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_workers</span></code> x
<code class="xref py py-obj docutils literal notranslate"><span class="pre">num_envs_per_worker</span></code>).
If not None, will make sure that the ratio between timesteps inserted
into and sampled from the buffer matches the given values.
Example:
training_intensity=1000.0
train_batch_size=250
rollout_fragment_length=1
num_workers=1 (or 0)
num_envs_per_worker=1
-&gt; natural value = 250 / 1 = 250.0
-&gt; will make sure that replay+train op will be executed 4x asoften as
rollout+insert op (4 * 250 = 1000).
See: rllib/algorithms/dqn/dqn.py::calculate_rr_weights for further
details.</p></li>
<li><p><strong>replay_buffer_config</strong>  Replay buffer config.
Examples:
{
_enable_replay_buffer_api: True,
type: MultiAgentReplayBuffer,
capacity: 50000,
replay_sequence_length: 1,
}
- OR -
{
_enable_replay_buffer_api: True,
type: MultiAgentPrioritizedReplayBuffer,
capacity: 50000,
prioritized_replay_alpha: 0.6,
prioritized_replay_beta: 0.4,
prioritized_replay_eps: 1e-6,
replay_sequence_length: 1,
}
- Where -
prioritized_replay_alpha: Alpha parameter controls the degree of
prioritization in the buffer. In other words, when a buffer sample has
a higher temporal-difference error, with how much more probability
should it drawn to use to update the parametrized Q-network. 0.0
corresponds to uniform probability. Setting much above 1.0 may quickly
result as the sampling distribution could become heavily pointy with
low entropy.
prioritized_replay_beta: Beta parameter controls the degree of
importance sampling which suppresses the influence of gradient updates
from samples that have higher probability of being sampled via alpha
parameter and the temporal-difference error.
prioritized_replay_eps: Epsilon parameter sets the baseline probability
for sampling so that when the temporal-difference error of a sample is
zero, there is still a chance of drawing the sample.</p></li>
<li><p><strong>max_requests_in_flight_per_replay_worker</strong>  Max number of inflight requests
to each replay (shard) worker. See the FaultTolerantActorManager class
for more details.
Tuning these values is important when running experimens
with large sample batches, where there is the risk that the object store
may fill up, causing spilling of objects to disk. This can cause any
asynchronous requests to become very slow, making your experiment run
slow as well. You can inspect the object store during your experiment
via a call to ray memory on your headnode, and by using the ray
dashboard. If youre seeing that the object store is filling up,
turn down the number of remote requests in flight, or enable compression
in your experiment of timesteps.</p></li>
<li><p><strong>timeout_s_sampler_manager</strong>  The timeout for waiting for sampling results
for workers  typically if this is too low, the manager wont be able
to retrieve ready sampling results.</p></li>
<li><p><strong>timeout_s_replay_manager</strong>  The timeout for waiting for replay worker
results  typically if this is too low, the manager wont be able to
retrieve ready replay requests.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="recurrent-replay-distributed-dqn-r2d2">
<span id="r2d2"></span><h3>Recurrent Replay Distributed DQN (R2D2)<a class="headerlink" href="rllib-algorithms.html#recurrent-replay-distributed-dqn-r2d2" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://openreview.net/pdf?id=r1lyTjAqYX">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/r2d2/r2d2.py">[implementation]</a>
R2D2 can be scaled by increasing the number of workers. All of the DQN improvements evaluated in <a class="reference external" href="https://arxiv.org/abs/1710.02298">Rainbow</a> are available, though not all are enabled by default.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/r2d2/stateless-cartpole-r2d2.yaml">Stateless CartPole-v1</a></p>
</section>
<section id="deep-q-networks-dqn-rainbow-parametric-dqn">
<span id="dqn"></span><h3>Deep Q Networks (DQN, Rainbow, Parametric DQN)<a class="headerlink" href="rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1312.5602">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/dqn/dqn.py">[implementation]</a>
DQN can be scaled by increasing the number of workers or using Ape-X. Memory usage is reduced by compressing samples in the replay buffer with LZ4. All of the DQN improvements evaluated in <a class="reference external" href="https://arxiv.org/abs/1710.02298">Rainbow</a> are available, though not all are enabled by default. See also how to use <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">parametric-actions in DQN</a>.</p>
<figure class="align-default" id="id13">
<img alt="../_images/dqn-arch.svg" src="../_images/dqn-arch.svg" /><figcaption>
<p><span class="caption-text">DQN architecture</span><a class="headerlink" href="rllib-algorithms.html#id13" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/dqn/pong-dqn.yaml">PongDeterministic-v4</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/dqn/pong-rainbow.yaml">Rainbow configuration</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/dqn/atari-dqn.yaml">{BeamRider,Breakout,Qbert,SpaceInvaders}NoFrameskip-v4</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/dqn/atari-duel-ddqn.yaml">with Dueling and Double-Q</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/dqn/atari-dist-dqn.yaml">with Distributional DQN</a>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Consider using <a class="reference external" href="rllib-algorithms.html#distributed-prioritized-experience-replay-ape-x">Ape-X</a> for faster training with similar timestep efficiency.</p>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>For a complete <a class="reference external" href="https://arxiv.org/pdf/1710.02298.pdf">rainbow</a> setup,
make the following changes to the default DQN config:
<code class="docutils literal notranslate"><span class="pre">&quot;n_step&quot;:</span> <span class="pre">[between</span> <span class="pre">1</span> <span class="pre">and</span> <span class="pre">10],</span>
<span class="pre">&quot;noisy&quot;:</span> <span class="pre">True,</span>
<span class="pre">&quot;num_atoms&quot;:</span> <span class="pre">[more</span> <span class="pre">than</span> <span class="pre">1],</span>
<span class="pre">&quot;v_min&quot;:</span> <span class="pre">-10.0,</span>
<span class="pre">&quot;v_max&quot;:</span> <span class="pre">10.0</span></code>
(set <code class="docutils literal notranslate"><span class="pre">v_min</span></code> and <code class="docutils literal notranslate"><span class="pre">v_max</span></code> according to your expected range of returns).</p>
</div>
<p><strong>Atari results &#64;10M steps</strong>: <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 10%" />
<col style="width: 19%" />
<col style="width: 23%" />
<col style="width: 24%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Atari env</p></th>
<th class="head"><p>RLlib DQN</p></th>
<th class="head"><p>RLlib Dueling DDQN</p></th>
<th class="head"><p>RLlib Dist. DQN</p></th>
<th class="head"><p>Hessel et al. DQN</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BeamRider</p></td>
<td><p>2869</p></td>
<td><p>1910</p></td>
<td><p>4447</p></td>
<td><p>~2000</p></td>
</tr>
<tr class="row-odd"><td><p>Breakout</p></td>
<td><p>287</p></td>
<td><p>312</p></td>
<td><p>410</p></td>
<td><p>~150</p></td>
</tr>
<tr class="row-even"><td><p>Qbert</p></td>
<td><p>3921</p></td>
<td><p>7968</p></td>
<td><p>15780</p></td>
<td><p>~4000</p></td>
</tr>
<tr class="row-odd"><td><p>SpaceInvaders</p></td>
<td><p>650</p></td>
<td><p>1001</p></td>
<td><p>1025</p></td>
<td><p>~500</p></td>
</tr>
</tbody>
</table>
<p><strong>DQN-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.dqn.dqn.DQNConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.dqn.dqn.</span></span><span class="sig-name descname"><span class="pre">DQNConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/dqn/dqn.html#DQNConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.dqn.dqn.DQNConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a DQN Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.dqn.dqn</span> <span class="kn">import</span> <span class="n">DQNConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">DQNConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">replay_buffer_config</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">replay_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">replay_buffer_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span> 
<span class="gp">... </span>    <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;capacity&quot;</span><span class="p">:</span> <span class="mi">60000</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;prioritized_replay_alpha&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;prioritized_replay_beta&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;prioritized_replay_eps&quot;</span><span class="p">:</span> <span class="mf">3e-6</span><span class="p">,</span>
<span class="gp">... </span>    <span class="p">}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">replay_buffer_config</span><span class="o">=</span><span class="n">replay_config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">trainer</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.dqn.dqn</span> <span class="kn">import</span> <span class="n">DQNConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">DQNConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span> 
<span class="gp">... </span>    <span class="n">num_atoms</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">))))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;DQN&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span><span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.dqn.dqn</span> <span class="kn">import</span> <span class="n">DQNConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">DQNConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">exploration_config</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">explore_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">exploration_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span> 
<span class="gp">... </span>    <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;initial_epsilon&quot;</span><span class="p">:</span> <span class="mf">1.5</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;final_epsilon&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;epsilone_timesteps&quot;</span><span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>
<span class="gp">... </span>    <span class="p">}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr_schedule</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="p">[</span><span class="mi">500</span><span class="p">,</span> <span class="mf">5e-3</span><span class="p">]])</span>\ 
<span class="gp">... </span>      <span class="o">.</span><span class="n">exploration</span><span class="p">(</span><span class="n">exploration_config</span><span class="o">=</span><span class="n">explore_config</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.dqn.dqn</span> <span class="kn">import</span> <span class="n">DQNConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">DQNConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">exploration_config</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">explore_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">exploration_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span> 
<span class="gp">... </span>    <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;softq&quot;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">],</span>
<span class="gp">... </span>    <span class="p">}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr_schedule</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="p">[</span><span class="mi">500</span><span class="p">,</span> <span class="mf">5e-3</span><span class="p">]])</span>\ 
<span class="gp">... </span>      <span class="o">.</span><span class="n">exploration</span><span class="p">(</span><span class="n">exploration_config</span><span class="o">=</span><span class="n">explore_config</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.dqn.dqn.DQNConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">*,</span> <span class="pre">num_atoms:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">v_min:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">v_max:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">noisy:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">sigma0:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">dueling:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">hiddens:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">double_q:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">n_step:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">before_learn_on_batch:</span> <span class="pre">Callable[[Type[ray.rllib.policy.sample_batch.MultiAgentBatch],</span> <span class="pre">List[Type[ray.rllib.policy.policy.Policy]],</span> <span class="pre">Type[int]],</span> <span class="pre">Type[ray.rllib.policy.sample_batch.MultiAgentBatch]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">training_intensity:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">td_error_loss_fn:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">categorical_distribution_temperature:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">**kwargs</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.dqn.dqn.DQNConfig" title="ray.rllib.algorithms.dqn.dqn.DQNConfig"><span class="pre">ray.rllib.algorithms.dqn.dqn.DQNConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/dqn/dqn.html#DQNConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.dqn.dqn.DQNConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_atoms</strong>  Number of atoms for representing the distribution of return.
When this is greater than 1, distributional Q-learning is used.</p></li>
<li><p><strong>v_min</strong>  Minimum value estimation</p></li>
<li><p><strong>v_max</strong>  Maximum value estimation</p></li>
<li><p><strong>noisy</strong>  Whether to use noisy network to aid exploration. This adds parametric
noise to the model weights.</p></li>
<li><p><strong>sigma0</strong>  Control the initial parameter noise for noisy nets.</p></li>
<li><p><strong>dueling</strong>  Whether to use dueling DQN.</p></li>
<li><p><strong>hiddens</strong>  Dense-layer setup for each the advantage branch and the value
branch</p></li>
<li><p><strong>double_q</strong>  Whether to use double DQN.</p></li>
<li><p><strong>n_step</strong>  N-step for Q-learning.</p></li>
<li><p><strong>before_learn_on_batch</strong>  Callback to run before learning on a multi-agent
batch of experiences.</p></li>
<li><p><strong>training_intensity</strong>  The intensity with which to update the model (vs
collecting samples from the env).
If None, uses natural values of:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">train_batch_size</span></code> / (<code class="xref py py-obj docutils literal notranslate"><span class="pre">rollout_fragment_length</span></code> x <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_workers</span></code> x
<code class="xref py py-obj docutils literal notranslate"><span class="pre">num_envs_per_worker</span></code>).
If not None, will make sure that the ratio between timesteps inserted
into and sampled from the buffer matches the given values.
Example:
training_intensity=1000.0
train_batch_size=250
rollout_fragment_length=1
num_workers=1 (or 0)
num_envs_per_worker=1
-&gt; natural value = 250 / 1 = 250.0
-&gt; will make sure that replay+train op will be executed 4x asoften as
rollout+insert op (4 * 250 = 1000).
See: rllib/algorithms/dqn/dqn.py::calculate_rr_weights for further
details.</p></li>
<li><p><strong>replay_buffer_config</strong>  Replay buffer config.
Examples:
{
_enable_replay_buffer_api: True,
type: MultiAgentReplayBuffer,
capacity: 50000,
replay_sequence_length: 1,
}
- OR -
{
_enable_replay_buffer_api: True,
type: MultiAgentPrioritizedReplayBuffer,
capacity: 50000,
prioritized_replay_alpha: 0.6,
prioritized_replay_beta: 0.4,
prioritized_replay_eps: 1e-6,
replay_sequence_length: 1,
}
- Where -
prioritized_replay_alpha: Alpha parameter controls the degree of
prioritization in the buffer. In other words, when a buffer sample has
a higher temporal-difference error, with how much more probability
should it drawn to use to update the parametrized Q-network. 0.0
corresponds to uniform probability. Setting much above 1.0 may quickly
result as the sampling distribution could become heavily pointy with
low entropy.
prioritized_replay_beta: Beta parameter controls the degree of
importance sampling which suppresses the influence of gradient updates
from samples that have higher probability of being sampled via alpha
parameter and the temporal-difference error.
prioritized_replay_eps: Epsilon parameter sets the baseline probability
for sampling so that when the temporal-difference error of a sample is
zero, there is still a chance of drawing the sample.</p></li>
<li><p><strong>td_error_loss_fn</strong>  huber or mse. loss function for calculating TD error
when num_atoms is 1. Note that if num_atoms is &gt; 1, this parameter
is simply ignored, and softmax cross entropy loss will be used.</p></li>
<li><p><strong>categorical_distribution_temperature</strong>  Set the temperature parameter used
by Categorical action distribution. A valid temperature is in the range
of [0, 1]. Note that this mostly affects evaluation since TD error uses
argmax for return calculation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="deep-deterministic-policy-gradients-ddpg">
<span id="ddpg"></span><h3>Deep Deterministic Policy Gradients (DDPG)<a class="headerlink" href="rllib-algorithms.html#deep-deterministic-policy-gradients-ddpg" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1509.02971">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/ddpg/ddpg.py">[implementation]</a>
DDPG is implemented similarly to DQN (below). The algorithm can be scaled by increasing the number of workers or using Ape-X.
The improvements from <a class="reference external" href="https://spinningup.openai.com/en/latest/algorithms/td3.html">TD3</a> are available as <code class="docutils literal notranslate"><span class="pre">TD3</span></code>.</p>
<figure class="align-default" id="id14">
<img alt="../_images/dqn-arch.svg" src="../_images/dqn-arch.svg" /><figcaption>
<p><span class="caption-text">DDPG architecture (same as DQN)</span><a class="headerlink" href="rllib-algorithms.html#id14" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ddpg/pendulum-ddpg.yaml">Pendulum-v1</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ddpg/mountaincarcontinuous-ddpg.yaml">MountainCarContinuous-v0</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ddpg/halfcheetah-ddpg.yaml">HalfCheetah-v2</a>.</p>
<p><strong>DDPG-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.ddpg.ddpg.DDPGConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.ddpg.ddpg.</span></span><span class="sig-name descname"><span class="pre">DDPGConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/ddpg/ddpg.html#DDPGConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.ddpg.ddpg.DDPGConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a DDPG Trainer can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ddpg.ddpg</span> <span class="kn">import</span> <span class="n">DDPGConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">DDPGConfig</span><span class="p">()</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Trainer object from the config and run one training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;Pendulum-v1&quot;</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ddpg.ddpg</span> <span class="kn">import</span> <span class="n">DDPGConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">DDPGConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span> 
<span class="go">0.0004</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;Pendulum-v1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;DDPG&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.ddpg.ddpg.DDPGConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">twin_q:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_delay:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smooth_target_policy:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_noise:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_noise_clip:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_state_preprocessor:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_hiddens:</span> <span class="pre">Optional[List[int]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_hidden_activation:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic_hiddens:</span> <span class="pre">Optional[List[int]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic_hidden_activation:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_step:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic_lr:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_lr:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_huber:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">huber_threshold:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l2_reg:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_intensity:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.ddpg.ddpg.DDPGConfig" title="ray.rllib.algorithms.ddpg.ddpg.DDPGConfig"><span class="pre">ray.rllib.algorithms.ddpg.ddpg.DDPGConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/ddpg/ddpg.html#DDPGConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.ddpg.ddpg.DDPGConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<p>=== Twin Delayed DDPG (TD3) and Soft Actor-Critic (SAC) tricks ===
TD3: <a class="reference external" href="https://spinningup.openai.com/en/latest/algorithms/td3.html">https://spinningup.openai.com/en/latest/algorithms/td3.html</a>
In addition to settings below, you can use exploration_noise_type and
exploration_gauss_act_noise to get IID Gaussian exploration noise
instead of OrnsteinUhlenbeck exploration noise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>twin_q</strong>  Use twin Q-net.</p></li>
<li><p><strong>policy_delay</strong>  Delayed policy update.</p></li>
<li><p><strong>smooth_target_policy</strong>  Target policy smoothing (this also replaces
OrnsteinUhlenbeck exploration noise with IID Gaussian exploration
noise, for now).</p></li>
<li><p><strong>target_noise</strong>  Gaussian stddev of target action noise for smoothing.</p></li>
<li><p><strong>target_noise_clip</strong>  Target noise limit (bound).</p></li>
<li><p><strong>use_state_preprocessor</strong>  Apply a state preprocessor with spec given by the
model config option
(like other RL algorithms). This is mostly useful if you have a weird
observation shape, like an image. Disabled by default.</p></li>
<li><p><strong>actor_hiddens</strong>  Postprocess the policy network model output with these
hidden layers. If use_state_preprocessor is False, then these will
be the <em>only</em> hidden layers in the network.</p></li>
<li><p><strong>actor_hidden_activation</strong>  Hidden layers activation of the postprocessing
stage of the policy network</p></li>
<li><p><strong>critic_hiddens</strong>  Postprocess the critic network model output with these
hidden layers; again, if use_state_preprocessor is True, then the
state will be preprocessed by the model specified with the model
config option first.</p></li>
<li><p><strong>critic_hidden_activation</strong>  Hidden layers activation of the postprocessing
state of the critic.</p></li>
<li><p><strong>n_step</strong>  N-step Q learning</p></li>
<li><p><strong>critic_lr</strong>  Learning rate for the critic (Q-function) optimizer.</p></li>
<li><p><strong>actor_lr</strong>  Learning rate for the actor (policy) optimizer.</p></li>
<li><p><strong>tau</strong>  Update the target by   au * policy + (1-       au) * target_policy</p></li>
<li><p><strong>use_huber</strong>  Conventionally, no need to clip gradients if using a huber loss</p></li>
<li><p><strong>huber_threshold</strong>  Threshold of a huber loss</p></li>
<li><p><strong>l2_reg</strong>  Weights for L2 regularization</p></li>
<li><p><strong>training_intensity</strong>  <p>The intensity with which to update the model
(vs collecting samples from
the env). If None, uses the natural value of:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">train_batch_size</span></code> / (<code class="xref py py-obj docutils literal notranslate"><span class="pre">rollout_fragment_length</span></code> x <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_workers</span></code> x
<code class="xref py py-obj docutils literal notranslate"><span class="pre">num_envs_per_worker</span></code>).
If provided, will make sure that the ratio between ts inserted into and
sampled from the buffer matches the given value.
.. rubric:: Example</p>
<p>training_intensity=1000.0
train_batch_size=250 rollout_fragment_length=1
num_workers=1 (or 0) num_envs_per_worker=1
-&gt; natural value = 250 / 1 = 250.0
-&gt; will make sure that replay+train op will be executed 4x as
often as rollout+insert op (4 * 250 = 1000).</p>
<p>See: rllib/algorithms/dqn/dqn.py::calculate_rr_weights for further
details.</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated DDPGConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="twin-delayed-ddpg-td3">
<span id="td3"></span><h3>Twin Delayed DDPG (TD3)<a class="headerlink" href="rllib-algorithms.html#twin-delayed-ddpg-td3" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1509.02971">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/td3/td3.py">[implementation]</a>
TD3 represents an improvement over DDPG. Its implementation is available in RLlib as <a class="reference external" href="https://spinningup.openai.com/en/latest/algorithms/td3.html">TD3</a>.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/td3/pendulum-td3.yaml">TD3 Pendulum-v1</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/td3/invertedpendulum-td3.yaml">TD3 InvertedPendulum-v2</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/td3/mujoco-td3.yaml">TD3 Mujoco suite (Ant-v2, HalfCheetah-v2, Hopper-v2, Walker2d-v2)</a>.</p>
<p><strong>TD3-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.td3.td3.TD3Config">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.td3.td3.</span></span><span class="sig-name descname"><span class="pre">TD3Config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/td3/td3.html#TD3Config"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.td3.td3.TD3Config" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a TD3 Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.td3</span> <span class="kn">import</span> <span class="n">TD3Config</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">TD3Config</span><span class="p">()</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run one training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;Pendulum-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.td3</span> <span class="kn">import</span> <span class="n">TD3Config</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">TD3Config</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]))</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;Pendulum-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;TD3&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.td3.td3.TD3Config.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">twin_q:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_delay:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smooth_target_policy:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_noise:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_noise_clip:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_state_preprocessor:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_hiddens:</span> <span class="pre">Optional[List[int]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_hidden_activation:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic_hiddens:</span> <span class="pre">Optional[List[int]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic_hidden_activation:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_step:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic_lr:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_lr:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_huber:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">huber_threshold:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l2_reg:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_intensity:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.ddpg.ddpg.DDPGConfig" title="ray.rllib.algorithms.ddpg.ddpg.DDPGConfig"><span class="pre">ray.rllib.algorithms.ddpg.ddpg.DDPGConfig</span></a></span></span><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.td3.td3.TD3Config.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<p>=== Twin Delayed DDPG (TD3) and Soft Actor-Critic (SAC) tricks ===
TD3: <a class="reference external" href="https://spinningup.openai.com/en/latest/algorithms/td3.html">https://spinningup.openai.com/en/latest/algorithms/td3.html</a>
In addition to settings below, you can use exploration_noise_type and
exploration_gauss_act_noise to get IID Gaussian exploration noise
instead of OrnsteinUhlenbeck exploration noise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>twin_q</strong>  Use twin Q-net.</p></li>
<li><p><strong>policy_delay</strong>  Delayed policy update.</p></li>
<li><p><strong>smooth_target_policy</strong>  Target policy smoothing (this also replaces
OrnsteinUhlenbeck exploration noise with IID Gaussian exploration
noise, for now).</p></li>
<li><p><strong>target_noise</strong>  Gaussian stddev of target action noise for smoothing.</p></li>
<li><p><strong>target_noise_clip</strong>  Target noise limit (bound).</p></li>
<li><p><strong>use_state_preprocessor</strong>  Apply a state preprocessor with spec given by the
model config option
(like other RL algorithms). This is mostly useful if you have a weird
observation shape, like an image. Disabled by default.</p></li>
<li><p><strong>actor_hiddens</strong>  Postprocess the policy network model output with these
hidden layers. If use_state_preprocessor is False, then these will
be the <em>only</em> hidden layers in the network.</p></li>
<li><p><strong>actor_hidden_activation</strong>  Hidden layers activation of the postprocessing
stage of the policy network</p></li>
<li><p><strong>critic_hiddens</strong>  Postprocess the critic network model output with these
hidden layers; again, if use_state_preprocessor is True, then the
state will be preprocessed by the model specified with the model
config option first.</p></li>
<li><p><strong>critic_hidden_activation</strong>  Hidden layers activation of the postprocessing
state of the critic.</p></li>
<li><p><strong>n_step</strong>  N-step Q learning</p></li>
<li><p><strong>critic_lr</strong>  Learning rate for the critic (Q-function) optimizer.</p></li>
<li><p><strong>actor_lr</strong>  Learning rate for the actor (policy) optimizer.</p></li>
<li><p><strong>tau</strong>  Update the target by   au * policy + (1-       au) * target_policy</p></li>
<li><p><strong>use_huber</strong>  Conventionally, no need to clip gradients if using a huber loss</p></li>
<li><p><strong>huber_threshold</strong>  Threshold of a huber loss</p></li>
<li><p><strong>l2_reg</strong>  Weights for L2 regularization</p></li>
<li><p><strong>training_intensity</strong>  <p>The intensity with which to update the model
(vs collecting samples from
the env). If None, uses the natural value of:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">train_batch_size</span></code> / (<code class="xref py py-obj docutils literal notranslate"><span class="pre">rollout_fragment_length</span></code> x <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_workers</span></code> x
<code class="xref py py-obj docutils literal notranslate"><span class="pre">num_envs_per_worker</span></code>).
If provided, will make sure that the ratio between ts inserted into and
sampled from the buffer matches the given value.
.. rubric:: Example</p>
<p>training_intensity=1000.0
train_batch_size=250 rollout_fragment_length=1
num_workers=1 (or 0) num_envs_per_worker=1
-&gt; natural value = 250 / 1 = 250.0
-&gt; will make sure that replay+train op will be executed 4x as
often as rollout+insert op (4 * 250 = 1000).</p>
<p>See: rllib/algorithms/dqn/dqn.py::calculate_rr_weights for further
details.</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated DDPGConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="soft-actor-critic-sac">
<span id="sac"></span><h3>Soft Actor Critic (SAC)<a class="headerlink" href="rllib-algorithms.html#soft-actor-critic-sac" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/pdf/1801.01290">[original paper]</a>, <a class="reference external" href="https://arxiv.org/pdf/1812.05905.pdf">[follow up paper]</a>, <a class="reference external" href="https://arxiv.org/pdf/1910.07207v2.pdf">[discrete actions paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/sac/sac.py">[implementation]</a></p>
<figure class="align-default" id="id15">
<img alt="../_images/dqn-arch.svg" src="../_images/dqn-arch.svg" /><figcaption>
<p><span class="caption-text">SAC architecture (same as DQN)</span><a class="headerlink" href="rllib-algorithms.html#id15" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>RLlibs soft-actor critic implementation is ported from the <a class="reference external" href="https://github.com/rail-berkeley/softlearning">official SAC repo</a> to better integrate with RLlib APIs.
Note that SAC has two fields to configure for custom models: <code class="docutils literal notranslate"><span class="pre">policy_model_config</span></code> and <code class="docutils literal notranslate"><span class="pre">q_model_config</span></code>, the <code class="docutils literal notranslate"><span class="pre">model</span></code> field of the config will be ignored.</p>
<p>Tuned examples (continuous actions):
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/sac/pendulum-sac.yaml">Pendulum-v1</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/sac/halfcheetah-sac.yaml">HalfCheetah-v3</a>,
Tuned examples (discrete actions):
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/sac/cartpole-sac.yaml">CartPole-v1</a></p>
<p><strong>MuJoCo results &#64;3M steps:</strong> <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 31%" />
<col style="width: 24%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>MuJoCo env</p></th>
<th class="head"><p>RLlib SAC</p></th>
<th class="head"><p>Haarnoja et al SAC</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>HalfCheetah</p></td>
<td><p>13000</p></td>
<td><p>~15000</p></td>
</tr>
</tbody>
</table>
<p><strong>SAC-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.sac.sac.SACConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.sac.sac.</span></span><span class="sig-name descname"><span class="pre">SACConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/sac/sac.html#SACConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.sac.sac.SACConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which an SAC Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">SACConfig</span><span class="p">()</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.sac.sac.SACConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">twin_q:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_model_config:</span> <span class="pre">Optional[Dict[str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Any]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_model_config:</span> <span class="pre">Optional[Dict[str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Any]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_alpha:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_entropy:</span> <span class="pre">Optional[Union[str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_step:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">store_buffer_in_checkpoints:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replay_buffer_config:</span> <span class="pre">Optional[Dict[str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Any]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_intensity:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_actions:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimization_config:</span> <span class="pre">Optional[Dict[str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Any]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_network_update_freq:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_deterministic_loss:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_use_beta_distribution:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_steps_sampled_before_learning_starts:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.sac.sac.SACConfig" title="ray.rllib.algorithms.sac.sac.SACConfig"><span class="pre">ray.rllib.algorithms.sac.sac.SACConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/sac/sac.html#SACConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.sac.sac.SACConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>twin_q</strong>  Use two Q-networks (instead of one) for action-value estimation.
Note: Each Q-network will have its own target network.</p></li>
<li><p><strong>q_model_config</strong>  Model configs for the Q network(s). These will override
MODEL_DEFAULTS. This is treated just as the top-level <code class="xref py py-obj docutils literal notranslate"><span class="pre">model</span></code> dict in
setting up the Q-network(s) (2 if twin_q=True).
That means, you can do for different observation spaces:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">obs=Box(1D)</span></code> -&gt; <code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple(Box(1D)</span> <span class="pre">+</span> <span class="pre">Action)</span></code> -&gt; <code class="xref py py-obj docutils literal notranslate"><span class="pre">concat</span></code> -&gt; <code class="xref py py-obj docutils literal notranslate"><span class="pre">post_fcnet</span></code>
obs=Box(3D) -&gt; Tuple(Box(3D) + Action) -&gt; vision-net -&gt; concat w/ action
-&gt; post_fcnet
obs=Tuple(Box(1D), Box(3D)) -&gt; Tuple(Box(1D), Box(3D), Action)
-&gt; vision-net -&gt; concat w/ Box(1D) and action -&gt; post_fcnet
You can also have SAC use your custom_model as Q-model(s), by simply
specifying the <code class="xref py py-obj docutils literal notranslate"><span class="pre">custom_model</span></code> sub-key in below dict (just like you would
do in the top-level <code class="xref py py-obj docutils literal notranslate"><span class="pre">model</span></code> dict.</p></li>
<li><p><strong>policy_model_config</strong>  Model options for the policy function (see
<code class="xref py py-obj docutils literal notranslate"><span class="pre">q_model_config</span></code> above for details). The difference to <code class="xref py py-obj docutils literal notranslate"><span class="pre">q_model_config</span></code>
above is that no action concating is performed before the post_fcnet
stack.</p></li>
<li><p><strong>tau</strong>  Update the target by   au * policy + (1-       au) * target_policy.</p></li>
<li><p><strong>initial_alpha</strong>  Initial value to use for the entropy weight alpha.</p></li>
<li><p><strong>target_entropy</strong>  Target entropy lower bound. If auto, will be set
to <code class="xref py py-obj docutils literal notranslate"><span class="pre">-|A|</span></code> (e.g. -2.0 for Discrete(2), -3.0 for Box(shape=(3,))).
This is the inverse of reward scale, and will be optimized
automatically.</p></li>
<li><p><strong>n_step</strong>  N-step target updates. If &gt;1, sars tuples in trajectories will be
postprocessed to become sa[discounted sum of R][s t+n] tuples.</p></li>
<li><p><strong>store_buffer_in_checkpoints</strong>  Set this to True, if you want the contents of
your buffer(s) to be stored in any saved checkpoints as well.
Warnings will be created if:
- This is True AND restoring from a checkpoint that contains no buffer
data.
- This is False AND restoring from a checkpoint that does contain
buffer data.</p></li>
<li><p><strong>replay_buffer_config</strong>  Replay buffer config.
Examples:
{
_enable_replay_buffer_api: True,
type: MultiAgentReplayBuffer,
capacity: 50000,
replay_batch_size: 32,
replay_sequence_length: 1,
}
- OR -
{
_enable_replay_buffer_api: True,
type: MultiAgentPrioritizedReplayBuffer,
capacity: 50000,
prioritized_replay_alpha: 0.6,
prioritized_replay_beta: 0.4,
prioritized_replay_eps: 1e-6,
replay_sequence_length: 1,
}
- Where -
prioritized_replay_alpha: Alpha parameter controls the degree of
prioritization in the buffer. In other words, when a buffer sample has
a higher temporal-difference error, with how much more probability
should it drawn to use to update the parametrized Q-network. 0.0
corresponds to uniform probability. Setting much above 1.0 may quickly
result as the sampling distribution could become heavily pointy with
low entropy.
prioritized_replay_beta: Beta parameter controls the degree of
importance sampling which suppresses the influence of gradient updates
from samples that have higher probability of being sampled via alpha
parameter and the temporal-difference error.
prioritized_replay_eps: Epsilon parameter sets the baseline probability
for sampling so that when the temporal-difference error of a sample is
zero, there is still a chance of drawing the sample.</p></li>
<li><p><strong>training_intensity</strong>  The intensity with which to update the model (vs
collecting samples from the env).
If None, uses natural values of:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">train_batch_size</span></code> / (<code class="xref py py-obj docutils literal notranslate"><span class="pre">rollout_fragment_length</span></code> x <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_workers</span></code> x
<code class="xref py py-obj docutils literal notranslate"><span class="pre">num_envs_per_worker</span></code>).
If not None, will make sure that the ratio between timesteps inserted
into and sampled from th buffer matches the given values.
Example:
training_intensity=1000.0
train_batch_size=250
rollout_fragment_length=1
num_workers=1 (or 0)
num_envs_per_worker=1
-&gt; natural value = 250 / 1 = 250.0
-&gt; will make sure that replay+train op will be executed 4x asoften as
rollout+insert op (4 * 250 = 1000).
See: rllib/algorithms/dqn/dqn.py::calculate_rr_weights for further
details.</p></li>
<li><p><strong>clip_actions</strong>  Whether to clip actions. If actions are already normalized,
this should be set to False.</p></li>
<li><p><strong>grad_clip</strong>  If not None, clip gradients during optimization at this value.</p></li>
<li><p><strong>optimization_config</strong>  Config dict for optimization. Set the supported keys
<code class="xref py py-obj docutils literal notranslate"><span class="pre">actor_learning_rate</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">critic_learning_rate</span></code>, and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">entropy_learning_rate</span></code> in here.</p></li>
<li><p><strong>target_network_update_freq</strong>  Update the target network every
<code class="xref py py-obj docutils literal notranslate"><span class="pre">target_network_update_freq</span></code> steps.</p></li>
<li><p><strong>_deterministic_loss</strong>  Whether the loss should be calculated deterministically
(w/o the stochastic action sampling step). True only useful for
continuous actions and for debugging.</p></li>
<li><p><strong>_use_beta_distribution</strong>  Use a Beta-distribution instead of a
<code class="xref py py-obj docutils literal notranslate"><span class="pre">SquashedGaussian</span></code> for bounded, continuous action spaces (not
recommended; for debugging only).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="model-based-rl">
<h2>Model-based RL<a class="headerlink" href="rllib-algorithms.html#model-based-rl" title="Permalink to this headline">#</a></h2>
<section id="dreamer">
<span id="id1"></span><h3>Dreamer<a class="headerlink" href="rllib-algorithms.html#dreamer" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1912.01603">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/dreamer/dreamer.py">[implementation]</a></p>
<p>Dreamer is an image-only model-based RL method that learns by imagining trajectories in the future and is evaluated on the DeepMind Control Suite <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/env/dm_control_suite.py">environments</a>. RLlibs Dreamer is adapted from the <a class="reference external" href="https://github.com/google-research/dreamer">official Google research repo</a>.</p>
<p>To visualize learning, RLlib Dreamers imagined trajectories are logged as gifs in TensorBoard. Examples of such can be seen <a class="reference external" href="https://github.com/ray-project/rl-experiments">here</a>.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/dreamer/dreamer-deepmind-control.yaml">Deepmind Control Environments</a></p>
<p><strong>Deepmind Control results &#64;1M steps:</strong> <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 27%" />
<col style="width: 29%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>DMC env</p></th>
<th class="head"><p>RLlib Dreamer</p></th>
<th class="head"><p>Danijar et al Dreamer</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Walker-Walk</p></td>
<td><p>920</p></td>
<td><p>~930</p></td>
</tr>
<tr class="row-odd"><td><p>Cheetah-Run</p></td>
<td><p>640</p></td>
<td><p>~800</p></td>
</tr>
</tbody>
</table>
<p><strong>Dreamer-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.dreamer.dreamer.DreamerConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.dreamer.dreamer.</span></span><span class="sig-name descname"><span class="pre">DreamerConfig</span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/dreamer/dreamer.html#DreamerConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.dreamer.dreamer.DreamerConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a Dreamer Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.dreamer</span> <span class="kn">import</span> <span class="n">DreamerConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">DreamerConfig</span><span class="p">()</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.dreamer</span> <span class="kn">import</span> <span class="n">DreamerConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">DreamerConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">clip_param</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]),</span> <span class="n">clip_param</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;Dreamer&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.dreamer.dreamer.DreamerConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">td_model_lr:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_lr:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic_lr:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dreamer_train_iters:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_length:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">imagine_horizon:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">free_nats:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kl_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_timesteps:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">explore_noise:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dreamer_model:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_steps_sampled_before_learning_starts:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.dreamer.dreamer.DreamerConfig" title="ray.rllib.algorithms.dreamer.dreamer.DreamerConfig"><span class="pre">ray.rllib.algorithms.dreamer.dreamer.DreamerConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/dreamer/dreamer.html#DreamerConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.dreamer.dreamer.DreamerConfig.training" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>td_model_lr</strong>  PlaNET (transition dynamics) model learning rate.</p></li>
<li><p><strong>actor_lr</strong>  Actor model learning rate.</p></li>
<li><p><strong>critic_lr</strong>  Critic model learning rate.</p></li>
<li><p><strong>grad_clip</strong>  If specified, clip the global norm of gradients by this amount.</p></li>
<li><p><strong>lambda</strong>  The GAE (lambda) parameter.</p></li>
<li><p><strong>dreamer_train_iters</strong>  Training iterations per data collection from real env.</p></li>
<li><p><strong>batch_size</strong>  Number of episodes to sample for loss calculation.</p></li>
<li><p><strong>batch_length</strong>  Length of each episode to sample for loss calculation.</p></li>
<li><p><strong>imagine_horizon</strong>  Imagination horizon for training Actor and Critic.</p></li>
<li><p><strong>free_nats</strong>  Free nats.</p></li>
<li><p><strong>kl_coeff</strong>  KL coefficient for the model Loss.</p></li>
<li><p><strong>prefill_timesteps</strong>  Prefill timesteps.</p></li>
<li><p><strong>explore_noise</strong>  Exploration Gaussian noise.</p></li>
<li><p><strong>dreamer_model</strong>  Custom model config.</p></li>
<li><p><strong>num_steps_sampled_before_learning_starts</strong>  Number of timesteps to collect
from rollout workers before we start sampling from replay buffers for
learning. Whether we count this in agent steps  or environment steps
depends on config.multi_agent(count_steps_by=..).</p></li>
</ul>
</dd>
</dl>
<p>Returns:</p>
</dd></dl>

</dd></dl>

</section>
<section id="model-based-meta-policy-optimization-mb-mpo">
<span id="mbmpo"></span><h3>Model-Based Meta-Policy-Optimization (MB-MPO)<a class="headerlink" href="rllib-algorithms.html#model-based-meta-policy-optimization-mb-mpo" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/pdf/1809.05214.pdf">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/mbmpo/mbmpo.py">[implementation]</a></p>
<p>RLlibs MBMPO implementation is a Dyna-styled model-based RL method that learns based on the predictions of an ensemble of transition-dynamics models. Similar to MAML, MBMPO metalearns an optimal policy by treating each dynamics model as a different task. Code here is adapted from <a class="reference external" href="https://github.com/jonasrothfuss/model_ensemble_meta_learning">https://github.com/jonasrothfuss/model_ensemble_meta_learning</a>. Similar to the original paper, MBMPO is evaluated on MuJoCo, with the horizon set to 200 instead of the default 1000.</p>
<p>Additional statistics are logged in MBMPO. Each MBMPO iteration corresponds to multiple MAML iterations, and <code class="docutils literal notranslate"><span class="pre">MAMLIter$i$_DynaTrajInner_$j$_episode_reward_mean</span></code> measures the agents returns across the dynamics models at iteration <code class="docutils literal notranslate"><span class="pre">i</span></code> of MAML and step <code class="docutils literal notranslate"><span class="pre">j</span></code> of inner adaptation. Examples can be seen <a class="reference external" href="https://github.com/ray-project/rl-experiments/tree/master/mbmpo">here</a>.</p>
<p>Tuned examples (continuous actions):
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/mbmpo/pendulum-mbmpo.yaml">Pendulum-v1</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/mbmpo/halfcheetah-mbmpo.yaml">HalfCheetah</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/mbmpo/hopper-mbmpo.yaml">Hopper</a>,
Tuned examples (discrete actions):
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/mbmpo/cartpole-mbmpo.yaml">CartPole-v1</a></p>
<p><strong>MuJoCo results &#64;100K steps:</strong> <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 29%" />
<col style="width: 27%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>MuJoCo env</p></th>
<th class="head"><p>RLlib MBMPO</p></th>
<th class="head"><p>Clavera et al MBMPO</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>HalfCheetah</p></td>
<td><p>520</p></td>
<td><p>~550</p></td>
</tr>
<tr class="row-odd"><td><p>Hopper</p></td>
<td><p>620</p></td>
<td><p>~650</p></td>
</tr>
</tbody>
</table>
<p><strong>MBMPO-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.mbmpo.mbmpo.MBMPOConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.mbmpo.mbmpo.</span></span><span class="sig-name descname"><span class="pre">MBMPOConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/mbmpo/mbmpo.html#MBMPOConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.mbmpo.mbmpo.MBMPOConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which an MBMPO Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.mbmpo</span> <span class="kn">import</span> <span class="n">MBMPOConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">MBMPOConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0003</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.mbmpo</span> <span class="kn">import</span> <span class="n">MBMPOConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">MBMPOConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vtrace</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>\  
<span class="gp">... </span>    <span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.0003</span><span class="p">]),</span> <span class="n">grad_clip</span><span class="o">=</span><span class="mf">20.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;AlphaStar&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.mbmpo.mbmpo.MBMPOConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_gae:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kl_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_loss_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entropy_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_param:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_clip_param:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kl_target:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inner_adaptation_steps:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maml_optimizer_steps:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inner_lr:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dynamics_model:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_vector_env:</span> <span class="pre">Optional[type]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_maml_steps:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.mbmpo.mbmpo.MBMPOConfig" title="ray.rllib.algorithms.mbmpo.mbmpo.MBMPOConfig"><span class="pre">ray.rllib.algorithms.mbmpo.mbmpo.MBMPOConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/mbmpo/mbmpo.html#MBMPOConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.mbmpo.mbmpo.MBMPOConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_gae</strong>  If true, use the Generalized Advantage Estimator (GAE)
with a value function, see <a class="reference external" href="https://arxiv.org/pdf/1506.02438.pdf">https://arxiv.org/pdf/1506.02438.pdf</a>.</p></li>
<li><p><strong>lambda</strong>  The GAE (lambda) parameter.</p></li>
<li><p><strong>kl_coeff</strong>  Initial coefficient for KL divergence.</p></li>
<li><p><strong>vf_loss_coeff</strong>  Coefficient of the value function loss.</p></li>
<li><p><strong>entropy_coeff</strong>  Coefficient of the entropy regularizer.</p></li>
<li><p><strong>clip_param</strong>  PPO clip parameter.</p></li>
<li><p><strong>vf_clip_param</strong>  Clip param for the value function. Note that this is
sensitive to the scale of the rewards. If your expected V is large,
increase this.</p></li>
<li><p><strong>grad_clip</strong>  If specified, clip the global norm of gradients by this amount.</p></li>
<li><p><strong>kl_target</strong>  Target value for KL divergence.</p></li>
<li><p><strong>inner_adaptation_steps</strong>  Number of Inner adaptation steps for the MAML
algorithm.</p></li>
<li><p><strong>maml_optimizer_steps</strong>  Number of MAML steps per meta-update iteration
(PPO steps).</p></li>
<li><p><strong>inner_lr</strong>  Inner adaptation step size.</p></li>
<li><p><strong>dynamics_model</strong>  Dynamics ensemble hyperparameters.</p></li>
<li><p><strong>custom_vector_env</strong>  Workers sample from dynamics models, not from actual
envs.</p></li>
<li><p><strong>num_maml_steps</strong>  How many iterations through MAML per MBMPO iteration.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="derivative-free">
<h2>Derivative-free<a class="headerlink" href="rllib-algorithms.html#derivative-free" title="Permalink to this headline">#</a></h2>
<section id="augmented-random-search-ars">
<span id="ars"></span><h3>Augmented Random Search (ARS)<a class="headerlink" href="rllib-algorithms.html#augmented-random-search-ars" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1803.07055">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/ars/ars.py">[implementation]</a>
ARS is a random search method for training linear policies for continuous control problems. Code here is adapted from <a class="reference external" href="https://github.com/modestyachts/ARS">https://github.com/modestyachts/ARS</a> to integrate with RLlib APIs.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ars/cartpole-ars.yaml">CartPole-v1</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ars/swimmer-ars.yaml">Swimmer-v2</a></p>
<p><strong>ARS-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.ars.ars.ARSConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.ars.ars.</span></span><span class="sig-name descname"><span class="pre">ARSConfig</span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/ars/ars.html#ARSConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.ars.ars.ARSConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which an ARS Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ars</span> <span class="kn">import</span> <span class="n">ARSConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">ARSConfig</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">report_length</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ars</span> <span class="kn">import</span> <span class="n">ARSConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">ARSConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">action_noise_std</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="n">rollouts_used</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">]),</span> <span class="n">eval_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;ARS&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.ars.ars.ARSConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_noise_std:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">noise_stdev:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_rollouts:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rollouts_used:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sgd_stepsize:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">noise_size:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_prob:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">report_length:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offset:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tf_single_threaded:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.ars.ars.ARSConfig" title="ray.rllib.algorithms.ars.ars.ARSConfig"><span class="pre">ray.rllib.algorithms.ars.ars.ARSConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/ars/ars.html#ARSConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.ars.ars.ARSConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>action_noise_std</strong>  Std. deviation to be used when adding (standard normal)
noise to computed actions. Action noise is only added, if
<code class="xref py py-obj docutils literal notranslate"><span class="pre">compute_actions</span></code> is called with the <code class="xref py py-obj docutils literal notranslate"><span class="pre">add_noise</span></code> arg set to True.</p></li>
<li><p><strong>noise_stdev</strong>  Std. deviation of parameter noise.</p></li>
<li><p><strong>num_rollouts</strong>  Number of perturbs to try.</p></li>
<li><p><strong>rollouts_used</strong>  Number of perturbs to keep in gradient estimate.</p></li>
<li><p><strong>sgd_stepsize</strong>  SGD step-size used for the Adam optimizer.</p></li>
<li><p><strong>noise_size</strong>  Number of rows in the noise table (shared across workers).
Each row contains a gaussian noise value for each model parameter.</p></li>
<li><p><strong>eval_prob</strong>  Probability of evaluating the parameter rewards.</p></li>
<li><p><strong>report_length</strong>  How many of the last rewards we average over.</p></li>
<li><p><strong>offset</strong>  Value to subtract from the reward (e.g. survival bonus
from humanoid) during rollouts.</p></li>
<li><p><strong>tf_single_threaded</strong>  Whether the tf-session should be generated without any
parallelism options.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="evolution-strategies-es">
<span id="es"></span><h3>Evolution Strategies (ES)<a class="headerlink" href="rllib-algorithms.html#evolution-strategies-es" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1703.03864">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/es/es.py">[implementation]</a>
Code here is adapted from <a class="reference external" href="https://github.com/openai/evolution-strategies-starter">https://github.com/openai/evolution-strategies-starter</a> to execute in the distributed setting with Ray.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/es/humanoid-es.yaml">Humanoid-v1</a></p>
<p><strong>Scalability:</strong></p>
<figure class="align-default" id="id16">
<a class="reference internal image-reference" href="../_images/es.png"><img alt="../_images/es.png" src="../_images/es.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-text">RLlibs ES implementation scales further and is faster than a reference Redis implementation on solving the Humanoid-v1 task.</span><a class="headerlink" href="rllib-algorithms.html#id16" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong>ES-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.es.es.ESConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.es.es.</span></span><span class="sig-name descname"><span class="pre">ESConfig</span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/es/es.html#ESConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.es.es.ESConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which an ES Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.es</span> <span class="kn">import</span> <span class="n">ESConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">ESConfig</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">sgd_stepsize</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">report_length</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.es</span> <span class="kn">import</span> <span class="n">ESConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">ESConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">action_noise_std</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="n">rollouts_used</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">]),</span> <span class="n">eval_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;ES&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">ray</span><span class="o">.</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.es.es.ESConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_noise_std:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l2_coeff:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">noise_stdev:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">episodes_per_batch:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_prob:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stepsize:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">noise_size:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">report_length:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tf_single_threaded:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.es.es.ESConfig" title="ray.rllib.algorithms.es.es.ESConfig"><span class="pre">ray.rllib.algorithms.es.es.ESConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/es/es.html#ESConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.es.es.ESConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>action_noise_std</strong>  Std. deviation to be used when adding (standard normal)
noise to computed actions. Action noise is only added, if
<code class="xref py py-obj docutils literal notranslate"><span class="pre">compute_actions</span></code> is called with the <code class="xref py py-obj docutils literal notranslate"><span class="pre">add_noise</span></code> arg set to True.</p></li>
<li><p><strong>l2_coeff</strong>  Coefficient to multiply current weights with inside the globalg
optimizer update term.</p></li>
<li><p><strong>noise_stdev</strong>  Std. deviation of parameter noise.</p></li>
<li><p><strong>episodes_per_batch</strong>  Minimum number of episodes to pack into the train batch.</p></li>
<li><p><strong>eval_prob</strong>  Probability of evaluating the parameter rewards.</p></li>
<li><p><strong>stepsize</strong>  SGD step-size used for the Adam optimizer.</p></li>
<li><p><strong>noise_size</strong>  Number of rows in the noise table (shared across workers).
Each row contains a gaussian noise value for each model parameter.</p></li>
<li><p><strong>report_length</strong>  How many of the last rewards we average over.</p></li>
<li><p><strong>tf_single_threaded</strong>  Whether the tf-session should be generated without any
parallelism options.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="rl-for-recommender-systems">
<h2>RL for recommender systems<a class="headerlink" href="rllib-algorithms.html#rl-for-recommender-systems" title="Permalink to this headline">#</a></h2>
<section id="slateq">
<span id="id2"></span><h3>SlateQ<a class="headerlink" href="rllib-algorithms.html#slateq" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/9f91de1fa0ac351ecb12e4062a37afb896aa1463.pdf">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/slateq/slateq.py">[implementation]</a></p>
<p>SlateQ is a model-free RL method that builds on top of DQN and generates recommendation slates for recommender system environments. Since these types of environments come with large combinatorial action spaces, SlateQ mitigates this by decomposing the Q-value into single-item Q-values and solves the decomposed objective via mixing integer programming and deep learning optimization. SlateQ can be evaluated on Googles RecSim <a class="reference external" href="https://github.com/google-research/recsim">environment</a>. <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/env/wrappers/recsim_wrapper.py">An RLlib wrapper for RecSim can be found here &lt;</a>.</p>
<p>RecSim environment wrapper: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/env/wrappers/recsim_wrapper.py">Google RecSim</a></p>
<p><strong>SlateQ-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.slateq.slateq.SlateQConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.slateq.slateq.</span></span><span class="sig-name descname"><span class="pre">SlateQConfig</span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/slateq/slateq.html#SlateQConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.slateq.slateq.SlateQConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a SlateQ Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.slateq</span> <span class="kn">import</span> <span class="n">SlateQConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">SlateQConfig</span><span class="p">()</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.slateq</span> <span class="kn">import</span> <span class="n">SlateQConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">SlateQConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;SlateQ&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mf">160.0</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.slateq.slateq.SlateQConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replay_buffer_config:</span> <span class="pre">Optional[Dict[str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Any]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fcnet_hiddens_per_candidate:</span> <span class="pre">Optional[List[int]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_network_update_freq:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_huber:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">huber_threshold:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_intensity:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_schedule:</span> <span class="pre">Optional[List[List[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_choice_model:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rmsprop_epsilon:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_step:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_steps_sampled_before_learning_starts:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.slateq.slateq.SlateQConfig" title="ray.rllib.algorithms.slateq.slateq.SlateQConfig"><span class="pre">ray.rllib.algorithms.slateq.slateq.SlateQConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/slateq/slateq.html#SlateQConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.slateq.slateq.SlateQConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>replay_buffer_config</strong>  The config dict to specify the replay buffer used.
May contain a <code class="xref py py-obj docutils literal notranslate"><span class="pre">type</span></code> key (default: <code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiAgentPrioritizedReplayBuffer</span></code>)
indicating the class being used. All other keys specify the names
and values of kwargs passed to to this class constructor.</p></li>
<li><p><strong>fcnet_hiddens_per_candidate</strong>  Dense-layer setup for each the n (document)
candidate Q-network stacks.</p></li>
<li><p><strong>target_network_update_freq</strong>  Update the target network every
<code class="xref py py-obj docutils literal notranslate"><span class="pre">target_network_update_freq</span></code> sample steps.</p></li>
<li><p><strong>tau</strong>  Update the target by   au * policy + (1-       au) * target_policy.</p></li>
<li><p><strong>use_huber</strong>  If True, use huber loss instead of squared loss for critic
network. Conventionally, no need to clip gradients if using a huber
loss.</p></li>
<li><p><strong>huber_threshold</strong>  The threshold for the Huber loss.</p></li>
<li><p><strong>training_intensity</strong>  If set, this will fix the ratio of replayed from a
buffer and learned on timesteps to sampled from an environment and
stored in the replay buffer timesteps. Otherwise, the replay will
proceed at the native ratio determined by
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(train_batch_size</span> <span class="pre">/</span> <span class="pre">rollout_fragment_length)</span></code>.</p></li>
<li><p><strong>lr_schedule</strong>  Learning rate schedule. In the format of
[[timestep, lr-value], [timestep, lr-value], ]
Intermediary timesteps will be assigned to interpolated learning rate
values. A schedule should normally start from timestep 0.</p></li>
<li><p><strong>lr_choice_model</strong>  Learning rate for adam optimizer for the user choice model.
So far, only relevant/supported for framework=torch.</p></li>
<li><p><strong>rmsprop_epsilon</strong>  RMSProp epsilon hyperparameter.</p></li>
<li><p><strong>grad_clip</strong>  If not None, clip gradients during optimization at this value.</p></li>
<li><p><strong>n_step</strong>  N-step parameter for Q-learning.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="contextual-bandits">
<h2>Contextual Bandits<a class="headerlink" href="rllib-algorithms.html#contextual-bandits" title="Permalink to this headline">#</a></h2>
<p id="bandits">The Multi-armed bandit (MAB) problem provides a simplified RL setting that
involves learning to act under one situation only, i.e. the context (observation/state) and arms (actions/items-to-select) are both fixed.
Contextual bandit is an extension of the MAB problem, where at each
round the agent has access not only to a set of bandit arms/actions but also
to a context (state) associated with this iteration. The context changes
with each iteration, but, is not affected by the action that the agent takes.
The objective of the agent is to maximize the cumulative rewards, by
collecting  enough information about how the context and the rewards of the
arms are related to each other. The agent does this by balancing the
trade-off between exploration and exploitation.</p>
<p>Contextual bandit algorithms typically consist of an action-value model (Q
model) and an exploration strategy (epsilon-greedy, LinUCB, Thompson Sampling etc.)</p>
<p>RLlib supports the following online contextual bandit algorithms,
named after the exploration strategies that they employ:</p>
<section id="linear-upper-confidence-bound-banditlinucb">
<span id="lin-ucb"></span><h3>Linear Upper Confidence Bound (BanditLinUCB)<a class="headerlink" href="rllib-algorithms.html#linear-upper-confidence-bound-banditlinucb" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="http://rob.schapire.net/papers/www10.pdf">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/bandit/bandit.py">[implementation]</a>
LinUCB assumes a linear dependency between the expected reward of an action and
its context. It estimates the Q value of each action using ridge regression.
It constructs a confidence region around the weights of the linear
regression model and uses this confidence ellipsoid to estimate the
uncertainty of action values.</p>
<p>Tuned examples:
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/bandit/tests/test_bandits.py">SimpleContextualBandit</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/bandit/tune_lin_ucb_train_recsim_env.py">UCB Bandit on RecSim</a>.
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/bandit/tune_lin_ucb_train_recommendation.py">ParametricItemRecoEnv</a>.</p>
<p><strong>LinUCB-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.bandit.bandit.BanditLinUCBConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.bandit.bandit.</span></span><span class="sig-name descname"><span class="pre">BanditLinUCBConfig</span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/bandit/bandit.html#BanditLinUCBConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.bandit.bandit.BanditLinUCBConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a config class from which an upper confidence bound bandit can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.bandit</span> <span class="kn">import</span> <span class="n">BanditLinUCBConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.examples.env.bandit_envs_discrete</span> <span class="kn">import</span> <span class="n">WheelBanditEnv</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">BanditLinUCBConfig</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">WheelBanditEnv</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.bandit.bandit.BanditLinUCBConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr:</span> <span class="pre">Optional[Union[float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">List[List[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]]]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip_by:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_batch_size:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_requests_in_flight_per_sampler_worker:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_enable_learner_api:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learner_class:</span> <span class="pre">Optional[Type[Learner]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.bandit.bandit.BanditLinUCBConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gamma</strong>  Float specifying the discount factor of the Markov Decision process.</p></li>
<li><p><strong>lr</strong>  The learning rate (float) or learning rate schedule in the format of
[[timestep, lr-value], [timestep, lr-value], ]
In case of a schedule, intermediary timesteps will be assigned to
linearly interpolated learning rate values. A schedule configs first
entry must start with timestep 0, i.e.: [[0, initial_value], []].
Note: If you require a) more than one optimizer (per RLModule),
b) optimizer types that are not Adam, c) a learning rate schedule that
is not a linearly interpolated, piecewise schedule as described above,
or d) specifying ctor arguments of the optimizer that are not the
learning rate (e.g. Adams epsilon), then you must override your
Learners <code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_optimizer_for_module()</span></code> method and handle
lr-scheduling yourself.</p></li>
<li><p><strong>grad_clip</strong>  If None, no gradient clipping will be applied. Otherwise,
depending on the setting of <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip_by</span></code>, the (float) value of
<code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip</span></code> will have the following effect:
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip_by=value</span></code>: Will clip all computed gradients individually
inside the interval [-<code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip</span></code>, +`grad_clip`].
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip_by=norm</span></code>, will compute the L2-norm of each weight/bias
gradient tensor individually and then clip all gradients such that these
L2-norms do not exceed <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip</span></code>. The L2-norm of a tensor is computed
via: <code class="xref py py-obj docutils literal notranslate"><span class="pre">sqrt(SUM(w0^2,</span> <span class="pre">w1^2,</span> <span class="pre">...,</span> <span class="pre">wn^2))</span></code> where w[i] are the elements of
the tensor (no matter what the shape of this tensor is).
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip_by=global_norm</span></code>, will compute the square of the L2-norm of
each weight/bias gradient tensor individually, sum up all these squared
L2-norms across all given gradient tensors (e.g. the entire module to
be updated), square root that overall sum, and then clip all gradients
such that this global L2-norm does not exceed the given value.
The global L2-norm over a list of tensors (e.g. W and V) is computed
via:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">sqrt[SUM(w0^2,</span> <span class="pre">w1^2,</span> <span class="pre">...,</span> <span class="pre">wn^2)</span> <span class="pre">+</span> <span class="pre">SUM(v0^2,</span> <span class="pre">v1^2,</span> <span class="pre">...,</span> <span class="pre">vm^2)]</span></code>, where
w[i] and v[j] are the elements of the tensors W and V (no matter what
the shapes of these tensors are).</p></li>
<li><p><strong>grad_clip_by</strong>  See <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip</span></code> for the effect of this setting on gradient
clipping. Allowed values are <code class="xref py py-obj docutils literal notranslate"><span class="pre">value</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">norm</span></code>, and <code class="xref py py-obj docutils literal notranslate"><span class="pre">global_norm</span></code>.</p></li>
<li><p><strong>train_batch_size</strong>  Training batch size, if applicable.</p></li>
<li><p><strong>model</strong>  Arguments passed into the policy model. See models/catalog.py for a
full list of the available model options.
TODO: Provide ModelConfig objects instead of dicts.</p></li>
<li><p><strong>optimizer</strong>  Arguments to pass to the policy optimizer. This setting is not
used when <code class="xref py py-obj docutils literal notranslate"><span class="pre">_enable_learner_api=True</span></code>.</p></li>
<li><p><strong>max_requests_in_flight_per_sampler_worker</strong>  Max number of inflight requests
to each sampling worker. See the FaultTolerantActorManager class for
more details.
Tuning these values is important when running experimens with
large sample batches, where there is the risk that the object store may
fill up, causing spilling of objects to disk. This can cause any
asynchronous requests to become very slow, making your experiment run
slow as well. You can inspect the object store during your experiment
via a call to ray memory on your headnode, and by using the ray
dashboard. If youre seeing that the object store is filling up,
turn down the number of remote requests in flight, or enable compression
in your experiment of timesteps.</p></li>
<li><p><strong>_enable_learner_api</strong>  Whether to enable the LearnerGroup and Learner
for training. This API uses ray.train to run the training loop which
allows for a more flexible distributed training.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="linear-thompson-sampling-banditlints">
<span id="lints"></span><h3>Linear Thompson Sampling (BanditLinTS)<a class="headerlink" href="rllib-algorithms.html#linear-thompson-sampling-banditlints" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="http://proceedings.mlr.press/v28/agrawal13.pdf">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/bandit/bandit.py">[implementation]</a>
Like LinUCB, LinTS also assumes a linear dependency between the expected
reward of an action and its context and uses online ridge regression to
estimate the Q values of actions given the context. It assumes a Gaussian
prior on the weights and a Gaussian likelihood function. For deciding which
action to take, the agent samples weights for each arm, using
the posterior distributions, and plays the arm that produces the highest reward.</p>
<p>Tuned examples:
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/bandit/tests/test_bandits.py">SimpleContextualBandit</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/bandit/tune_lin_ts_train_wheel_env.py">WheelBandit</a>.</p>
<p><strong>LinTS-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.bandit.bandit.BanditLinTSConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.bandit.bandit.</span></span><span class="sig-name descname"><span class="pre">BanditLinTSConfig</span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/bandit/bandit.html#BanditLinTSConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.bandit.bandit.BanditLinTSConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a Thompson-sampling bandit can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.bandit</span> <span class="kn">import</span> <span class="n">BanditLinTSConfig</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.examples.env.bandit_envs_discrete</span> <span class="kn">import</span> <span class="n">WheelBanditEnv</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">BanditLinTSConfig</span><span class="p">()</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">WheelBanditEnv</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.bandit.bandit.BanditLinTSConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr:</span> <span class="pre">Optional[Union[float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">List[List[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]]]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip_by:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_batch_size:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_requests_in_flight_per_sampler_worker:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_enable_learner_api:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learner_class:</span> <span class="pre">Optional[Type[Learner]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.bandit.bandit.BanditLinTSConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gamma</strong>  Float specifying the discount factor of the Markov Decision process.</p></li>
<li><p><strong>lr</strong>  The learning rate (float) or learning rate schedule in the format of
[[timestep, lr-value], [timestep, lr-value], ]
In case of a schedule, intermediary timesteps will be assigned to
linearly interpolated learning rate values. A schedule configs first
entry must start with timestep 0, i.e.: [[0, initial_value], []].
Note: If you require a) more than one optimizer (per RLModule),
b) optimizer types that are not Adam, c) a learning rate schedule that
is not a linearly interpolated, piecewise schedule as described above,
or d) specifying ctor arguments of the optimizer that are not the
learning rate (e.g. Adams epsilon), then you must override your
Learners <code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_optimizer_for_module()</span></code> method and handle
lr-scheduling yourself.</p></li>
<li><p><strong>grad_clip</strong>  If None, no gradient clipping will be applied. Otherwise,
depending on the setting of <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip_by</span></code>, the (float) value of
<code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip</span></code> will have the following effect:
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip_by=value</span></code>: Will clip all computed gradients individually
inside the interval [-<code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip</span></code>, +`grad_clip`].
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip_by=norm</span></code>, will compute the L2-norm of each weight/bias
gradient tensor individually and then clip all gradients such that these
L2-norms do not exceed <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip</span></code>. The L2-norm of a tensor is computed
via: <code class="xref py py-obj docutils literal notranslate"><span class="pre">sqrt(SUM(w0^2,</span> <span class="pre">w1^2,</span> <span class="pre">...,</span> <span class="pre">wn^2))</span></code> where w[i] are the elements of
the tensor (no matter what the shape of this tensor is).
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip_by=global_norm</span></code>, will compute the square of the L2-norm of
each weight/bias gradient tensor individually, sum up all these squared
L2-norms across all given gradient tensors (e.g. the entire module to
be updated), square root that overall sum, and then clip all gradients
such that this global L2-norm does not exceed the given value.
The global L2-norm over a list of tensors (e.g. W and V) is computed
via:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">sqrt[SUM(w0^2,</span> <span class="pre">w1^2,</span> <span class="pre">...,</span> <span class="pre">wn^2)</span> <span class="pre">+</span> <span class="pre">SUM(v0^2,</span> <span class="pre">v1^2,</span> <span class="pre">...,</span> <span class="pre">vm^2)]</span></code>, where
w[i] and v[j] are the elements of the tensors W and V (no matter what
the shapes of these tensors are).</p></li>
<li><p><strong>grad_clip_by</strong>  See <code class="xref py py-obj docutils literal notranslate"><span class="pre">grad_clip</span></code> for the effect of this setting on gradient
clipping. Allowed values are <code class="xref py py-obj docutils literal notranslate"><span class="pre">value</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">norm</span></code>, and <code class="xref py py-obj docutils literal notranslate"><span class="pre">global_norm</span></code>.</p></li>
<li><p><strong>train_batch_size</strong>  Training batch size, if applicable.</p></li>
<li><p><strong>model</strong>  Arguments passed into the policy model. See models/catalog.py for a
full list of the available model options.
TODO: Provide ModelConfig objects instead of dicts.</p></li>
<li><p><strong>optimizer</strong>  Arguments to pass to the policy optimizer. This setting is not
used when <code class="xref py py-obj docutils literal notranslate"><span class="pre">_enable_learner_api=True</span></code>.</p></li>
<li><p><strong>max_requests_in_flight_per_sampler_worker</strong>  Max number of inflight requests
to each sampling worker. See the FaultTolerantActorManager class for
more details.
Tuning these values is important when running experimens with
large sample batches, where there is the risk that the object store may
fill up, causing spilling of objects to disk. This can cause any
asynchronous requests to become very slow, making your experiment run
slow as well. You can inspect the object store during your experiment
via a call to ray memory on your headnode, and by using the ray
dashboard. If youre seeing that the object store is filling up,
turn down the number of remote requests in flight, or enable compression
in your experiment of timesteps.</p></li>
<li><p><strong>_enable_learner_api</strong>  Whether to enable the LearnerGroup and Learner
for training. This API uses ray.train to run the training loop which
allows for a more flexible distributed training.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="multi-agent">
<h2>Multi-agent<a class="headerlink" href="rllib-algorithms.html#multi-agent" title="Permalink to this headline">#</a></h2>
<section id="parameter-sharing">
<span id="parameter"></span><h3>Parameter Sharing<a class="headerlink" href="rllib-algorithms.html#parameter-sharing" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="http://ala2017.it.nuigalway.ie/papers/ALA2017_Gupta.pdf">[paper]</a>, <a class="reference external" href="https://arxiv.org/abs/2005.13625">[paper]</a> and <a class="reference external" href="../rllib-env.html#multi-agent-and-hierarchical">[instructions]</a>. Parameter sharing refers to a class of methods that take a base single agent method, and use it to learn a single policy for all agents. This simple approach has been shown to achieve state of the art performance in cooperative games, and is usually how you should start trying to learn a multi-agent problem.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/PettingZoo-Team/PettingZoo">PettingZoo</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_parameter_sharing.py">waterworld</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/rock_paper_scissors_multiagent.py">rock-paper-scissors</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_cartpole.py">multi-agent cartpole</a></p>
</section>
<section id="qmix-monotonic-value-factorisation-qmix-vdn-iqn">
<span id="qmix"></span><h3>QMIX Monotonic Value Factorisation (QMIX, VDN, IQN)<a class="headerlink" href="rllib-algorithms.html#qmix-monotonic-value-factorisation-qmix-vdn-iqn" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1803.11485">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/qmix/qmix.py">[implementation]</a> Q-Mix is a specialized multi-agent algorithm. Code here is adapted from <a class="reference external" href="https://github.com/oxwhirl/pymarl_alpha">https://github.com/oxwhirl/pymarl_alpha</a>  to integrate with RLlib multi-agent APIs. To use Q-Mix, you must specify an agent <a class="reference external" href="../rllib-env.html#grouping-agents">grouping</a> in the environment (see the <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/two_step_game.py">two-step game example</a>). Currently, all agents in the group must be homogeneous. The algorithm can be scaled by increasing the number of workers or using Ape-X.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/two_step_game.py">Two-step game</a></p>
<p><strong>QMIX-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.qmix.qmix.QMixConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.qmix.qmix.</span></span><span class="sig-name descname"><span class="pre">QMixConfig</span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/qmix/qmix.html#QMixConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.qmix.qmix.QMixConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which QMix can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.examples.env.two_step_game</span> <span class="kn">import</span> <span class="n">TwoStepGame</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.qmix</span> <span class="kn">import</span> <span class="n">QMixConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">QMixConfig</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">kl_coeff</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build an Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">TwoStepGame</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.examples.env.two_step_game</span> <span class="kn">import</span> <span class="n">TwoStepGame</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.qmix</span> <span class="kn">import</span> <span class="n">QMixConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">QMixConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">optim_alpha</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]),</span> <span class="n">optim_alpha</span><span class="o">=</span><span class="mf">0.97</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">TwoStepGame</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;QMix&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.qmix.qmix.QMixConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mixer:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mixing_embed_dim:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">double_q:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_network_update_freq:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replay_buffer_config:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim_alpha:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim_eps:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_norm_clipping=-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.qmix.qmix.QMixConfig" title="ray.rllib.algorithms.qmix.qmix.QMixConfig"><span class="pre">ray.rllib.algorithms.qmix.qmix.QMixConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/qmix/qmix.html#QMixConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.qmix.qmix.QMixConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mixer</strong>  Mixing network. Either qmix, vdn, or None.</p></li>
<li><p><strong>mixing_embed_dim</strong>  Size of the mixing network embedding.</p></li>
<li><p><strong>double_q</strong>  Whether to use Double_Q learning.</p></li>
<li><p><strong>target_network_update_freq</strong>  Update the target network every
<code class="xref py py-obj docutils literal notranslate"><span class="pre">target_network_update_freq</span></code> sample steps.</p></li>
<li><p><strong>replay_buffer_config</strong>  </p></li>
<li><p><strong>optim_alpha</strong>  RMSProp alpha.</p></li>
<li><p><strong>optim_eps</strong>  RMSProp epsilon.</p></li>
<li><p><strong>grad_clip</strong>  If not None, clip gradients during optimization at
this value.</p></li>
<li><p><strong>grad_norm_clipping</strong>  Depcrecated in favor of grad_clip</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="multi-agent-deep-deterministic-policy-gradient-maddpg">
<span id="maddpg"></span><h3>Multi-Agent Deep Deterministic Policy Gradient (MADDPG)<a class="headerlink" href="rllib-algorithms.html#multi-agent-deep-deterministic-policy-gradient-maddpg" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1706.02275">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/maddpg/maddpg.py">[implementation]</a> MADDPG is a DDPG centralized/shared critic algorithm. Code here is adapted from <a class="reference external" href="https://github.com/openai/maddpg">https://github.com/openai/maddpg</a> to integrate with RLlib multi-agent APIs. Please check <a class="reference external" href="https://github.com/jkterry1/maddpg-rllib">justinkterry/maddpg-rllib</a> for examples and more information. Note that the implementation here is based on OpenAIs, and is intended for use with the discrete MPE environments. Please also note that people typically find this method difficult to get to work, even with all applicable optimizations for their environment applied. This method should be viewed as for research purposes, and for reproducing the results of the paper introducing it.</p>
<p><strong>MADDPG-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/wsjeon/maddpg-rllib/tree/master/plots">Multi-Agent Particle Environment</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/two_step_game.py">Two-step game</a></p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.maddpg.maddpg.MADDPGConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.maddpg.maddpg.</span></span><span class="sig-name descname"><span class="pre">MADDPGConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/maddpg/maddpg.html#MADDPGConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.maddpg.maddpg.MADDPGConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a MADDPG Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.maddpg.maddpg</span> <span class="kn">import</span> <span class="n">MADDPGConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">MADDPGConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">replay_buffer_config</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">replay_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">replay_buffer_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;capacity&quot;</span><span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;prioritized_replay_alpha&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;prioritized_replay_beta&quot;</span><span class="p">:</span> <span class="mf">0.45</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;prioritized_replay_eps&quot;</span><span class="p">:</span> <span class="mf">2e-6</span><span class="p">,</span>
<span class="gp">... </span>    <span class="p">}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">replay_buffer_config</span><span class="o">=</span><span class="n">replay_config</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.maddpg.maddpg</span> <span class="kn">import</span> <span class="n">MADDPGConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">MADDPGConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">n_step</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]))</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>  
<span class="gp">... </span>    <span class="s2">&quot;MADDPG&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span><span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.maddpg.maddpg.MADDPGConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">agent_id:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_local_critic:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_state_preprocessor:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_hiddens:</span> <span class="pre">Optional[List[int]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_hidden_activation:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic_hiddens:</span> <span class="pre">Optional[List[int]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic_hidden_activation:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_step:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">good_policy:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adv_policy:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replay_buffer_config:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_intensity:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_steps_sampled_before_learning_starts:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic_lr:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_lr:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_network_update_freq:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_feature_reg:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_norm_clipping:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.maddpg.maddpg.MADDPGConfig" title="ray.rllib.algorithms.maddpg.maddpg.MADDPGConfig"><span class="pre">ray.rllib.algorithms.maddpg.maddpg.MADDPGConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/maddpg/maddpg.html#MADDPGConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.maddpg.maddpg.MADDPGConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>agent_id</strong>  ID of the agent controlled by this policy.</p></li>
<li><p><strong>use_local_critic</strong>  Use a local critic for this policy.</p></li>
<li><p><strong>use_state_preprocessor</strong>  Apply a state preprocessor with spec given by the
model config option (like other RL algorithms). This is mostly useful
if you have a weird observation shape, like an image. Disabled by
default.</p></li>
<li><p><strong>actor_hiddens</strong>  Postprocess the policy network model output with these hidden
layers. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">use_state_preprocessor</span></code> is False, then these will be the
<em>only</em> hidden layers in the network.</p></li>
<li><p><strong>actor_hidden_activation</strong>  Hidden layers activation of the postprocessing
stage of the policy network.</p></li>
<li><p><strong>critic_hiddens</strong>  Postprocess the critic network model output with these
hidden layers; again, if use_state_preprocessor is True, then the state
will be preprocessed by the model specified with the model config
option first.</p></li>
<li><p><strong>critic_hidden_activation</strong>  Hidden layers activation of the postprocessing
state of the critic.</p></li>
<li><p><strong>n_step</strong>  N-step for Q-learning.</p></li>
<li><p><strong>good_policy</strong>  Algorithm for good policies.</p></li>
<li><p><strong>adv_policy</strong>  Algorithm for adversary policies.</p></li>
<li><p><strong>replay_buffer_config</strong>  Replay buffer config.
Examples:
{
_enable_replay_buffer_api: True,
type: MultiAgentReplayBuffer,
capacity: 50000,
replay_sequence_length: 1,
}
- OR -
{
_enable_replay_buffer_api: True,
type: MultiAgentPrioritizedReplayBuffer,
capacity: 50000,
prioritized_replay_alpha: 0.6,
prioritized_replay_beta: 0.4,
prioritized_replay_eps: 1e-6,
replay_sequence_length: 1,
}
- Where -
prioritized_replay_alpha: Alpha parameter controls the degree of
prioritization in the buffer. In other words, when a buffer sample has
a higher temporal-difference error, with how much more probability
should it drawn to use to update the parametrized Q-network. 0.0
corresponds to uniform probability. Setting much above 1.0 may quickly
result as the sampling distribution could become heavily pointy with
low entropy.
prioritized_replay_beta: Beta parameter controls the degree of
importance sampling which suppresses the influence of gradient updates
from samples that have higher probability of being sampled via alpha
parameter and the temporal-difference error.
prioritized_replay_eps: Epsilon parameter sets the baseline probability
for sampling so that when the temporal-difference error of a sample is
zero, there is still a chance of drawing the sample.</p></li>
<li><p><strong>training_intensity</strong>  If set, this will fix the ratio of replayed from a
buffer and learned on timesteps to sampled from an environment and
stored in the replay buffer timesteps. Otherwise, the replay will
proceed at the native ratio determined by
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(train_batch_size</span> <span class="pre">/</span> <span class="pre">rollout_fragment_length)</span></code>.</p></li>
<li><p><strong>num_steps_sampled_before_learning_starts</strong>  Number of timesteps to collect
from rollout workers before we start sampling from replay buffers for
learning. Whether we count this in agent steps  or environment steps
depends on config.multi_agent(count_steps_by=..).</p></li>
<li><p><strong>critic_lr</strong>  Learning rate for the critic (Q-function) optimizer.</p></li>
<li><p><strong>actor_lr</strong>  Learning rate for the actor (policy) optimizer.</p></li>
<li><p><strong>target_network_update_freq</strong>  Update the target network every
<code class="xref py py-obj docutils literal notranslate"><span class="pre">target_network_update_freq</span></code> sample steps.</p></li>
<li><p><strong>tau</strong>  Update the target by   au * policy + (1-       au) * target_policy.</p></li>
<li><p><strong>actor_feature_reg</strong>  Weights for feature regularization for the actor.</p></li>
<li><p><strong>grad_norm_clipping</strong>  If not None, clip gradients during optimization at this
value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="shared-critic-methods">
<span id="sc"></span><h3>Shared Critic Methods<a class="headerlink" href="rllib-algorithms.html#shared-critic-methods" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="../rllib-env.html#implementing-a-centralized-critic">[instructions]</a> Shared critic methods are when all agents use a single parameter shared critic network (in some cases with access to more of the observation space than agents can see). Note that many specialized multi-agent algorithms such as MADDPG are mostly shared critic forms of their single-agent algorithm (DDPG in the case of MADDPG).</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/centralized_critic_2.py">TwoStepGame</a></p>
</section>
</section>
<section id="others">
<h2>Others<a class="headerlink" href="rllib-algorithms.html#others" title="Permalink to this headline">#</a></h2>
<section id="single-player-alpha-zero-alphazero">
<span id="alphazero"></span><h3>Single-Player Alpha Zero (AlphaZero)<a class="headerlink" href="rllib-algorithms.html#single-player-alpha-zero-alphazero" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1712.01815">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/alpha_zero">[implementation]</a> AlphaZero is an RL agent originally designed for two-player games. This version adapts it to handle single player games. The code can be scaled to any number of workers. It also implements the ranked rewards <a class="reference external" href="https://arxiv.org/abs/1807.01672">(R2)</a> strategy to enable self-play even in the one-player setting. The code is mainly purposed to be used for combinatorial optimization.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/alpha_zero/cartpole-sparse-rewards-alpha-zero.yaml">Sparse reward CartPole</a></p>
<p><strong>AlphaZero-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.alpha_zero.alpha_zero.AlphaZeroConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.alpha_zero.alpha_zero.</span></span><span class="sig-name descname"><span class="pre">AlphaZeroConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/alpha_zero/alpha_zero.html#AlphaZeroConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.alpha_zero.alpha_zero.AlphaZeroConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which an AlphaZero Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.alpha_zero</span> <span class="kn">import</span> <span class="n">AlphaZeroConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AlphaZeroConfig</span><span class="p">()</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">sgd_minibatch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">..</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">..</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> 
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.alpha_zero</span> <span class="kn">import</span> <span class="n">AlphaZeroConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AlphaZeroConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">shuffle_sequences</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]))</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span> 
<span class="gp">... </span>    <span class="s2">&quot;AlphaZero&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;episode_reward_mean&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.alpha_zero.alpha_zero.AlphaZeroConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sgd_minibatch_size:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_sequences:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_sgd_iter:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replay_buffer_config:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_schedule:</span> <span class="pre">Optional[List[List[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_share_layers:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mcts_config:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ranked_rewards:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_steps_sampled_before_learning_starts:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.alpha_zero.alpha_zero.AlphaZeroConfig" title="ray.rllib.algorithms.alpha_zero.alpha_zero.AlphaZeroConfig"><span class="pre">ray.rllib.algorithms.alpha_zero.alpha_zero.AlphaZeroConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/alpha_zero/alpha_zero.html#AlphaZeroConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.alpha_zero.alpha_zero.AlphaZeroConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sgd_minibatch_size</strong>  Total SGD batch size across all devices for SGD.</p></li>
<li><p><strong>shuffle_sequences</strong>  Whether to shuffle sequences in the batch when training
(recommended).</p></li>
<li><p><strong>num_sgd_iter</strong>  Number of SGD iterations in each outer loop.</p></li>
<li><p><strong>replay_buffer_config</strong>  Replay buffer config.
Examples:
{
_enable_replay_buffer_api: True,
type: MultiAgentReplayBuffer,
learning_starts: 1000,
capacity: 50000,
replay_sequence_length: 1,
}
- OR -
{
_enable_replay_buffer_api: True,
type: MultiAgentPrioritizedReplayBuffer,
capacity: 50000,
prioritized_replay_alpha: 0.6,
prioritized_replay_beta: 0.4,
prioritized_replay_eps: 1e-6,
replay_sequence_length: 1,
}
- Where -
prioritized_replay_alpha: Alpha parameter controls the degree of
prioritization in the buffer. In other words, when a buffer sample has
a higher temporal-difference error, with how much more probability
should it drawn to use to update the parametrized Q-network. 0.0
corresponds to uniform probability. Setting much above 1.0 may quickly
result as the sampling distribution could become heavily pointy with
low entropy.
prioritized_replay_beta: Beta parameter controls the degree of
importance sampling which suppresses the influence of gradient updates
from samples that have higher probability of being sampled via alpha
parameter and the temporal-difference error.
prioritized_replay_eps: Epsilon parameter sets the baseline probability
for sampling so that when the temporal-difference error of a sample is
zero, there is still a chance of drawing the sample.</p></li>
<li><p><strong>lr_schedule</strong>  Learning rate schedule. In the format of
[[timestep, lr-value], [timestep, lr-value], ]
Intermediary timesteps will be assigned to interpolated learning rate
values. A schedule should normally start from timestep 0.</p></li>
<li><p><strong>vf_share_layers</strong>  Share layers for value function. If you set this to True,
its important to tune vf_loss_coeff.</p></li>
<li><p><strong>mcts_config</strong>  MCTS specific settings.</p></li>
<li><p><strong>ranked_rewards</strong>  Settings for the ranked reward (r2) algorithm
from: <a class="reference external" href="https://arxiv.org/pdf/1807.01672.pdf">https://arxiv.org/pdf/1807.01672.pdf</a></p></li>
<li><p><strong>num_steps_sampled_before_learning_starts</strong>  Number of timesteps to collect
from rollout workers before we start sampling from replay buffers for
learning. Whether we count this in agent steps  or environment steps
depends on config.multi_agent(count_steps_by=..).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="multiagent-leelachesszero-leelachesszero">
<span id="leelachesszero"></span><h3>MultiAgent LeelaChessZero (LeelaChessZero)<a class="headerlink" href="rllib-algorithms.html#multiagent-leelachesszero-leelachesszero" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://github.com/LeelaChessZero/lc0/">[source]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/leela_chess_zero">[implementation]</a> LeelaChessZero is an RL agent originally inspired by AlphaZero for playing chess. This version adapts it to handle a MultiAgent competitive environment of chess. The code can be scaled to any number of workers.</p>
<p>Tuned examples: tbd</p>
<p><strong>LeelaChessZero-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<dl class="py class">
<dt class="sig sig-object py" id="ray.rllib.algorithms.leela_chess_zero.leela_chess_zero.LeelaChessZeroConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ray.rllib.algorithms.leela_chess_zero.leela_chess_zero.</span></span><span class="sig-name descname"><span class="pre">LeelaChessZeroConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ray/rllib/algorithms/leela_chess_zero/leela_chess_zero.html#LeelaChessZeroConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.leela_chess_zero.leela_chess_zero.LeelaChessZeroConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a configuration class from which a LeelaChessZero Algorithm can be built.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.leela_chess_zero</span> <span class="k">as</span> <span class="n">lc0</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">lc0</span> <span class="kn">import</span> <span class="n">LeelaChessZeroConfig</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">LeelaChessZeroConfig</span><span class="p">()</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">sgd_minibatch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">..</span><span class="n">resources</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="o">..</span><span class="n">rollouts</span><span class="p">(</span><span class="n">num_rollout_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build a Algorithm object from the config and run 1 training iteration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> 
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.leela_chess_zero</span> <span class="k">as</span> <span class="n">lc0</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">lc0</span> <span class="kn">import</span> <span class="n">LeelaChessZeroConfig</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">LeelaChessZeroConfig</span><span class="p">()</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print out some default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">shuffle_sequences</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update the config object.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]))</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set the config object&#39;s env.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>   
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use to_dict() to get the old-style python config dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># when running with tune.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span> 
<span class="gp">... </span>    <span class="s2">&quot;LeelaChessZero&quot;</span><span class="p">,</span> 
<span class="gp">... </span>    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">{</span> 
<span class="go">            &quot;episode_reward_mean&quot;: 200}), </span>
<span class="gp">... </span>    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span> 
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span> 
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ray.rllib.algorithms.leela_chess_zero.leela_chess_zero.LeelaChessZeroConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sgd_minibatch_size:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_sequences:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_sgd_iter:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replay_buffer_config:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr:</span> <span class="pre">Optional[float]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_schedule:</span> <span class="pre">Optional[List[List[Union[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]]]]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_share_layers:</span> <span class="pre">Optional[bool]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mcts_config:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_steps_sampled_before_learning_starts:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">Optional[dict]</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="rllib-algorithms.html#ray.rllib.algorithms.leela_chess_zero.leela_chess_zero.LeelaChessZeroConfig" title="ray.rllib.algorithms.leela_chess_zero.leela_chess_zero.LeelaChessZeroConfig"><span class="pre">ray.rllib.algorithms.leela_chess_zero.leela_chess_zero.LeelaChessZeroConfig</span></a></span></span><a class="reference internal" href="../_modules/ray/rllib/algorithms/leela_chess_zero/leela_chess_zero.html#LeelaChessZeroConfig.training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="rllib-algorithms.html#ray.rllib.algorithms.leela_chess_zero.leela_chess_zero.LeelaChessZeroConfig.training" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sgd_minibatch_size</strong>  Total SGD batch size across all devices for SGD.</p></li>
<li><p><strong>shuffle_sequences</strong>  Whether to shuffle sequences in the batch when training
(recommended).</p></li>
<li><p><strong>num_sgd_iter</strong>  Number of SGD iterations in each outer loop.</p></li>
<li><p><strong>replay_buffer_config</strong>  Replay buffer config.
Examples:
{
_enable_replay_buffer_api: True,
type: MultiAgentReplayBuffer,
learning_starts: 1000,
capacity: 50000,
replay_sequence_length: 1,
}
- OR -
{
_enable_replay_buffer_api: True,
type: MultiAgentPrioritizedReplayBuffer,
capacity: 50000,
prioritized_replay_alpha: 0.6,
prioritized_replay_beta: 0.4,
prioritized_replay_eps: 1e-6,
replay_sequence_length: 1,
}
- Where -
prioritized_replay_alpha: Alpha parameter controls the degree of
prioritization in the buffer. In other words, when a buffer sample has
a higher temporal-difference error, with how much more probability
should it drawn to use to update the parametrized Q-network. 0.0
corresponds to uniform probability. Setting much above 1.0 may quickly
result as the sampling distribution could become heavily pointy with
low entropy.
prioritized_replay_beta: Beta parameter controls the degree of
importance sampling which suppresses the influence of gradient updates
from samples that have higher probability of being sampled via alpha
parameter and the temporal-difference error.
prioritized_replay_eps: Epsilon parameter sets the baseline probability
for sampling so that when the temporal-difference error of a sample is
zero, there is still a chance of drawing the sample.</p></li>
<li><p><strong>lr_schedule</strong>  Learning rate schedule. In the format of
[[timestep, lr-value], [timestep, lr-value], ]
Intermediary timesteps will be assigned to interpolated learning rate
values. A schedule should normally start from timestep 0.</p></li>
<li><p><strong>vf_share_layers</strong>  Share layers for value function. If you set this to True,
its important to tune vf_loss_coeff.</p></li>
<li><p><strong>mcts_config</strong>  MCTS specific settings.</p></li>
<li><p><strong>num_steps_sampled_before_learning_starts</strong>  Number of timesteps to collect
from rollout workers before we start sampling from replay buffers for
learning. Whether we count this in agent steps  or environment steps
depends on config.multi_agent(count_steps_by=..).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="curiosity-icm-intrinsic-curiosity-module">
<span id="curiosity"></span><h3>Curiosity (ICM: Intrinsic Curiosity Module)<a class="headerlink" href="rllib-algorithms.html#curiosity-icm-intrinsic-curiosity-module" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/pdf/1705.05363.pdf">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/utils/exploration/curiosity.py">[implementation]</a></p>
<p>Tuned examples:
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/unity3d_env_local.py">Pyramids (Unity3D)</a> (use <code class="docutils literal notranslate"><span class="pre">--env</span> <span class="pre">Pyramids</span></code> command line option)
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/utils/exploration/tests/test_curiosity.py#L184">Test case with MiniGrid example</a> (UnitTest case: <code class="docutils literal notranslate"><span class="pre">test_curiosity_on_partially_observable_domain</span></code>)</p>
<p><strong>Activating Curiosity</strong>
The curiosity plugin can be easily activated by specifying it as the Exploration class to-be-used
in the main Algorithm config. Most of its parameters usually do not have to be specified
as the module uses the values from the paper by default. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">ppo</span><span class="o">.</span><span class="n">DEFAULT_CONFIG</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">config</span><span class="p">[</span><span class="s2">&quot;exploration_config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;Curiosity&quot;</span><span class="p">,</span>  <span class="c1"># &lt;- Use the Curiosity module for exploring.</span>
    <span class="s2">&quot;eta&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>  <span class="c1"># Weight for intrinsic rewards before being added to extrinsic ones.</span>
    <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>  <span class="c1"># Learning rate of the curiosity (ICM) module.</span>
    <span class="s2">&quot;feature_dim&quot;</span><span class="p">:</span> <span class="mi">288</span><span class="p">,</span>  <span class="c1"># Dimensionality of the generated feature vectors.</span>
    <span class="c1"># Setup of the feature net (used to encode observations into feature (latent) vectors).</span>
    <span class="s2">&quot;feature_net_config&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;fcnet_hiddens&quot;</span><span class="p">:</span> <span class="p">[],</span>
        <span class="s2">&quot;fcnet_activation&quot;</span><span class="p">:</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;inverse_net_hiddens&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">256</span><span class="p">],</span>  <span class="c1"># Hidden layers of the &quot;inverse&quot; model.</span>
    <span class="s2">&quot;inverse_net_activation&quot;</span><span class="p">:</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>  <span class="c1"># Activation of the &quot;inverse&quot; model.</span>
    <span class="s2">&quot;forward_net_hiddens&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">256</span><span class="p">],</span>  <span class="c1"># Hidden layers of the &quot;forward&quot; model.</span>
    <span class="s2">&quot;forward_net_activation&quot;</span><span class="p">:</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>  <span class="c1"># Activation of the &quot;forward&quot; model.</span>
    <span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>  <span class="c1"># Weight for the &quot;forward&quot; loss (beta) over the &quot;inverse&quot; loss (1.0 - beta).</span>
    <span class="c1"># Specify, which exploration sub-type to use (usually, the algo&#39;s &quot;default&quot;</span>
    <span class="c1"># exploration, e.g. EpsilonGreedy for DQN, StochasticSampling for PG/SAC).</span>
    <span class="s2">&quot;sub_exploration&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;StochasticSampling&quot;</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Functionality</strong>
RLlibs Curiosity is based on <a class="reference external" href="https://arxiv.org/pdf/1705.05363.pdf">ICM (intrinsic curiosity module) described in this paper here</a>.
It allows agents to learn in sparse-reward- or even no-reward environments by
calculating so-called intrinsic rewards, purely based on the information content that is incoming via the observation channel.
Sparse-reward environments are envs where almost all reward signals are 0.0, such as these <a class="reference external" href="https://github.com/maximecb/gym-minigrid">[MiniGrid env examples here]</a>.
In such environments, agents have to navigate (and change the underlying state of the environment) over long periods of time, without receiving much (or any) feedback.
For example, the task could be to find a key in some room, pick it up, find a matching door (matching the color of the key), and eventually unlock this door with the key to reach a goal state,
all the while not seeing any rewards.
Such problems are impossible to solve with standard RL exploration methods like epsilon-greedy or stochastic sampling.
The Curiosity module - when configured as the Exploration class to use via the Algorithms config (see above on how to do this) - automatically adds three simple models to the Policys <code class="docutils literal notranslate"><span class="pre">self.model</span></code>:
a) a latent space learning (feature) model, taking an environment observation and outputting a latent vector, which represents this observation and
b) a forward model, predicting the next latent vector, given the current observation vector and an action to take next.
c) a so-called inverse net, only used to train the feature net. The inverse net tries to predict the action taken between two latent vectors (obs and next obs).</p>
<p>All the above extra Models are trained inside the modified <code class="docutils literal notranslate"><span class="pre">Exploration.postprocess_trajectory()</span></code> call.</p>
<p>Using the (ever changing) forward model, our Curiosity module calculates an artificial (intrinsic) reward signal, weights it via the <code class="docutils literal notranslate"><span class="pre">eta</span></code> parameter, and then adds it to the environments (extrinsic) reward.
Intrinsic rewards for each env-step are calculated by taking the euclidian distance between the latent-space encoded next observation (feature model) and the <strong>predicted</strong> latent-space encoding for the next observation
(forward model).
This allows the agent to explore areas of the environment, where the forward model still performs poorly (are not understood yet), whereas exploration to these areas will taper down after the agent has visited them
often: The forward model will eventually get better at predicting these next latent vectors, which in turn will diminish the intrinsic rewards (decrease the euclidian distance between predicted and actual vectors).</p>
</section>
<section id="re3-random-encoders-for-efficient-exploration">
<span id="re3"></span><h3>RE3 (Random Encoders for Efficient Exploration)<a class="headerlink" href="rllib-algorithms.html#re3-random-encoders-for-efficient-exploration" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/pdf/2102.09430.pdf">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/utils/exploration/random_encoder.py">[implementation]</a></p>
<p>Examples:
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/re3_exploration.py">LunarLanderContinuous-v2</a> (use <code class="docutils literal notranslate"><span class="pre">--env</span> <span class="pre">LunarLanderContinuous-v2</span></code> command line option)
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/utils/exploration/tests/test_random_encoder.py">Test case with Pendulum-v1 example</a></p>
<p><strong>Activating RE3</strong>
The RE3 plugin can be easily activated by specifying it as the Exploration class to-be-used
in the main Algorithm config and inheriting the <code class="xref py py-obj docutils literal notranslate"><span class="pre">RE3UpdateCallbacks</span></code> as shown in this <a class="reference external" href="https://github.com/ray-project/ray/blob/c9c3f0745a9291a4de0872bdfa69e4ffdfac3657/rllib/utils/exploration/tests/test_random_encoder.py#L35">example</a>. Most of its parameters usually do not have to be specified as the module uses the values from the paper by default. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">sac</span><span class="o">.</span><span class="n">DEFAULT_CONFIG</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">config</span><span class="p">[</span><span class="s2">&quot;env&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;Pendulum-v1&quot;</span>
<span class="n">config</span><span class="p">[</span><span class="s2">&quot;seed&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12345</span>
<span class="n">config</span><span class="p">[</span><span class="s2">&quot;callbacks&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">RE3Callbacks</span>
<span class="n">config</span><span class="p">[</span><span class="s2">&quot;exploration_config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;RE3&quot;</span><span class="p">,</span>
     <span class="c1"># the dimensionality of the observation embedding vectors in latent space.</span>
     <span class="s2">&quot;embeds_dim&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
     <span class="s2">&quot;rho&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="c1"># Beta decay factor, used for on-policy algorithm.</span>
     <span class="s2">&quot;k_nn&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="c1"># Number of neighbours to set for K-NN entropy estimation.</span>
     <span class="c1"># Configuration for the encoder network, producing embedding vectors from observations.</span>
     <span class="c1"># This can be used to configure fcnet- or conv_net setups to properly process any</span>
     <span class="c1"># observation space. By default uses the Policy model configuration.</span>
     <span class="s2">&quot;encoder_net_config&quot;</span><span class="p">:</span> <span class="p">{</span>
         <span class="s2">&quot;fcnet_hiddens&quot;</span><span class="p">:</span> <span class="p">[],</span>
         <span class="s2">&quot;fcnet_activation&quot;</span><span class="p">:</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
     <span class="p">},</span>
     <span class="c1"># Hyperparameter to choose between exploration and exploitation. A higher value of beta adds</span>
     <span class="c1"># more importance to the intrinsic reward, as per the following equation</span>
     <span class="c1"># `reward = r + beta * intrinsic_reward`</span>
     <span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
     <span class="c1"># Schedule to use for beta decay, one of constant&quot; or &quot;linear_decay&quot;.</span>
     <span class="s2">&quot;beta_schedule&quot;</span><span class="p">:</span> <span class="s1">&#39;constant&#39;</span><span class="p">,</span>
     <span class="c1"># Specify, which exploration sub-type to use (usually, the algo&#39;s &quot;default&quot;</span>
     <span class="c1"># exploration, e.g. EpsilonGreedy for DQN, StochasticSampling for PG/SAC).</span>
     <span class="s2">&quot;sub_exploration&quot;</span><span class="p">:</span> <span class="p">{</span>
         <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;StochasticSampling&quot;</span><span class="p">,</span>
     <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Functionality</strong>
RLlibs RE3 is based on <a class="reference external" href="https://arxiv.org/pdf/2102.09430.pdf">Random Encoders for Efficient Exploration described in this paper here</a>.
RE3 quantifies exploration based on state entropy. The entropy of a state is calculated based on its distance from K nearest neighbor states present in the replay buffer in the latent space (With this implementation, KNN is implemented using training samples from the same batch).
The state entropy is considered as an intrinsic reward and for policy optimization added to the extrinsic reward when available.  If the extrinsic reward is not available then the state entropy is used as intrinsic reward for unsupervised pre-training of the RL agent.
RE3 further allows agents to learn in sparse-reward or even no-reward environments by
using the state entropy as intrinsic rewards.</p>
<p>This exploration objective can be used with both model-free and model-based RL algorithms.
RE3 uses a randomly initialized encoder to get the states latent representation, thus taking away the complexity of training the representation learning method. The encoder weights are fixed during the entire duration of the training process.</p>
</section>
<section id="fully-independent-learning">
<span id="fil"></span><h3>Fully Independent Learning<a class="headerlink" href="rllib-algorithms.html#fully-independent-learning" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="../rllib-env.html#multi-agent-and-hierarchical">[instructions]</a> Fully independent learning involves a collection of agents learning independently of each other via single agent methods. This typically works, but can be less effective than dedicated multi-agent RL methods, since they do not account for the non-stationarity of the multi-agent environment.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_independent_learning.py">waterworld</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_cartpole.py">multiagent-cartpole</a></p>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../rllib-env.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Environments</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="user-guides.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">User Guides</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>