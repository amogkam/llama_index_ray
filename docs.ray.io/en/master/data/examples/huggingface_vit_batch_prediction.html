
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Image Classification Batch Inference with Huggingface Vision Transformer &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/js/versionwarning.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../_static/js/docsearch.js"></script>
    <script src="../../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../_static/js/custom.js"></script>
    <script defer="defer" src="../../_static/js/top-navigation.js"></script>
    <script src="../../_static/js/tags.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script async="async" src="../../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/data/examples/huggingface_vit_batch_prediction.html" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Image Classification Batch Inference with PyTorch" href="pytorch_resnet_batch_prediction.html" />
    <link rel="prev" title="Ray Data Examples" href="index.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "data/examples/huggingface_vit_batch_prediction", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".ipynb", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../data.html">
   Ray Data
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="../overview.html">
     Overview
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../getting-started.html">
     Getting Started
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../key-concepts.html">
     Key Concepts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../user-guide.html">
     User Guides
    </a>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="index.html">
     Ray Data Examples
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="huggingface_vit_batch_prediction.html#">
       Image Classification Batch Inference with Huggingface Vision Transformer
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="pytorch_resnet_batch_prediction.html">
       Image Classification Batch Inference with PyTorch
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="batch_inference_object_detection.html">
       Object Detection Batch Inference with PyTorch
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="nyc_taxi_basic_processing.html">
       Processing NYC taxi data using Ray Data
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="batch_training.html">
       Batch Training with Ray Data
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="ocr_example.html">
       Scaling OCR using Ray Data
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="random-access.html">
       Random Data Access (Experimental)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="custom-datasource.html">
       Implementing a Custom Datasource
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../faq.html">
     FAQ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/api.html">
     Ray Data API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Fdata/examples/huggingface_vit_batch_prediction.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/data/examples/huggingface_vit_batch_prediction.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/data/examples/huggingface_vit_batch_prediction.ipynb.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="huggingface_vit_batch_prediction.html#step-1-reading-the-dataset-from-s3">
   Step 1: Reading the Dataset from S3
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="huggingface_vit_batch_prediction.html#step-2-inference-on-a-single-batch">
   Step 2: Inference on a single batch
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="huggingface_vit_batch_prediction.html#step-3-scaling-up-to-the-full-dataset-with-ray-data">
   Step 3: Scaling up to the full Dataset with Ray Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="huggingface_vit_batch_prediction.html#verify-and-save-results">
     Verify and Save Results
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Image Classification Batch Inference with Huggingface Vision Transformer</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="huggingface_vit_batch_prediction.html#step-1-reading-the-dataset-from-s3">
   Step 1: Reading the Dataset from S3
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="huggingface_vit_batch_prediction.html#step-2-inference-on-a-single-batch">
   Step 2: Inference on a single batch
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="huggingface_vit_batch_prediction.html#step-3-scaling-up-to-the-full-dataset-with-ray-data">
   Step 3: Scaling up to the full Dataset with Ray Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="huggingface_vit_batch_prediction.html#verify-and-save-results">
     Verify and Save Results
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="image-classification-batch-inference-with-huggingface-vision-transformer">
<h1>Image Classification Batch Inference with Huggingface Vision Transformer<a class="headerlink" href="huggingface_vit_batch_prediction.html#image-classification-batch-inference-with-huggingface-vision-transformer" title="Permalink to this headline">#</a></h1>
<p>In this example, we will introduce how to use the <a class="reference internal" href="../data.html#data"><span class="std std-ref">Ray Data</span></a> for <strong>large-scale image classification batch inference with multiple GPU workers.</strong></p>
<p>In particular, we will:</p>
<ul class="simple">
<li><p>Load Imagenette dataset from S3 bucket and create a <a class="reference internal" href="../api/doc/ray.data.Dataset.html#ray.data.Dataset" title="ray.data.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ray</span> <span class="pre">Dataset</span></code></a>.</p></li>
<li><p>Load a pretrained Vision Transformer from Huggingface that’s been trained on ImageNet.</p></li>
<li><p>Use <a class="reference internal" href="../data.html#data"><span class="std std-ref">Ray Data</span></a> to preprocess the dataset and do model inference parallelizing across multiple GPUs</p></li>
<li><p>Evaluate the predictions and save results to S3/local disk.</p></li>
</ul>
<p>This example will still work even if you do not have GPUs available, but overall performance will be slower.</p>
<p>To run this example, you will need to install the following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip install -q -U <span class="s2">&quot;ray[data]&quot;</span> transformers
</pre></div>
</div>
</div>
</div>
<section id="step-1-reading-the-dataset-from-s3">
<h2>Step 1: Reading the Dataset from S3<a class="headerlink" href="huggingface_vit_batch_prediction.html#step-1-reading-the-dataset-from-s3" title="Permalink to this headline">#</a></h2>
<p><a class="reference external" href="https://github.com/fastai/imagenette">Imagenette</a> is a subset of Imagenet with 10 classes. We have this dataset hosted publicly in an S3 bucket. Since we are only doing inference here, we load in just the validation split.</p>
<p>Here, we use <a class="reference internal" href="../api/doc/ray.data.read_images.html#ray.data.read_images" title="ray.data.read_images"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.data.read_images</span></code></a> to load the validation set from S3. <a class="reference internal" href="../data.html#data"><span class="std std-ref">Ray Data</span></a> also supports reading from a variety of other <a class="reference internal" href="../loading-data.html#loading-data"><span class="std std-ref">datasources and formats</span></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>

<span class="n">s3_uri</span> <span class="o">=</span> <span class="s2">&quot;s3://anonymous@air-example-data-2/imagenette2/val/&quot;</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_images</span><span class="p">(</span>
    <span class="n">s3_uri</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;RGB&quot;</span>
<span class="p">)</span>
<span class="n">ds</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2023-05-24 11:25:47]  INFO ray._private.worker::Connecting to existing Ray cluster at address: 10.0.33.149:6379...
[2023-05-24 11:25:47]  INFO ray._private.worker::Connected to Ray cluster. View the dashboard at <span class=" -Color -Color-Bold -Color-Bold-Green">https://console.anyscale-staging.com/api/v2/sessions/ses_6h5a4kl2xhfgtdy4w41he6iwyw/services?redirect_to=dashboard </span>
[2023-05-24 11:25:47]  INFO ray._private.runtime_env.packaging::Pushing file package &#39;gcs://_ray_pkg_2429254893b10da6df2b65ceaf858894.zip&#39; (8.71MiB) to Ray cluster...
[2023-05-24 11:25:47]  INFO ray._private.runtime_env.packaging::Successfully pushed file package &#39;gcs://_ray_pkg_2429254893b10da6df2b65ceaf858894.zip&#39;.
[2023-05-24 11:25:50] [Ray Data] WARNING ray.data.dataset::<span class=" -Color -Color-Yellow">Important: Ray Data requires schemas for all datasets in Ray 2.5. This means that standalone Python objects are no longer supported. In addition, the default batch format is fixed to NumPy. To revert to legacy behavior temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes.</span>

<span class=" -Color -Color-Yellow">Learn more here: https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode</span>
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "65441ade9218437f80a49201adddc111", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<p>Inspecting the schema, we can see that there is 1 column in the dataset containing the images stored as Numpy arrays.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ds</span><span class="o">.</span><span class="n">schema</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Column  Type
------  ----
image   numpy.ndarray(ndim=3, dtype=uint8)
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-2-inference-on-a-single-batch">
<h2>Step 2: Inference on a single batch<a class="headerlink" href="huggingface_vit_batch_prediction.html#step-2-inference-on-a-single-batch" title="Permalink to this headline">#</a></h2>
<p>Next, we can do inference on a single batch of data, using a pre-trained Vision Transformer from Huggingface following <a class="reference external" href="https://huggingface.co/docs/transformers/tasks/image_classification#inference">this Huggingface example</a>.</p>
<p>Let’s get a batch of 10 from our dataset. Each image in the batch is represented as a Numpy array.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">single_batch</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">take_batch</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can visualize 1 image from this batch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">single_batch</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="n">img</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/huggingface_vit_batch_prediction_12_0.png" src="../../_images/huggingface_vit_batch_prediction_12_0.png" />
</div>
</div>
<p>Now, let’s create a Huggingface Image Classification pipeline from a pre-trained Vision Transformer model.</p>
<p>We specify the following configurations:</p>
<ol class="simple">
<li><p>Set the device to “cuda:0” to use GPU for inference</p></li>
<li><p>We set the batch size to 10 so that we can maximize GPU utilization and do inference on the entire batch at once.</p></li>
</ol>
<p>We also convert the image Numpy arrays into PIL Images since that’s what Huggingface expects.</p>
<p>From the results, we see that all of the images in the batch are correctly being classified as “tench” which is a type of fish.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="c1"># If doing CPU inference, set device=&quot;cpu&quot; instead.</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;image-classification&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;google/vit-base-patch16-224&quot;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">([</span><span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">image_array</span><span class="p">)</span> <span class="k">for</span> <span class="n">image_array</span> <span class="ow">in</span> <span class="n">single_batch</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">]],</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="k">del</span> <span class="n">classifier</span> <span class="c1"># Delete the classifier to free up GPU memory.</span>
<span class="n">outputs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[{&#39;score&#39;: 0.9997267127037048, &#39;label&#39;: &#39;tench, Tinca tinca&#39;}],
 [{&#39;score&#39;: 0.9993537068367004, &#39;label&#39;: &#39;tench, Tinca tinca&#39;}],
 [{&#39;score&#39;: 0.9997393488883972, &#39;label&#39;: &#39;tench, Tinca tinca&#39;}],
 [{&#39;score&#39;: 0.99950110912323, &#39;label&#39;: &#39;tench, Tinca tinca&#39;}],
 [{&#39;score&#39;: 0.9986729621887207, &#39;label&#39;: &#39;tench, Tinca tinca&#39;}],
 [{&#39;score&#39;: 0.999290943145752, &#39;label&#39;: &#39;tench, Tinca tinca&#39;}],
 [{&#39;score&#39;: 0.9997896552085876, &#39;label&#39;: &#39;tench, Tinca tinca&#39;}],
 [{&#39;score&#39;: 0.9997585415840149, &#39;label&#39;: &#39;tench, Tinca tinca&#39;}],
 [{&#39;score&#39;: 0.9985774755477905, &#39;label&#39;: &#39;tench, Tinca tinca&#39;}],
 [{&#39;score&#39;: 0.9996065497398376, &#39;label&#39;: &#39;tench, Tinca tinca&#39;}]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-3-scaling-up-to-the-full-dataset-with-ray-data">
<h2>Step 3: Scaling up to the full Dataset with Ray Data<a class="headerlink" href="huggingface_vit_batch_prediction.html#step-3-scaling-up-to-the-full-dataset-with-ray-data" title="Permalink to this headline">#</a></h2>
<p>By using Ray Data, we can apply the same logic in the previous section to scale up to the entire dataset, leveraging all the GPUs in our cluster.</p>
<p>There are a couple unique properties about the inference step:</p>
<ol class="simple">
<li><p>Model initialization is usually pretty expensive</p></li>
<li><p>We want to do inference in batches to maximize GPU utilization.</p></li>
</ol>
<p>To address 1, we package the inference code in a <code class="docutils literal notranslate"><span class="pre">ImageClassifier</span></code> class. Using a class allows us to put the expensive pipeline loading and initialization code in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> constructor, which will run only once.
The actual model inference logic is in the <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method, which will be called for each batch.</p>
<p>To address 2, we do our inference in batches, specifying a <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> to the Huggingface Pipeline.
The <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method takes a batch of data items, instead of a single one.
In this case, the batch is a dict that has one key named “image”, and the value is a Numpy array of images represented in <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> format. This is the same format in step 2, and we can reuse the same inferencing logic from step 2.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="c1"># Pick the largest batch size that can fit on our GPUs</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">1024</span>

<span class="k">class</span> <span class="nc">ImageClassifier</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># If doing CPU inference, set `device=&quot;cpu&quot;` instead.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;image-classification&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;google/vit-base-patch16-224&quot;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]):</span>
        <span class="c1"># Convert the numpy array of images into a list of PIL images which is the format the HF pipeline expects.</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span>
            <span class="p">[</span><span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">image_array</span><span class="p">)</span> <span class="k">for</span> <span class="n">image_array</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">]],</span> 
            <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
            <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
        
        <span class="c1"># `outputs` is a list of length-one lists. For example:</span>
        <span class="c1"># [[{&#39;score&#39;: &#39;...&#39;, &#39;label&#39;: &#39;...&#39;}], ..., [{&#39;score&#39;: &#39;...&#39;, &#39;label&#39;: &#39;...&#39;}]]</span>
        <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;score&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;score&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">]</span>
        <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;label&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
</div>
</div>
<p>Then we use the <a class="reference internal" href="../api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches" title="ray.data.Dataset.map_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">map_batches</span></code></a> API to apply the model to the whole dataset.</p>
<p>The first parameter of <code class="docutils literal notranslate"><span class="pre">map_batches</span></code> is the user-defined function (UDF), which can either be a function or a class. Since we are using a class in this case, the UDF will run as long-running <a class="reference external" href="https://docs.ray.io/en/latest/ray-core/key-concepts.html#actors">Ray actors</a>. For class-based UDFs, we use the <code class="docutils literal notranslate"><span class="pre">compute</span></code> argument to specify <code class="xref py py-class docutils literal notranslate"><span class="pre">ActorPoolStrategy</span></code> with the number of parallel actors. And the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> argument indicates the number of images in each batch.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">num_gpus</span></code> argument specifies the number of GPUs needed for each <code class="docutils literal notranslate"><span class="pre">ImageClassifier</span></code> instance. In this case, we want 1 GPU for each model replica.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span>
    <span class="n">ImageClassifier</span><span class="p">,</span>
    <span class="n">compute</span><span class="o">=</span><span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">ActorPoolStrategy</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span> <span class="c1"># Use 4 GPUs. Change this number based on the number of GPUs in your cluster.</span>
    <span class="n">num_gpus</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># Specify 1 GPU per model replica.</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span> <span class="c1"># Use the largest batch size that can fit on our GPUs</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="verify-and-save-results">
<h3>Verify and Save Results<a class="headerlink" href="huggingface_vit_batch_prediction.html#verify-and-save-results" title="Permalink to this headline">#</a></h3>
<p>Let’s take a small batch and verify the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prediction_batch</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">take_batch</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2023-05-24 12:08:44] [Ray Data] INFO ray.data._internal.execution.streaming_executor.logfile::Executing DAG InputDataBuffer[Input] -&gt; TaskPoolMapOperator[ReadImage] -&gt; ActorPoolMapOperator[MapBatches(ImageClassifier)]
[2023-05-24 12:08:44] [Ray Data] INFO ray.data._internal.execution.streaming_executor.logfile::Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
[2023-05-24 12:08:44] [Ray Data] INFO ray.data._internal.execution.streaming_executor.logfile::Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
[2023-05-24 12:08:44] [Ray Data] INFO ray.data._internal.execution.operators.actor_pool_map_operator.logfile::MapBatches(ImageClassifier): Waiting for 4 pool actors to start...
(_MapWorker pid=137172) 2023-05-24 12:08:49.035713: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libnvinfer_plugin.so.7&#39;; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]
(_MapWorker pid=137172) 2023-05-24 12:08:49.035721: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
(_MapWorker pid=131332) /home/ray/anaconda3/lib/python3.10/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. [repeated 2x across cluster]
(_MapWorker pid=131332)   from pandas import MultiIndex, Int64Index [repeated 2x across cluster]
(_MapWorker pid=137169) 2023-05-24 12:08:48.988387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
(_MapWorker pid=137170) 2023-05-24 12:08:49.136309: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libnvinfer_plugin.so.7&#39;; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 6x across cluster]
(_MapWorker pid=137169)   from pandas import MultiIndex, Int64Index
(_MapWorker pid=137169)   from pandas import MultiIndex, Int64Index
(_MapWorker pid=137170) 2023-05-24 12:08:49.136316: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. [repeated 2x across cluster]
(_MapWorker pid=137171) /home/ray/anaconda3/lib/python3.10/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.
(_MapWorker pid=137171)   from pandas import MultiIndex, Int64Index
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "f9f698caae8c4ef083243c20c0d294d1", "version_major": 2, "version_minor": 0}
</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2023-05-24 12:09:22] [Ray Data] INFO ray.data._internal.execution.streaming_executor.logfile::Shutting down &lt;StreamingExecutor(Thread-759, started daemon 140117905766144)&gt;.
[2023-05-24 12:09:22] [Ray Data] WARNING ray.data._internal.execution.operators.actor_pool_map_operator.logfile::To ensure full parallelization across an actor pool of size 4, the specified batch size should be at most 255. Your configured batch size for this operator was 1024.
</pre></div>
</div>
</div>
</div>
<p>We see that all the images are correctly classified as “tench”, which is a type of fish.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="k">for</span> <span class="n">image</span><span class="p">,</span> <span class="n">prediction</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">prediction_batch</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">],</span> <span class="n">prediction_batch</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">display</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Label: &quot;</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/huggingface_vit_batch_prediction_25_0.png" src="../../_images/huggingface_vit_batch_prediction_25_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Label:  tench, Tinca tinca
</pre></div>
</div>
<img alt="../../_images/huggingface_vit_batch_prediction_25_2.png" src="../../_images/huggingface_vit_batch_prediction_25_2.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Label:  tench, Tinca tinca
</pre></div>
</div>
<img alt="../../_images/huggingface_vit_batch_prediction_25_4.png" src="../../_images/huggingface_vit_batch_prediction_25_4.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Label:  tench, Tinca tinca
</pre></div>
</div>
<img alt="../../_images/huggingface_vit_batch_prediction_25_6.png" src="../../_images/huggingface_vit_batch_prediction_25_6.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Label:  tench, Tinca tinca
</pre></div>
</div>
<img alt="../../_images/huggingface_vit_batch_prediction_25_8.png" src="../../_images/huggingface_vit_batch_prediction_25_8.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Label:  tench, Tinca tinca
</pre></div>
</div>
</div>
</div>
<p>If the samples look good, we can proceed with saving the results to an external storage, e.g., S3 or local disks. See <a class="reference external" href="https://docs.ray.io/en/latest/data/api/input_output.html">Ray Data Input/Output</a> for all supported stoarges and file formats.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ds</span><span class="o">.</span><span class="n">write_parquet</span><span class="p">(</span><span class="s2">&quot;local://tmp/inference_results&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="index.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Ray Data Examples</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="pytorch_resnet_batch_prediction.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Image Classification Batch Inference with PyTorch</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>