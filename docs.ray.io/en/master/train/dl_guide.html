
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Distributed Deep Learning with Ray Train User Guide &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/versionwarning.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../_static/js/docsearch.js"></script>
    <script src="../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../_static/js/termynal.js"></script>
    <script defer="defer" src="../_static/js/custom.js"></script>
    <script defer="defer" src="../_static/js/top-navigation.js"></script>
    <script src="../_static/js/tags.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/train/dl_guide.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="XGBoost &amp; LightGBM User Guide for Ray Train" href="gbdt.html" />
    <link rel="prev" title="Ray Train Configuration User Guide" href="config_guide.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "train/dl_guide", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="train.html">
   Ray Train
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="getting-started.html">
     Getting Started
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="key-concepts.html">
     Key Concepts
    </a>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="user-guides.html">
     User Guides
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3">
      <a class="reference internal" href="config_guide.html">
       Configuring Ray Train
      </a>
     </li>
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="dl_guide.html#">
       Deep Learning Guide
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="gbdt.html">
       XGBoost/LightGBM guide
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="architecture.html">
       Ray Train Architecture
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="examples.html">
     Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="faq.html">
     Ray Train FAQ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="api/api.html">
     Ray Train API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Ftrain/dl_guide.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/train/dl_guide.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/train/dl_guide.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#using-deep-learning-frameworks-as-backends">
   Using Deep Learning Frameworks as Backends
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#porting-code-from-pytorch-tensorflow-or-horovod-to-ray-train">
   Porting code from PyTorch, TensorFlow, or Horovod to Ray Train
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#updating-your-training-function">
     Updating your training function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#creating-a-ray-train-trainer">
     Creating a Ray Train Trainer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#running-your-training-function">
     Running your training function
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#configuring-training">
   Configuring Training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#accessing-training-results">
   Accessing Training Results
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#log-directory-structure">
     Log Directory Structure
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#distributed-data-ingest-with-ray-data-and-ray-train">
   Distributed Data Ingest with Ray Data and Ray Train
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#logging-checkpointing-and-callbacks-in-ray-train">
   Logging, Checkpointing and Callbacks in Ray Train
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#reporting-intermediate-results-and-handling-checkpoints">
     Reporting intermediate results and handling checkpoints
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="dl_guide.html#saving-checkpoints">
       Saving checkpoints
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="dl_guide.html#configuring-checkpoints">
       Configuring checkpoints
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="dl_guide.html#loading-checkpoints">
       Loading checkpoints
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#callbacks">
     Callbacks
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="dl_guide.html#example-logging-to-mlflow-and-tensorboard">
       Example: Logging to MLflow and TensorBoard
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="dl_guide.html#custom-callbacks">
       Custom Callbacks
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#how-to-obtain-and-aggregate-results-from-different-workers">
     How to obtain and aggregate results from different workers?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#fault-tolerance">
   Fault Tolerance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#automatically-recover-from-train-worker-failures">
     Automatically Recover from Train Worker Failures
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#restore-a-ray-train-experiment">
     Restore a Ray Train Experiment
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="dl_guide.html#auto-resume">
       Auto-resume
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#hyperparameter-tuning-ray-tune">
   Hyperparameter tuning (Ray Tune)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#automatic-mixed-precision">
   Automatic Mixed Precision
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#reproducibility">
   Reproducibility
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Distributed Deep Learning with Ray Train User Guide</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#using-deep-learning-frameworks-as-backends">
   Using Deep Learning Frameworks as Backends
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#porting-code-from-pytorch-tensorflow-or-horovod-to-ray-train">
   Porting code from PyTorch, TensorFlow, or Horovod to Ray Train
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#updating-your-training-function">
     Updating your training function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#creating-a-ray-train-trainer">
     Creating a Ray Train Trainer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#running-your-training-function">
     Running your training function
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#configuring-training">
   Configuring Training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#accessing-training-results">
   Accessing Training Results
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#log-directory-structure">
     Log Directory Structure
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#distributed-data-ingest-with-ray-data-and-ray-train">
   Distributed Data Ingest with Ray Data and Ray Train
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#logging-checkpointing-and-callbacks-in-ray-train">
   Logging, Checkpointing and Callbacks in Ray Train
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#reporting-intermediate-results-and-handling-checkpoints">
     Reporting intermediate results and handling checkpoints
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="dl_guide.html#saving-checkpoints">
       Saving checkpoints
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="dl_guide.html#configuring-checkpoints">
       Configuring checkpoints
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="dl_guide.html#loading-checkpoints">
       Loading checkpoints
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#callbacks">
     Callbacks
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="dl_guide.html#example-logging-to-mlflow-and-tensorboard">
       Example: Logging to MLflow and TensorBoard
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="dl_guide.html#custom-callbacks">
       Custom Callbacks
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#how-to-obtain-and-aggregate-results-from-different-workers">
     How to obtain and aggregate results from different workers?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#fault-tolerance">
   Fault Tolerance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#automatically-recover-from-train-worker-failures">
     Automatically Recover from Train Worker Failures
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="dl_guide.html#restore-a-ray-train-experiment">
     Restore a Ray Train Experiment
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="dl_guide.html#auto-resume">
       Auto-resume
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#hyperparameter-tuning-ray-tune">
   Hyperparameter tuning (Ray Tune)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#automatic-mixed-precision">
   Automatic Mixed Precision
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="dl_guide.html#reproducibility">
   Reproducibility
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="distributed-deep-learning-with-ray-train-user-guide">
<span id="train-dl-guide"></span><h1>Distributed Deep Learning with Ray Train User Guide<a class="headerlink" href="dl_guide.html#distributed-deep-learning-with-ray-train-user-guide" title="Permalink to this headline">#</a></h1>
<p>This guide explains how to use Train to scale PyTorch, TensorFlow and Horovod.</p>
<p>In this guide, we cover examples for the following use cases:</p>
<ul class="simple">
<li><p>How do I <a class="reference internal" href="dl_guide.html#train-porting-code"><span class="std std-ref">port my code</span></a> to use Ray Train?</p></li>
<li><p>How do I use Ray Train to <a class="reference internal" href="dl_guide.html#train-datasets"><span class="std std-ref">train with a large dataset</span></a>?</p></li>
<li><p>How do I <a class="reference internal" href="dl_guide.html#train-monitoring"><span class="std std-ref">monitor</span></a> my training?</p></li>
<li><p>How do I run my training on pre-emptible instances
(<a class="reference internal" href="dl_guide.html#train-fault-tolerance"><span class="std std-ref">fault tolerance</span></a>)?</p></li>
<li><p>How do I <a class="reference internal" href="dl_guide.html#train-tune"><span class="std std-ref">tune</span></a> my Ray Train model?</p></li>
</ul>
<section id="using-deep-learning-frameworks-as-backends">
<span id="train-backends"></span><h2>Using Deep Learning Frameworks as Backends<a class="headerlink" href="dl_guide.html#using-deep-learning-frameworks-as-backends" title="Permalink to this headline">#</a></h2>
<p>Ray Train provides a thin API around different backend frameworks for
distributed deep learning. At the moment, Ray Train allows you to perform
training with:</p>
<ul class="simple">
<li><p><strong>PyTorch:</strong> Ray Train initializes your distributed process group, allowing
you to run your <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> training script. See <a class="reference external" href="https://pytorch.org/tutorials/beginner/dist_overview.html">PyTorch
Distributed Overview</a>
for more information.</p></li>
<li><p><strong>TensorFlow:</strong>  Ray Train configures <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> for you, allowing you to run
your <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> training script. See <a class="reference external" href="https://www.tensorflow.org/guide/distributed_training">Distributed
training with TensorFlow</a>
for more information.</p></li>
<li><p><strong>Horovod:</strong> Ray Train configures the Horovod environment and Rendezvous
server for you, allowing you to run your <code class="docutils literal notranslate"><span class="pre">DistributedOptimizer</span></code> training
script. See <a class="reference external" href="https://horovod.readthedocs.io/en/stable/index.html">Horovod documentation</a>
for more information.</p></li>
</ul>
</section>
<section id="porting-code-from-pytorch-tensorflow-or-horovod-to-ray-train">
<span id="train-porting-code"></span><h2>Porting code from PyTorch, TensorFlow, or Horovod to Ray Train<a class="headerlink" href="dl_guide.html#porting-code-from-pytorch-tensorflow-or-horovod-to-ray-train" title="Permalink to this headline">#</a></h2>
<p>The following instructions assume you have a training function
that can already be run on a single worker for one of the supported
<a class="reference internal" href="dl_guide.html#train-backends"><span class="std std-ref">backend</span></a> frameworks.</p>
<section id="updating-your-training-function">
<h3>Updating your training function<a class="headerlink" href="dl_guide.html#updating-your-training-function" title="Permalink to this headline">#</a></h3>
<p>First, you’ll want to update your training function to support distributed
training.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-0">
PyTorch</label><div class="sd-tab-content docutils">
<p>Ray Train will set up your distributed process group for you and also provides utility methods
to automatically prepare your model and data for distributed training.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Ray Train will still work even if you don’t use the <a class="reference internal" href="api/doc/ray.train.torch.prepare_model.html#ray.train.torch.prepare_model" title="ray.train.torch.prepare_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">ray.train.torch.prepare_model()</span></code></a>
and <a class="reference internal" href="api/doc/ray.train.torch.prepare_data_loader.html#ray.train.torch.prepare_data_loader" title="ray.train.torch.prepare_data_loader"><code class="xref py py-func docutils literal notranslate"><span class="pre">ray.train.torch.prepare_data_loader()</span></code></a> utilities below,
and instead handle the logic directly inside your training function.</p>
</div>
<p>First, use the <a class="reference internal" href="api/doc/ray.train.torch.prepare_model.html#ray.train.torch.prepare_model" title="ray.train.torch.prepare_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">prepare_model()</span></code></a> function to automatically move your model to the right device and wrap it in
<code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code>:</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>import torch<span class="w"></span>
<span class="w"> </span>from torch.nn.parallel import DistributedDataParallel<span class="w"></span>
<span class="gi">+from ray.air import session</span><span class="w"></span>
<span class="gi">+from ray import train</span><span class="w"></span>
<span class="gi">+import ray.train.torch</span><span class="w"></span>


<span class="w"> </span>def train_func():<span class="w"></span>
<span class="gd">-    device = torch.device(f&quot;cuda:{session.get_local_rank()}&quot; if</span><span class="w"></span>
<span class="gd">-        torch.cuda.is_available() else &quot;cpu&quot;)</span><span class="w"></span>
<span class="gd">-    torch.cuda.set_device(device)</span><span class="w"></span>

<span class="w"> </span>    # Create model.<span class="w"></span>
<span class="w"> </span>    model = NeuralNetwork()<span class="w"></span>

<span class="gd">-    model = model.to(device)</span><span class="w"></span>
<span class="gd">-    model = DistributedDataParallel(model,</span><span class="w"></span>
<span class="gd">-        device_ids=[session.get_local_rank()] if torch.cuda.is_available() else None)</span><span class="w"></span>

<span class="gi">+    model = train.torch.prepare_model(model)</span><span class="w"></span>

<span class="w"> </span>    ...<span class="w"></span>
</pre></div>
</div>
<p>Then, use the <code class="docutils literal notranslate"><span class="pre">prepare_data_loader</span></code> function to automatically add a <code class="docutils literal notranslate"><span class="pre">DistributedSampler</span></code> to your <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>
and move the batches to the right device. This step is not necessary if you are passing in Ray Data to your Trainer
(see <a class="reference internal" href="dl_guide.html#train-datasets"><span class="std std-ref">Distributed Data Ingest with Ray Data and Ray Train</span></a>):</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>import torch<span class="w"></span>
<span class="w"> </span>from torch.utils.data import DataLoader, DistributedSampler<span class="w"></span>
<span class="gi">+from ray.air import session</span><span class="w"></span>
<span class="gi">+from ray import train</span><span class="w"></span>
<span class="gi">+import ray.train.torch</span><span class="w"></span>


<span class="w"> </span>def train_func():<span class="w"></span>
<span class="gd">-    device = torch.device(f&quot;cuda:{session.get_local_rank()}&quot; if</span><span class="w"></span>
<span class="gd">-        torch.cuda.is_available() else &quot;cpu&quot;)</span><span class="w"></span>
<span class="gd">-    torch.cuda.set_device(device)</span><span class="w"></span>

<span class="w"> </span>    ...<span class="w"></span>

<span class="gd">-    data_loader = DataLoader(my_dataset, batch_size=worker_batch_size, sampler=DistributedSampler(dataset))</span><span class="w"></span>

<span class="gi">+    data_loader = DataLoader(my_dataset, batch_size=worker_batch_size)</span><span class="w"></span>
<span class="gi">+    data_loader = train.torch.prepare_data_loader(data_loader)</span><span class="w"></span>

<span class="w"> </span>    for X, y in data_loader:<span class="w"></span>
<span class="gd">-        X = X.to_device(device)</span><span class="w"></span>
<span class="gd">-        y = y.to_device(device)</span><span class="w"></span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Keep in mind that <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> takes in a <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> which is the batch size for each worker.
The global batch size can be calculated from the worker batch size (and vice-versa) with the following equation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">global_batch_size</span> <span class="o">=</span> <span class="n">worker_batch_size</span> <span class="o">*</span> <span class="n">session</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-1">
TensorFlow</label><div class="sd-tab-content docutils">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The current TensorFlow implementation supports
<code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> (and <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code>). If there are
other strategies you wish to see supported by Ray Train, please let us know
by submitting a <a class="reference external" href="https://github.com/ray-project/ray/issues">feature request on GitHub</a>.</p>
</div>
<p>These instructions closely follow TensorFlow’s <a class="reference external" href="https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras">Multi-worker training
with Keras</a>
tutorial. One key difference is that Ray Train will handle the environment
variable set up for you.</p>
<p><strong>Step 1:</strong> Wrap your model in <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code>.</p>
<p>The <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy">MultiWorkerMirroredStrategy</a>
enables synchronous distributed training. The <code class="docutils literal notranslate"><span class="pre">Model</span></code> <em>must</em> be built and
compiled within the scope of the strategy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MultiWorkerMirroredStrategy</span><span class="p">()</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># build model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Step 2:</strong> Update your <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> batch size to the <em>global</em> batch
size.</p>
<p>The <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch">batch</a>
will be split evenly across worker processes, so <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> should be
set appropriately.</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="gd">-batch_size = worker_batch_size</span><span class="w"></span>
<span class="gi">+batch_size = worker_batch_size * session.get_world_size()</span><span class="w"></span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-2" name="sd-tab-set-0" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-2">
Horovod</label><div class="sd-tab-content docutils">
<p>If you have a training function that already runs with the <a class="reference external" href="https://horovod.readthedocs.io/en/stable/ray_include.html#horovod-ray-executor">Horovod Ray
Executor</a>,
you should not need to make any additional changes!</p>
<p>To onboard onto Horovod, please visit the <a class="reference external" href="https://horovod.readthedocs.io/en/stable/index.html#get-started">Horovod guide</a>.</p>
</div>
</div>
</section>
<section id="creating-a-ray-train-trainer">
<h3>Creating a Ray Train Trainer<a class="headerlink" href="dl_guide.html#creating-a-ray-train-trainer" title="Permalink to this headline">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Trainer</span></code>s are the primary Ray Train classes that are used to manage state and
execute training. You can create a simple <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> for the backend of choice
with one of the following:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-3" name="sd-tab-set-1" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-3">
PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>
<span class="c1"># For GPU Training, set `use_gpu` to True.</span>
<span class="n">use_gpu</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="n">use_gpu</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-4" name="sd-tab-set-1" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-4">
TensorFlow</label><div class="sd-tab-content docutils">
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Ray will not automatically set any environment variables or configuration
related to local parallelism / threading
<a class="reference internal" href="../ray-core/configure.html#omp-num-thread-note"><span class="std std-ref">aside from “OMP_NUM_THREADS”</span></a>.
If you desire greater control over TensorFlow threading, use
the <code class="docutils literal notranslate"><span class="pre">tf.config.threading</span></code> module (eg.
<code class="docutils literal notranslate"><span class="pre">tf.config.threading.set_inter_op_parallelism_threads(num_cpus)</span></code>)
at the beginning of your <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code> function.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.tensorflow</span> <span class="kn">import</span> <span class="n">TensorflowTrainer</span>
<span class="c1"># For GPU Training, set `use_gpu` to True.</span>
<span class="n">use_gpu</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TensorflowTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="n">use_gpu</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-5" name="sd-tab-set-1" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-5">
Horovod</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.horovod</span> <span class="kn">import</span> <span class="n">HorovodTrainer</span>
<span class="c1"># For GPU Training, set `use_gpu` to True.</span>
<span class="n">use_gpu</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">HorovodTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="n">use_gpu</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To customize the backend setup, you can use the <a class="reference internal" href="api/api.html#train-integration-api"><span class="std std-ref">framework-specific config objects</span></a>.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-6" name="sd-tab-set-2" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-6">
PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span><span class="p">,</span> <span class="n">TorchConfig</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">torch_backend</span><span class="o">=</span><span class="n">TorchConfig</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-7" name="sd-tab-set-2" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-7">
TensorFlow</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.tensorflow</span> <span class="kn">import</span> <span class="n">TensorflowTrainer</span><span class="p">,</span> <span class="n">TensorflowConfig</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TensorflowTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">tensorflow_backend</span><span class="o">=</span><span class="n">TensorflowConfig</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-8" name="sd-tab-set-2" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-8">
Horovod</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.horovod</span> <span class="kn">import</span> <span class="n">HorovodTrainer</span><span class="p">,</span> <span class="n">HorovodConfig</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">HorovodTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">tensorflow_backend</span><span class="o">=</span><span class="n">HorovodConfig</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>For more configurability, please reference the <a class="reference internal" href="api/doc/ray.train.data_parallel_trainer.DataParallelTrainer.html#ray.train.data_parallel_trainer.DataParallelTrainer" title="ray.train.data_parallel_trainer.DataParallelTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataParallelTrainer</span></code></a> API.</p>
</section>
<section id="running-your-training-function">
<h3>Running your training function<a class="headerlink" href="dl_guide.html#running-your-training-function" title="Permalink to this headline">#</a></h3>
<p>With a distributed training function and a Ray Train <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>, you are now
ready to start training!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="configuring-training">
<h2>Configuring Training<a class="headerlink" href="dl_guide.html#configuring-training" title="Permalink to this headline">#</a></h2>
<p>With Ray Train, you can execute a training function (<code class="docutils literal notranslate"><span class="pre">train_func</span></code>) in a
distributed manner by calling <code class="docutils literal notranslate"><span class="pre">Trainer.fit</span></code>. To pass arguments
into the training function, you can expose a single <code class="docutils literal notranslate"><span class="pre">config</span></code> dictionary parameter:</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="gd">-def train_func():</span><span class="w"></span>
<span class="gi">+def train_func(config):</span><span class="w"></span>
</pre></div>
</div>
<p>Then, you can pass in the config dictionary as an argument to <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>:</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="gi">+config = {} # This should be populated.</span><span class="w"></span>
<span class="w"> </span>trainer = TorchTrainer(<span class="w"></span>
<span class="w"> </span>    train_func,<span class="w"></span>
<span class="gi">+    train_loop_config=config,</span><span class="w"></span>
<span class="w"> </span>    scaling_config=ScalingConfig(num_workers=2)<span class="w"></span>
<span class="w"> </span>)<span class="w"></span>
</pre></div>
</div>
<p>Putting this all together, you can run your training function with different
configurations. As an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">]):</span>
        <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">({</span><span class="s2">&quot;epoch&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">})</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">])</span>
<span class="c1"># 1</span>
</pre></div>
</div>
<p>A primary use-case for <code class="docutils literal notranslate"><span class="pre">config</span></code> is to try different hyperparameters. To
perform hyperparameter tuning with Ray Train, please refer to the
<a class="reference internal" href="dl_guide.html#train-tune"><span class="std std-ref">Ray Tune integration</span></a>.</p>
</section>
<section id="accessing-training-results">
<span id="train-result-object"></span><h2>Accessing Training Results<a class="headerlink" href="dl_guide.html#accessing-training-results" title="Permalink to this headline">#</a></h2>
<p>The return of a <code class="docutils literal notranslate"><span class="pre">Trainer.fit</span></code> is a <a class="reference internal" href="../tune/api/doc/ray.air.Result.html#ray.air.Result" title="ray.air.result.Result"><code class="xref py py-class docutils literal notranslate"><span class="pre">Result</span></code></a> object, containing
information about the training run. You can access it to obtain saved checkpoints,
metrics and other relevant data.</p>
<p>For example, you can:</p>
<ul class="simple">
<li><p>Print the metrics for the last training iteration:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
<span class="c1"># {&#39;_time_this_iter_s&#39;: 0.001016855239868164,</span>
<span class="c1">#  &#39;_timestamp&#39;: 1657829125,</span>
<span class="c1">#  &#39;_training_iteration&#39;: 2,</span>
<span class="c1">#  &#39;config&#39;: {},</span>
<span class="c1">#  &#39;date&#39;: &#39;2022-07-14_20-05-25&#39;,</span>
<span class="c1">#  &#39;done&#39;: True,</span>
<span class="c1">#  &#39;episodes_total&#39;: None,</span>
<span class="c1">#  &#39;epoch&#39;: 1,</span>
<span class="c1">#  &#39;experiment_id&#39;: &#39;5a3f8b9bf875437881a8ddc7e4dd3340&#39;,</span>
<span class="c1">#  &#39;experiment_tag&#39;: &#39;0&#39;,</span>
<span class="c1">#  &#39;hostname&#39;: &#39;ip-172-31-43-110&#39;,</span>
<span class="c1">#  &#39;iterations_since_restore&#39;: 2,</span>
<span class="c1">#  &#39;node_ip&#39;: &#39;172.31.43.110&#39;,</span>
<span class="c1">#  &#39;pid&#39;: 654068,</span>
<span class="c1">#  &#39;time_since_restore&#39;: 3.4353830814361572,</span>
<span class="c1">#  &#39;time_this_iter_s&#39;: 0.00809168815612793,</span>
<span class="c1">#  &#39;time_total_s&#39;: 3.4353830814361572,</span>
<span class="c1">#  &#39;timestamp&#39;: 1657829125,</span>
<span class="c1">#  &#39;timesteps_since_restore&#39;: 0,</span>
<span class="c1">#  &#39;timesteps_total&#39;: None,</span>
<span class="c1">#  &#39;training_iteration&#39;: 2,</span>
<span class="c1">#  &#39;trial_id&#39;: &#39;4913f_00000&#39;,</span>
<span class="c1">#  &#39;warmup_time&#39;: 0.003167867660522461}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>View the dataframe containing the metrics from all iterations:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">metrics_dataframe</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Obtain the <a class="reference internal" href="../ray-air/api/doc/ray.air.checkpoint.Checkpoint.html#ray.air.checkpoint.Checkpoint" title="ray.air.checkpoint.Checkpoint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Checkpoint</span></code></a>, used for resuming training, prediction and serving.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">result</span><span class="o">.</span><span class="n">checkpoint</span>  <span class="c1"># last saved checkpoint</span>
<span class="n">result</span><span class="o">.</span><span class="n">best_checkpoints</span>  <span class="c1"># N best saved checkpoints, as configured in run_config</span>
</pre></div>
</div>
<section id="log-directory-structure">
<span id="train-log-dir"></span><h3>Log Directory Structure<a class="headerlink" href="dl_guide.html#log-directory-structure" title="Permalink to this headline">#</a></h3>
<p>Each <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> will have a local directory created for logs and checkpoints.</p>
<p>You can obtain the path to the directory by accessing the <code class="docutils literal notranslate"><span class="pre">log_dir</span></code> attribute
of the <a class="reference internal" href="../tune/api/doc/ray.air.Result.html#ray.air.Result" title="ray.air.result.Result"><code class="xref py py-class docutils literal notranslate"><span class="pre">Result</span></code></a> object returned by <code class="docutils literal notranslate"><span class="pre">Trainer.fit()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">log_dir</span><span class="p">)</span>
<span class="c1"># &#39;/home/ubuntu/ray_results/TorchTrainer_2022-06-13_20-31-06/checkpoint_000003&#39;</span>
</pre></div>
</div>
</section>
</section>
<section id="distributed-data-ingest-with-ray-data-and-ray-train">
<span id="train-datasets"></span><h2>Distributed Data Ingest with Ray Data and Ray Train<a class="headerlink" href="dl_guide.html#distributed-data-ingest-with-ray-data-and-ray-train" title="Permalink to this headline">#</a></h2>
<p><a class="reference internal" href="../data/data.html#data"><span class="std std-ref">Ray Data</span></a> is the recommended way to work with large datasets in Ray Train. Ray Data provides automatic loading, sharding, and streamed ingest of Data across multiple Train workers.
To get started, pass in one or more datasets under the <code class="docutils literal notranslate"><span class="pre">datasets</span></code> keyword argument for Trainer (e.g., <code class="docutils literal notranslate"><span class="pre">Trainer(datasets={...})</span></code>).</p>
<p>Here’s a simple code overview of the Ray Data integration:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span>

<span class="c1"># Datasets can be accessed in your train_func via ``get_dataset_shard``.</span>
<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="n">train_data_shard</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="n">validation_data_shard</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
    <span class="o">...</span>

<span class="c1"># Random split the dataset into 80% training data and 20% validation data.</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;...&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="s2">&quot;validation&quot;</span><span class="p">:</span> <span class="n">validation_dataset</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>For more details on how to configure data ingest for Train, please refer to <a class="reference internal" href="../ray-air/check-ingest.html#air-ingest"><span class="std std-ref">Configuring Training Datasets</span></a>.</p>
</section>
<section id="logging-checkpointing-and-callbacks-in-ray-train">
<span id="train-monitoring"></span><h2>Logging, Checkpointing and Callbacks in Ray Train<a class="headerlink" href="dl_guide.html#logging-checkpointing-and-callbacks-in-ray-train" title="Permalink to this headline">#</a></h2>
<p>Ray Train has mechanisms to easily collect intermediate results from the training workers during the training run
and also has a <a class="reference internal" href="dl_guide.html#train-callbacks"><span class="std std-ref">Callback interface</span></a> to perform actions on these intermediate results (such as logging, aggregations, etc.).
You can use either the <a class="reference internal" href="../ray-air/api/experiment-tracking.html#air-builtin-callbacks"><span class="std std-ref">built-in callbacks</span></a> that Ray AIR provides,
or implement a <a class="reference internal" href="dl_guide.html#train-custom-callbacks"><span class="std std-ref">custom callback</span></a> for your use case. The callback API
is shared with Ray Tune.</p>
<p id="train-checkpointing">Ray Train also provides a way to save <a class="reference internal" href="../ray-air/key-concepts.html#air-checkpoints-doc"><span class="std std-ref">Checkpoints</span></a> during the training process. This is
useful for:</p>
<ol class="arabic simple">
<li><p><a class="reference internal" href="dl_guide.html#train-tune"><span class="std std-ref">Integration with Ray Tune</span></a> to use certain Ray Tune
schedulers.</p></li>
<li><p>Running a long-running training job on a cluster of pre-emptible machines/pods.</p></li>
<li><p>Persisting trained model state to later use for serving/inference.</p></li>
<li><p>In general, storing any model artifacts.</p></li>
</ol>
<section id="reporting-intermediate-results-and-handling-checkpoints">
<h3>Reporting intermediate results and handling checkpoints<a class="headerlink" href="dl_guide.html#reporting-intermediate-results-and-handling-checkpoints" title="Permalink to this headline">#</a></h3>
<p>Ray AIR provides a <em>Session</em> API for reporting intermediate
results and checkpoints from the training function (run on distributed workers) up to the
<code class="docutils literal notranslate"><span class="pre">Trainer</span></code> (where your python script is executed) by calling <code class="docutils literal notranslate"><span class="pre">session.report(metrics)</span></code>.
The results will be collected from the distributed workers and passed to the driver to
be logged and displayed.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Only the results from rank 0 worker will be used. However, in order to ensure
consistency, <code class="docutils literal notranslate"><span class="pre">session.report()</span></code> has to be called on each worker. If you
want to aggregate results from multiple workers, see <a class="reference internal" href="dl_guide.html#train-aggregating-results"><span class="std std-ref">How to obtain and aggregate results from different workers?</span></a>.</p>
</div>
<p>The primary use-case for reporting is for metrics (accuracy, loss, etc.) at
the end of each training epoch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">():</span>
    <span class="o">...</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">({</span><span class="s2">&quot;result&quot;</span><span class="p">:</span> <span class="n">result</span><span class="p">})</span>
</pre></div>
</div>
<p>The session concept exists on several levels: The execution layer (called <code class="xref py py-obj docutils literal notranslate"><span class="pre">Tune</span> <span class="pre">Session</span></code>) and the Data Parallel training layer
(called <code class="xref py py-obj docutils literal notranslate"><span class="pre">Train</span> <span class="pre">Session</span></code>).
The following figure shows how these two sessions look like in a Data Parallel training scenario.</p>
<a class="reference internal image-reference" href="../_images/session.svg"><img alt="../_images/session.svg" class="align-center" src="../_images/session.svg" width="650px" /></a>
<section id="saving-checkpoints">
<span id="train-dl-saving-checkpoints"></span><h4>Saving checkpoints<a class="headerlink" href="dl_guide.html#saving-checkpoints" title="Permalink to this headline">#</a></h4>
<p><a class="reference internal" href="../ray-air/key-concepts.html#air-checkpoints-doc"><span class="std std-ref">Checkpoints</span></a> can be saved by calling <code class="docutils literal notranslate"><span class="pre">session.report(metrics,</span> <span class="pre">checkpoint=Checkpoint(...))</span></code> in the
training function. This will cause the checkpoint state from the distributed
workers to be saved on the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> (where your python script is executed).</p>
<p>The latest saved checkpoint can be accessed through the <code class="docutils literal notranslate"><span class="pre">checkpoint</span></code> attribute of
the <a class="reference internal" href="../tune/api/doc/ray.air.Result.html#ray.air.Result" title="ray.air.result.Result"><code class="xref py py-class docutils literal notranslate"><span class="pre">Result</span></code></a>, and the best saved checkpoints can be accessed by the <code class="docutils literal notranslate"><span class="pre">best_checkpoints</span></code>
attribute.</p>
<p>Concrete examples are provided to demonstrate how checkpoints (model weights but not models) are saved
appropriately in distributed training.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-9" name="sd-tab-set-3" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-9">
PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray.train.torch</span>
<span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span><span class="p">,</span> <span class="n">Checkpoint</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="c1"># create a toy dataset</span>
    <span class="c1"># data   : X - dim = (n, 4)</span>
    <span class="c1"># target : Y - dim = (n, 1)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="c1"># toy neural network : 1-layer</span>
    <span class="c1"># wrap the model in DDP</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">]):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># compute loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="c1"># back-propagate loss</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
            <span class="nb">dict</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">model_weights</span><span class="o">=</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">({},</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="hll">
</span><span class="hll"><span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
</span><span class="hll">    <span class="n">train_func</span><span class="p">,</span>
</span><span class="hll">    <span class="n">train_loop_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span>
</span><span class="hll">    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
</span><span class="hll"><span class="p">)</span>
</span><span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
<span class="c1"># {&#39;epoch&#39;: 4, &#39;model_weights&#39;: OrderedDict([(&#39;bias&#39;, tensor([-0.1215])), (&#39;weight&#39;, tensor([[0.3253, 0.1979, 0.4525, 0.2850]]))]), &#39;_timestamp&#39;: 1656107095, &#39;_preprocessor&#39;: None, &#39;_current_checkpoint_id&#39;: 4}</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-10" name="sd-tab-set-3" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-10">
TensorFlow</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span><span class="p">,</span> <span class="n">Checkpoint</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.tensorflow</span> <span class="kn">import</span> <span class="n">TensorflowTrainer</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="c1"># create a toy dataset</span>
    <span class="c1"># data   : X - dim = (n, 4)</span>
    <span class="c1"># target : Y - dim = (n, 1)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">MultiWorkerMirroredStrategy</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
        <span class="c1"># toy neural network : 1-layer</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,))])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;Adam&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mean_squared_error&quot;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mse&quot;</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">]):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="hll">        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
</span>            <span class="nb">dict</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">model_weights</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">get_weights</span><span class="p">())</span>
        <span class="p">)</span>
        <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">({},</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TensorflowTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
<span class="c1"># {&#39;epoch&#39;: 4, &#39;model_weights&#39;: [array([[-0.31858477],</span>
<span class="c1">#    [ 0.03747174],</span>
<span class="c1">#    [ 0.28266194],</span>
<span class="c1">#    [ 0.8626015 ]], dtype=float32), array([0.02230084], dtype=float32)], &#39;_timestamp&#39;: 1656107383, &#39;_preprocessor&#39;: None, &#39;_current_checkpoint_id&#39;: 4}</span>
</pre></div>
</div>
</div>
</div>
<p>By default, checkpoints will be persisted to local disk in the <a class="reference internal" href="dl_guide.html#train-log-dir"><span class="std std-ref">log
directory</span></a> of each run.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">get_internal_representation</span><span class="p">())</span>
<span class="c1"># (&#39;local_path&#39;, &#39;/home/ubuntu/ray_results/TorchTrainer_2022-06-24_21-34-49/TorchTrainer_7988b_00000_0_2022-06-24_21-34-49/checkpoint_000003&#39;)</span>
</pre></div>
</div>
</section>
<section id="configuring-checkpoints">
<h4>Configuring checkpoints<a class="headerlink" href="dl_guide.html#configuring-checkpoints" title="Permalink to this headline">#</a></h4>
<p>For more configurability of checkpointing behavior (specifically saving
checkpoints to disk), a <a class="reference internal" href="../ray-air/api/doc/ray.air.CheckpointConfig.html#ray.air.CheckpointConfig" title="ray.air.config.CheckpointConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CheckpointConfig</span></code></a> can be passed into
<code class="docutils literal notranslate"><span class="pre">Trainer</span></code>.</p>
<p>As an example, to completely disable writing checkpoints to disk:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span><span class="p">,</span> <span class="n">RunConfig</span><span class="p">,</span> <span class="n">CheckpointConfig</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">))</span>
        <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">({},</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="hll"><span class="n">checkpoint_config</span> <span class="o">=</span> <span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">num_to_keep</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="hll">    <span class="n">run_config</span><span class="o">=</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">checkpoint_config</span><span class="o">=</span><span class="n">checkpoint_config</span><span class="p">)</span>
</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>You may also config <code class="docutils literal notranslate"><span class="pre">CheckpointConfig</span></code> to keep the “N best” checkpoints persisted to disk. The following example shows how you could keep the 2 checkpoints with the lowest “loss” value:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span><span class="p">,</span> <span class="n">Checkpoint</span><span class="p">,</span> <span class="n">RunConfig</span><span class="p">,</span> <span class="n">CheckpointConfig</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">():</span>
    <span class="c1"># first checkpoint</span>
    <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="mi">2</span><span class="p">)))</span>
    <span class="c1"># second checkpoint</span>
    <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="mi">4</span><span class="p">)))</span>
    <span class="c1"># third checkpoint</span>
    <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>
    <span class="c1"># fourth checkpoint</span>
    <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="mi">3</span><span class="p">)))</span>

<span class="c1"># Keep the 2 checkpoints with the smallest &quot;loss&quot; value.</span>
<span class="n">checkpoint_config</span> <span class="o">=</span> <span class="n">CheckpointConfig</span><span class="p">(</span>
    <span class="n">num_to_keep</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">checkpoint_score_attribute</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="n">checkpoint_score_order</span><span class="o">=</span><span class="s2">&quot;min&quot;</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">checkpoint_config</span><span class="o">=</span><span class="n">checkpoint_config</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">best_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_internal_representation</span><span class="p">())</span>
<span class="c1"># (&#39;local_path&#39;, &#39;/home/ubuntu/ray_results/TorchTrainer_2022-06-24_21-34-49/TorchTrainer_7988b_00000_0_2022-06-24_21-34-49/checkpoint_000000&#39;)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">best_checkpoints</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_internal_representation</span><span class="p">())</span>
<span class="c1"># (&#39;local_path&#39;, &#39;/home/ubuntu/ray_results/TorchTrainer_2022-06-24_21-34-49/TorchTrainer_7988b_00000_0_2022-06-24_21-34-49/checkpoint_000002&#39;)</span>
</pre></div>
</div>
</section>
<section id="loading-checkpoints">
<span id="train-dl-loading-checkpoints"></span><h4>Loading checkpoints<a class="headerlink" href="dl_guide.html#loading-checkpoints" title="Permalink to this headline">#</a></h4>
<p>Checkpoints can be loaded into the training function in 2 steps:</p>
<ol class="arabic simple">
<li><p>From the training function, <a class="reference internal" href="../ray-air/api/doc/ray.air.session.get_checkpoint.html#ray.air.session.get_checkpoint" title="ray.air.session.get_checkpoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">ray.air.session.get_checkpoint()</span></code></a> can be used to access
the most recently saved <a class="reference internal" href="../ray-air/api/doc/ray.air.checkpoint.Checkpoint.html#ray.air.checkpoint.Checkpoint" title="ray.air.checkpoint.Checkpoint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Checkpoint</span></code></a>. This is useful to continue training even
if there’s a worker failure.</p></li>
<li><p>The checkpoint to start training with can be bootstrapped by passing in a
<a class="reference internal" href="../ray-air/api/doc/ray.air.checkpoint.Checkpoint.html#ray.air.checkpoint.Checkpoint" title="ray.air.checkpoint.Checkpoint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Checkpoint</span></code></a> to <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> as the <code class="docutils literal notranslate"><span class="pre">resume_from_checkpoint</span></code> argument.</p></li>
</ol>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-11" name="sd-tab-set-4" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-11">
PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray.train.torch</span>
<span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span><span class="p">,</span> <span class="n">Checkpoint</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="c1"># create a toy dataset</span>
    <span class="c1"># data   : X - dim = (n, 4)</span>
    <span class="c1"># target : Y - dim = (n, 1)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

    <span class="c1"># toy neural network : 1-layer</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">)</span>
    <span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">0</span>
<span class="hll">
</span>    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get_checkpoint</span><span class="p">()</span>
<span class="hll">    <span class="k">if</span> <span class="n">checkpoint</span><span class="p">:</span>
</span><span class="hll">        <span class="c1"># assume that we have run the session.report() example</span>
</span>        <span class="c1"># and successfully save some model weights</span>
        <span class="n">checkpoint_dict</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
<span class="hll">        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;model_weights&quot;</span><span class="p">))</span>
</span><span class="hll">        <span class="n">start_epoch</span> <span class="o">=</span> <span class="n">checkpoint_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span><span class="hll">
</span>    <span class="c1"># wrap the model in DDP</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_epoch</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">]):</span>
<span class="hll">        <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span>        <span class="c1"># compute loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="c1"># back-propagate loss</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
            <span class="nb">dict</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">model_weights</span><span class="o">=</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">({},</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
<span class="c1"># save a checkpoint</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># load checkpoint</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
<span class="c1"># {&#39;epoch&#39;: 3, &#39;model_weights&#39;: OrderedDict([(&#39;bias&#39;, tensor([0.0902])), (&#39;weight&#39;, tensor([[-0.1549, -0.0861,  0.4353, -0.4116]]))]), &#39;_timestamp&#39;: 1656108265, &#39;_preprocessor&#39;: None, &#39;_current_checkpoint_id&#39;: 2}</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-12" name="sd-tab-set-4" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-12">
TensorFlow</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span><span class="p">,</span> <span class="n">Checkpoint</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.tensorflow</span> <span class="kn">import</span> <span class="n">TensorflowTrainer</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="c1"># create a toy dataset</span>
    <span class="c1"># data   : X - dim = (n, 4)</span>
    <span class="c1"># target : Y - dim = (n, 1)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="hll">    <span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">0</span>
</span>    <span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">MultiWorkerMirroredStrategy</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
        <span class="c1"># toy neural network : 1-layer</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,))])</span>
<span class="hll">        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get_checkpoint</span><span class="p">()</span>
</span><span class="hll">        <span class="k">if</span> <span class="n">checkpoint</span><span class="p">:</span>
</span>            <span class="c1"># assume that we have run the session.report() example</span>
            <span class="c1"># and successfully save some model weights</span>
<span class="hll">            <span class="n">checkpoint_dict</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
</span><span class="hll">            <span class="n">model</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">checkpoint_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;model_weights&quot;</span><span class="p">))</span>
</span><span class="hll">            <span class="n">start_epoch</span> <span class="o">=</span> <span class="n">checkpoint_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span>        <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;Adam&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mean_squared_error&quot;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mse&quot;</span><span class="p">])</span>

<span class="hll">    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_epoch</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">]):</span>
</span>        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
            <span class="nb">dict</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">model_weights</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">get_weights</span><span class="p">())</span>
        <span class="p">)</span>
        <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">({},</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TensorflowTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
<span class="c1"># save a checkpoint</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># load a checkpoint</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TensorflowTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
<span class="c1"># {&#39;epoch&#39;: 4, &#39;model_weights&#39;: [array([[-0.70056134],</span>
<span class="c1">#    [-0.8839263 ],</span>
<span class="c1">#    [-1.0043601 ],</span>
<span class="c1">#    [-0.61634773]], dtype=float32), array([0.01889327], dtype=float32)], &#39;_timestamp&#39;: 1656108446, &#39;_preprocessor&#39;: None, &#39;_current_checkpoint_id&#39;: 3}</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="callbacks">
<span id="train-callbacks"></span><h3>Callbacks<a class="headerlink" href="dl_guide.html#callbacks" title="Permalink to this headline">#</a></h3>
<p>You may want to plug in your training code with your favorite experiment management framework.
Ray AIR provides an interface to fetch intermediate results and callbacks to process/log your intermediate results
(the values passed into <a class="reference internal" href="../ray-air/api/doc/ray.air.session.report.html#ray.air.session.report" title="ray.air.session.report"><code class="xref py py-func docutils literal notranslate"><span class="pre">ray.air.session.report()</span></code></a>).</p>
<p>Ray AIR contains <a class="reference internal" href="../ray-air/api/experiment-tracking.html#air-builtin-callbacks"><span class="std std-ref">built-in callbacks</span></a> for popular tracking frameworks, or you can implement your own callback via the <a class="reference internal" href="../tune/api/callbacks.html#tune-callbacks-docs"><span class="std std-ref">Callback</span></a> interface.</p>
<section id="example-logging-to-mlflow-and-tensorboard">
<h4>Example: Logging to MLflow and TensorBoard<a class="headerlink" href="dl_guide.html#example-logging-to-mlflow-and-tensorboard" title="Permalink to this headline">#</a></h4>
<p><strong>Step 1: Install the necessary packages</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ pip install mlflow
$ pip install tensorboardX
</pre></div>
</div>
<p><strong>Step 2: Run the following training script</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">ScalingConfig</span><span class="p">,</span> <span class="n">RunConfig</span><span class="p">,</span> <span class="n">session</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>
<span class="kn">from</span> <span class="nn">ray.air.integrations.mlflow</span> <span class="kn">import</span> <span class="n">MLflowLoggerCallback</span>
<span class="kn">from</span> <span class="nn">ray.tune.logger</span> <span class="kn">import</span> <span class="n">TBXLoggerCallback</span>


<span class="k">def</span> <span class="nf">train_func</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="n">i</span><span class="p">))</span>


<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">RunConfig</span><span class="p">(</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
            <span class="n">MLflowLoggerCallback</span><span class="p">(</span><span class="n">experiment_name</span><span class="o">=</span><span class="s2">&quot;train_experiment&quot;</span><span class="p">),</span>
            <span class="n">TBXLoggerCallback</span><span class="p">(),</span>
        <span class="p">],</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Run the training function, logging all the intermediate results</span>
<span class="c1"># to MLflow and Tensorboard.</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># For MLFLow logs:</span>

<span class="c1"># MLFlow logs will by default be saved in an `mlflow` directory</span>
<span class="c1"># in the current working directory.</span>

<span class="c1"># $ cd mlflow</span>
<span class="c1"># # View the MLflow UI.</span>
<span class="c1"># $ mlflow ui</span>

<span class="c1"># You can change the directory by setting the `tracking_uri` argument</span>
<span class="c1"># in `MLflowLoggerCallback`.</span>

<span class="c1"># For TensorBoard logs:</span>

<span class="c1"># Print the latest run directory and keep note of it.</span>
<span class="c1"># For example: /home/ubuntu/ray_results/TorchTrainer_2022-06-13_20-31-06</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Run directory:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">log_dir</span><span class="o">.</span><span class="n">parent</span><span class="p">)</span>  <span class="c1"># TensorBoard is saved in parent dir</span>

<span class="c1"># How to visualize the logs</span>

<span class="c1"># Navigate to the run directory of the trainer.</span>
<span class="c1"># For example `cd /home/ubuntu/ray_results/TorchTrainer_2022-06-13_20-31-06`</span>
<span class="c1"># $ cd &lt;TRAINER_RUN_DIR&gt;</span>
<span class="c1">#</span>
<span class="c1"># # View the tensorboard UI.</span>
<span class="c1"># $ tensorboard --logdir .</span>
</pre></div>
</div>
</section>
<section id="custom-callbacks">
<span id="train-custom-callbacks"></span><h4>Custom Callbacks<a class="headerlink" href="dl_guide.html#custom-callbacks" title="Permalink to this headline">#</a></h4>
<p>If the provided callbacks do not cover your desired integrations or use-cases,
you may always implement a custom callback by subclassing <a class="reference internal" href="../tune/api/doc/ray.tune.logger.LoggerCallback.html#ray.tune.logger.LoggerCallback" title="ray.tune.logger.LoggerCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">LoggerCallback</span></code></a>. If
the callback is general enough, please feel welcome to <a class="reference internal" href="../ray-contribute/getting-involved.html#getting-involved"><span class="std std-ref">add it</span></a>
to the <code class="docutils literal notranslate"><span class="pre">ray</span></code> <a class="reference external" href="https://github.com/ray-project/ray">repository</a>.</p>
<p>A simple example for creating a callback that will print out results:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span>

<span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span><span class="p">,</span> <span class="n">RunConfig</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>
<span class="kn">from</span> <span class="nn">ray.tune.logger</span> <span class="kn">import</span> <span class="n">LoggerCallback</span>

<span class="c1"># LoggerCallback is a higher level API of Callback.</span>
<span class="k">class</span> <span class="nc">LoggingCallback</span><span class="p">(</span><span class="n">LoggerCallback</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">log_trial_result</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">iteration</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">trial</span><span class="p">:</span> <span class="s2">&quot;Trial&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">:</span> <span class="n">Dict</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">trial</span><span class="o">.</span><span class="n">last_result</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">({</span><span class="s2">&quot;epoch&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">})</span>

<span class="n">callback</span> <span class="o">=</span> <span class="n">LoggingCallback</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">callback</span><span class="p">]),</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">callback</span><span class="o">.</span><span class="n">results</span><span class="p">]))</span>
<span class="c1"># {&#39;trial_id&#39;: &#39;0f1d0_00000&#39;, &#39;experiment_id&#39;: &#39;494a1d050b4a4d11aeabd87ba475fcd3&#39;, &#39;date&#39;: &#39;2022-06-27_17-03-28&#39;, &#39;timestamp&#39;: 1656349408, &#39;pid&#39;: 23018, &#39;hostname&#39;: &#39;ip-172-31-43-110&#39;, &#39;node_ip&#39;: &#39;172.31.43.110&#39;, &#39;config&#39;: {}}</span>
<span class="c1"># {&#39;epoch&#39;: 0, &#39;_timestamp&#39;: 1656349412, &#39;_time_this_iter_s&#39;: 0.0026497840881347656, &#39;_training_iteration&#39;: 1, &#39;time_this_iter_s&#39;: 3.433483362197876, &#39;done&#39;: False, &#39;timesteps_total&#39;: None, &#39;episodes_total&#39;: None, &#39;training_iteration&#39;: 1, &#39;trial_id&#39;: &#39;0f1d0_00000&#39;, &#39;experiment_id&#39;: &#39;494a1d050b4a4d11aeabd87ba475fcd3&#39;, &#39;date&#39;: &#39;2022-06-27_17-03-32&#39;, &#39;timestamp&#39;: 1656349412, &#39;time_total_s&#39;: 3.433483362197876, &#39;pid&#39;: 23018, &#39;hostname&#39;: &#39;ip-172-31-43-110&#39;, &#39;node_ip&#39;: &#39;172.31.43.110&#39;, &#39;config&#39;: {}, &#39;time_since_restore&#39;: 3.433483362197876, &#39;timesteps_since_restore&#39;: 0, &#39;iterations_since_restore&#39;: 1, &#39;warmup_time&#39;: 0.003779172897338867, &#39;experiment_tag&#39;: &#39;0&#39;}</span>
<span class="c1"># {&#39;epoch&#39;: 1, &#39;_timestamp&#39;: 1656349412, &#39;_time_this_iter_s&#39;: 0.0013833045959472656, &#39;_training_iteration&#39;: 2, &#39;time_this_iter_s&#39;: 0.016670703887939453, &#39;done&#39;: False, &#39;timesteps_total&#39;: None, &#39;episodes_total&#39;: None, &#39;training_iteration&#39;: 2, &#39;trial_id&#39;: &#39;0f1d0_00000&#39;, &#39;experiment_id&#39;: &#39;494a1d050b4a4d11aeabd87ba475fcd3&#39;, &#39;date&#39;: &#39;2022-06-27_17-03-32&#39;, &#39;timestamp&#39;: 1656349412, &#39;time_total_s&#39;: 3.4501540660858154, &#39;pid&#39;: 23018, &#39;hostname&#39;: &#39;ip-172-31-43-110&#39;, &#39;node_ip&#39;: &#39;172.31.43.110&#39;, &#39;config&#39;: {}, &#39;time_since_restore&#39;: 3.4501540660858154, &#39;timesteps_since_restore&#39;: 0, &#39;iterations_since_restore&#39;: 2, &#39;warmup_time&#39;: 0.003779172897338867, &#39;experiment_tag&#39;: &#39;0&#39;}</span>
</pre></div>
</div>
</section>
</section>
<section id="how-to-obtain-and-aggregate-results-from-different-workers">
<span id="train-aggregating-results"></span><h3>How to obtain and aggregate results from different workers?<a class="headerlink" href="dl_guide.html#how-to-obtain-and-aggregate-results-from-different-workers" title="Permalink to this headline">#</a></h3>
<p>In real applications, you may want to calculate optimization metrics besides accuracy and loss: recall, precision, Fbeta, etc.
You may also want to collect metrics from multiple workers. While Ray Train currently only reports metrics from the rank 0
worker, you can use third-party libraries or distributed primitives of your machine learning framework to report
metrics from multiple workers.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-13" name="sd-tab-set-5" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-13">
PyTorch</label><div class="sd-tab-content docutils">
<p>Ray Train natively supports <a class="reference external" href="https://torchmetrics.readthedocs.io/en/latest/">TorchMetrics</a>, which provides a collection of machine learning metrics for distributed, scalable PyTorch models.</p>
<p>Here is an example of reporting both the aggregated R2 score and mean train and validation loss from all workers.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="c1"># First, pip install torchmetrics</span>
<span class="c1"># This code is tested with torchmetrics==0.7.3 and torch==1.12.1</span>

<span class="kn">import</span> <span class="nn">ray.train.torch</span>
<span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torchmetrics</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="c1"># create a toy dataset</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
    <span class="n">X_valid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="n">Y_valid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="c1"># toy neural network : 1-layer</span>
    <span class="c1"># wrap the model in DDP</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

    <span class="n">mape</span> <span class="o">=</span> <span class="n">torchmetrics</span><span class="o">.</span><span class="n">MeanAbsolutePercentageError</span><span class="p">()</span>
    <span class="c1"># for averaging loss</span>
    <span class="n">mean_valid_loss</span> <span class="o">=</span> <span class="n">torchmetrics</span><span class="o">.</span><span class="n">MeanMetric</span><span class="p">()</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">]):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># compute loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

        <span class="c1"># back-propagate loss</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># evaluate</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_valid</span><span class="p">)</span>
            <span class="n">valid_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">Y_valid</span><span class="p">)</span>
            <span class="c1"># save loss in aggregator</span>
            <span class="n">mean_valid_loss</span><span class="p">(</span><span class="n">valid_loss</span><span class="p">)</span>
            <span class="n">mape</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">Y_valid</span><span class="p">)</span>

        <span class="c1"># collect all metrics</span>
        <span class="c1"># use .item() to obtain a value that can be reported</span>
        <span class="n">valid_loss</span> <span class="o">=</span> <span class="n">valid_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">mape_collected</span> <span class="o">=</span> <span class="n">mape</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">mean_valid_loss_collected</span> <span class="o">=</span> <span class="n">mean_valid_loss</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;mape_collected&quot;</span><span class="p">:</span> <span class="n">mape_collected</span><span class="p">,</span>
                <span class="s2">&quot;valid_loss&quot;</span><span class="p">:</span> <span class="n">valid_loss</span><span class="p">,</span>
                <span class="s2">&quot;mean_valid_loss_collected&quot;</span><span class="p">:</span> <span class="n">mean_valid_loss_collected</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">)</span>

        <span class="c1"># reset for next epoch</span>
        <span class="n">mape</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">mean_valid_loss</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>


<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;valid_loss&quot;</span><span class="p">],</span> <span class="n">result</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;mean_valid_loss_collected&quot;</span><span class="p">])</span>
<span class="c1"># 0.5109779238700867 0.5512474775314331</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-14" name="sd-tab-set-5" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-14">
TensorFlow</label><div class="sd-tab-content docutils">
<p>TensorFlow Keras automatically aggregates metrics from all workers. If you wish to have more
control over that, consider implementing a <a class="reference external" href="https://www.tensorflow.org/tutorials/distribute/custom_training">custom training loop</a>.</p>
</div>
</div>
</section>
</section>
<section id="fault-tolerance">
<span id="train-fault-tolerance"></span><h2>Fault Tolerance<a class="headerlink" href="dl_guide.html#fault-tolerance" title="Permalink to this headline">#</a></h2>
<section id="automatically-recover-from-train-worker-failures">
<h3>Automatically Recover from Train Worker Failures<a class="headerlink" href="dl_guide.html#automatically-recover-from-train-worker-failures" title="Permalink to this headline">#</a></h3>
<p>Ray Train has built-in fault tolerance to recover from worker failures (i.e.
<code class="docutils literal notranslate"><span class="pre">RayActorError</span></code>s). When a failure is detected, the workers will be shut
down and new workers will be added in.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Elastic Training is not yet supported.</p>
</div>
<p>The training function will be restarted, but progress from the previous execution can
be resumed through checkpointing.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>In order to retain progress when recovery, your training function
<strong>must</strong> implement logic for both <a class="reference internal" href="dl_guide.html#train-dl-saving-checkpoints"><span class="std std-ref">saving</span></a>
<em>and</em> <a class="reference internal" href="dl_guide.html#train-dl-loading-checkpoints"><span class="std std-ref">loading checkpoints</span></a>.</p>
</div>
<p>Each instance of recovery from a worker failure is considered a retry. The
number of retries is configurable through the <code class="docutils literal notranslate"><span class="pre">max_failures</span></code> attribute of the
<a class="reference internal" href="../ray-air/api/doc/ray.air.FailureConfig.html#ray.air.FailureConfig" title="ray.air.FailureConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FailureConfig</span></code></a> argument set in the <a class="reference internal" href="../ray-air/api/doc/ray.air.RunConfig.html#ray.air.RunConfig" title="ray.air.RunConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RunConfig</span></code></a>
passed to the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">RunConfig</span><span class="p">,</span> <span class="n">FailureConfig</span>

<span class="n">run_config</span> <span class="o">=</span> <span class="n">RunConfig</span><span class="p">(</span>
    <span class="n">failure_config</span><span class="o">=</span><span class="n">FailureConfig</span><span class="p">(</span>
        <span class="c1"># Tries to recover a run up to this many times.</span>
        <span class="n">max_failures</span><span class="o">=</span><span class="mi">2</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="restore-a-ray-train-experiment">
<span id="train-restore-guide"></span><h3>Restore a Ray Train Experiment<a class="headerlink" href="dl_guide.html#restore-a-ray-train-experiment" title="Permalink to this headline">#</a></h3>
<p>At the experiment level, <a class="reference internal" href="api/api.html#trainer-restore"><span class="std std-ref">Trainer restoration</span></a>
allows you to resume a previously interrupted experiment from where it left off.</p>
<p>A Train experiment may be interrupted due to one of the following reasons:</p>
<ul class="simple">
<li><p>The experiment was manually interrupted (e.g., Ctrl+C, or pre-empted head node instance).</p></li>
<li><p>The head node crashed (e.g., OOM or some other runtime error).</p></li>
<li><p>The entire cluster went down (e.g., network error affecting all nodes).</p></li>
</ul>
<p>Trainer restoration is possible for all of Ray Train’s built-in trainers,
but we use <code class="docutils literal notranslate"><span class="pre">TorchTrainer</span></code> in the examples for demonstration.
We also use <code class="docutils literal notranslate"><span class="pre">&lt;Framework&gt;Trainer</span></code> to refer to methods that are shared across all
built-in trainers.</p>
<p>Let’s say your initial Train experiment is configured as follows.
The actual training loop is just for demonstration purposes: the important detail is that
<a class="reference internal" href="dl_guide.html#train-dl-saving-checkpoints"><span class="std std-ref">saving</span></a> <em>and</em> <a class="reference internal" href="dl_guide.html#train-dl-loading-checkpoints"><span class="std std-ref">loading checkpoints</span></a>
has been implemented.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span>
<span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchCheckpoint</span><span class="p">,</span> <span class="n">TorchTrainer</span>


<span class="k">def</span> <span class="nf">get_datasets</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">]:</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">from_items</span><span class="p">([{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">}</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])}</span>


<span class="k">def</span> <span class="nf">train_loop_per_worker</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet18</span>

    <span class="c1"># Checkpoint loading</span>
    <span class="n">checkpoint</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchCheckpoint</span><span class="p">]</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get_checkpoint</span><span class="p">()</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">get_model</span><span class="p">()</span> <span class="k">if</span> <span class="n">checkpoint</span> <span class="k">else</span> <span class="n">resnet18</span><span class="p">()</span>
    <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="n">train_ds</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="c1"># Do some training...</span>

        <span class="c1"># Checkpoint saving</span>
        <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;epoch&quot;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">},</span>
            <span class="n">checkpoint</span><span class="o">=</span><span class="n">TorchCheckpoint</span><span class="o">.</span><span class="n">from_model</span><span class="p">(</span><span class="n">model</span><span class="p">),</span>
        <span class="p">)</span>


<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_loop_per_worker</span><span class="o">=</span><span class="n">train_loop_per_worker</span><span class="p">,</span>
    <span class="n">datasets</span><span class="o">=</span><span class="n">get_datasets</span><span class="p">(),</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span>
        <span class="n">storage_path</span><span class="o">=</span><span class="s2">&quot;~/ray_results&quot;</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dl_trainer_restore&quot;</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>The results and checkpoints of the experiment are saved to the path configured by <a class="reference internal" href="../ray-air/api/doc/ray.air.RunConfig.html#ray.air.RunConfig" title="ray.air.config.RunConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RunConfig</span></code></a>.
If the experiment has been interrupted due to one of the reasons listed above, use this path to resume:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>

<span class="n">restored_trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span>
    <span class="n">path</span><span class="o">=</span><span class="s2">&quot;~/ray_results/dl_trainer_restore&quot;</span><span class="p">,</span>
    <span class="n">datasets</span><span class="o">=</span><span class="n">get_datasets</span><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can also restore from a remote path (e.g., from an experiment directory stored in a s3 bucket).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">original_trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="c1"># ...</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span>
        <span class="c1"># Configure cloud storage</span>
        <span class="n">storage_path</span><span class="o">=</span><span class="s2">&quot;s3://results-bucket&quot;</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dl_trainer_restore&quot;</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">restored_trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span>
    <span class="s2">&quot;s3://results-bucket/dl_trainer_restore&quot;</span><span class="p">,</span>
    <span class="n">datasets</span><span class="o">=</span><span class="n">get_datasets</span><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Different trainers may allow more parameters to be optionally re-specified on restore.
Only <strong>datasets</strong> are required to be re-specified on restore, if they were supplied originally.</p>
<p>See <a class="reference internal" href="api/api.html#train-framework-specific-restore"><span class="std std-ref">Restoration API for Built-in Trainers</span></a> for more details.</p>
</div>
<section id="auto-resume">
<h4>Auto-resume<a class="headerlink" href="dl_guide.html#auto-resume" title="Permalink to this headline">#</a></h4>
<p>Adding the branching logic below will allow you to run the same script after the interrupt,
picking up training from where you left on the previous run. Notice that we use the
<a class="reference internal" href="api/doc/ray.train.trainer.BaseTrainer.can_restore.html#ray.train.trainer.BaseTrainer.can_restore" title="ray.train.trainer.BaseTrainer.can_restore"><code class="xref py py-meth docutils literal notranslate"><span class="pre">&lt;Framework&gt;Trainer.can_restore</span></code></a> utility method
to determine the existence and validity of the given experiment directory.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">TorchTrainer</span><span class="o">.</span><span class="n">can_restore</span><span class="p">(</span><span class="s2">&quot;~/ray_results/dl_restore_autoresume&quot;</span><span class="p">):</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span>
        <span class="s2">&quot;~/ray_results/dl_restore_autoresume&quot;</span><span class="p">,</span>
        <span class="n">datasets</span><span class="o">=</span><span class="n">get_datasets</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
        <span class="n">train_loop_per_worker</span><span class="o">=</span><span class="n">train_loop_per_worker</span><span class="p">,</span>
        <span class="n">datasets</span><span class="o">=</span><span class="n">get_datasets</span><span class="p">(),</span>
        <span class="n">scaling_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
        <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span>
            <span class="n">storage_path</span><span class="o">=</span><span class="s2">&quot;~/ray_results&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dl_restore_autoresume&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <a class="reference internal" href="api/doc/ray.train.trainer.BaseTrainer.restore.html#ray.train.trainer.BaseTrainer.restore" title="ray.train.trainer.BaseTrainer.restore"><code class="xref py py-meth docutils literal notranslate"><span class="pre">BaseTrainer.restore</span></code></a> docstring
for a full example.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">&lt;Framework&gt;Trainer.restore</span></code> is different from
<a class="reference internal" href="api/doc/ray.train.trainer.BaseTrainer.html#ray.train.trainer.BaseTrainer" title="ray.train.trainer.BaseTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">&lt;Framework&gt;Trainer(...,</span> <span class="pre">resume_from_checkpoint=...)</span></code></a>.
<code class="xref py py-obj docutils literal notranslate"><span class="pre">resume_from_checkpoint</span></code> is meant to be used to start a <em>new</em> Train experiment,
which writes results to a new directory and starts over from iteration 0.</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">&lt;Framework&gt;Trainer.restore</span></code> is used to continue an existing experiment, where
new results will continue to be appended to existing logs.</p>
</div>
</section>
</section>
</section>
<section id="hyperparameter-tuning-ray-tune">
<span id="train-tune"></span><h2>Hyperparameter tuning (Ray Tune)<a class="headerlink" href="dl_guide.html#hyperparameter-tuning-ray-tune" title="Permalink to this headline">#</a></h2>
<p>Hyperparameter tuning with <a class="reference internal" href="../tune.html#tune-main"><span class="std std-ref">Ray Tune</span></a> is natively supported
with Ray Train. Specifically, you can take an existing <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> and simply
pass it into a <a class="reference internal" href="../tune/api/doc/ray.tune.Tuner.html#ray.tune.Tuner" title="ray.tune.tuner.Tuner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tuner</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>
<span class="kn">from</span> <span class="nn">ray.tune.tuner</span> <span class="kn">import</span> <span class="n">Tuner</span><span class="p">,</span> <span class="n">TuneConfig</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="c1"># In this example, nothing is expected to change over epochs,</span>
    <span class="c1"># and the output metric is equivalent to the input value.</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">]):</span>
        <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">output</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]))</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span><span class="n">train_func</span><span class="p">,</span> <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">Tuner</span><span class="p">(</span>
    <span class="n">trainer</span><span class="p">,</span>
    <span class="n">param_space</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;train_loop_config&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span>
        <span class="p">}</span>
    <span class="p">},</span>
    <span class="n">tune_config</span><span class="o">=</span><span class="n">TuneConfig</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">result_grid</span> <span class="o">=</span> <span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result_grid</span><span class="o">.</span><span class="n">get_best_result</span><span class="p">()</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">])</span>
<span class="c1"># 3</span>
</pre></div>
</div>
</section>
<section id="automatic-mixed-precision">
<span id="torch-amp"></span><h2>Automatic Mixed Precision<a class="headerlink" href="dl_guide.html#automatic-mixed-precision" title="Permalink to this headline">#</a></h2>
<p>Automatic mixed precision (AMP) lets you train your models faster by using a lower
precision datatype for operations like linear layers and convolutions.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-15" name="sd-tab-set-6" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-15">
PyTorch</label><div class="sd-tab-content docutils">
<p>You can train your Torch model with AMP by:</p>
<ol class="arabic simple">
<li><p>Adding <a class="reference internal" href="api/doc/ray.train.torch.accelerate.html#ray.train.torch.accelerate" title="ray.train.torch.accelerate"><code class="xref py py-func docutils literal notranslate"><span class="pre">ray.train.torch.accelerate()</span></code></a> with <code class="docutils literal notranslate"><span class="pre">amp=True</span></code> to the top of your training function.</p></li>
<li><p>Wrapping your optimizer with <a class="reference internal" href="api/doc/ray.train.torch.prepare_optimizer.html#ray.train.torch.prepare_optimizer" title="ray.train.torch.prepare_optimizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">ray.train.torch.prepare_optimizer()</span></code></a>.</p></li>
<li><p>Replacing your backward call with <a class="reference internal" href="api/doc/ray.train.torch.backward.html#ray.train.torch.backward" title="ray.train.torch.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">ray.train.torch.backward()</span></code></a>.</p></li>
</ol>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>def train_func():<span class="w"></span>
<span class="gi">+    train.torch.accelerate(amp=True)</span><span class="w"></span>

<span class="w"> </span>    model = NeuralNetwork()<span class="w"></span>
<span class="w"> </span>    model = train.torch.prepare_model(model)<span class="w"></span>

<span class="w"> </span>    data_loader = DataLoader(my_dataset, batch_size=worker_batch_size)<span class="w"></span>
<span class="w"> </span>    data_loader = train.torch.prepare_data_loader(data_loader)<span class="w"></span>

<span class="w"> </span>    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)<span class="w"></span>
<span class="gi">+    optimizer = train.torch.prepare_optimizer(optimizer)</span><span class="w"></span>

<span class="w"> </span>    model.train()<span class="w"></span>
<span class="w"> </span>    for epoch in range(90):<span class="w"></span>
<span class="w"> </span>        for images, targets in dataloader:<span class="w"></span>
<span class="w"> </span>            optimizer.zero_grad()<span class="w"></span>

<span class="w"> </span>            outputs = model(images)<span class="w"></span>
<span class="w"> </span>            loss = torch.nn.functional.cross_entropy(outputs, targets)<span class="w"></span>

<span class="gd">-            loss.backward()</span><span class="w"></span>
<span class="gi">+            train.torch.backward(loss)</span><span class="w"></span>
<span class="w"> </span>            optimizer.step()<span class="w"></span>
<span class="w"> </span>   ...<span class="w"></span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The performance of AMP varies based on GPU architecture, model type,
and data shape. For certain workflows, AMP may perform worse than
full-precision training.</p>
</div>
</section>
<section id="reproducibility">
<span id="train-reproducibility"></span><h2>Reproducibility<a class="headerlink" href="dl_guide.html#reproducibility" title="Permalink to this headline">#</a></h2>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-16" name="sd-tab-set-7" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-16">
PyTorch</label><div class="sd-tab-content docutils">
<p>To limit sources of nondeterministic behavior, add
<a class="reference internal" href="api/doc/ray.train.torch.enable_reproducibility.html#ray.train.torch.enable_reproducibility" title="ray.train.torch.enable_reproducibility"><code class="xref py py-func docutils literal notranslate"><span class="pre">ray.train.torch.enable_reproducibility()</span></code></a> to the top of your training
function.</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>def train_func():<span class="w"></span>
<span class="gi">+    train.torch.enable_reproducibility()</span><span class="w"></span>

<span class="w"> </span>    model = NeuralNetwork()<span class="w"></span>
<span class="w"> </span>    model = train.torch.prepare_model(model)<span class="w"></span>

<span class="w"> </span>    ...<span class="w"></span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="api/doc/ray.train.torch.enable_reproducibility.html#ray.train.torch.enable_reproducibility" title="ray.train.torch.enable_reproducibility"><code class="xref py py-func docutils literal notranslate"><span class="pre">ray.train.torch.enable_reproducibility()</span></code></a> can’t guarantee
completely reproducible results across executions. To learn more, read
the <a class="reference external" href="https://pytorch.org/docs/stable/notes/randomness.html">PyTorch notes on randomness</a>.</p>
</div>
</div>
</div>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="config_guide.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Ray Train Configuration User Guide</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="gbdt.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">XGBoost &amp; LightGBM User Guide for Ray Train</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>