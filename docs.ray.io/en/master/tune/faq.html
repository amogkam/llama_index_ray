
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Ray Tune FAQ &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/versionwarning.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../_static/js/docsearch.js"></script>
    <script src="../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../_static/js/termynal.js"></script>
    <script defer="defer" src="../_static/js/custom.js"></script>
    <script defer="defer" src="../_static/js/top-navigation.js"></script>
    <script src="../_static/js/tags.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/tune/faq.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Ray Tune API" href="api/api.html" />
    <link rel="prev" title="Tune Exercises" href="examples/exercises.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": true, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "tune/faq", "programming_language": "py", "project": "ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../tune.html">
   Ray Tune
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="getting-started.html">
     Getting Started
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="key-concepts.html">
     Key Concepts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="tutorials/overview.html">
     User Guides
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="examples/index.html">
     Ray Tune Examples
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="faq.html#">
     Ray Tune FAQ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="api/api.html">
     Ray Tune API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Ftune/faq.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/tune/faq.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/tune/faq.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#what-are-hyperparameters">
   What are Hyperparameters?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#which-search-algorithm-scheduler-should-i-choose">
   Which search algorithm/scheduler should I choose?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-do-i-choose-hyperparameter-ranges">
   How do I choose hyperparameter ranges?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-use-nested-conditional-search-spaces">
   How can I use nested/conditional search spaces?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="faq.html#nested-spaces">
     Nested spaces
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="faq.html#conditional-spaces">
     Conditional spaces
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="faq.html#conditional-grid-search">
     Conditional grid search
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-does-early-termination-e-g-hyperband-asha-work">
   How does early termination (e.g. Hyperband/ASHA) work?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#why-are-all-my-trials-returning-1-iteration">
   Why are all my trials returning “1” iteration?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#what-are-all-these-extra-outputs">
   What are all these extra outputs?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-do-i-set-resources">
   How do I set resources?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#why-is-my-training-stuck-and-ray-reporting-that-pending-actor-or-tasks-cannot-be-scheduled">
   Why is my training stuck and Ray reporting that pending actor or tasks cannot be scheduled?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-pass-further-parameter-values-to-my-trainable">
   How can I pass further parameter values to my trainable?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-reproduce-experiments">
   How can I reproduce experiments?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-avoid-bottlenecks">
   How can I avoid bottlenecks?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-develop-and-test-tune-locally">
   How can I develop and test Tune locally?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-get-started-contributing-to-tune">
   How can I get started contributing to Tune?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-make-my-tune-experiments-reproducible">
   How can I make my Tune experiments reproducible?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-use-large-datasets-in-tune">
   How can I use large datasets in Tune?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-upload-my-tune-results-to-cloud-storage">
   How can I upload my Tune results to cloud storage?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-use-the-awscli-or-gsutil-command-line-commands-for-syncing">
   How can I use the awscli or gsutil command line commands for syncing?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="faq.html#aws-s3">
     AWS S3
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="faq.html#google-cloud-storage">
     Google cloud storage
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="faq.html#hdfs">
     HDFS
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-use-tune-with-docker">
   How can I use Tune with Docker?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-use-tune-with-kubernetes">
   How can I use Tune with Kubernetes?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-do-i-configure-search-spaces">
   How do I configure search spaces?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-do-i-access-relative-filepaths-in-my-tune-training-function">
   How do I access relative filepaths in my Tune training function?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-run-multiple-ray-tune-jobs-on-the-same-cluster-at-the-same-time-multi-tenancy">
   How can I run multiple Ray Tune jobs on the same cluster at the same time (multi-tenancy)?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-continue-training-a-completed-tune-experiment-for-longer-and-with-new-configurations-iterative-experimentation">
   How can I continue training a completed Tune experiment for longer and with new configurations (iterative experimentation)?
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Ray Tune FAQ</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#what-are-hyperparameters">
   What are Hyperparameters?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#which-search-algorithm-scheduler-should-i-choose">
   Which search algorithm/scheduler should I choose?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-do-i-choose-hyperparameter-ranges">
   How do I choose hyperparameter ranges?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-use-nested-conditional-search-spaces">
   How can I use nested/conditional search spaces?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="faq.html#nested-spaces">
     Nested spaces
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="faq.html#conditional-spaces">
     Conditional spaces
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="faq.html#conditional-grid-search">
     Conditional grid search
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-does-early-termination-e-g-hyperband-asha-work">
   How does early termination (e.g. Hyperband/ASHA) work?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#why-are-all-my-trials-returning-1-iteration">
   Why are all my trials returning “1” iteration?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#what-are-all-these-extra-outputs">
   What are all these extra outputs?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-do-i-set-resources">
   How do I set resources?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#why-is-my-training-stuck-and-ray-reporting-that-pending-actor-or-tasks-cannot-be-scheduled">
   Why is my training stuck and Ray reporting that pending actor or tasks cannot be scheduled?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-pass-further-parameter-values-to-my-trainable">
   How can I pass further parameter values to my trainable?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-reproduce-experiments">
   How can I reproduce experiments?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-avoid-bottlenecks">
   How can I avoid bottlenecks?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-develop-and-test-tune-locally">
   How can I develop and test Tune locally?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-get-started-contributing-to-tune">
   How can I get started contributing to Tune?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-make-my-tune-experiments-reproducible">
   How can I make my Tune experiments reproducible?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-use-large-datasets-in-tune">
   How can I use large datasets in Tune?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-upload-my-tune-results-to-cloud-storage">
   How can I upload my Tune results to cloud storage?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-use-the-awscli-or-gsutil-command-line-commands-for-syncing">
   How can I use the awscli or gsutil command line commands for syncing?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="faq.html#aws-s3">
     AWS S3
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="faq.html#google-cloud-storage">
     Google cloud storage
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="faq.html#hdfs">
     HDFS
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-use-tune-with-docker">
   How can I use Tune with Docker?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-use-tune-with-kubernetes">
   How can I use Tune with Kubernetes?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-do-i-configure-search-spaces">
   How do I configure search spaces?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-do-i-access-relative-filepaths-in-my-tune-training-function">
   How do I access relative filepaths in my Tune training function?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-run-multiple-ray-tune-jobs-on-the-same-cluster-at-the-same-time-multi-tenancy">
   How can I run multiple Ray Tune jobs on the same cluster at the same time (multi-tenancy)?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="faq.html#how-can-i-continue-training-a-completed-tune-experiment-for-longer-and-with-new-configurations-iterative-experimentation">
   How can I continue training a completed Tune experiment for longer and with new configurations (iterative experimentation)?
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="ray-tune-faq">
<span id="tune-faq"></span><h1>Ray Tune FAQ<a class="headerlink" href="faq.html#ray-tune-faq" title="Permalink to this headline">#</a></h1>
<p>Here we try to answer questions that come up often.
If you still have questions after reading this FAQ, let us know!</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="faq.html#what-are-hyperparameters" id="id1">What are Hyperparameters?</a></p></li>
<li><p><a class="reference internal" href="faq.html#which-search-algorithm-scheduler-should-i-choose" id="id2">Which search algorithm/scheduler should I choose?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-do-i-choose-hyperparameter-ranges" id="id3">How do I choose hyperparameter ranges?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-can-i-use-nested-conditional-search-spaces" id="id4">How can I use nested/conditional search spaces?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-does-early-termination-e-g-hyperband-asha-work" id="id5">How does early termination (e.g. Hyperband/ASHA) work?</a></p></li>
<li><p><a class="reference internal" href="faq.html#why-are-all-my-trials-returning-1-iteration" id="id6">Why are all my trials returning “1” iteration?</a></p></li>
<li><p><a class="reference internal" href="faq.html#what-are-all-these-extra-outputs" id="id7">What are all these extra outputs?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-do-i-set-resources" id="id8">How do I set resources?</a></p></li>
<li><p><a class="reference internal" href="faq.html#why-is-my-training-stuck-and-ray-reporting-that-pending-actor-or-tasks-cannot-be-scheduled" id="id9">Why is my training stuck and Ray reporting that pending actor or tasks cannot be scheduled?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-can-i-pass-further-parameter-values-to-my-trainable" id="id10">How can I pass further parameter values to my trainable?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-can-i-reproduce-experiments" id="id11">How can I reproduce experiments?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-can-i-avoid-bottlenecks" id="id12">How can I avoid bottlenecks?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-can-i-develop-and-test-tune-locally" id="id13">How can I develop and test Tune locally?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-can-i-get-started-contributing-to-tune" id="id14">How can I get started contributing to Tune?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-can-i-make-my-tune-experiments-reproducible" id="id15">How can I make my Tune experiments reproducible?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-can-i-use-large-datasets-in-tune" id="id16">How can I use large datasets in Tune?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-can-i-upload-my-tune-results-to-cloud-storage" id="id17">How can I upload my Tune results to cloud storage?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-can-i-use-the-awscli-or-gsutil-command-line-commands-for-syncing" id="id18">How can I use the awscli or gsutil command line commands for syncing?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-can-i-use-tune-with-docker" id="id19">How can I use Tune with Docker?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-can-i-use-tune-with-kubernetes" id="id20">How can I use Tune with Kubernetes?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-do-i-configure-search-spaces" id="id21">How do I configure search spaces?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-do-i-access-relative-filepaths-in-my-tune-training-function" id="id22">How do I access relative filepaths in my Tune training function?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-can-i-run-multiple-ray-tune-jobs-on-the-same-cluster-at-the-same-time-multi-tenancy" id="id23">How can I run multiple Ray Tune jobs on the same cluster at the same time (multi-tenancy)?</a></p></li>
<li><p><a class="reference internal" href="faq.html#how-can-i-continue-training-a-completed-tune-experiment-for-longer-and-with-new-configurations-iterative-experimentation" id="id24">How can I continue training a completed Tune experiment for longer and with new configurations (iterative experimentation)?</a></p></li>
</ul>
</div>
<section id="what-are-hyperparameters">
<h2><a class="toc-backref" href="faq.html#id1">What are Hyperparameters?</a><a class="headerlink" href="faq.html#what-are-hyperparameters" title="Permalink to this headline">#</a></h2>
<p>What are <em>hyperparameters?</em> And how are they different from <em>model parameters</em>?</p>
<p>In supervised learning, we train a model with labeled data so the model can properly identify new data values.
Everything about the model is defined by a set of parameters, such as the weights in a linear regression. These
are <em>model parameters</em>; they are learned during training.</p>
<img alt="../_images/hyper-model-parameters.png" src="../_images/hyper-model-parameters.png" />
<p>In contrast, the <em>hyperparameters</em> define structural details about the kind of model itself, like whether or not
we are using a linear regression or classification, what architecture is best for a neural network,
how many layers, what kind of filters, etc. They are defined before training, not learned.</p>
<img alt="../_images/hyper-network-params.png" src="../_images/hyper-network-params.png" />
<p>Other quantities considered <em>hyperparameters</em> include learning rates, discount rates, etc. If we want our training
process and resulting model to work well, we first need to determine the optimal or near-optimal set of <em>hyperparameters</em>.</p>
<p>How do we determine the optimal <em>hyperparameters</em>? The most direct approach is to perform a loop where we pick
a candidate set of values from some reasonably inclusive list of possible values, train a model, compare the results
achieved with previous loop iterations, and pick the set that performed best. This process is called
<em>Hyperparameter Tuning</em> or <em>Optimization</em> (HPO). And <em>hyperparameters</em> are specified over a configured and confined
search space, collectively defined for each <em>hyperparameter</em> in a <code class="docutils literal notranslate"><span class="pre">config</span></code> dictionary.</p>
</section>
<section id="which-search-algorithm-scheduler-should-i-choose">
<h2><a class="toc-backref" href="faq.html#id2">Which search algorithm/scheduler should I choose?</a><a class="headerlink" href="faq.html#which-search-algorithm-scheduler-should-i-choose" title="Permalink to this headline">#</a></h2>
<p>Ray Tune offers <a class="reference internal" href="api/suggestion.html#tune-search-alg"><span class="std std-ref">many different search algorithms</span></a>
and <a class="reference internal" href="api/schedulers.html#tune-schedulers"><span class="std std-ref">schedulers</span></a>.
Deciding on which to use mostly depends on your problem:</p>
<ul class="simple">
<li><p>Is it a small or large problem (how long does it take to train? How costly
are the resources, like GPUs)? Can you run many trials in parallel?</p></li>
<li><p>How many hyperparameters would you like to tune?</p></li>
<li><p>What values are valid for hyperparameters?</p></li>
</ul>
<p><strong>If your model returns incremental results</strong> (eg. results per epoch in deep learning,
results per each added tree in GBDTs, etc.) using early stopping usually allows for sampling
more configurations, as unpromising trials are pruned before they run their full course.
Please note that not all search algorithms can use information from pruned trials.
Early stopping cannot be used without incremental results - in case of the functional API,
that means that <code class="docutils literal notranslate"><span class="pre">session.report()</span></code> has to be called more than once - usually in a loop.</p>
<p><strong>If your model is small</strong>, you can usually try to run many different configurations.
A <strong>random search</strong> can be used to generate configurations. You can also grid search
over some values. You should probably still use
<a class="reference internal" href="api/schedulers.html#tune-scheduler-hyperband"><span class="std std-ref">ASHA for early termination of bad trials</span></a> (if your problem
supports early stopping).</p>
<p><strong>If your model is large</strong>, you can try to either use
<strong>Bayesian Optimization-based search algorithms</strong> like <a class="reference internal" href="api/suggestion.html#bayesopt"><span class="std std-ref">BayesOpt</span></a> or
<a class="reference internal" href="api/suggestion.html#dragonfly"><span class="std std-ref">Dragonfly</span></a> to get good parameter configurations after few
trials. <a class="reference internal" href="api/suggestion.html#tune-ax"><span class="std std-ref">Ax</span></a> is similar but more robust to noisy data.
Please note that these algorithms only work well with <strong>a small number of hyperparameters</strong>.
Alternatively, you can use <a class="reference internal" href="api/schedulers.html#tune-scheduler-pbt"><span class="std std-ref">Population Based Training</span></a> which
works well with few trials, e.g. 8 or even 4. However, this will output a hyperparameter <em>schedule</em> rather
than one fixed set of hyperparameters.</p>
<p><strong>If you have a small number of hyperparameters</strong>, Bayesian Optimization methods
work well. Take a look at <a class="reference internal" href="api/schedulers.html#tune-scheduler-bohb"><span class="std std-ref">BOHB</span></a> or <a class="reference internal" href="api/suggestion.html#tune-optuna"><span class="std std-ref">Optuna</span></a>
with the <a class="reference internal" href="api/schedulers.html#tune-scheduler-hyperband"><span class="std std-ref">ASHA</span></a> scheduler to combine the
benefits of Bayesian Optimization with early stopping.</p>
<p><strong>If you only have continuous values for hyperparameters</strong> this will work well
with most Bayesian Optimization methods. Discrete or categorical variables still
work, but less good with an increasing number of categories.</p>
<p><strong>If you have many categorical values for hyperparameters</strong>, consider using random search,
or a TPE-based Bayesian Optimization algorithm such as <a class="reference internal" href="api/suggestion.html#tune-optuna"><span class="std std-ref">Optuna</span></a> or
<a class="reference internal" href="api/suggestion.html#tune-hyperopt"><span class="std std-ref">HyperOpt</span></a>.</p>
<p><strong>Our go-to solution</strong> is usually to use <strong>random search</strong> with
<a class="reference internal" href="api/schedulers.html#tune-scheduler-hyperband"><span class="std std-ref">ASHA for early stopping</span></a> for smaller problems.
Use <a class="reference internal" href="api/schedulers.html#tune-scheduler-bohb"><span class="std std-ref">BOHB</span></a> for <strong>larger problems</strong> with a <strong>small number of hyperparameters</strong>
and <a class="reference internal" href="api/schedulers.html#tune-scheduler-pbt"><span class="std std-ref">Population Based Training</span></a> for <strong>larger problems</strong> with a
<strong>large number of hyperparameters</strong> if a learning schedule is acceptable.</p>
</section>
<section id="how-do-i-choose-hyperparameter-ranges">
<h2><a class="toc-backref" href="faq.html#id3">How do I choose hyperparameter ranges?</a><a class="headerlink" href="faq.html#how-do-i-choose-hyperparameter-ranges" title="Permalink to this headline">#</a></h2>
<p>A good start is to look at the papers that introduced the algorithms, and also
to see what other people are using.</p>
<p>Most algorithms also have sensible defaults for some of their parameters.
For instance, <a class="reference external" href="https://xgboost.readthedocs.io/en/latest/parameter.html">XGBoost’s parameter overview</a>
reports to use <code class="docutils literal notranslate"><span class="pre">max_depth=6</span></code> for the maximum decision tree depth. Here, anything
between 2 and 10 might make sense (though that naturally depends on your problem).</p>
<p>For <strong>learning rates</strong>, we suggest using a <strong>loguniform distribution</strong> between
<strong>1e-5</strong> and <strong>1e-1</strong>: <code class="docutils literal notranslate"><span class="pre">tune.loguniform(1e-5,</span> <span class="pre">1e-1)</span></code>.</p>
<p>For <strong>batch sizes</strong>, we suggest trying <strong>powers of 2</strong>, for instance, 2, 4, 8,
16, 32, 64, 128, 256, etc. The magnitude depends on your problem. For easy
problems with lots of data, use higher batch sizes, for harder problems with
not so much data, use lower batch sizes.</p>
<p>For <strong>layer sizes</strong> we also suggest trying <strong>powers of 2</strong>. For small problems
(e.g. Cartpole), use smaller layer sizes. For larger problems, try larger ones.</p>
<p>For <strong>discount factors</strong> in reinforcement learning we suggest sampling uniformly
between 0.9 and 1.0. Depending on the problem, a much stricter range above 0.97
or oeven above 0.99 can make sense (e.g. for Atari).</p>
</section>
<section id="how-can-i-use-nested-conditional-search-spaces">
<h2><a class="toc-backref" href="faq.html#id4">How can I use nested/conditional search spaces?</a><a class="headerlink" href="faq.html#how-can-i-use-nested-conditional-search-spaces" title="Permalink to this headline">#</a></h2>
<p>Sometimes you might need to define parameters whose value depend on the value
of other parameters. Ray Tune offers some methods to define these.</p>
<section id="nested-spaces">
<h3>Nested spaces<a class="headerlink" href="faq.html#nested-spaces" title="Permalink to this headline">#</a></h3>
<p>You can nest hyperparameter definition in sub dictionaries:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)},</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])}</span>
</pre></div>
</div>
<p>The trial config will be nested exactly like the input config.</p>
</section>
<section id="conditional-spaces">
<h3>Conditional spaces<a class="headerlink" href="faq.html#conditional-spaces" title="Permalink to this headline">#</a></h3>
<p><a class="reference internal" href="tutorials/tune-search-spaces.html#tune-custom-search"><span class="std std-ref">Custom and conditional search spaces are explained in detail here</span></a>.
In short, you can pass custom functions to <code class="docutils literal notranslate"><span class="pre">tune.sample_from()</span></code> that can
return values that depend on other values:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">sample_from</span><span class="p">(</span><span class="k">lambda</span> <span class="n">spec</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">a</span><span class="p">)),</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="conditional-grid-search">
<h3>Conditional grid search<a class="headerlink" href="faq.html#conditional-grid-search" title="Permalink to this headline">#</a></h3>
<p>If you would like to grid search over two parameters that depend on each other,
this might not work out of the box. For instance say that <em>a</em> should be a value
between 5 and 10 and <em>b</em> should be a value between 0 and a. In this case, we
cannot use <code class="docutils literal notranslate"><span class="pre">tune.sample_from</span></code> because it doesn’t support grid searching.</p>
<p>The solution here is to create a list of valid <em>tuples</em> with the help of a
helper function, like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_iter</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span>


<span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;ab&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">_iter</span><span class="p">())),</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Your trainable then can do something like <code class="docutils literal notranslate"><span class="pre">a,</span> <span class="pre">b</span> <span class="pre">=</span> <span class="pre">config[&quot;ab&quot;]</span></code> to split
the a and b variables and use them afterwards.</p>
</section>
</section>
<section id="how-does-early-termination-e-g-hyperband-asha-work">
<h2><a class="toc-backref" href="faq.html#id5">How does early termination (e.g. Hyperband/ASHA) work?</a><a class="headerlink" href="faq.html#how-does-early-termination-e-g-hyperband-asha-work" title="Permalink to this headline">#</a></h2>
<p>Early termination algorithms look at the intermediately reported values,
e.g. what is reported to them via <code class="docutils literal notranslate"><span class="pre">session.report()</span></code> after each training
epoch. After a certain number of steps, they then remove the worst
performing trials and keep only the best performing trials. Goodness of a trial
is determined by ordering them by the objective metric, for instance accuracy
or loss.</p>
<p>In ASHA, you can decide how many trials are early terminated.
<code class="docutils literal notranslate"><span class="pre">reduction_factor=4</span></code> means that only 25% of all trials are kept each
time they are reduced. With <code class="docutils literal notranslate"><span class="pre">grace_period=n</span></code> you can force ASHA to
train each trial at least for <code class="docutils literal notranslate"><span class="pre">n</span></code> epochs.</p>
</section>
<section id="why-are-all-my-trials-returning-1-iteration">
<h2><a class="toc-backref" href="faq.html#id6">Why are all my trials returning “1” iteration?</a><a class="headerlink" href="faq.html#why-are-all-my-trials-returning-1-iteration" title="Permalink to this headline">#</a></h2>
<p><strong>This is most likely applicable for the Tune function API.</strong></p>
<p>Ray Tune counts iterations internally every time <code class="docutils literal notranslate"><span class="pre">session.report()</span></code> is
called. If you only call <code class="docutils literal notranslate"><span class="pre">session.report()</span></code> once at the end of the training,
the counter has only been incremented once. If you’re using the class API,
the counter is increased after calling <code class="docutils literal notranslate"><span class="pre">step()</span></code>.</p>
<p>Note that it might make sense to report metrics more often than once. For
instance, if you train your algorithm for 1000 timesteps, consider reporting
intermediate performance values every 100 steps. That way, schedulers
like Hyperband/ASHA can terminate bad performing trials early.</p>
</section>
<section id="what-are-all-these-extra-outputs">
<h2><a class="toc-backref" href="faq.html#id7">What are all these extra outputs?</a><a class="headerlink" href="faq.html#what-are-all-these-extra-outputs" title="Permalink to this headline">#</a></h2>
<p>You’ll notice that Ray Tune not only reports hyperparameters (from the
<code class="docutils literal notranslate"><span class="pre">config</span></code>) or metrics (passed to <code class="docutils literal notranslate"><span class="pre">session.report()</span></code>), but also some other
outputs.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Result <span class="k">for</span> easy_objective_c64c9112:
  date: <span class="m">2020</span>-10-07_13-29-18
  <span class="k">done</span>: <span class="nb">false</span>
  experiment_id: 6edc31257b564bf8985afeec1df618ee
  experiment_tag: <span class="nv">7_activation</span><span class="o">=</span>tanh,height<span class="o">=</span>-53.116,steps<span class="o">=</span><span class="m">100</span>,width<span class="o">=</span><span class="m">13</span>.885
  hostname: ubuntu
  iterations: <span class="m">0</span>
  iterations_since_restore: <span class="m">1</span>
  mean_loss: <span class="m">4</span>.688385317424468
  neg_mean_loss: -4.688385317424468
  node_ip: <span class="m">192</span>.168.1.115
  pid: <span class="m">5973</span>
  time_since_restore: <span class="m">7</span>.605552673339844e-05
  time_this_iter_s: <span class="m">7</span>.605552673339844e-05
  time_total_s: <span class="m">7</span>.605552673339844e-05
  timestamp: <span class="m">1602102558</span>
  timesteps_since_restore: <span class="m">0</span>
  training_iteration: <span class="m">1</span>
  trial_id: c64c9112
</pre></div>
</div>
<p>See the <a class="reference internal" href="tutorials/tune-metrics.html#tune-autofilled-metrics"><span class="std std-ref">How to use log metrics in Tune?</span></a> section for a glossary.</p>
</section>
<section id="how-do-i-set-resources">
<h2><a class="toc-backref" href="faq.html#id8">How do I set resources?</a><a class="headerlink" href="faq.html#how-do-i-set-resources" title="Permalink to this headline">#</a></h2>
<p>If you want to allocate specific resources to a trial, you can use the
<code class="docutils literal notranslate"><span class="pre">tune.with_resources</span></code> and wrap it around you trainable together with
a dict or a <a class="reference internal" href="api/doc/ray.tune.execution.placement_groups.PlacementGroupFactory.html#ray.tune.execution.placement_groups.PlacementGroupFactory" title="ray.tune.execution.placement_groups.PlacementGroupFactory"><code class="xref py py-class docutils literal notranslate"><span class="pre">PlacementGroupFactory</span></code></a> object:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="n">tune</span><span class="o">.</span><span class="n">with_resources</span><span class="p">(</span>
        <span class="n">train_fn</span><span class="p">,</span> <span class="n">resources</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;cpu&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;gpu&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">&quot;custom_resources&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;hdd&quot;</span><span class="p">:</span> <span class="mi">80</span><span class="p">}}</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>The example above showcases three things:</p>
<ol class="arabic simple">
<li><p>The <code class="xref py py-obj docutils literal notranslate"><span class="pre">cpu</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">gpu</span></code> options set how many CPUs and GPUs are available for
each trial, respectively. <strong>Trials cannot request more resources</strong> than these
(exception: see 3).</p></li>
<li><p>It is possible to request <strong>fractional GPUs</strong>. A value of 0.5 means that
half of the memory of the GPU is made available to the trial. You will have
to make sure yourself that your model still fits on the fractional memory.</p></li>
<li><p>You can request custom resources you supplied to Ray when starting the cluster.
Trials will only be scheduled on single nodes that can provide all resources you
requested.</p></li>
</ol>
<p>One important thing to keep in mind is that each Ray worker (and thus each
Ray Tune Trial) will only be scheduled on <strong>one machine</strong>. That means if
you for instance request 2 GPUs for your trial, but your cluster consists
of 4 machines with 1 GPU each, the trial will never be scheduled.</p>
<p>In other words, you will have to make sure that your Ray cluster
has machines that can actually fulfill your resource requests.</p>
<p>In some cases your trainable might want to start other remote actors, for instance if you’re
leveraging distributed training via Ray Train. In these cases, you can use
<a class="reference internal" href="../ray-core/scheduling/placement-group.html#ray-placement-group-doc-ref"><span class="std std-ref">placement groups</span></a> to request additional resources:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="n">tune</span><span class="o">.</span><span class="n">with_resources</span><span class="p">(</span>
        <span class="n">train_fn</span><span class="p">,</span>
        <span class="n">resources</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">PlacementGroupFactory</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="p">{</span><span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">&quot;hdd&quot;</span><span class="p">:</span> <span class="mi">80</span><span class="p">},</span>
                <span class="p">{</span><span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
                <span class="p">{</span><span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
            <span class="p">],</span>
            <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;PACK&quot;</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>Here, you’re requesting 2 additional CPUs for remote tasks. These two additional
actors do not necessarily have to live on the same node as your main trainable.
In fact, you can control this via the <code class="docutils literal notranslate"><span class="pre">strategy</span></code> parameter. In this example, <code class="docutils literal notranslate"><span class="pre">PACK</span></code>
will try to schedule the actors on the same node, but allows them to be scheduled
on other nodes as well. Please refer to the
<a class="reference internal" href="../ray-core/scheduling/placement-group.html#ray-placement-group-doc-ref"><span class="std std-ref">placement groups documentation</span></a> to learn more
about these placement strategies.</p>
<p>You can also use the <a class="reference internal" href="../train/config_guide.html#train-config"><span class="std std-ref">ScalingConfig</span></a> to achieve the same results:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="n">tune</span><span class="o">.</span><span class="n">with_resources</span><span class="p">(</span>
        <span class="n">train_fn</span><span class="p">,</span>
        <span class="n">resources</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span>
            <span class="n">trainer_resources</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">&quot;hdd&quot;</span><span class="p">:</span> <span class="mi">80</span><span class="p">},</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">resources_per_worker</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
        <span class="p">),</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>You can also allocate specific resources to a trial based on a custom rule via lambda functions.
For instance, if you want to allocate GPU resources to trials based on a setting in your param space:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="n">tune</span><span class="o">.</span><span class="n">with_resources</span><span class="p">(</span>
        <span class="n">train_fn</span><span class="p">,</span>
        <span class="n">resources</span><span class="o">=</span><span class="k">lambda</span> <span class="n">config</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span> <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;use_gpu&quot;</span><span class="p">]</span> <span class="k">else</span> <span class="p">{</span><span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span>
    <span class="p">),</span>
    <span class="n">param_space</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;use_gpu&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="why-is-my-training-stuck-and-ray-reporting-that-pending-actor-or-tasks-cannot-be-scheduled">
<h2><a class="toc-backref" href="faq.html#id9">Why is my training stuck and Ray reporting that pending actor or tasks cannot be scheduled?</a><a class="headerlink" href="faq.html#why-is-my-training-stuck-and-ray-reporting-that-pending-actor-or-tasks-cannot-be-scheduled" title="Permalink to this headline">#</a></h2>
<p>This is usually caused by Ray actors or tasks being started by the
trainable without the trainable resources accounting for them, leading to a deadlock.
This can also be “stealthly” caused by using other libraries in the trainable that are
based on Ray, such as Modin. In order to fix the issue, request additional resources for
the trial using <a class="reference internal" href="../ray-core/scheduling/placement-group.html#ray-placement-group-doc-ref"><span class="std std-ref">placement groups</span></a>, as outlined in
the section above.</p>
<p>For example, if your trainable is using Modin dataframes, operations on those will spawn
Ray tasks. By allocating an additional CPU bundle to the trial, those tasks will be able
to run without being starved of resources.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_fn</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">checkpoint_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># some Modin operations here</span>
    <span class="c1"># import modin.pandas as pd</span>
    <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">({</span><span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="n">metric</span><span class="p">})</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="n">tune</span><span class="o">.</span><span class="n">with_resources</span><span class="p">(</span>
        <span class="n">train_fn</span><span class="p">,</span>
        <span class="n">resources</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">PlacementGroupFactory</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="p">{</span><span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>  <span class="c1"># this bundle will be used by the trainable itself</span>
                <span class="p">{</span><span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>  <span class="c1"># this bundle will be used by Modin</span>
            <span class="p">],</span>
            <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;PACK&quot;</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="how-can-i-pass-further-parameter-values-to-my-trainable">
<h2><a class="toc-backref" href="faq.html#id10">How can I pass further parameter values to my trainable?</a><a class="headerlink" href="faq.html#how-can-i-pass-further-parameter-values-to-my-trainable" title="Permalink to this headline">#</a></h2>
<p>Ray Tune expects your trainable functions to accept only up to two parameters,
<code class="docutils literal notranslate"><span class="pre">config</span></code> and <code class="docutils literal notranslate"><span class="pre">checkpoint_dir</span></code>. But sometimes there are cases where
you want to pass constant arguments, like the number of epochs to run,
or a dataset to train on. Ray Tune offers a wrapper function to achieve
just that, called <a class="reference internal" href="api/doc/ray.tune.with_parameters.html#ray.tune.with_parameters" title="ray.tune.with_parameters"><code class="xref py py-func docutils literal notranslate"><span class="pre">tune.with_parameters()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">checkpoint_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="c1"># ... train on sample</span>
            <span class="k">pass</span>


<span class="c1"># Some huge dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100000000</span><span class="p">)</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span><span class="n">tune</span><span class="o">.</span><span class="n">with_parameters</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">))</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>This function works similarly to <code class="docutils literal notranslate"><span class="pre">functools.partial</span></code>, but it stores
the parameters directly in the Ray object store. This means that you
can pass even huge objects like datasets, and Ray makes sure that these
are efficiently stored and retrieved on your cluster machines.</p>
<p><a class="reference internal" href="api/doc/ray.tune.with_parameters.html#ray.tune.with_parameters" title="ray.tune.with_parameters"><code class="xref py py-func docutils literal notranslate"><span class="pre">tune.with_parameters()</span></code></a>
also works with class trainables. Please see
<a class="reference internal" href="api/doc/ray.tune.with_parameters.html#ray.tune.with_parameters" title="ray.tune.with_parameters"><code class="xref py py-func docutils literal notranslate"><span class="pre">tune.with_parameters()</span></code></a> for more details and examples.</p>
</section>
<section id="how-can-i-reproduce-experiments">
<h2><a class="toc-backref" href="faq.html#id11">How can I reproduce experiments?</a><a class="headerlink" href="faq.html#how-can-i-reproduce-experiments" title="Permalink to this headline">#</a></h2>
<p>Reproducing experiments and experiment results means that you get the exact same
results when running an experiment again and again. To achieve this, the
conditions have to be exactly the same each time you run the exeriment.
In terms of ML training and tuning, this mostly concerns
the random number generators that are used for sampling in various places of the
training and tuning lifecycle.</p>
<p>Random number generators are used to create randomness, for instance to sample a hyperparameter
value for a parameter you defined. There is no true randomness in computing, rather
there are sophisticated algorithms that generate numbers that <em>seem</em> to be random and
fulfill all properties of a random distribution. These algorithms can be <em>seeded</em> with
an initial state, after which the generated random numbers are always the same.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>

<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>

<span class="c1"># The output will always be the same.</span>
<span class="k">assert</span> <span class="n">output</span> <span class="o">==</span> <span class="p">[</span><span class="mi">99</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">74</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">85</span><span class="p">,</span> <span class="mi">88</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
<p>The most commonly used random number generators from Python libraries are those in the
native <code class="docutils literal notranslate"><span class="pre">random</span></code> submodule and the <code class="docutils literal notranslate"><span class="pre">numpy.random</span></code> module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># This should suffice to initialize the RNGs for most Python-based libraries</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">5678</span><span class="p">)</span>
</pre></div>
</div>
<p>In your tuning and training run, there are several places where randomness occurs, and
at all these places we will have to introduce seeds to make sure we get the same behavior.</p>
<ul class="simple">
<li><p><strong>Search algorithm</strong>: Search algorithms have to be seeded to generate the same
hyperparameter configurations in each run. Some search algorithms can be explicitly instantiated with a
random seed (look for a <code class="docutils literal notranslate"><span class="pre">seed</span></code> parameter in the constructor). For others, try to use
the above code block.</p></li>
<li><p><strong>Schedulers</strong>: Schedulers like Population Based Training rely on resampling some
of the parameters, requiring randomness. Use the code block above to set the initial
seeds.</p></li>
<li><p><strong>Training function</strong>: In addition to initializing the configurations, the training
functions themselves have to use seeds. This could concern e.g. the data splitting.
You should make sure to set the seed at the start of your training function.</p></li>
</ul>
<p>PyTorch and TensorFlow use their own RNGs, which have to be initialized, too:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>You should thus seed both Ray Tune’s schedulers and search algorithms, and the
training code. The schedulers and search algorithms should always be seeded with the
same seed. This is also true for the training code, but often it is beneficial that
the seeds differ <em>between different training runs</em>.</p>
<p>Here’s a blueprint on how to do all this in your training code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>


<span class="k">def</span> <span class="nf">trainable</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="c1"># config[&quot;seed&quot;] is set deterministically, but differs between training runs</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;seed&quot;</span><span class="p">])</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;seed&quot;</span><span class="p">])</span>
    <span class="c1"># torch.manual_seed(config[&quot;seed&quot;])</span>
    <span class="c1"># ... training code</span>


<span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;seed&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10000</span><span class="p">),</span>
    <span class="c1"># ...</span>
<span class="p">}</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Set seed for the search algorithms/schedulers</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
    <span class="c1"># Don&#39;t forget to check if the search alg has a `seed` parameter</span>
    <span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span><span class="n">trainable</span><span class="p">,</span> <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
    <span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Please note</strong> that it is not always possible to control all sources of non-determinism.
For instance, if you use schedulers like ASHA or PBT, some trials might finish earlier
than other trials, affecting the behavior of the schedulers. Which trials finish first
can however depend on the current system load, network communication, or other factors
in the envrionment that we cannot control with random seeds. This is also true for search
algorithms such as Bayesian Optimization, which take previous results into account when
sampling new configurations. This can be tackled by
using the <strong>synchronous modes</strong> of PBT and Hyperband, where the schedulers wait for all trials to
finish an epoch before deciding which trials to promote.</p>
<p>We strongly advise to try reproduction on smaller toy problems first before relying
on it for larger experiments.</p>
</section>
<section id="how-can-i-avoid-bottlenecks">
<span id="tune-bottlenecks"></span><h2><a class="toc-backref" href="faq.html#id12">How can I avoid bottlenecks?</a><a class="headerlink" href="faq.html#how-can-i-avoid-bottlenecks" title="Permalink to this headline">#</a></h2>
<p>Sometimes you might run into a message like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>The `experiment_checkpoint` operation took 2.43 seconds to complete, which may be a performance bottleneck
</pre></div>
</div>
<p>Most commonly, the <code class="docutils literal notranslate"><span class="pre">experiment_checkpoint</span></code> operation is throwing this warning, but it might be something else,
like <code class="docutils literal notranslate"><span class="pre">process_trial_result</span></code>.</p>
<p>These operations should usually take less than 500ms to complete. When it consistently takes longer, this might
indicate a problem or inefficiencies. To get rid of this message, it is important to understand where it comes
from.</p>
<p>These are the main reasons this problem comes up:</p>
<p><strong>The Trial config is very large</strong></p>
<p>This is the case if you e.g. try to pass a dataset or other large object via the <code class="docutils literal notranslate"><span class="pre">config</span></code> parameter.
If this is the case, the dataset is serialized and written to disk repeatedly during experiment
checkpointing, which takes a long time.</p>
<p><strong>Solution</strong>: Use <a class="reference internal" href="api/doc/ray.tune.with_parameters.html#ray.tune.with_parameters" title="ray.tune.with_parameters"><code class="xref py py-func docutils literal notranslate"><span class="pre">tune.with_parameters</span></code></a> to pass large objects to
function trainables via the objects store. For class trainables you can do this manually via <code class="docutils literal notranslate"><span class="pre">ray.put()</span></code>
and <code class="docutils literal notranslate"><span class="pre">ray.get()</span></code>. If you need to pass a class definition, consider passing an
indicator (e.g. a string) instead and let the trainable select the class instead. Generally, your config
dictionary should only contain primitive types, like numbers or strings.</p>
<p><strong>The Trial result is very large</strong></p>
<p>This is the case if you return objects, data, or other large objects via the return value of <code class="docutils literal notranslate"><span class="pre">step()</span></code> in
your class trainable or to <code class="docutils literal notranslate"><span class="pre">session.report()</span></code> in your function trainable. The effect is the same as above:
The results are repeatedly serialized and written to disk, and this can take a long time.</p>
<p><strong>Solution</strong>: Use checkpoint by writing data to the trainable’s current working directory instead. There are various ways
to do that depending on whether you are using class or functional Trainable API.</p>
<p><strong>You are training a large number of trials on a cluster, or you are saving huge checkpoints</strong></p>
<p>Checkpoints and logs are synced between nodes
- usually at least to the driver on the head node, but sometimes between worker nodes if needed (e.g. when
using <a class="reference internal" href="api/schedulers.html#tune-scheduler-pbt"><span class="std std-ref">Population Based Training</span></a>). If these checkpoints are very large (e.g. for
NLP models), or if you are training a large number of trials, this syncing can take a long time.</p>
<p>If nothing else is specified, syncing happens via SSH, which can lead to network overhead as connections are
not kept open by Ray Tune.</p>
<p><strong>Solution</strong>: There are multiple solutions, depending on your needs:</p>
<ol class="arabic simple">
<li><p>You can disable syncing to the driver in the <code class="xref py py-class docutils literal notranslate"><span class="pre">tune.SyncConfig</span></code>. In this case,
logs and checkpoints will not be synced to the driver, so if you need to access them later, you will have to
transfer them where you need them manually.</p></li>
<li><p>You can use <a class="reference internal" href="tutorials/tune-storage.html#tune-cloud-checkpointing"><span class="std std-ref">cloud checkpointing</span></a> to save logs and checkpoints to a specified <code class="xref py py-obj docutils literal notranslate"><span class="pre">storage_path</span></code>.
This is the preferred way to deal with this. All syncing will be taken care of automatically, as all nodes
are able to access the cloud storage. Additionally, your results will be safe, so even when you’re working on
pre-emptible instances, you won’t lose any of your data.</p></li>
</ol>
<p><strong>You are reporting results too often</strong></p>
<p>Each result is processed by the search algorithm, trial scheduler, and callbacks (including loggers and the
trial syncer). If you’re reporting a large number of results per trial (e.g. multiple results per second),
this can take a long time.</p>
<p><strong>Solution</strong>: The solution here is obvious: Just don’t report results that often. In class trainables, <code class="docutils literal notranslate"><span class="pre">step()</span></code>
should maybe process a larger chunk of data. In function trainables, you can report only every n-th iteration
of the training loop. Try to balance the number of results you really need to make scheduling or searching
decisions. If you need more fine grained metrics for logging or tracking, consider using a separate logging
mechanism for this instead of the Ray Tune-provided progress logging of results.</p>
</section>
<section id="how-can-i-develop-and-test-tune-locally">
<h2><a class="toc-backref" href="faq.html#id13">How can I develop and test Tune locally?</a><a class="headerlink" href="faq.html#how-can-i-develop-and-test-tune-locally" title="Permalink to this headline">#</a></h2>
<p>First, follow the instructions in <a class="reference internal" href="../ray-contribute/development.html#python-develop"><span class="std std-ref">Building Ray (Python Only)</span></a> to develop Tune without compiling Ray.
After Ray is set up, run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-r</span> <span class="pre">ray/python/ray/tune/requirements-dev.txt</span></code> to install all packages
required for Tune development. Now, to run all Tune tests simply run:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pytest ray/python/ray/tune/tests/
</pre></div>
</div>
<p>If you plan to submit a pull request, we recommend you to run unit tests locally beforehand to speed up the review process.
Even though we have hooks to run unit tests automatically for each pull request, it’s usually quicker to run them
on your machine first to avoid any obvious mistakes.</p>
</section>
<section id="how-can-i-get-started-contributing-to-tune">
<h2><a class="toc-backref" href="faq.html#id14">How can I get started contributing to Tune?</a><a class="headerlink" href="faq.html#how-can-i-get-started-contributing-to-tune" title="Permalink to this headline">#</a></h2>
<p>We use Github to track issues, feature requests, and bugs. Take a look at the
ones labeled <a class="reference external" href="https://github.com/ray-project/ray/issues?utf8=%E2%9C%93&amp;q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">“good first issue”</a> and <a class="reference external" href="https://github.com/ray-project/ray/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22">“help wanted”</a> for a place to start.
Look for issues with “[tune]” in the title.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If raising a new issue or PR related to Tune, be sure to include “[tune]” in the title and add a <code class="docutils literal notranslate"><span class="pre">tune</span></code> label.</p>
</div>
<p>For project organization, Tune maintains a relatively up-to-date organization of
issues on the <a class="reference external" href="https://github.com/ray-project/ray/projects/4">Tune Github Project Board</a>.
Here, you can track and identify how issues are organized.</p>
</section>
<section id="how-can-i-make-my-tune-experiments-reproducible">
<span id="tune-reproducible"></span><h2><a class="toc-backref" href="faq.html#id15">How can I make my Tune experiments reproducible?</a><a class="headerlink" href="faq.html#how-can-i-make-my-tune-experiments-reproducible" title="Permalink to this headline">#</a></h2>
<p>Exact reproducibility of machine learning runs is hard to achieve. This
is even more true in a distributed setting, as more non-determinism is
introduced. For instance, if two trials finish at the same time, the
convergence of the search algorithm might be influenced by which trial
result is processed first. This depends on the searcher - for random search,
this shouldn’t make a difference, but for most other searchers it will.</p>
<p>If you try to achieve some amount of reproducibility, there are two
places where you’ll have to set random seeds:</p>
<ol class="arabic simple">
<li><p>On the driver program, e.g. for the search algorithm. This will ensure
that at least the initial configurations suggested by the search
algorithms are the same.</p></li>
<li><p>In the trainable (if required). Neural networks are usually initialized
with random numbers, and many classical ML algorithms, like GBDTs, make use of
randomness. Thus you’ll want to make sure to set a seed here
so that the initialization is always the same.</p></li>
</ol>
<p>Here is an example that will always produce the same result (except for trial
runtimes).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>
<span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">session</span><span class="p">,</span> <span class="n">ScalingConfig</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="c1"># Set seed for trainable random result.</span>
    <span class="c1"># If you remove this line, you will get different results</span>
    <span class="c1"># each time you run the trial, even if the configuration</span>
    <span class="c1"># is the same.</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;seed&quot;</span><span class="p">])</span>
    <span class="n">random_result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">({</span><span class="s2">&quot;result&quot;</span><span class="p">:</span> <span class="n">random_result</span><span class="p">})</span>


<span class="c1"># Set seed for Ray Tune&#39;s random search.</span>
<span class="c1"># If you remove this line, you will get different configurations</span>
<span class="c1"># each time you run the script.</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="n">train</span><span class="p">,</span>
    <span class="n">tune_config</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">TuneConfig</span><span class="p">(</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">search_alg</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">search</span><span class="o">.</span><span class="n">BasicVariantGenerator</span><span class="p">(),</span>
    <span class="p">),</span>
    <span class="n">param_space</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;seed&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)},</span>
<span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>Some searchers use their own random states to sample new configurations.
These searchers usually accept a <code class="docutils literal notranslate"><span class="pre">seed</span></code> parameter that can be passed on
initialization. Other searchers use Numpy’s <code class="docutils literal notranslate"><span class="pre">np.random</span></code> interface -
these seeds can be then set with <code class="docutils literal notranslate"><span class="pre">np.random.seed()</span></code>. We don’t offer an
interface to do this in the searcher classes as setting a random seed
globally could have side effects. For instance, it could influence the
way your dataset is split. Thus, we leave it up to the user to make
these global configuration changes.</p>
</section>
<section id="how-can-i-use-large-datasets-in-tune">
<h2><a class="toc-backref" href="faq.html#id16">How can I use large datasets in Tune?</a><a class="headerlink" href="faq.html#how-can-i-use-large-datasets-in-tune" title="Permalink to this headline">#</a></h2>
<p>You often will want to compute a large object (e.g., training data, model weights) on the driver and use that
object within each trial.</p>
<p>Tune provides a wrapper function <code class="docutils literal notranslate"><span class="pre">tune.with_parameters()</span></code> that allows you to broadcast large objects to your trainable.
Objects passed with this wrapper will be stored on the <a class="reference internal" href="../ray-core/objects.html#objects-in-ray"><span class="std std-ref">Ray object store</span></a> and will
be automatically fetched and passed to your trainable as a parameter.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If the objects are small in size or already exist in the <a class="reference internal" href="../ray-core/objects.html#objects-in-ray"><span class="std std-ref">Ray Object Store</span></a>, there’s no need to use <code class="docutils literal notranslate"><span class="pre">tune.with_parameters()</span></code>. You can use <a class="reference external" href="https://docs.python.org/3/library/functools.html#functools.partial">partials</a> or pass in directly to <code class="docutils literal notranslate"><span class="pre">config</span></code> instead.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span><span class="p">,</span> <span class="n">tune</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">pass</span>
    <span class="c1"># use data</span>


<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100000000</span><span class="p">)</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span><span class="n">tune</span><span class="o">.</span><span class="n">with_parameters</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">))</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="how-can-i-upload-my-tune-results-to-cloud-storage">
<span id="tune-cloud-syncing"></span><h2><a class="toc-backref" href="faq.html#id17">How can I upload my Tune results to cloud storage?</a><a class="headerlink" href="faq.html#how-can-i-upload-my-tune-results-to-cloud-storage" title="Permalink to this headline">#</a></h2>
<p>If an upload directory is provided, Tune will automatically sync results from the <code class="docutils literal notranslate"><span class="pre">RAY_AIR_LOCAL_CACHE_DIR</span></code> to the given directory,
natively supporting standard URIs for systems like S3, gsutil or HDFS. You can add more filesystems by installing
<a class="reference external" href="https://filesystem-spec.readthedocs.io/en/latest/">fs-spec</a>-compatible filesystems e.g. using pip.</p>
<p>Here is an example of uploading to S3, using a bucket called <code class="docutils literal notranslate"><span class="pre">my-log-dir</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="n">MyTrainableClass</span><span class="p">,</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">storage_path</span><span class="o">=</span><span class="s2">&quot;s3://my-log-dir&quot;</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>You can customize synchronization behavior by implementing your own Syncer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.tune.syncer</span> <span class="kn">import</span> <span class="n">Syncer</span>

<span class="k">class</span> <span class="nc">CustomSyncer</span><span class="p">(</span><span class="n">Syncer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">sync_up</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">local_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">remote_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">exclude</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># sync up</span>

    <span class="k">def</span> <span class="nf">sync_down</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">remote_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">local_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">exclude</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># sync down</span>

    <span class="k">def</span> <span class="nf">delete</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">remote_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># delete</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="n">MyTrainableClass</span><span class="p">,</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">storage_path</span><span class="o">=</span><span class="s2">&quot;s3://my-log-dir&quot;</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>By default, syncing occurs whenever one of the following conditions are met:</p>
<ul class="simple">
<li><p>if you have used a <a class="reference internal" href="../ray-air/api/doc/ray.air.CheckpointConfig.html#ray.air.CheckpointConfig" title="ray.air.config.CheckpointConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CheckpointConfig</span></code></a> with <code class="docutils literal notranslate"><span class="pre">num_to_keep</span></code> and a trial has checkpointed more than <code class="docutils literal notranslate"><span class="pre">num_to_keep</span></code> times since last sync,</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">sync_period</span></code> of seconds (default 300) has passed since last sync.</p></li>
</ul>
<p>To change the frequency of syncing, set the <code class="docutils literal notranslate"><span class="pre">sync_period</span></code> attribute of the sync config to the desired syncing period.</p>
<p>Note that uploading only happens when global experiment state is collected, and the frequency of this is
determined by the experiment checkpoint period. So the true upload period is given by <code class="docutils literal notranslate"><span class="pre">max(sync</span> <span class="pre">period,</span> <span class="pre">TUNE_GLOBAL_CHECKPOINT_S)</span></code>.</p>
<p>Make sure that worker nodes have the write access to the cloud storage.
Failing to do so would cause error messages like <code class="docutils literal notranslate"><span class="pre">Error</span> <span class="pre">message</span> <span class="pre">(1):</span> <span class="pre">fatal</span> <span class="pre">error:</span> <span class="pre">Unable</span> <span class="pre">to</span> <span class="pre">locate</span> <span class="pre">credentials</span></code>.
For AWS set up, this involves adding an IamInstanceProfile configuration for worker nodes.
Please <a class="reference internal" href="../cluster/vms/user-guides/launching-clusters/aws.html#aws-cluster-s3"><span class="std std-ref">see here for more tips</span></a>.</p>
</section>
<section id="how-can-i-use-the-awscli-or-gsutil-command-line-commands-for-syncing">
<span id="tune-cloud-syncing-command-line-example"></span><h2><a class="toc-backref" href="faq.html#id18">How can I use the awscli or gsutil command line commands for syncing?</a><a class="headerlink" href="faq.html#how-can-i-use-the-awscli-or-gsutil-command-line-commands-for-syncing" title="Permalink to this headline">#</a></h2>
<p>Some users reported to run into problems with the default pyarrow-based syncing.
In this case, a custom syncer that invokes the respective command line tools
for transferring files between nodes and cloud storage can be implemented.</p>
<p>Here is an example for a syncer that uses string templates that will be run
as a command:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">from</span> <span class="nn">ray.tune.syncer</span> <span class="kn">import</span> <span class="n">Syncer</span>

<span class="k">class</span> <span class="nc">CustomCommandSyncer</span><span class="p">(</span><span class="n">Syncer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sync_up_template</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">sync_down_template</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">delete_template</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">sync_period</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">300.0</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sync_up_template</span> <span class="o">=</span> <span class="n">sync_up_template</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sync_down_template</span> <span class="o">=</span> <span class="n">sync_down_template</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">delete_template</span> <span class="o">=</span> <span class="n">delete_template</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">sync_period</span><span class="o">=</span><span class="n">sync_period</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sync_up</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">local_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">remote_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">exclude</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="n">cmd_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sync_up_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">source</span><span class="o">=</span><span class="n">local_dir</span><span class="p">,</span>
            <span class="n">target</span><span class="o">=</span><span class="n">remote_dir</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">subprocess</span><span class="o">.</span><span class="n">check_call</span><span class="p">(</span><span class="n">cmd_str</span><span class="p">,</span> <span class="n">shell</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exception when syncing up </span><span class="si">{</span><span class="n">local_dir</span><span class="si">}</span><span class="s2"> to </span><span class="si">{</span><span class="n">remote_dir</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">sync_down</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">remote_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">local_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">exclude</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="n">cmd_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sync_down_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">source</span><span class="o">=</span><span class="n">remote_dir</span><span class="p">,</span>
            <span class="n">target</span><span class="o">=</span><span class="n">local_dir</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">subprocess</span><span class="o">.</span><span class="n">check_call</span><span class="p">(</span><span class="n">cmd_str</span><span class="p">,</span> <span class="n">shell</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exception when syncing down </span><span class="si">{</span><span class="n">remote_dir</span><span class="si">}</span><span class="s2"> to </span><span class="si">{</span><span class="n">local_dir</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">delete</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">remote_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="n">cmd_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">delete_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">target</span><span class="o">=</span><span class="n">remote_dir</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">subprocess</span><span class="o">.</span><span class="n">check_call</span><span class="p">(</span><span class="n">cmd_str</span><span class="p">,</span> <span class="n">shell</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exception when deleting </span><span class="si">{</span><span class="n">remote_dir</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">retry</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">wait</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

<span class="n">sync_config</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">SyncConfig</span><span class="p">(</span>
    <span class="n">syncer</span><span class="o">=</span><span class="n">CustomCommandSyncer</span><span class="p">(</span>
        <span class="n">sync_up_template</span><span class="o">=</span><span class="s2">&quot;aws s3 sync </span><span class="si">{source}</span><span class="s2"> </span><span class="si">{target}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">sync_down_template</span><span class="o">=</span><span class="s2">&quot;aws s3 sync </span><span class="si">{source}</span><span class="s2"> </span><span class="si">{target}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">delete_template</span><span class="o">=</span><span class="s2">&quot;aws s3 rm </span><span class="si">{target}</span><span class="s2"> --recursive&quot;</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>For different cloud services, these are example templates you can use with this syncer:</p>
<section id="aws-s3">
<h3>AWS S3<a class="headerlink" href="faq.html#aws-s3" title="Permalink to this headline">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sync_up_template</span><span class="o">=</span><span class="s2">&quot;aws s3 sync </span><span class="si">{source}</span><span class="s2"> </span><span class="si">{target}</span><span class="s2"> --exact-timestamps --only-show-errors&quot;</span>
<span class="n">sync_down_template</span><span class="o">=</span><span class="s2">&quot;aws s3 sync </span><span class="si">{source}</span><span class="s2"> </span><span class="si">{target}</span><span class="s2"> --exact-timestamps --only-show-errors&quot;</span>
<span class="n">delete_template</span><span class="o">=</span><span class="s2">&quot;aws s3 rm </span><span class="si">{target}</span><span class="s2"> --recursive --only-show-errors&quot;</span>
</pre></div>
</div>
</section>
<section id="google-cloud-storage">
<h3>Google cloud storage<a class="headerlink" href="faq.html#google-cloud-storage" title="Permalink to this headline">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sync_up_template</span><span class="o">=</span><span class="s2">&quot;gsutil rsync -r </span><span class="si">{source}</span><span class="s2"> </span><span class="si">{target}</span><span class="s2">&quot;</span>
<span class="n">sync_down_template</span><span class="o">=</span><span class="s2">&quot;down&quot;</span><span class="p">:</span> <span class="s2">&quot;gsutil rsync -r </span><span class="si">{source}</span><span class="s2"> </span><span class="si">{target}</span><span class="s2">&quot;</span>
<span class="n">delete_template</span><span class="o">=</span><span class="s2">&quot;delete&quot;</span><span class="p">:</span> <span class="s2">&quot;gsutil rm -r </span><span class="si">{target}</span><span class="s2">&quot;</span>
</pre></div>
</div>
</section>
<section id="hdfs">
<h3>HDFS<a class="headerlink" href="faq.html#hdfs" title="Permalink to this headline">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sync_up_template</span><span class="o">=</span><span class="s2">&quot;hdfs dfs -put -f </span><span class="si">{source}</span><span class="s2"> </span><span class="si">{target}</span><span class="s2">&quot;</span>
<span class="n">sync_down_template</span><span class="o">=</span><span class="s2">&quot;down&quot;</span><span class="p">:</span> <span class="s2">&quot;hdfs dfs -get -f </span><span class="si">{source}</span><span class="s2"> </span><span class="si">{target}</span><span class="s2">&quot;</span>
<span class="n">delete_template</span><span class="o">=</span><span class="s2">&quot;delete&quot;</span><span class="p">:</span> <span class="s2">&quot;hdfs dfs -rm -r </span><span class="si">{target}</span><span class="s2">&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="how-can-i-use-tune-with-docker">
<span id="tune-docker"></span><h2><a class="toc-backref" href="faq.html#id19">How can I use Tune with Docker?</a><a class="headerlink" href="faq.html#how-can-i-use-tune-with-docker" title="Permalink to this headline">#</a></h2>
<p>Tune automatically syncs files and checkpoints between different remote
containers as needed.</p>
</section>
<section id="how-can-i-use-tune-with-kubernetes">
<span id="tune-kubernetes"></span><h2><a class="toc-backref" href="faq.html#id20">How can I use Tune with Kubernetes?</a><a class="headerlink" href="faq.html#how-can-i-use-tune-with-kubernetes" title="Permalink to this headline">#</a></h2>
<p>Ray Tune automatically synchronizes files and checkpoints between different remote nodes as needed.
This usually happens via the Ray object store, but this can be a <a class="reference internal" href="faq.html#tune-bottlenecks"><span class="std std-ref">performance bottleneck</span></a>,
especially when running many trials in parallel.</p>
<p>Instead you should use shared storage for checkpoints so that no additional synchronization across nodes
is necessary. There are two main options.</p>
<p>First, you can use the <a class="reference internal" href="api/syncing.html#tune-sync-config"><span class="std std-ref">SyncConfig</span></a> to store your
logs and checkpoints on cloud storage, such as AWS S3 or Google Cloud Storage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">tune</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="n">train_fn</span><span class="p">,</span>
    <span class="c1"># ...,</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">storage_path</span><span class="o">=</span><span class="s2">&quot;s3://your-s3-bucket/durable-trial/&quot;</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>Second, you can set up a shared file system like NFS. If you do this, disable automatic trial syncing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span><span class="p">,</span> <span class="n">tune</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="n">train_fn</span><span class="p">,</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span>
        <span class="n">storage_path</span><span class="o">=</span><span class="s2">&quot;/path/to/shared/storage&quot;</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">sync_config</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">SyncConfig</span><span class="p">(</span>
        <span class="c1"># Do not sync because we are on shared storage</span>
        <span class="n">syncer</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>Please note that we strongly encourage you to use one of these two options, as they will
result in less overhead and provide naturally durable checkpoint storage.</p>
</section>
<section id="how-do-i-configure-search-spaces">
<span id="tune-default-search-space"></span><h2><a class="toc-backref" href="faq.html#id21">How do I configure search spaces?</a><a class="headerlink" href="faq.html#how-do-i-configure-search-spaces" title="Permalink to this headline">#</a></h2>
<p>You can specify a grid search or sampling distribution via the dict passed into <code class="docutils literal notranslate"><span class="pre">Tuner(param_space=...)</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;qux&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">sample_from</span><span class="p">(</span><span class="k">lambda</span> <span class="n">spec</span><span class="p">:</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="p">),</span>
    <span class="s2">&quot;bar&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]),</span>
    <span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span>
    <span class="s2">&quot;baz&quot;</span><span class="p">:</span> <span class="s2">&quot;asd&quot;</span><span class="p">,</span>  <span class="c1"># a constant value</span>
<span class="p">}</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span><span class="n">train_fn</span><span class="p">,</span> <span class="n">param_space</span><span class="o">=</span><span class="n">parameters</span><span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>By default, each random variable and grid search point is sampled once.
To take multiple random samples, add <code class="docutils literal notranslate"><span class="pre">num_samples:</span> <span class="pre">N</span></code> to the experiment config.
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">grid_search</span></code> is provided as an argument, the grid will be repeated <code class="docutils literal notranslate"><span class="pre">num_samples</span></code> of times.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># num_samples=10 repeats the 3x3 grid search 10 times, for a total of 90 trials</span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="n">train_fn</span><span class="p">,</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;my_trainable&quot;</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">param_space</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
        <span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">sample_from</span><span class="p">(</span><span class="k">lambda</span> <span class="n">spec</span><span class="p">:</span> <span class="n">spec</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">()),</span>
        <span class="s2">&quot;nn_layers&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">]),</span>
            <span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">]),</span>
        <span class="p">],</span>
    <span class="p">},</span>
    <span class="n">tune_config</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">TuneConfig</span><span class="p">(</span>
<span class="hll">        <span class="n">num_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
</span>    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Note that search spaces may not be interoperable across different search algorithms.
For example, for many search algorithms, you will not be able to use a <code class="docutils literal notranslate"><span class="pre">grid_search</span></code> or <code class="docutils literal notranslate"><span class="pre">sample_from</span></code> parameters.
Read about this in the <a class="reference internal" href="api/search_space.html#tune-search-space"><span class="std std-ref">Search Space API</span></a> page.</p>
</section>
<section id="how-do-i-access-relative-filepaths-in-my-tune-training-function">
<span id="tune-working-dir"></span><h2><a class="toc-backref" href="faq.html#id22">How do I access relative filepaths in my Tune training function?</a><a class="headerlink" href="faq.html#how-do-i-access-relative-filepaths-in-my-tune-training-function" title="Permalink to this headline">#</a></h2>
<p>Let’s say you launch a Tune experiment from <code class="docutils literal notranslate"><span class="pre">~/code/my_script.py</span></code>. By default, Tune
changes the working directory of each worker from <code class="docutils literal notranslate"><span class="pre">~/code</span></code> to its corresponding trial
directory (e.g. <code class="docutils literal notranslate"><span class="pre">~/ray_results/exp_name/trial_0000x</span></code>). This default
guarantees separate working directories for each worker process, avoiding conflicts when
saving trial-specific outputs.</p>
<p>You can configure this by setting <code class="xref py py-obj docutils literal notranslate"><span class="pre">chdir_to_trial_dir=False</span></code> in <code class="xref py py-obj docutils literal notranslate"><span class="pre">tune.TuneConfig</span></code>.
This explicitly tells Tune to not change the working directory
to the trial directory, giving access to paths relative to the original working directory.
One caveat is that the working directory is now shared between workers, so the
<a class="reference internal" href="../ray-air/api/doc/ray.air.session.get_trial_dir.html#ray.air.session.get_trial_dir" title="ray.air.session.get_trial_dir"><code class="xref py py-meth docutils literal notranslate"><span class="pre">session.get_trial_dir()</span></code></a>
API should be used to get the path for saving trial-specific outputs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="c1"># Read from relative paths</span>
<span class="hll">    <span class="nb">print</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s2">&quot;./read.txt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
</span>
    <span class="c1"># The working directory shouldn&#39;t have changed from the original</span>
    <span class="c1"># NOTE: The `TUNE_ORIG_WORKING_DIR` environment variable is deprecated.</span>
    <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span> <span class="o">==</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;TUNE_ORIG_WORKING_DIR&quot;</span><span class="p">]</span>

    <span class="c1"># Write to the Tune trial directory, not the shared working dir</span>
<span class="hll">    <span class="n">tune_trial_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">session</span><span class="o">.</span><span class="n">get_trial_dir</span><span class="p">())</span>
</span><span class="hll">    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">tune_trial_dir</span> <span class="o">/</span> <span class="s2">&quot;write.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span><span class="hll">        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;trial saved artifact&quot;</span><span class="p">)</span>
</span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
<span class="hll">    <span class="n">tune_config</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">TuneConfig</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">chdir_to_trial_dir</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The <code class="xref py py-obj docutils literal notranslate"><span class="pre">TUNE_ORIG_WORKING_DIR</span></code> environment variable was the original workaround for
accessing paths relative to the original working directory. This environment
variable is deprecated, and the <code class="xref py py-obj docutils literal notranslate"><span class="pre">chdir_to_trial_dir</span></code> flag described above should be
used instead.</p>
</div>
</section>
<section id="how-can-i-run-multiple-ray-tune-jobs-on-the-same-cluster-at-the-same-time-multi-tenancy">
<span id="tune-multi-tenancy"></span><h2><a class="toc-backref" href="faq.html#id23">How can I run multiple Ray Tune jobs on the same cluster at the same time (multi-tenancy)?</a><a class="headerlink" href="faq.html#how-can-i-run-multiple-ray-tune-jobs-on-the-same-cluster-at-the-same-time-multi-tenancy" title="Permalink to this headline">#</a></h2>
<p>Running multiple Ray Tune runs on the same cluster at the same
time is not officially supported. We do not test this workflow and we recommend
using a separate cluster for each tuning job.</p>
<p>The reasons for this are:</p>
<ol class="arabic simple">
<li><p>When multiple Ray Tune jobs run at the same time, they compete for resources.
One job could run all its trials at the same time, while the other job waits
for a long time until it gets resources to run the first trial.</p></li>
<li><p>If it is easy to start a new Ray cluster on your infrastructure, there is often
no cost benefit to running one large cluster instead of multiple smaller
clusters. For instance, running one cluster of 32 instances incurs almost the same
cost as running 4 clusters with 8 instances each.</p></li>
<li><p>Concurrent jobs are harder to debug. If a trial of job A fills the disk,
trials from job B on the same node are impacted. In practice, it’s hard
to reason about these conditions from the logs if something goes wrong.</p></li>
</ol>
<p>Previously, some internal implementations in Ray Tune assumed that you only have one job
running at a time. A symptom was when trials from job A used parameters specified in job B,
leading to unexpected results.</p>
<p>Please refer to
[this github issue](<a class="reference external" href="https://github.com/ray-project/ray/issues/30091#issuecomment-1431676976">https://github.com/ray-project/ray/issues/30091#issuecomment-1431676976</a>)
for more context and a workaround if you run into this issue.</p>
</section>
<section id="how-can-i-continue-training-a-completed-tune-experiment-for-longer-and-with-new-configurations-iterative-experimentation">
<span id="tune-iterative-experimentation"></span><h2><a class="toc-backref" href="faq.html#id24">How can I continue training a completed Tune experiment for longer and with new configurations (iterative experimentation)?</a><a class="headerlink" href="faq.html#how-can-i-continue-training-a-completed-tune-experiment-for-longer-and-with-new-configurations-iterative-experimentation" title="Permalink to this headline">#</a></h2>
<p>Let’s say that I have a Tune experiment that has completed with the following configurations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">air</span><span class="p">,</span> <span class="n">tune</span>
<span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">Checkpoint</span><span class="p">,</span> <span class="n">session</span>
<span class="kn">import</span> <span class="nn">random</span>


<span class="k">def</span> <span class="nf">trainable</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">]):</span>
        <span class="c1"># Do some training...</span>

        <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;score&quot;</span><span class="p">:</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()},</span>
            <span class="n">checkpoint</span><span class="o">=</span><span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span><span class="s2">&quot;model_state_dict&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}}),</span>
        <span class="p">)</span>


<span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="n">trainable</span><span class="p">,</span>
    <span class="n">param_space</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">&quot;hyperparam&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])},</span>
    <span class="n">tune_config</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">TuneConfig</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="s2">&quot;score&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">result_grid</span> <span class="o">=</span> <span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="n">best_result</span> <span class="o">=</span> <span class="n">result_grid</span><span class="o">.</span><span class="n">get_best_result</span><span class="p">()</span>
<span class="n">best_checkpoint</span> <span class="o">=</span> <span class="n">best_result</span><span class="o">.</span><span class="n">checkpoint</span>
</pre></div>
</div>
<p>Now, I want to continue training from a checkpoint (e.g., the best one) generated by the previous experiment,
and search over a new hyperparameter search space, for another <code class="docutils literal notranslate"><span class="pre">10</span></code> epochs.</p>
<p><a class="reference internal" href="tutorials/tune-fault-tolerance.html#tune-fault-tolerance-ref"><span class="std std-ref">How to Enable Fault Tolerance in Ray Tune</span></a> explains that the usage of <a class="reference internal" href="api/doc/ray.tune.Tuner.restore.html#ray.tune.Tuner.restore" title="ray.tune.Tuner.restore"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Tuner.restore</span></code></a>
is meant for resuming an <em>unfinished</em> experiment that was interrupted in the middle,
according to the <em>exact configuration</em> that was supplied in the initial training run.</p>
<p>Therefore, <code class="docutils literal notranslate"><span class="pre">Tuner.restore</span></code> is not suitable for our desired behavior.
This style of “iterative experimentation” should be done with <em>new</em> Tune experiments
rather than restoring a single experiment over and over and modifying the experiment spec.</p>
<p>See the following for an example of how to create a new experiment that builds off of the old one:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>


<span class="k">def</span> <span class="nf">trainable</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="c1"># Add logic to handle the initial checkpoint.</span>
    <span class="n">checkpoint_ref</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;start_from_checkpoint&quot;</span><span class="p">]</span>
    <span class="n">checkpoint</span><span class="p">:</span> <span class="n">Checkpoint</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">checkpoint_ref</span><span class="p">)</span>
    <span class="n">model_state_dict</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()[</span><span class="s2">&quot;model_state_dict&quot;</span><span class="p">]</span>
    <span class="c1"># Initialize a model from the checkpoint...</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">]):</span>
        <span class="c1"># Do some training...</span>

        <span class="n">session</span><span class="o">.</span><span class="n">report</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;score&quot;</span><span class="p">:</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()},</span>
            <span class="n">checkpoint</span><span class="o">=</span><span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span><span class="s2">&quot;model_state_dict&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}}),</span>
        <span class="p">)</span>


<span class="n">new_tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="n">trainable</span><span class="p">,</span>
    <span class="n">param_space</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="s2">&quot;hyperparam&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">grid_search</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span>
        <span class="c1"># Put the best checkpoint from above into the object store.</span>
        <span class="c1"># This way, all trials will be able to access the checkpoint,</span>
        <span class="c1"># regardless of which node they are on.</span>
        <span class="s2">&quot;start_from_checkpoint&quot;</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">best_checkpoint</span><span class="p">),</span>
    <span class="p">},</span>
    <span class="n">tune_config</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">TuneConfig</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="s2">&quot;score&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">result_grid</span> <span class="o">=</span> <span class="n">new_tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="examples/exercises.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Tune Exercises</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="api/api.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Ray Tune API</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>